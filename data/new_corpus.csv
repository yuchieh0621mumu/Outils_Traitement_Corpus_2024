context,questions,answers
"A cognitive bias is a systematic pattern of deviation from norm or rationality in judgment.[1] Individuals create their own ""subjective reality"" from their perception of the input. An individual's construction of reality, not the objective input, may dictate their behavior in the world. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, and irrationality.[2][3][4]
 While cognitive biases may initially appear to be negative, some are adaptive. They may lead to more effective actions in a given context.[5] Furthermore, allowing cognitive biases enables faster decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics.[6] Other cognitive biases are a ""by-product"" of human processing limitations,[1] resulting from a lack of appropriate mental mechanisms (bounded rationality), the impact of an individual's constitution and biological state (see embodied cognition), or simply from a limited capacity for information processing.[7][8] Research suggests that cognitive biases can make individuals more inclined to endorsing pseudoscientific beliefs by requiring less evidence for claims that confirm their preconceptions. This can potentially distort their perceptions and lead to inaccurate judgments.[9]
 A continually evolving list of cognitive biases has been identified over the last six decades of research on human judgment and decision-making in cognitive science, social psychology, and behavioral economics. The study of cognitive biases has practical implications for areas including clinical judgment, entrepreneurship, finance, and management.[10][11]
 The notion of cognitive biases was introduced by Amos Tversky and Daniel Kahneman in 1972[12] and grew out of their experience of people's innumeracy, or inability to reason intuitively with the greater orders of magnitude. Tversky, Kahneman, and colleagues demonstrated several replicable ways in which human judgments and decisions differ from rational choice theory. Tversky and Kahneman explained human differences in judgment and decision-making in terms of heuristics. Heuristics involve mental shortcuts which provide swift estimates about the possibility of uncertain occurrences.[13] Heuristics are simple for the brain to compute but sometimes introduce ""severe and systematic errors.""[6] For example, the representativeness heuristic is defined as ""The tendency to judge the frequency or likelihood"" of an occurrence by the extent of which the event ""resembles the typical case.""[13]
 The ""Linda Problem"" illustrates the representativeness heuristic (Tversky & Kahneman, 1983[14]). Participants were given a description of ""Linda"" that suggests Linda might well be a feminist (e.g., she is said to be concerned about discrimination and social justice issues). They were then asked whether they thought Linda was more likely to be (a) a ""bank teller"" or (b) a ""bank teller and active in the feminist movement."" A majority chose answer (b). Independent of the information given about Linda, though, the more restrictive answer (b) is under any circumstance statistically less likely than answer (a). This is an example of the ""conjunction fallacy"". Tversky and Kahneman argued that respondents chose (b) because it seemed more ""representative"" or typical of persons who might fit the description of Linda. The representativeness heuristic may lead to errors such as activating stereotypes and inaccurate judgments of others (Haselton et al., 2005, p. 726).
 Critics of Kahneman and Tversky, such as Gerd Gigerenzer, alternatively argued that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases. They should rather conceive rationality as an adaptive tool, not identical to the rules of formal logic or the probability calculus.[15] Nevertheless, experiments such as the ""Linda problem"" grew into heuristics and biases research programs, which spread beyond academic psychology into other disciplines including medicine and political science.
 Biases can be distinguished on a number of dimensions. Examples of cognitive biases include -
 Other biases are due to the particular way the brain perceives, forms memories and makes judgments. This distinction is sometimes described as ""hot cognition"" versus ""cold cognition"", as motivated reasoning can involve a state of arousal. Among the ""cold"" biases,
 As some biases reflect motivation specifically the motivation to have positive attitudes to oneself.[20] It accounts for the fact that many biases are self-motivated or self-directed (e.g., illusion of asymmetric insight, self-serving bias). There are also biases in how subjects evaluate in-groups or out-groups; evaluating in-groups as more diverse and ""better"" in many respects, even when those groups are arbitrarily defined (ingroup bias, outgroup homogeneity bias).
 Some cognitive biases belong to the subgroup of attentional biases, which refers to paying increased attention to certain stimuli. It has been shown, for example, that people addicted to alcohol and other drugs pay more attention to drug-related stimuli. Common psychological tests to measure those biases are the Stroop task[21][22] and the dot probe task.
 Individuals' susceptibility to some types of cognitive biases can be measured by the Cognitive Reflection Test (CRT) developed by Shane Frederick (2005).[23][24]
 The following is a list of the more commonly studied cognitive biases:
 Many social institutions rely on individuals to make rational judgments.
 The securities regulation regime largely assumes that all investors act as perfectly rational persons. In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.
 A fair jury trial, for example, requires that the jury ignore irrelevant features of the case, weigh the relevant features appropriately, consider different possibilities open-mindedly and resist fallacies such as appeal to emotion. The various biases demonstrated in these psychological experiments suggest that people will frequently fail to do all these things.[35] However, they fail to do so in systematic, directional ways that are predictable.[4]
 In some academic disciplines, the study of bias is very popular. For instance, bias is a wide spread and well studied phenomenon because most decisions that concern the minds and hearts of entrepreneurs are computationally intractable.[11]
 Cognitive biases can create other issues that arise in everyday life. One study showed the connection between cognitive bias, specifically approach bias, and inhibitory control on how much unhealthy snack food a person would eat.[36] They found that the participants who ate more of the unhealthy snack food, tended to have less inhibitory control and more reliance on approach bias. Others have also hypothesized that cognitive biases could be linked to various eating disorders and how people view their bodies and their body image.[37][38]
 It has also been argued that cognitive biases can be used in destructive ways.[39] Some believe that there are people in authority who use cognitive biases and heuristics in order to manipulate others so that they can reach their end goals. Some medications and other health care treatments rely on cognitive biases in order to persuade others who are susceptible to cognitive biases to use their products. Many see this as taking advantage of one's natural struggle of judgement and decision-making. They also believe that it is the government's responsibility to regulate these misleading ads.
 Cognitive biases also seem to play a role in property sale price and value. Participants in the experiment were shown a residential property.[40] Afterwards, they were shown another property that was completely unrelated to the first property. They were asked to say what they believed the value and the sale price of the second property would be. They found that showing the participants an unrelated property did have an effect on how they valued the second property.
 Cognitive biases can be used in non-destructive ways. In team science and collective problem-solving, the superiority bias can be beneficial. It leads to a diversity of solutions within a group, especially in complex problems, by preventing premature consensus on suboptimal solutions. This example demonstrates how a cognitive bias, typically seen as a hindrance, can enhance collective decision-making by encouraging a wider exploration of possibilities.[41]
 Because they cause systematic errors, cognitive biases cannot be compensated for using a wisdom of the crowd technique of averaging answers from several people.[42] Debiasing is the reduction of biases in judgment and decision-making through incentives, nudges, and training. Cognitive bias mitigation and cognitive bias modification are forms of debiasing specifically applicable to cognitive biases and their effects. Reference class forecasting is a method for systematically debiasing estimates and decisions, based on what Daniel Kahneman has dubbed the outside view.
 Similar to Gigerenzer (1996),[43] Haselton et al. (2005) state the content and direction of cognitive biases are not ""arbitrary"" (p. 730).[1] Moreover, cognitive biases can be controlled. One debiasing technique aims to decrease biases by encouraging individuals to use controlled processing compared to automatic processing.[25] In relation to reducing the FAE, monetary incentives[44] and informing participants they will be held accountable for their attributions[45] have been linked to the increase of accurate attributions. Training has also shown to reduce cognitive bias. Carey K. Morewedge and colleagues (2015) found that research participants exposed to one-shot training interventions, such as educational videos and debiasing games that taught mitigating strategies, exhibited significant reductions in their commission of six cognitive biases immediately and up to 3 months later.[46]
 Cognitive bias modification refers to the process of modifying cognitive biases in healthy people and also refers to a growing area of psychological (non-pharmaceutical) therapies for anxiety, depression and addiction called cognitive bias modification therapy (CBMT). CBMT is sub-group of therapies within a growing area of psychological therapies based on modifying cognitive processes with or without accompanying medication and talk therapy, sometimes referred to as applied cognitive processing therapies (ACPT). Although cognitive bias modification can refer to modifying cognitive processes in healthy individuals, CBMT is a growing area of evidence-based psychological therapy, in which cognitive processes are modified to relieve suffering[47][48] from serious depression,[49] anxiety,[50] and addiction.[51] CBMT techniques are technology-assisted therapies that are delivered via a computer with or without clinician support. CBM combines evidence and theory from the cognitive model of anxiety,[52] cognitive neuroscience,[53] and attentional models.[54]
 Cognitive bias modification has also been used to help those with obsessive-compulsive beliefs and obsessive-compulsive disorder.[55][56] This therapy has shown that it decreases the obsessive-compulsive beliefs and behaviors.
 Bias arises from various processes that are sometimes difficult to distinguish. These include:
 People do appear to have stable individual differences in their susceptibility to decision biases such as overconfidence, temporal discounting, and bias blind spot.[65] That said, these stable levels of bias within individuals are possible to change. Participants in experiments who watched training videos and played debiasing games showed medium to large reductions both immediately and up to three months later in the extent to which they exhibited susceptibility to six cognitive biases: anchoring, bias blind spot, confirmation bias, fundamental attribution error, projection bias, and representativeness.[66]
 Individual differences in cognitive bias have also been linked to varying levels of cognitive abilities and functions.[67] The Cognitive Reflection Test (CRT) has been used to help understand the connection between cognitive biases and cognitive ability. There have been inconclusive results when using the Cognitive Reflection Test to understand ability. However, there does seem to be a correlation; those who gain a higher score on the Cognitive Reflection Test, have higher cognitive ability and rational-thinking skills. This in turn helps predict the performance on cognitive bias and heuristic tests. Those with higher CRT scores tend to be able to answer more correctly on different heuristic and cognitive bias tests and tasks.[68]
 Age is another individual difference that has an effect on one's ability to be susceptible to cognitive bias. Older individuals tend to be more susceptible to cognitive biases and have less cognitive flexibility. However, older individuals were able to decrease their susceptibility to cognitive biases throughout ongoing trials.[69] These experiments had both young and older adults complete a framing task. Younger adults had more cognitive flexibility than older adults. Cognitive flexibility is linked to helping overcome pre-existing biases.
 Cognitive bias theory loses the sight of any distinction between reason and bias. If every bias can be seen as a reason, and every reason can be seen as a bias, then the distinction is lost.[70]
 Criticism against theories of cognitive biases is usually founded in the fact that both sides of a debate often claim the other's thoughts to be subject to human nature and the result of cognitive bias, while claiming their own point of view to be above the cognitive bias and the correct way to ""overcome"" the issue. This rift ties to a more fundamental issue that stems from a lack of consensus in the field, thereby creating arguments that can be non-falsifiably used to validate any contradicting viewpoint.[citation needed]
 Gerd Gigerenzer is one of the main opponents to cognitive biases and heuristics.[71][72][73] Gigerenzer believes that cognitive biases are not biases, but rules of thumb, or as he would put it ""gut feelings"" that can actually help us make accurate decisions in our lives. His view shines a much more positive light on cognitive biases than many other researchers. Many view cognitive biases and heuristics as irrational ways of making decisions and judgements.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['discrimination and social justice issues', 'Amos Tversky and Daniel Kahneman', 'systematic pattern of deviation from norm or rationality in judgment', 'various eating disorders', 'practical implications for areas including clinical judgment, entrepreneurship, finance, and management'], 'answer_start': [], 'answer_end': []}"
"
 Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.
 AI technology is widely used throughout industry, government, and science. Some high-profile applications include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go).[2] However, many AI applications are not perceived as AI: ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""[3][4]
 Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.[5] Artificial intelligence was founded as an academic discipline in 1956.[6] The field went through multiple cycles of optimism,[7][8] followed by periods of disappointment and loss of funding, known as AI winter.[9][10] Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques,[11] and after 2017 with the transformer architecture.[12] This led to the AI boom of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.[13]
 The growing use of artificial intelligence in the 21st century is influencing a societal and economic shift towards increased automation, data-driven decision-making, and the integration of AI systems into various economic sectors and areas of life, impacting job markets, healthcare, government, industry, and education. This raises questions about the long-term effects, ethical implications, and risks of AI, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. 
 The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics.[a] General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field's long-term goals.[14]
 To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[15]
 The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]
 Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[16] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[17]
 Many of these algorithms are insufficient for solving large reasoning problems because they experience a ""combinatorial explosion"": They become exponentially slower as the problems grow.[18] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[19] Accurate and efficient reasoning is an unsolved problem.
 Knowledge representation and knowledge engineering[20] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[21] scene interpretation,[22] clinical decision support,[23] knowledge discovery (mining ""interesting"" and actionable inferences from large databases),[24] and other areas.[25]
 A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[26] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[27] situations, events, states, and time;[28] causes and effects;[29] knowledge about knowledge (what we know about what other people know);[30] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[31] and many other aspects and domains of knowledge.
 Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[32] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).[19] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]
 An ""agent"" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][35] In automated planning, the agent has a specific goal.[36] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the ""utility"") that measures how much the agent prefers it. For each possible action, it can calculate the ""expected utility"": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[37]
 In classical planning, the agent knows exactly what the effect of any action will be.[38] In most real-world problems, however, the agent may not be certain about the situation they are in (it is ""unknown"" or ""unobservable"") and it may not know for certain what will happen after each possible action (it is not ""deterministic""). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[39]
 In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[40] Information value theory can be used to weigh the value of exploratory or experimental actions.[41] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.
 A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[42]
 Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[43]
 Machine learning is the study of programs that can improve their performance on a given task automatically.[44] It has been a part of AI from the beginning.[e]
 There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[47] Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[48]
 In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as ""good"".[49] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[50] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[51]
 Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[52]
 Natural language processing (NLP)[53] allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[54]
 Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called ""micro-worlds"" (due to the common sense knowledge problem[32]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.
 Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[55] transformers (a deep learning architecture using an attention mechanism),[56] and others.[57] In 2019, generative pre-trained transformer (or ""GPT"") language models began to generate coherent text,[58][59] and by 2023 these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[60]
 Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[61]
 The field includes speech recognition,[62] image classification,[63] facial recognition, object recognition,[64] and robotic perception.[65]
 Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[67] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
 However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[68] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.[69]
 A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[14]
 AI research uses a wide variety of techniques to accomplish the goals above.[b]
 AI can solve many problems by intelligently searching through many possible solutions.[70] There are two very different kinds of search used in AI: state space search and local search.
 State space search searches through a tree of possible states to try to find a goal state.[71] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[72]
 Simple exhaustive searches[73] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[18] ""Heuristics"" or ""rules of thumb"" can help prioritize choices that are more likely to reach a goal.[74]
 Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.[75]
 Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[76]
 Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.[77]
 Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by ""mutating"" and ""recombining"" them, selecting only the fittest to survive each generation.[78]
 Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[79]
 Formal logic is used for reasoning and knowledge representation.[80]
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as ""and"", ""or"", ""not"" and ""implies"")[81] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as ""Every X is a Y"" and ""There are some Xs that are Ys"").[82]
 Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[83] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.
 Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[84] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[85]
 Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[86]
 Fuzzy logic assigns a ""degree of truth"" between 0 and 1. It can therefore handle propositions that are vague and partially true.[87]
 Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[31]
Other specialized versions of logic have been developed to describe many complex domains.
 Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[88] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[89] and information value theory.[90] These tools include models such as Markov decision processes,[91] dynamic decision networks,[92] game theory and mechanism design.[93]
 Bayesian networks[94] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][96] learning (using the expectation-maximization algorithm),[h][98] planning (using decision networks)[99] and perception (using dynamic Bayesian networks).[92]
 
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[92] The simplest AI applications can be divided into two types: classifiers (e.g., ""if shiny then diamond""), on one hand, and controllers (e.g., ""if diamond then pick up""), on the other hand. Classifiers[100] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an ""observation"") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[48]
 There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]
The naive Bayes classifier is reportedly the ""most widely used learner""[103] at Google, due in part to its scalability.[104]
Neural networks are also used as classifiers.[105]
 An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]
 Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106]
Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107]
 In feedforward neural networks the signal passes in only one direction.[108] Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.[109]
Perceptrons[110]
use only a single layer of neurons, deep learning[111] uses multiple layers.
Convolutional neural networks strengthen the connection between neurons that are ""close"" to each other—this is especially important in image processing, where a local set of neurons must identify an ""edge"" before the network can identify an object.[112]
 Deep learning[111]
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[113]
 Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[114] and others. The reason that deep learning performs so well in so many applications is not known as of 2023.[115]
The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i]
but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]
 Generative pre-trained transformers (GPT) are large language models that are based on the semantic relationships between words in sentences (natural language processing). Text-based GPT models are pre-trained on a large corpus of text which can be from the internet. The pre-training consists in predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pre-training, GPT models accumulate knowledge about the world, and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are still prone to generating falsehoods called ""hallucinations"", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow you to ask a question or request a task in simple text.[124][125]
 Current models and services include: Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot and LLaMA.[126] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[127]
 In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[128] Historically, specialized languages, such as Lisp, Prolog, Python and others, had been used.
 AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok).
 The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[129] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.
 For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[130] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[130] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[131] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[132] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[133][134]
 Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[135] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[136] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[137] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then in 2017 it defeated Ke Jie, who was the best Go player in the world.[138] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[139] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[140] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[141] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[142]
 Various countries are deploying AI military applications.[143] The main applications enhance command and control, communications, sensors, integration and interoperability.[144] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[143] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[144] AI was incorporated into military operations in Iraq and Syria.[143]
 In November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.[145]
 In the early 2020s, generative AI gained widespread prominence. In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it.[146] The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.[147][148]
 There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated ""AI"" in some offerings or processes.[149] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.
 In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.
 Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights"" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
 AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to ""solve intelligence, and then use that to solve everything else"".[150] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[151] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[152]
 Machine-learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
 Technology companies collect a wide range of data from their users, including online activity, geolocation data, video and audio.[153]
For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[154] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[155]
 AI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[156] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted ""from the question of 'what they know' to the question of 'what they're doing with it'.""[157]
 Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of ""fair use"". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"".[158][159] Website owners who do not wish to have their content scraped can indicate it in a ""robots.txt"" file.[160] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[161][162] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[163]
 YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[164] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[165] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem.
 In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.[166] AI pioneer Geoffrey Hinton expressed concern about AI enabling ""authoritarian leaders to manipulate their electorates"" on a large scale, among other risks.[167]
 Machine learning applications will be biased if they learn from biased data.[168] The developers may not be aware that the bias exists.[169]
Bias can be introduced by the way training data is selected and by the way a model is deployed.[170][168] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[171]
Fairness in machine learning is the study of how to prevent the harm caused by algorithmic bias. It has become serious area of academic study within AI. Researchers have discovered it is not always possible to define ""fairness"" in a way that satisfies all stakeholders.[172]
 On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as ""gorillas"" because they were black. The system was trained on a dataset that contained very few images of black people,[173] a problem called ""sample size disparity"".[174] Google ""fixed"" this problem by preventing the system from labelling anything as a ""gorilla"". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[175]
 COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist.
In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[176] In 2017, several researchers[k] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[178]
 A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as ""race"" or ""gender""). The feature will correlate with other features (like ""address"", ""shopping history"" or ""first name""), and the program will make the same decisions based on these features as it would on ""race"" or ""gender"".[179]
Moritz Hardt said ""the most robust fact in this research area is that fairness through blindness doesn't work.""[180]
 Criticism of COMPAS highlighted that machine learning models are designed to make ""predictions"" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these ""recommendations"" will likely be racist.[181] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is necessarily descriptive and not proscriptive.[l]
 Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[174]
 At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[183]
 Many AI systems are so complex that their designers cannot explain how they reach their decisions.[184] Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[185]
 It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as ""cancerous"", because pictures of malignancies typically include a ruler to show the scale.[186] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at ""low risk"" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[187]
 People who have been harmed by an algorithm's decision have a right to an explanation.[188] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[m] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[189]
 DARPA established the XAI (""Explainable Artificial Intelligence"") program in 2014 to try and solve these problems.[190]
 There are several possible solutions to the transparency problem. SHAP tried to solve the transparency problems by visualising the contribution of each feature to the output.[191] LIME can locally approximate a model with a simpler, interpretable model.[192] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[193] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network have learned and produce output that can suggest what the network is learning.[194]
 Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
 A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[n] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[196] Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person.[196] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[197] By 2015, over fifty countries were reported to be researching battlefield robots.[198]
 AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[199] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[200][201]
 There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[202]
 Training AI systems requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller startups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.[203]
 Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[204]
 In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI.[205] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[206] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9% of U.S. jobs as ""high risk"".[o][208] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[204] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[209][210]
 Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"".[211] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[212]
 From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[213]
 It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, ""spell the end of the human race"".[214] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like ""self-awareness"" (or ""sentience"" or ""consciousness"") and becomes a malevolent character.[p] These sci-fi scenarios are misleading in several ways.
 First, AI does not require human-like ""sentience"" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager).[216] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that ""you can't fetch the coffee if you're dead.""[217] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is ""fundamentally on our side"".[218]
 Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[219]
 The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[220] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk have expressed concern about existential risk from AI.[221]
AI pioneers including Fei-Fei Li, Geoffrey Hinton, Yoshua Bengio, Cynthia Breazeal, Rana el Kaliouby, Demis Hassabis, Joy Buolamwini, and Sam Altman have expressed concerns about the risks of AI. In 2023, many leading AI experts issued the joint statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"".[222]
 Other researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making ""human lives longer and healthier and easier.""[223] While the tools that are now being used to improve lives can also be used by bad actors, ""they can also be used against the bad actors.""[224][225] Andrew Ng also argued that ""it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.""[226] Yann LeCun ""scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.""[227] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[228] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[229]
 Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[230]
 Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[231]
The field of machine ethics is also called computational morality,[231]
and was founded at an AAAI symposium in 2005.[232]
 Other approaches include Wendell Wallach's ""artificial moral agents""[233] and Stuart J. Russell's three principles for developing provably beneficial machines.[234]
 Active organizations in the AI open-source community include Hugging Face,[235] Google,[236] EleutherAI and Meta.[237] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[238][239] meaning that their architecture and trained parameters (the ""weights"") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[240] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism), and that once released on the Internet, they can't be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[241]
 Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:[242][243]
 Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[244] however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.[245]
 Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[246]
 The AI Safety Institute in the UK has released a testing toolset called ‘Inspect’ for AI safety evaluations available under a MIT open-source licence which is freely available on Github and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[247]
 The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[248] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[249] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[250][251] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[252] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[252] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[252] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[253] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[254] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[255]
 In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"".[250] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[256] In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".[257][258]
 In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[259] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[260][261]
 The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable form of mathematical reasoning.[262][5] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an ""electronic brain"".[q] 
They developed several areas of research that would become part of AI,[264]
such as McCullouch and Pitts design for ""artificial neurons"" in 1943,[265] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that ""machine intelligence"" was plausible.[266][5]
 The field of AI research was founded at a workshop at Dartmouth College in 1956.[r][6] The attendees became the leaders of AI research in the 1960s.[s] They and their students produced programs that the press described as ""astonishing"":[t] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[u][7] Artificial intelligence laboratories were set up at a number of British and U.S. Universities in the latter 1950s and early 1960s.[5]
 Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[270] Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"".[271] Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".[272] They had, however, underestimated the difficulty of the problem.[v] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[274] and ongoing pressure from the U.S. Congress to fund more productive projects.[275] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[276] The ""AI winter"", a period when obtaining funding for AI projects was difficult, followed.[9]
 In the early 1980s, AI research was revived by the commercial success of expert systems,[277] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]
 Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[278] and began to look into ""sub-symbolic"" approaches.[279] Rodney Brooks rejected ""representation"" in general and focussed directly on engineering machines that move and survive.[w] Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[88][284] But the most important development was the revival of ""connectionism"", including neural network research, by Geoffrey Hinton and others.[285] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[286]
 AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[287] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".[288]
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.[14]
 Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]
For many specific tasks, other methods were abandoned.[x]
Deep learning's success was based on both hardware improvements (faster computers,[290] graphics processing units, cloud computing[291]) and access to large amounts of data[292] (including curated datasets,[291] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[y] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[252]
 In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[229]
 In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[293] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in ""AI"" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in ""AI"".[294]
About 800,000 ""AI""-related U.S. job openings existed in 2022.[295]
 Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""[296] He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".[296] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[266] Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes that we can not determine these things about other people but ""it is usual to have a polite convention that everyone thinks""[297]
 Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. ""Aeronautical engineering texts,"" they wrote, ""do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'""[298] AI founder John McCarthy agreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".[299]
 McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world"".[300] Another AI founder, Marvin Minsky similarly describes it as ""the ability to solve hard problems"".[301] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machine—and no other philosophical discussion is required, or may not even be possible.
 Another definition has been adopted by Google,[302] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.
 No established unifying theory or paradigm has guided AI research for most of its history.[z] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.
 Symbolic AI (or ""GOFAI"")[304] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""[305]
 However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.[306] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.[307] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[aa][19]
 The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[309][310] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
 ""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[311] but eventually was seen as irrelevant. Modern AI has elements of both.
 Finding a provably correct or optimal solution is intractable for many important problems.[18] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
 AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[312][313] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.
 The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that ""[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.""[314] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.
 David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness.[315] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[316]
 Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[317]
 Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""[ab] Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[321]
 It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[322] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[323][324] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[323] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[325]
 In 2017, the European Union considered granting ""electronic personhood"" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[326] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.[327][328]
 Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[324][323]
 A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[313]
 If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an ""intelligence explosion"" and Vernor Vinge called a ""singularity"".[329]
 However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[330]
 Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[331]
 Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[332]
 Thought-capable artificial beings have appeared as storytelling devices since antiquity,[333] and have been a persistent theme in science fiction.[334]
 A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[335]
 Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[336] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[337]
 Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[338]
 The two most widely used textbooks in 2023. (See the Open Syllabus).
 These were the four of the most widely used AI textbooks in 2008:
 Later editions.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['machine ethics', 'Brad Rutter and Ken Jennings', 'The feature will correlate with other features', 'situations, events, states, and time', 'social and ethical'], 'answer_start': [], 'answer_end': []}"
"
 Computer security, cybersecurity, digital security or information technology security (IT security) is the protection of computer systems and networks from attacks by malicious actors that may result in unauthorized information disclosure, theft of, or damage to hardware, software, or data, as well as from the disruption or misdirection of the services they provide.[1][2]
 The field is significant due to the expanded reliance on computer systems, the Internet,[3] and wireless network standards such as Bluetooth and Wi-Fi. It is also significant due to the growth of smart devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is one of the most significant challenges of the contemporary world, due to both the complexity of information systems and the societies they support. Security is of especially high importance for systems that govern large-scale systems with far-reaching physical effects, such as power distribution, elections, and finance.[4][5]
 While most aspects of computer security involve digital measures such as electronic passwords and encryption, physical security measures such as metal locks are still used to prevent unauthorized tampering.
 A vulnerability refers to a flaw in the structure, execution, functioning, or internal oversight of a computer or system that compromises its security. Most of the vulnerabilities that have been discovered are documented in the Common Vulnerabilities and Exposures (CVE) database.[6] An exploitable vulnerability is one for which at least one working attack or exploit exists.[7] Vulnerabilities can be researched, reverse-engineered, hunted, or exploited using automated tools or customized scripts.[8][9]
 Various people or parties are vulnerable to cyber attacks; however, different groups are likely to experience different types of attacks more than others.[10]
 In April 2023, the United Kingdom Department for Science, Innovation & Technology released a report on cyber attacks over the last 12 months.[11] They surveyed 2,263 UK businesses, 1,174 UK registered charities and 554 education institutions. The research found that ""32% of businesses and 24% of charities overall recall any breaches or attacks from the last 12 months."" These figures were much higher for ""medium businesses (59%), large businesses (69%) and high-income charities with £500,000 or more in annual income (56%).""[11] Yet, although medium or large businesses are more often the victims, since larger companies have generally improved their security over the last decade, small and midsize businesses (SMBs) have also become increasingly vulnerable as they often ""do not have advanced tools to defend the business.""[10] SMBs are most likely to be affected by malware, ransomware, phishing, man-in-the-middle attacks, and Denial-of Service (DoS) Attacks.[10]
 Normal internet users are most likely to be affected by untargeted cyber attacks.[12] These are where attackers indiscriminately target as many devices, services or users as possible. They do this using techniques that take advantage of the openness of the Internet. These strategies mostly include phishing, ransomware, water holing and scanning.[12]
 To secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of the following categories:
 A backdoor in a computer system, a cryptosystem, or an algorithm, is any secret method of bypassing normal authentication or security controls. These weaknesses may exist for many reasons, including original design or poor configuration.[13] Due to the nature of backdoors, they are of greater concern to companies and databases as opposed to individuals.
 Backdoors may be added by an authorized party to allow some legitimate access, or by an attacker for malicious reasons. Criminals often use malware to install backdoors, giving them remote administrative access to a system.[14] Once they have access, cybercriminals can ""modify files, steal personal information, install unwanted software, and even take control of the entire computer.""[14]
 Backdoors can be very hard to detect, and are usually discovered by someone who has access to the application source code or intimate knowledge of the operating system of the computer.
 Denial-of-service attacks (DoS) are designed to make a machine or network resource unavailable to its intended users.[15] Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim's account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of distributed denial-of-service (DDoS) attacks are possible, where the attack comes from a large number of points. In this case defending against these attacks is much more difficult. Such attacks can originate from the zombie computers of a botnet or from a range of other possible techniques, including distributed reflective denial-of-service (DRDoS), where innocent systems are fooled into sending traffic to the victim.[15] With such attacks, the amplification factor makes the attack easier for the attacker because they have to use little bandwidth themselves. To understand why attackers may carry out these attacks, see the 'attacker motivation' section.
 A direct-access attack is when an unauthorized user (an attacker) gains physical access to a computer, most likely to directly copy data from it or to steal information.[16] Attackers may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless microphones. Even when the system is protected by standard security measures, these may be bypassed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.
 Direct service attackers are related in concept to direct memory attacks which allow an attacker to gain direct access to a computer's memory.[17] The attacks ""take advantage of a feature of modern computers that allows certain devices, such as external hard drives, graphics cards or network cards, to access the computer's memory directly.""[17]
 To help prevent these attacks, computer users must ensure that they have strong passwords, that their computer is locked at all times when they are not using it, and that they keep their computer with them at all times when traveling.[17]
 Eavesdropping is the act of surreptitiously listening to a private computer conversation (communication), usually between hosts on a network. It typically occurs when a user connects to a network where traffic is not secured or encrypted and sends sensitive business data to a colleague, which, when listened to by an attacker, could be exploited.[18] Data transmitted across an ""open network"" allows an attacker to exploit a vulnerability and intercept it via various methods.
 Unlike malware, direct-access attacks, or other forms of cyber attacks, eavesdropping attacks are unlikely to negatively affect the performance of networks or devices, making them difficult to notice.[18] In fact, ""the attacker does not need to have any ongoing connection to the software at all. The attacker can insert the software onto a compromised device, perhaps by direct insertion or perhaps by a virus or other malware, and then come back some time later to retrieve any data that is found or trigger the software to send the data at some determined time.""[19]
 Using a virtual private network (VPN), which encrypts data between two points, is one of the most common forms of protection against eavesdropping. Using the best form of encryption possible for wireless networks is best practice, as well as using HTTPS instead of an unencrypted HTTP.[20]
 Programs such as Carnivore and NarusInSight have been used by the Federal Bureau of Investigation (FBI) and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact with the outside world) can be eavesdropped upon by monitoring the faint electromagnetic transmissions generated by the hardware. TEMPEST is a specification by the NSA referring to these attacks.
 Malicious software (malware) is any software code or computer program ""intentionally written to harm a computer system or its users.""[21] Once present on a computer, it can leak sensitive details such as personal information, business information and passwords, can give control of the system to the attacker, and can corrupt or delete data permanently.[22] Another type of malware is ransomware, which is when ""malware installs itself onto a victim's machine, encrypts their files, and then turns around and demands a ransom (usually in Bitcoin) to return that data to the user.""[23]
 Types of malware include some of the following:
 Man-in-the-middle attacks (MITM) involve a malicious attacker trying to intercept, surveil or modify communications between two parties by spoofing one or both party's identities and injecting themselves in-between.[24] Types of MITM attacks include:
 Surfacing in 2017, a new class of multi-vector,[25] polymorphic[26] cyber threats combine several types of attacks and change form to avoid cybersecurity controls as they spread.
 Multi-vector polymorphic attacks, as the name describes, are both multi-vectored and polymorphic.[27] Firstly, they are a singular attack that involves multiple methods of attack. In this sense, they are “multi-vectored (i.e. the attack can use multiple means of propagation such as via the Web, email and applications."" However,  they are also multi-staged, meaning that “they can infiltrate networks and move laterally inside the network.”[27] The attacks can be polymorphic, meaning that the cyberattacks used such as viruses, worms or trojans “constantly change (“morph”) making it nearly impossible to detect them using signature-based defences.”[27]
 Phishing is the attempt of acquiring sensitive information such as usernames, passwords, and credit card details directly from users by deceiving the users.[28] Phishing is typically carried out by email spoofing, instant messaging, text message, or on a phone call. They often directs users to enter details at a fake website whose look and feel are almost identical to the legitimate one.[29] The fake website often asks for personal information, such as login details and passwords. This information can then be used to gain access to the individual's real account on the real website.
 Preying on a victim's trust, phishing can be classified as a form of social engineering. Attackers can use creative ways to gain access to real accounts. A common scam is for attackers to send fake electronic invoices[30] to individuals showing that they recently purchased music, apps, or others, and instructing them to click on a link if the purchases were not authorized. A more strategic type of phishing is spear-phishing which leverages personal or organization-specific details to make the attacker appear like a trusted source. Spear-phishing attacks target specific individuals, rather than the broad net cast by phishing attempts.[31]
 Privilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level.[32] For example, a standard computer user may be able to exploit a vulnerability in the system to gain access to restricted data; or even become root and have full unrestricted access to a system. The severity of attacks can range from attacks simply sending an unsolicited email to a ransomware attack on large amounts of data. Privilege escalation usually starts with social engineering techniques, often phishing.[32]
 Privilege escalation can be separated into two strategies, horizontal and vertical privilege escalation:
 Any computational system affects its environment in some form. This effect it has on its environment includes a wide range of criteria, which can range from electromagnetic radiation to residual effect on RAM cells which as a consequence make a Cold boot attack possible, to hardware implementation faults that allow for access and or guessing of other values that normally should be inaccessible. In Side-channel attack scenarios, the attacker would gather such information about a system or network to guess its internal state and as a result access the information which is assumed by the victim to be secure.
 Social engineering, in the context of computer security, aims to convince a user to disclose secrets such as passwords, card numbers, etc. or grant physical access by, for example, impersonating a senior executive, bank, a contractor, or a customer.[33] This generally involves exploiting people's trust, and relying on their cognitive biases. A common scam involves emails sent to accounting and finance department personnel, impersonating their CEO and urgently requesting some action. One of the main techniques of social engineering are phishing attacks.
 In early 2016, the FBI reported that such business email compromise (BEC) scams had cost US businesses more than $2 billion in about two years.[34]
 In May 2016, the Milwaukee Bucks NBA team was the victim of this type of cyber scam with a perpetrator impersonating the team's president Peter Feigin, resulting in the handover of all the team's employees' 2015 W-2 tax forms.[35]
 Spoofing is an act of pretending to be a valid entity through the falsification of data (such as an IP address or username), in order to gain access to information or resources that one is otherwise unauthorized to obtain. Spoofing is closely related to phishing.[36][37] There are several types of spoofing, including:
 In 2018, the cybersecurity firm Trellix published research on the life-threatening risk of spoofing in the healthcare industry.[39]
 Tampering describes a malicious modification or alteration of data. An intentional but unauthorized act resulting in the modification of a system, components of systems, its intended behavior, or data. So-called Evil Maid attacks and security services planting of surveillance capability into routers are examples.[40]
 HTML smuggling allows an attacker to ""smuggle"" a malicious code inside a particular HTML or web page.[41] HTML files can carry payloads concealed as benign, inert data in order to defeat content filters. These payloads can be reconstructed on the other side of the filter.[42]
 When a target user opens the HTML, the malicious code is activated; the web browser then ""decodes"" the script, which then unleashes the malware onto the target's device.[41]
 Employee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness toward information security within an organization. Information security culture is the ""...totality of patterns of behavior in an organization that contributes to the protection of information of all kinds.""[43]
 Andersson and Reimers (2014) found that employees often do not see themselves as part of their organization's information security effort and often take actions that impede organizational changes.[44] Indeed, the Verizon Data Breach Investigations Report 2020, which examined 3,950 security breaches, discovered 30% of cybersecurity incidents involved internal actors within a company.[45] Research shows information security culture needs to be improved continuously. In ""Information Security Culture from Analysis to Change"", authors commented, ""It's a never-ending process, a cycle of evaluation and change or maintenance."" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.[46]
 In computer security, a countermeasure is an action, device, procedure or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.[47][48][49]
 Some common countermeasures are listed in the following sections:
 Security by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered a main feature.
 The UK government's National Cyber Security Centre separate secure cyber design principles into five sections:[50]
 These design principles of security by design can include some of the following techniques:
 Security architecture can be defined as the ""practice of designing computer systems to achieve security goals.""[51] These goals have overlap with the principles of ""security by design"" explored above, including to ""make initial compromise of the system difficult,"" and to ""limit the impact of any compromise.""[51] In practice, the role of a security architect would be to ensure the structure of a system reinforces the security of the system, and that new changes are safe and meet the security requirements of the organization.[52][53]
 Similarly, Techopedia defines security architecture as ""a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment. It also specifies when and where to apply security controls. The design process is generally reproducible."" The key attributes of security architecture are:[54]
 Practicing security architecture provides the right foundation to systematically address business, IT and security concerns in an organization.
 A state of computer security is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:
 Today, computer security consists mainly of preventive measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet. They can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real-time filtering and blocking.[55] Another implementation is a so-called physical firewall, which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.
 Some organizations are turning to big data platforms, such as Apache Hadoop, to extend data accessibility and machine learning to detect advanced persistent threats.[57]
 In order to ensure adequate security, the confidentiality, integrity and availability of a network, better known as the CIA triad, must be protected and is considered the foundation to information security.[58] To achieve those objectives, administrative, physical and technical security measures should be employed. The amount of security afforded to an asset can only be determined when its value is known.[59]
 Vulnerability management is the cycle of identifying, fixing or mitigating vulnerabilities,[60] especially in software and firmware. Vulnerability management is integral to computer security and network security.
 Vulnerabilities can be discovered with a vulnerability scanner, which analyzes a computer system in search of known vulnerabilities,[61] such as open ports, insecure software configuration, and susceptibility to malware.  In order for these tools to be effective, they must be kept up to date with every new update the vendor release.  Typically, these updates will scan for the new vulnerabilities that were introduced recently.
 Beyond vulnerability scanning, many organizations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities. In some sectors, this is a contractual requirement.[62]
 The act of assessing and reducing vulnerabilities to cyber attacks is commonly referred to as information technology security assessments. They aims to assess systems for risk and to predict and test for their vulnerabilities. While formal verification of the correctness of computer systems is possible,[63][64] it is not yet common. Operating systems formally verified include seL4,[65] and SYSGO's PikeOS[66][67] – but these make up a very small percentage of the market.
 It is possible to reduce an attacker's chances by keeping systems up to date with security patches and updates and/or hiring people with expertise in security. Large companies with significant threats can hire  Security Operations Centre (SOC) Analysts. These are specialists in cyber defences, with their role ranging from ""conducting threat analysis to investigating reports of any new issues and preparing and testing disaster recovery plans.""[68]
 Whilst no measures can completely guarantee the prevention of an attack, these measures can help mitigate the damage of possible attacks. The effects of data loss/damage can be also reduced by careful backing up and insurance.
 Outside of formal assessments, there are various methods of reducing vulnerabilities. Two factor authentication is a method for mitigating unauthorized access to a system or sensitive information.[69] It requires something you know; a password or PIN, and something you have; a card, dongle, cellphone, or another piece of hardware. This increases security as an unauthorized person needs both of these to gain access.
 Protecting against social engineering and direct computer access (physical) attacks can only be occur by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk by improving people's knowledge of how to protect themselves and by increasing people's awareness of threats.[70] However, even in highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.
 Inoculation, derived from inoculation theory, seeks to prevent social engineering and other fraudulent tricks or traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts.[71]
 Hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.
 One use of the term computer security refers to technology that is used to implement secure operating systems. Using secure operating systems is a good way of ensuring computer security. These are systems that have  achieved certification from an external security-auditing organization, the most popular evaluations are Common Criteria (CC).[84]
 In software engineering, secure coding aims to guard against the accidental introduction of security vulnerabilities. It is also possible to create software designed from the ground up to be secure. Such systems are secure by design. Beyond this, formal verification aims to prove the correctness of the algorithms underlying a system;[85]
important for cryptographic protocols for example.
 Within computer systems, two of the main security models capable of enforcing privilege separation are access control lists (ACLs) and role-based access control (RBAC).
 An access-control list (ACL), with respect to a computer file system, is a list of permissions associated with an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.
 Role-based access control is an approach to restricting system access to authorized users,[86][87][88]  used by the majority of enterprises with more than 500 employees,[89] and can implement mandatory access control (MAC) or discretionary access control (DAC).
 A further approach, capability-based security has been mostly restricted to research operating systems. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open-source project in the area is the E language.
 The end-user is widely recognized as the weakest link in the security chain[90] and it is estimated that more than 90% of security incidents and breaches involve some kind of human error.[91][92] Among the most commonly recorded forms of errors and misjudgment are poor password management, sending emails containing sensitive data and attachments to the wrong recipient, the inability to recognize misleading URLs and to identify fake websites and dangerous email attachments.  A common mistake that users make is saving their user id/password in their browsers to make it easier to log in to banking sites.  This is a gift to attackers who have obtained access to a machine by some means.  The risk may be mitigated by the use of two-factor authentication.[93]
 As the human component of cyber risk is particularly relevant in determining the global cyber risk[94] an organization is facing, security awareness training, at all levels, not only provides formal compliance with regulatory and industry mandates but is considered essential[95] in reducing cyber risk and protecting individuals and companies from the great majority of cyber threats.
 The focus on the end-user represents a profound cultural change for many security practitioners, who have traditionally approached cybersecurity exclusively from a technical perspective, and moves along the lines suggested by major security centers[96] to develop a culture of cyber awareness within the organization, recognizing that a security-aware user provides an important line of defense against cyber attacks.
 Related to end-user training, digital hygiene or cyber hygiene is a fundamental principle relating to information security and, as the analogy with personal hygiene shows, is the equivalent of establishing simple routine measures to minimize the risks from cyber threats. The assumption is that good cyber hygiene practices can give networked users another layer of protection, reducing the risk that one vulnerable node will be used to either mount attacks or compromise another node or network, especially from common cyberattacks.[97] Cyber hygiene should also not be mistaken for proactive cyber defence, a military term.[98]
 The most common acts of digital hygiene can include updating malware protection, cloud back-ups, passwords, and ensuring restricted admin rights and network firewalls.[99] As opposed to a purely technology-based defense against threats, cyber hygiene mostly regards routine measures that are technically simple to implement and mostly dependent on discipline[100] or education.[101] It can be thought of as an abstract list of tips or measures that have been demonstrated as having a positive effect on personal and/or collective digital security. As such, these measures can be performed by laypeople, not just security experts.
 Cyber hygiene relates to personal hygiene as computer viruses relate to biological viruses (or pathogens). However, while the term computer virus was coined almost simultaneously with the creation of the first working computer viruses,[102] the term cyber hygiene is a much later invention, perhaps as late as 2000[103] by Internet pioneer Vint Cerf. It has since been adopted by the Congress[104] and Senate of the United States,[105] the FBI,[106] EU institutions[97] and heads of state.[98]
 Responding to attempted security breaches is often very difficult for a variety of reasons, including:
 Where an attack succeeds and a breach occurs, many jurisdictions now have in place mandatory security breach notification laws.
 The growth in the number of computer systems and the increasing reliance upon them by individuals, businesses, industries, and governments means that there are an increasing number of systems at risk.
 The computer systems of financial regulators and financial institutions like the U.S. Securities and Exchange Commission, SWIFT, investment banks, and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains.[107] Websites and apps that accept or store credit card numbers, brokerage accounts, and bank account information are also prominent hacking targets, because of the potential for immediate financial gain from transferring money, making purchases, or selling the information on the black market.[108] In-store payment systems and ATMs have also been tampered with in order to gather customer account data and PINs.
 The UCLA Internet Report: Surveying the Digital Future (2000) found that the privacy of personal data created barriers to online sales and that more than nine out of 10 internet users were somewhat or very concerned about credit card security.[109]
 The most common web technologies for improving security between browsers and websites are named SSL (Secure Sockets Layer), and its successor TLS (Transport Layer Security), identity management and authentication services, and domain name services allow companies and consumers to engage in secure communications and commerce. Several versions of SSL and TLS are commonly used today in applications such as web browsing, e-mail, internet faxing, instant messaging, and VoIP (voice-over-IP). There are various interoperable implementations of these technologies, including at least one implementation that is  open source. Open source allows anyone to view the application's source code, and look for and report vulnerabilities.
 The credit card companies Visa and MasterCard cooperated to develop the secure EMV chip which is embedded in credit cards. Further developments include the Chip Authentication Program where banks give customers hand-held card readers to perform online secure transactions. Other developments in this arena include the development of technology such as Instant Issuance which has enabled shopping mall kiosks acting on behalf of banks to issue on-the-spot credit cards to interested customers.
 Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.[110]
 The aviation industry is very reliant on a series of complex systems which could be attacked.[111] A simple power outage at one airport can cause repercussions worldwide,[112] much of the system relies on radio transmissions which could be disrupted,[113] and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore.[114] There is also potential for attack from within an aircraft.[115]
 Implementing fixes in aerospace systems poses a unique challenge because efficient air transportation is heavily affected by weight and volume. Improving security by adding physical devices to airplanes could increase their unloaded weight, and could potentially reduce cargo or passenger capacity.[116]
 In Europe, with the (Pan-European Network Service)[117] and NewPENS,[118] and in the US with the NextGen program,[119] air navigation service providers are moving to create their own dedicated networks.
 Many modern passports are now biometric passports, containing an embedded microchip that stores a digitized photograph and personal information such as name, gender, and date of birth. In addition, more countries[which?] are introducing facial recognition technology to reduce identity-related fraud. The introduction of the ePassport has assisted border officials in verifying the identity of the passport holder, thus allowing for quick passenger processing.[120] Plans are under way in the US, the UK, and Australia to introduce SmartGate kiosks with both retina and fingerprint recognition technology.[121] The airline industry is moving from the use of traditional paper tickets towards the use of electronic tickets (e-tickets). These have been made possible by advances in online credit card transactions in partnership with the airlines. Long-distance bus companies[which?] are also switching over to e-ticketing transactions today.
 The consequences of a successful attack range from loss of confidentiality to loss of system integrity, air traffic control outages, loss of aircraft, and even loss of life.
 Desktop computers and laptops are commonly targeted to gather passwords or financial account information or to construct a botnet to attack another target. Smartphones, tablet computers, smart watches, and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. WiFi, Bluetooth, and cell phone networks on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.[122]
 The increasing number of home automation devices such as the Nest thermostat are also potential targets.[122]
 Today many healthcare providers and health insurance companies use the internet to provide enhanced products and services, for example through use of tele-health to potentially offer better quality and access to healthcare, or fitness trackers to lower insurance premiums.
 The health care company Humana partners with WebMD, Oracle Corporation, EDS and Microsoft to enable its members to access their health care records, as well as to provide an overview of health care plans.[123] Patient records are increasingly being placed on secure in-house networks, alleviating the need for extra storage space.[124]
 Large corporations are common targets. In many cases attacks are aimed at financial gain through identity theft and involve data breaches. Examples include the loss of millions of clients' credit card and financial details by Home Depot,[125] Staples,[126] Target Corporation,[127] and Equifax.[128]
 Medical records have been targeted in general identify theft, health insurance fraud, and impersonating patients to obtain prescription drugs for recreational purposes or resale.[129] Although cyber threats continue to increase, 62% of all organizations did not increase security training for their business in 2015.[130]
 Not all attacks are financially motivated, however: security firm HBGary Federal had a serious series of attacks in 2011 from hacktivist group Anonymous in retaliation for the firm's CEO claiming to have infiltrated their group,[131][132] and Sony Pictures was hacked in 2014 with the apparent dual motive of embarrassing the company through data leaks and crippling the company by wiping workstations and servers.[133][134]
 Vehicles are increasingly computerized, with engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver-assistance systems on many models. Additionally, connected cars may use WiFi and Bluetooth to communicate with onboard consumer devices and the cell phone network.[135] Self-driving cars are expected to be even more complex. All of these systems carry some security risks, and such issues have gained wide attention.[136][137][138]
 Simple examples of risk include a malicious compact disc being used as an attack vector,[139] and the car's onboard microphones being used for eavesdropping. However, if access is gained to a car's internal controller area network, the danger is much greater[135] – and in a widely publicized 2015 test, hackers remotely carjacked a vehicle from 10 miles away and drove it into a ditch.[140][141]
 Manufacturers are reacting in numerous ways, with Tesla in 2016 pushing out some security fixes over the air into its cars' computer systems.[142] In the area of autonomous vehicles, in September 2016 the United States Department of Transportation announced some initial safety standards, and called for states to come up with uniform policies.[143][144][145]
 Additionally, e-Drivers' licenses are being developed using the same technology. For example, Mexico's licensing authority (ICV) has used a smart card platform to issue the first e-Drivers' licenses to the city of Monterrey, in the state of Nuevo León.[146]
 Shipping companies[147] have adopted RFID (Radio Frequency Identification) technology as an efficient, digitally secure, tracking device. Unlike a barcode, RFID can be read up to 20 feet away. RFID is used by FedEx[148] and UPS.[149]
 Government and military computer systems are commonly attacked by activists[150][151][152] and foreign powers.[153][154][155][156] Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, as well as student records.[157]
 The FBI, CIA, and Pentagon, all utilize secure controlled access technology for any of their buildings. However, the use of this form of technology is spreading into the entrepreneurial world. More and more companies are taking advantage of the development of digitally secure controlled access technology. GE's ACUVision, for example, offers a single panel platform for access control, alarm monitoring and digital recording.[158]
 The Internet of things (IoT) is the network of physical objects such as devices, vehicles, and buildings that are embedded with electronics, software, sensors, and network connectivity that enables them to collect and exchange data.[159] Concerns have been raised that this is being developed without appropriate consideration of the security challenges involved.[160][161]
 While the IoT creates opportunities for more direct integration of the physical world into computer-based systems,[162][163]
it also provides opportunities for misuse. In particular, as the Internet of Things spreads widely, cyberattacks are likely to become an increasingly physical (rather than simply virtual) threat.[164] If a front door's lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.[165]
 An attack that targets physical infrastructure and/or human lives is sometimes referred to as a cyber-kinetic attack. As IoT devices and appliances gain currency, cyber-kinetic attacks can become pervasive and significantly damaging.
 Medical devices have either been successfully attacked or had potentially deadly vulnerabilities demonstrated, including both in-hospital diagnostic equipment[166] and implanted devices including pacemakers[167] and insulin pumps.[168] There are many reports of hospitals and hospital organizations getting hacked, including ransomware attacks,[169][170][171][172] Windows XP exploits,[173][174] viruses,[175][176] and data breaches of sensitive data stored on hospital servers.[177][170][178][179] On 28 December 2016 the US Food and Drug Administration released its recommendations for how medical device manufacturers should maintain the security of Internet-connected devices – but no structure for enforcement.[180][181]
 In distributed generation systems, the risk of a cyber attack is real, according to Daily Energy Insider. An attack could cause a loss of power in a large area for a long period of time, and such an attack could have just as severe consequences as a natural disaster. The District of Columbia is considering creating a Distributed Energy Resources (DER) Authority within the city, with the goal being for customers to have more insight into their own energy use and giving the local electric utility, Pepco, the chance to better estimate energy demand. The D.C. proposal, however, would ""allow third-party vendors to create numerous points of energy distribution, which could potentially create more opportunities for cyber attackers to threaten the electric grid.""[182]
 Perhaps the most widely known digitally secure telecommunication device is the SIM (Subscriber Identity Module) card, a device that is embedded in most of the world's cellular devices before any service can be obtained. The SIM card is just the beginning of this digitally secure environment.
 The Smart Card Web Servers draft standard (SCWS) defines the interfaces to an HTTP server in a smart card.[183] Tests are being conducted to secure OTA (""over-the-air"") payment and credit card information from and to a mobile phone. 
Combination SIM/DVD devices are being developed through Smart Video Card technology which embeds a DVD-compliant optical disc into the card body of a regular SIM card.
 Other telecommunication developments involving digital security include mobile signatures, which use the embedded SIM card to generate a legally binding electronic signature.
 Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. ""Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal.""[184]
 However, reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions. According to the classic Gordon-Loeb Model analyzing the optimal investment level in information security, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).[185]
 As with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, some are activists, others are criminals looking for financial gain. State-sponsored attackers are now common and well resourced but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll in The Cuckoo's Egg.
 Attackers motivations can vary for all types of attacks from pleasure to for political goals.[15] For example, ""hacktivists"" may target a company a company or organization that carries out activities they do not agree with. This would be to create bad publicity for the company by having its website crash.
 High capability hackers, often with larger backing or state sponsorship, may attack based on the demands of their financial backers. These attacks are more likely to attempt more serious attack. An example of a more serious attack was the 2015 Ukraine power grid hack, which reportedly utilised the spear-phising, destruction of files, and denial-of-service attacks to carry out the full attack.[186][187]
 Additionally, recent attacker motivations can be traced back to extremist organizations seeking to gain political advantage or disrupt social agendas.[188] The growth of the internet, mobile technologies, and inexpensive computing devices have led to a rise in capabilities but also to the risk to environments that are deemed as vital to operations. All critical targeted environments are susceptible to compromise and this has led to a series of proactive studies on how to migrate the risk by taking into consideration motivations by these types of actors. Several stark differences exist between the hacker motivation and that of nation state actors seeking to attack based on an ideological preference.[189]
 A standard part of threat modeling for any particular system is to identify what might motivate an attack on that system, and who might be motivated to breach it. The level and detail of precautions will vary depending on the system to be secured. A home personal computer, bank, and classified military network face very different threats, even when the underlying technologies in use are similar.[190]
 Computer security incident management is an organized approach to addressing and managing the aftermath of a computer security incident or compromise with the goal of preventing a breach or thwarting a cyberattack. An incident that is not identified and managed at the time of intrusion typically escalates to a more damaging event such as a data breach or system failure. The intended outcome of a computer security incident response plan is to contain the incident, limit damage and assist recovery to business as usual. Responding to compromises quickly can mitigate exploited vulnerabilities, restore services and processes and minimize losses.[191]
Incident response planning allows an organization to establish a series of best practices to stop an intrusion before it causes damage. Typical incident response plans contain a set of written instructions that outline the organization's response to a cyberattack. Without a documented plan in place, an organization may not successfully detect an intrusion or compromise and stakeholders may not understand their roles, processes and procedures during an escalation, slowing the organization's response and resolution.
 There are four key components of a computer security incident response plan:
 Some illustrative examples of different types of computer security breaches are given below.
 In 1988, 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On 2 November 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers – the first internet computer worm.[193] The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris who said ""he wanted to count how many machines were connected to the Internet"".[193]
 In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.[194]
 In early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion[195] and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.[196]
 In 2010, the computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran's nuclear centrifuges.[197] It did so by disrupting industrial programmable logic controllers (PLCs) in a targeted attack. This is generally believed to have been launched by Israel and the United States to disrupt Iran's nuclear program[198][199][200][201] – although neither has publicly admitted this.
 In early 2013, documents provided by Edward Snowden were published by The Washington Post and The Guardian[202][203] exposing the massive scale of NSA global surveillance. There were also indications that the NSA may have inserted a backdoor in a NIST standard for encryption.[204] This standard was later withdrawn due to widespread criticism.[205] The NSA additionally were revealed to have tapped the links between Google's data centers.[206]
 A Ukrainian hacker known as Rescator broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards,[207] and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers.[208] Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. ""The malware utilized is absolutely unsophisticated and uninteresting,"" says Jim Walter, director of threat intelligence operations at security technology company McAfee – meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.
 In April 2015, the Office of Personnel Management discovered it had been hacked more than a year earlier in a data breach, resulting in the theft of approximately 21.5 million personnel records handled by the office.[209] The Office of Personnel Management hack has been described by federal officials as among the largest breaches of government data in the history of the United States.[210] Data targeted in the breach included personally identifiable information such as Social Security numbers, names, dates and places of birth, addresses, and fingerprints of current and former government employees as well as anyone who had undergone a government background check.[211][212] It is believed the hack was perpetrated by Chinese hackers.[213]
 In July 2015, a hacker group is known as The Impact Team successfully breached the extramarital relationship website Ashley Madison, created by Avid Life Media. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently.[214] When Avid Life Media did not take the site offline the group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned; but the website remained to function.
 In June 2021, the cyber attack took down the largest fuel pipeline in the U.S. and led to shortages across the East Coast.[215]
 International legal issues of cyber attacks are complicated in nature. There is no global base of common rules to judge, and eventually punish, cybercrimes and cybercriminals - and where security firms or agencies do locate the cybercriminal behind the creation of a particular piece of malware or form of cyber attack, often the local authorities cannot take action due to lack of laws under which to prosecute.[216][217] Proving attribution for cybercrimes and cyberattacks is also a major problem for all law enforcement agencies. ""Computer viruses switch from one country to another, from one jurisdiction to another – moving around the world, using the fact that we don't have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world.""[216] The use of techniques such as dynamic DNS, fast flux and bullet proof servers add to the difficulty of investigation and enforcement.
 The role of the government is to make regulations to force companies and organizations to protect their systems, infrastructure and information from any cyberattacks, but also to protect its own national infrastructure such as the national power-grid.[218]
 The government's regulatory role in cyberspace is complicated. For some, cyberspace was seen as a virtual space that was to remain free of government intervention, as can be seen in many of today's libertarian blockchain and bitcoin discussions.[219]
 Many government officials and experts think that the government should do more and that there is a crucial need for improved regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the ""industry only responds when you threaten regulation. If the industry doesn't respond (to the threat), you have to follow through.""[220] On the other hand, executives from the private sector agree that improvements are necessary, but think that government intervention would affect their ability to innovate efficiently. Daniel R. McCarthy analyzed this public-private partnership in cybersecurity and reflected on the role of cybersecurity in the broader constitution of political order.[221]
 On 22 May 2020, the UN Security Council held its second ever informal meeting on cybersecurity to focus on cyber challenges to international peace. According to UN Secretary-General António Guterres, new technologies are too often used to violate rights.[222]
 Many different teams and organizations exist, including:
 On 14 April 2016, the European Parliament and the Council of the European Union adopted the General Data Protection Regulation (GDPR). The GDPR, which came into force on 25 May 2018, grants individuals within the European Union (EU) and the European Economic Area (EEA) the right to the protection of personal data. The regulation requires that any entity that processes personal data incorporate data protection by design and by default. It also requires that certain organizations appoint a Data Protection Officer (DPO).
 Most countries have their own computer emergency response team to protect network security.
 Since 2010, Canada has had a cybersecurity strategy.[228][229] This functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure.[230] The strategy has three main pillars: securing government systems, securing vital private cyber systems, and helping Canadians to be secure online.[229][230] There is also a Cyber Incident Management Framework to provide a coordinated response in the event of a cyber incident.[231][232]
 The Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada's critical infrastructure and cyber systems. It provides support to mitigate cyber threats, technical support to respond & recover from targeted cyber attacks, and provides online tools for members of Canada's critical infrastructure sectors.[233] It posts regular cybersecurity bulletins[234] & operates an online reporting tool where individuals and organizations can report a cyber incident.[235]
 To inform the general public on how to protect themselves online, Public Safety Canada has partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations,[236] and launched the Cyber Security Cooperation Program.[237][238] They also run the GetCyberSafe portal for Canadian citizens, and Cyber Security Awareness Month during October.[239]
 Public Safety Canada aims to begin an evaluation of Canada's cybersecurity strategy in early 2015.[230]
 Australian federal government announced an $18.2 million investment to fortify the cybersecurity resilience of small and medium enterprises (SMEs) and enhance their capabilities in responding to cyber threats. This financial backing is an integral component of the soon-to-be-unveiled 2023-2030  Australian Cyber Security Strategy, slated for release within the current week. A substantial allocation of $7.2 million is earmarked for the establishment of a voluntary cyber health check program, facilitating businesses in conducting a comprehensive and tailored self-assessment of their cybersecurity upskill.
 This avant-garde health assessment serves as a diagnostic tool, enabling enterprises to ascertain the robustness of Australia's cyber security regulations. Furthermore, it affords them access to a repository of educational resources and materials, fostering the acquisition of skills necessary for an elevated cybersecurity posture. This groundbreaking initiative was jointly disclosed by Minister for Cyber Security Clare O'Neil and Minister for Small Business Julie Collins.
[240]
 
 Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000.[241]
 The National Cyber Security Policy 2013 is a policy framework by the Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard ""information, such as personal information (of web users), financial and banking information and sovereign data"". CERT- In is the nodal agency which monitors the cyber threats in the country. The post of National Cyber Security Coordinator has also been created in the Prime Minister's Office (PMO).
 The Indian Companies Act 2013 has also introduced cyber law and cybersecurity obligations on the part of Indian directors. Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000 Update in 2013.[242]
 Following cyberattacks in the first half of 2013, when the government, news media, television stations, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011,[243] and 2012, but Pyongyang denies the accusations.[244]
 The United States has its first fully formed cyber plan in 15 years, as a result of the release of this National Cyber plan.[245] In this policy, the US says it will: Protect the country by keeping networks, systems, functions, and data safe; Promote American wealth by building a strong digital economy and encouraging strong domestic innovation; Peace and safety should be kept by making it easier for the US to stop people from using computer tools for bad things, working with friends and partners to do this; and Increase the United States' impact around the world to support the main ideas behind an open, safe, reliable, and compatible Internet.[246] The new U.S. cyber strategy[247] seeks to allay some of those concerns by promoting responsible behavior in cyberspace, urging nations to adhere to a set of norms, both through international law and voluntary standards. It also calls for specific measures to harden U.S. government networks from attacks, like the June 2015 intrusion into the U.S. Office of Personnel Management (OPM), which compromised the records of about 4.2 million current and former government employees. And the strategy calls for the U.S. to continue to name and shame bad cyber actors, calling them out publicly for attacks when possible, along with the use of economic sanctions and diplomatic pressure.[248]
 The 1986 18 U.S.C. § 1030, the Computer Fraud and Abuse Act is the key legislation. It prohibits unauthorized access or damage of protected computers as defined in 18 U.S.C. § 1030(e)(2). Although various other measures have been proposed[249][250] – none has succeeded.
 In 2013, executive order 13636 Improving Critical Infrastructure Cybersecurity was signed, which prompted the creation of the NIST Cybersecurity Framework.
 In response to the Colonial Pipeline ransomware attack[251] President Joe Biden signed Executive Order 14028[252] on May 12, 2021, to increase software security standards for sales to the government, tighten detection and security on existing systems, improve information sharing and training, establish a Cyber Safety Review Board, and improve incident response.
 The General Services Administration (GSA) has[when?] standardized the penetration test service as a pre-vetted support service, to rapidly address potential vulnerabilities, and stop adversaries before they impact US federal, state and local governments. These services are commonly referred to as Highly Adaptive Cybersecurity Services (HACS).
 The Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division.[253][254] The division is home to US-CERT operations and the National Cyber Alert System.[254] The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.[255]
 The third priority of the FBI is to: ""Protect the United States against cyber-based attacks and high-technology crimes"",[256] and they, along with the National White Collar Crime Center (NW3C), and the Bureau of Justice Assistance (BJA) are part of the multi-agency task force, The Internet Crime Complaint Center, also known as IC3.[257]
 In addition to its own specific duties, the FBI participates alongside non-profit organizations such as InfraGard.[258][259]
 The Computer Crime and Intellectual Property Section (CCIPS) operates in the United States Department of Justice Criminal Division. The CCIPS is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks.[260] In 2017, CCIPS published A Framework for a Vulnerability Disclosure Program for Online Systems to help organizations ""clearly describe authorized vulnerability disclosure and discovery conduct, thereby substantially reducing the likelihood that such described activities will result in a civil or criminal violation of law under the Computer Fraud and Abuse Act (18 U.S.C. § 1030).""[261]
 The United States Cyber Command, also known as USCYBERCOM, ""has the mission to direct, synchronize, and coordinate cyberspace planning and operations to defend and advance national interests in collaboration with domestic and international partners.""[262] It has no role in the protection of civilian networks.[263][264]
 The U.S. Federal Communications Commission's role in cybersecurity is to strengthen the protection of critical communications infrastructure, to assist in maintaining the reliability of networks during disasters, to aid in swift recovery after, and to ensure that first responders have access to effective communications services.[265]
 The Food and Drug Administration has issued guidance for medical devices,[266] and the National Highway Traffic Safety Administration[267] is concerned with automotive cybersecurity. After being criticized by the Government Accountability Office,[268] and following successful attacks on airports and claimed attacks on airplanes, the Federal Aviation Administration has devoted funding to securing systems on board the planes of private manufacturers, and the Aircraft Communications Addressing and Reporting System.[269] Concerns have also been raised about the future Next Generation Air Transportation System.[270]
 The US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.[271]
 Computer emergency response team is a name given to expert groups that handle computer security incidents. In the US, two distinct organizations exist, although they do work closely together.
 In the context of U.S. nuclear power plants, the U.S. Nuclear Regulatory Commission (NRC) outlines cybersecurity requirements under 10 CFR Part 73, specifically in §73.54.[273]
 The Nuclear Energy Institute's NEI 08-09 document, Cyber Security Plan for Nuclear Power Reactors,[274]  outlines a comprehensive framework for cybersecurity in the nuclear power industry. Drafted with input from the U.S. NRC, this guideline is instrumental in aiding licensees to comply with the Code of Federal Regulations (CFR), which mandates robust protection of digital computers and equipment and communications systems at nuclear power plants against cyber threats.[275]
 There is growing concern that cyberspace will become the next theater of warfare. As Mark Clayton from The Christian Science Monitor wrote in a 2015 article titled ""The New Cyber Arms Race"":
 In the future, wars will not just be fought by soldiers with guns or with planes that drop bombs. They will also be fought with the click of a mouse a half a world away that unleashes carefully weaponized computer programs that disrupt or destroy critical industries like utilities, transportation, communications, and energy. Such attacks could also disable military networks that control the movement of troops, the path of jet fighters, the command and control of warships.[276] This has led to new terms such as cyberwarfare and cyberterrorism. The United States Cyber Command was created in 2009[277] and many other countries have similar forces.
 There are a few critical voices that question whether cybersecurity is as significant a threat as it is made out to be.[278][279][280]
 Cybersecurity is a fast-growing field of IT concerned with reducing organizations' risk of hack or data breaches.[281] According to research from the Enterprise Strategy Group, 46% of organizations say that they have a ""problematic shortage"" of cybersecurity skills in 2016, up from 28% in 2015.[282] Commercial, government and non-governmental organizations all employ cybersecurity professionals. The fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance, health care, and retail.[283] However, the use of the term cybersecurity is more prevalent in government job descriptions.[284]
 Typical cybersecurity job titles and descriptions include:[285]
 Student programs are also available for people interested in beginning a career in cybersecurity.[289][290] Meanwhile, a flexible and effective option for information security professionals of all experience levels to keep studying is online security training, including webcasts.[291][292] A wide range of certified courses are also available.[293]
 In the United Kingdom, a nationwide set of cybersecurity forums, known as the U.K Cyber Security Forum, were established supported by the Government's cybersecurity strategy[294] in order to encourage start-ups and innovation and to address the skills gap[295] identified by the U.K Government.
 In Singapore, the Cyber Security Agency has issued a Singapore Operational Technology (OT) Cybersecurity Competency Framework (OTCCF). The framework defines emerging cybersecurity roles in Operational Technology. The OTCCF was endorsed by the Infocomm Media Development Authority (IMDA). It outlines the different OT cybersecurity job positions as well as the technical skills and core competencies necessary. It also depicts the many career paths available, including vertical and lateral advancement opportunities.[296]
 The following terms used with regards to computer security are explained below:
 Since the Internet's arrival and with the digital transformation initiated in recent years, the notion of cybersecurity has become a familiar subject in both our professional and personal lives. Cybersecurity and cyber threats have been consistently present for the last 60 years of technological change. In the 1970s and 1980s, computer security was mainly limited to academia until the conception of the Internet, where, with increased connectivity, computer viruses and network intrusions began to take off. After the spread of viruses in the 1990s, the 2000s marked the institutionalization of organized attacks such as distributed denial of service.[300] This led to the formalization of cybersecurity as a professional discipline.[301]
 The April 1967 session organized by Willis Ware at the Spring Joint Computer Conference, and the later publication of the Ware Report, were foundational moments in the history of the field of computer security.[302] Ware's work straddled the intersection of material, cultural, political, and social concerns.[302]
 A 1977 NIST publication[303] introduced the CIA triad of confidentiality, integrity, and availability as a clear and simple way to describe key security goals.[304] While still relevant, many more elaborate frameworks have since been proposed.[305][306]
 However, in the 1970s and 1980s, there were no grave computer threats because computers and the internet were still developing, and security threats were easily identifiable. More often, threats came from malicious insiders who gained unauthorized access to sensitive documents and files. Although malware and network breaches existed during the early years, they did not use them for financial gain. By the second half of the 1970s, established computer firms like IBM started offering commercial access control systems and computer security software products.[307]
 One of the earliest examples of an attack on a computer network was the computer worm Creeper written by Bob Thomas at BBN, which propagated through the ARPANET in 1971.[citation needed] The program was purely experimental in nature and carried no malicious payload. A later program, Reaper, was created by Ray Tomlinson in 1972 and used to destroy Creeper.[citation needed]
 Between September 1986 and June 1987, a group of German hackers performed the first documented case of cyber espionage.[308] The group hacked into American defense contractors, universities, and military base networks and sold gathered information to the Soviet KGB. The group was led by Markus Hess, who was arrested on 29 June 1987. He was convicted of espionage (along with two co-conspirators) on 15 Feb 1990.
 In 1988, one of the first computer worms, called the Morris worm, was distributed via the Internet. It gained significant mainstream media attention.[309]
 In 1993, Netscape started developing the protocol SSL, shortly after the National Center for Supercomputing Applications (NCSA) launched Mosaic 1.0, the first web browser, in 1993.[citation needed] Netscape had SSL version 1.0 ready in 1994, but it was never released to the public due to many serious security vulnerabilities. These weaknesses included replay attacks and a vulnerability that allowed hackers to alter unencrypted communications sent by users. However, in February 1995, Netscape launched Version 2.0.[310]
 The National Security Agency (NSA) is responsible for the protection of U.S. information systems and also for collecting foreign intelligence.[311] The agency analyzes commonly used software and system configurations to find security flaws, which it can use for offensive purposes against competitors of the United States.[312]
 NSA contractors created and sold click-and-shoot attack tools to US agencies and close allies, but eventually, the tools made their way to foreign adversaries.[citation needed] In 2016, NSAs own hacking tools were hacked, and they have been used by Russia and North Korea.[citation needed] NSA's employees and contractors have been recruited at high salaries by adversaries, anxious to compete in cyberwarfare.[citation needed] In 2007, the United States and Israel began exploiting security flaws in the Microsoft Windows operating system to attack and damage equipment used in Iran to refine nuclear materials. Iran responded by heavily investing in their own cyberwarfare capability, which it began using against the United States.[312]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Next Generation Air Transportation System', 'Various people or parties', ""reducing organizations' risk of hack or data breaches"", 'internal actors within a company', 'prominent hacking targets'], 'answer_start': [], 'answer_end': []}"
"
 Quantum mechanics is a fundamental theory in physics that describes the behavior of nature at and below the scale of atoms.[2]: 1.1  It is the foundation of all quantum physics, which includes quantum chemistry, quantum field theory, quantum technology, and quantum information science.
 Quantum mechanics can describe many systems that classical physics cannot. Classical physics can describe many aspects of nature at an ordinary (macroscopic and (optical) microscopic) scale, but is not sufficient for describing them at very small submicroscopic (atomic and subatomic) scales. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic/microscopic) scale.[3]
 Quantum systems have bound states that are quantized to discrete values of energy, momentum, angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of both particles and waves (wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).
 Quantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the ""old quantum theory"", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.
 Quantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and sub-atomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms,[4] but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative.[5] Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known as quantum electrodynamics (QED), has been shown to agree with experiment to within 1 part in 1012 when predicting the magnetic properties of an electron.[6]
 A fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only give probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. The Schrödinger equation relates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.[7]: 67–87 
 One consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between different measurable quantities. The most famous form of this uncertainty principle says that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of its momentum.[7]: 427–435 
 Another consequence of the mathematical rules of quantum mechanics is the phenomenon of quantum interference, which is often illustrated with the double-slit experiment. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.[8]: 102–111 [2]: 1.1–1.8  The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles.[8] However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave).[8]: 109 [9][10] However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known as wave–particle duality. In addition to light, electrons, atoms, and molecules are all found to exhibit the same dual behavior when fired towards a double slit.[2]
 Another non-classical phenomenon predicted by quantum mechanics is quantum tunnelling: a particle that goes up against a potential barrier can cross it, even if its kinetic energy is smaller than the maximum of the potential.[11] In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enabling radioactive decay, nuclear fusion in stars, and applications such as scanning tunnelling microscopy and the tunnel diode.[12]
 When quantum systems interact, the result can be the creation of quantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrödinger called entanglement ""...the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought"".[13] Quantum entanglement enables quantum computing and is part of quantum communication protocols, such as quantum key distribution and superdense coding.[14] Contrary to popular misconception, entanglement does not allow sending signals faster than light, as demonstrated by the no-communication theorem.[14]
 Another possibility opened by entanglement is testing for ""hidden variables"", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory can provide. A collection of results, most significantly Bell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory of local hidden variables, then the results of a Bell test will be constrained in a particular, quantifiable way. Many Bell tests have been performed and they have shown results incompatible with the constraints imposed by local hidden variables.[15][16]
 It is not possible to present these concepts in more than a superficial way without introducing the actual mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but also linear algebra, differential equations, group theory, and other more advanced subjects.[17][18] Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.
 In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector 



ψ


{\displaystyle \psi }

 belonging to a (separable) complex Hilbert space 





H




{\displaystyle {\mathcal {H}}}

. This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys 



⟨
ψ
,
ψ
⟩
=
1


{\displaystyle \langle \psi ,\psi \rangle =1}

, and it is well-defined up to a complex number of modulus 1 (the global phase), that is, 



ψ


{\displaystyle \psi }

 and 




e

i
α


ψ


{\displaystyle e^{i\alpha }\psi }

 represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions 




L

2


(

C

)


{\displaystyle L^{2}(\mathbb {C} )}

, while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors 





C


2




{\displaystyle \mathbb {C} ^{2}}

 with the usual inner product.
 Physical quantities of interest – position, momentum, energy, spin – are represented by observables, which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A quantum state can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue 



λ


{\displaystyle \lambda }

 is non-degenerate and the probability is given by 




|

⟨



λ
→



,
ψ
⟩


|


2




{\displaystyle |\langle {\vec {\lambda }},\psi \rangle |^{2}}

, where 






λ
→





{\displaystyle {\vec {\lambda }}}

 is its associated eigenvector. More generally, the eigenvalue is degenerate and the probability is given by 



⟨
ψ
,

P

λ


ψ
⟩


{\displaystyle \langle \psi ,P_{\lambda }\psi \rangle }

, where 




P

λ




{\displaystyle P_{\lambda }}

 is the projector onto its associated eigenspace. In the continuous case, these formulas give instead the probability density.
 After the measurement, if result 



λ


{\displaystyle \lambda }

 was obtained, the quantum state is postulated to collapse to 






λ
→





{\displaystyle {\vec {\lambda }}}

, in the non-degenerate case, or to 




P

λ


ψ


/





⟨
ψ
,

P

λ


ψ
⟩




{\textstyle P_{\lambda }\psi {\big /}\!{\sqrt {\langle \psi ,P_{\lambda }\psi \rangle }}}

, in the general case. The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a ""measurement"" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of ""wave function collapse"" (see, for example, the many-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[19]
 The time evolution of a quantum state is described by the Schrödinger equation:
 Here 



H


{\displaystyle H}

 denotes the Hamiltonian, the observable corresponding to the total energy of the system, and 



ℏ


{\displaystyle \hbar }

 is the reduced Planck constant. The constant 



i
ℏ


{\displaystyle i\hbar }

 is introduced so that the Hamiltonian is reduced to the classical Hamiltonian in cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called the correspondence principle.
 The solution of this differential equation is given by
 The operator 



U
(
t
)
=

e

−
i
H
t

/

ℏ




{\displaystyle U(t)=e^{-iHt/\hbar }}

 is known as the time-evolution operator, and has the crucial property that it is unitary. This time evolution is deterministic in the sense that – given an initial quantum state 



ψ
(
0
)


{\displaystyle \psi (0)}

  – it makes a definite prediction of what the quantum state 



ψ
(
t
)


{\displaystyle \psi (t)}

 will be at any later time.[20]
 Some wave functions produce probability distributions that are independent of time, such as eigenstates of the Hamiltonian.[7]: 133–137  Many systems that are treated dynamically in classical mechanics are described by such ""static"" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as an s orbital (Fig. 1).
 Analytic solutions of the Schrödinger equation are known for very few relatively simple model Hamiltonians including the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom. Even the helium atom – which contains just two electrons – has defied all attempts at a fully analytic treatment, admitting no solution in closed form.[21][22][23]
 However, there are techniques for finding approximate solutions. One method, called perturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weak potential energy.[7]: 793  Another approximation method applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion.[7]: 849 
 One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum.[24][25] Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator 






X
^





{\displaystyle {\hat {X}}}

 and momentum operator 






P
^





{\displaystyle {\hat {P}}}

 do not commute, but rather satisfy the canonical commutation relation:
 Given a quantum state, the Born rule lets us compute expectation values for both 



X


{\displaystyle X}

 and 



P


{\displaystyle P}

, and moreover for powers of them. Defining
the uncertainty for an observable by a standard deviation, we have
 and likewise for the momentum:
 The uncertainty principle states that
 Either standard deviation can in principle be made arbitrarily small, but not both simultaneously.[26] This inequality generalizes to arbitrary pairs of self-adjoint operators 



A


{\displaystyle A}

 and 



B


{\displaystyle B}

. The commutator of these two operators is
 and this provides the lower bound on the product of standard deviations:
 Another consequence of the canonical commutation relation is that the position and momentum operators are Fourier transforms of each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to an 



i

/

ℏ


{\displaystyle i/\hbar }

 factor) to taking the derivative according to the position, since in Fourier analysis differentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentum 




p

i




{\displaystyle p_{i}}

 is replaced by 



−
i
ℏ


∂

∂
x





{\displaystyle -i\hbar {\frac {\partial }{\partial x}}}

, and in particular in the non-relativistic Schrödinger equation in position space the momentum-squared term is replaced with a Laplacian times 



−

ℏ

2




{\displaystyle -\hbar ^{2}}

.[24]
 When two different quantum systems are considered together, the Hilbert space of the combined system is the tensor product of the Hilbert spaces of the two components. For example, let A and B be two quantum systems, with Hilbert spaces 






H



A




{\displaystyle {\mathcal {H}}_{A}}

 and 






H



B




{\displaystyle {\mathcal {H}}_{B}}

, respectively. The Hilbert space of the composite system is then
 If the state for the first system is the vector 




ψ

A




{\displaystyle \psi _{A}}

 and the state for the second system is 




ψ

B




{\displaystyle \psi _{B}}

, then the state of the composite system is
 Not all states in the joint Hilbert space 






H



A
B




{\displaystyle {\mathcal {H}}_{AB}}

 can be written in this form, however, because the superposition principle implies that linear combinations of these ""separable"" or ""product states"" are also valid. For example, if 




ψ

A




{\displaystyle \psi _{A}}

 and 




ϕ

A




{\displaystyle \phi _{A}}

 are both possible states for system 



A


{\displaystyle A}

, and likewise 




ψ

B




{\displaystyle \psi _{B}}

 and 




ϕ

B




{\displaystyle \phi _{B}}

 are both possible states for system 



B


{\displaystyle B}

, then
 is a valid joint state that is not separable. States that are not separable are called entangled.[27][28]
 If the state for a composite system is entangled, it is impossible to describe either component system A or system B by a state vector. One can instead define reduced density matrices that describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system.[27][28] Just as density matrices specify the state of a subsystem of a larger system, analogously, positive operator-valued measures (POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.[27][29]
 As described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known as quantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic.[30]
 There are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the ""transformation theory"" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics – matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger).[31] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.[32]
 The Hamiltonian 



H


{\displaystyle H}

 is known as the generator of time evolution, since it defines a unitary time-evolution operator 



U
(
t
)
=

e

−
i
H
t

/

ℏ




{\displaystyle U(t)=e^{-iHt/\hbar }}

 for each value of 



t


{\displaystyle t}

. From this relation between 



U
(
t
)


{\displaystyle U(t)}

 and 



H


{\displaystyle H}

, it follows that any observable 



A


{\displaystyle A}

 that commutes with 



H


{\displaystyle H}

 will be conserved: its expectation value will not change over time.[7]: 471  This statement generalizes, as mathematically, any Hermitian operator 



A


{\displaystyle A}

 can generate a family of unitary operators parameterized by a variable 



t


{\displaystyle t}

. Under the evolution generated by 



A


{\displaystyle A}

, any observable 



B


{\displaystyle B}

 that commutes with 



A


{\displaystyle A}

 will be conserved. Moreover, if 



B


{\displaystyle B}

 is conserved by evolution under 



A


{\displaystyle A}

, then 



A


{\displaystyle A}

 is conserved under the evolution generated by 



B


{\displaystyle B}

. This implies a quantum version of the result proven by Emmy Noether in classical (Lagrangian) mechanics: for every differentiable symmetry of a Hamiltonian, there exists a corresponding conservation law.
 The simplest example of a quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:
 The general solution of the Schrödinger equation is given by
 which is a superposition of all possible plane waves 




e

i
(
k
x
−



ℏ

k

2




2
m



t
)




{\displaystyle e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}}

, which are eigenstates of the momentum operator with momentum 



p
=
ℏ
k


{\displaystyle p=\hbar k}

. The coefficients of the superposition are 






ψ
^



(
k
,
0
)


{\displaystyle {\hat {\psi }}(k,0)}

, which is the Fourier transform of the initial quantum state 



ψ
(
x
,
0
)


{\displaystyle \psi (x,0)}

.
 It is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states.[note 1] Instead, we can consider a Gaussian wave packet:
 which has Fourier transform, and therefore momentum distribution
 We see that as we make 



a


{\displaystyle a}

 smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by making 



a


{\displaystyle a}

 larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.
 As we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant.[33]
 The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere inside a certain region, and therefore infinite potential energy everywhere outside that region.[24]: 77–78  For the one-dimensional case in the 



x


{\displaystyle x}

 direction, the time-independent Schrödinger equation may be written
 With the differential operator defined by
 the previous equation is evocative of the classic kinetic energy analogue,
 with state 



ψ


{\displaystyle \psi }

 in this case having energy 



E


{\displaystyle E}

 coincident with the kinetic energy of the particle.
 The general solutions of the Schrödinger equation for the particle in a box are
 or, from Euler's formula,
 The infinite potential walls of the box determine the values of 



C
,
D
,


{\displaystyle C,D,}

 and 



k


{\displaystyle k}

 at 



x
=
0


{\displaystyle x=0}

 and 



x
=
L


{\displaystyle x=L}

 where 



ψ


{\displaystyle \psi }

 must be zero. Thus, at 



x
=
0


{\displaystyle x=0}

,
 and 



D
=
0


{\displaystyle D=0}

. At 



x
=
L


{\displaystyle x=L}

,
 in which 



C


{\displaystyle C}

 cannot be zero as this would conflict with the postulate that 



ψ


{\displaystyle \psi }

 has norm 1. Therefore, since 



sin
⁡
(
k
L
)
=
0


{\displaystyle \sin(kL)=0}

, 



k
L


{\displaystyle kL}

 must be an integer multiple of 



π


{\displaystyle \pi }

,
 This constraint on 



k


{\displaystyle k}

 implies a constraint on the energy levels, yielding
 




E

n


=




ℏ

2



π

2



n

2




2
m

L

2





=




n

2



h

2




8
m

L

2





.


{\displaystyle E_{n}={\frac {\hbar ^{2}\pi ^{2}n^{2}}{2mL^{2}}}={\frac {n^{2}h^{2}}{8mL^{2}}}.}


 A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of the rectangular potential barrier, which furnishes a model for the quantum tunneling effect that plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy.
 As in the classical case, the potential for the quantum harmonic oscillator is given by[7]: 234 
 This problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant ""ladder method"" first proposed by Paul Dirac. The eigenstates are given by
 where Hn are the Hermite polynomials
 and the corresponding energy levels are
 This is another example illustrating the discretization of energy for bound states.
 The Mach–Zehnder interferometer (MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in the delayed choice quantum eraser, the Elitzur–Vaidman bomb tester, and in studies of quantum entanglement.[34][35]
 We can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the ""lower"" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the ""upper"" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vector 



ψ
∈


C


2




{\displaystyle \psi \in \mathbb {C} ^{2}}

 that is a superposition of the ""lower"" path 




ψ

l


=


(



1




0



)




{\displaystyle \psi _{l}={\begin{pmatrix}1\\0\end{pmatrix}}}

 and the ""upper"" path 




ψ

u


=


(



0




1



)




{\displaystyle \psi _{u}={\begin{pmatrix}0\\1\end{pmatrix}}}

, that is, 



ψ
=
α

ψ

l


+
β

ψ

u




{\displaystyle \psi =\alpha \psi _{l}+\beta \psi _{u}}

 for complex 



α
,
β


{\displaystyle \alpha ,\beta }

. In order to respect the postulate that 



⟨
ψ
,
ψ
⟩
=
1


{\displaystyle \langle \psi ,\psi \rangle =1}

 we require that 




|

α


|


2


+

|

β


|


2


=
1


{\displaystyle |\alpha |^{2}+|\beta |^{2}=1}

.
 Both beam splitters are modelled as the unitary matrix 



B
=


1

2





(



1


i




i


1



)




{\displaystyle B={\frac {1}{\sqrt {2}}}{\begin{pmatrix}1&i\\i&1\end{pmatrix}}}

, which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of 



1

/



2




{\displaystyle 1/{\sqrt {2}}}

, or be reflected to the other path with a probability amplitude of 



i

/



2




{\displaystyle i/{\sqrt {2}}}

. The phase shifter on the upper arm is modelled as the unitary matrix 



P
=


(



1


0




0



e

i
Δ
Φ





)




{\displaystyle P={\begin{pmatrix}1&0\\0&e^{i\Delta \Phi }\end{pmatrix}}}

, which means that if the photon is on the ""upper"" path it will gain a relative phase of 



Δ
Φ


{\displaystyle \Delta \Phi }

, and it will stay unchanged if it is in the lower path.
 A photon that enters the interferometer from the left will then be acted upon with a beam splitter 



B


{\displaystyle B}

, a phase shifter 



P


{\displaystyle P}

, and another beam splitter 



B


{\displaystyle B}

, and so end up in the state
 and the probabilities that it will be detected at the right or at the top are given respectively by
 One can therefore use the Mach–Zehnder interferometer to estimate the phase shift by estimating these probabilities.
 It is interesting to consider what would happen if the photon were definitely in either the ""lower"" or ""upper"" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases, there will be no interference between the paths anymore, and the probabilities are given by 



p
(
u
)
=
p
(
l
)
=
1

/

2


{\displaystyle p(u)=p(l)=1/2}

, independently of the phase 



Δ
Φ


{\displaystyle \Delta \Phi }

. From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths.[36]
 Quantum mechanics has had enormous success in explaining many of the features of our universe, with regard to small-scale and discrete quantities and interactions which cannot be explained by classical methods.[note 2] Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Solid-state physics and materials science are dependent upon quantum mechanics.[37]
 In many aspects, modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, the optical amplifier and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy.[38] Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.
 The rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space – although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is the correspondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those of classical mechanics in the regime of large quantum numbers.[39] One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known as quantization.[40]: 299 [41]
 When quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.[7]: 234 
 Complications arise with chaotic systems, which do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.[40]: 353 
 Quantum decoherence is a mechanism through which quantum systems lose coherence, and thus become incapable of displaying many typically quantum effects: quantum superpositions become simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations.[7]: 687–730  Quantum coherence is not typically evident at macroscopic scales, though at temperatures approaching absolute zero quantum behavior may manifest macroscopically.[note 3]
 Many macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms and molecules which would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction of electric charges under the rules of quantum mechanics.[42]
 Early attempts to merge quantum mechanics with special relativity involved the replacement of the Schrödinger equation with a covariant equation such as the Klein–Gordon equation or the Dirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, quantum electrodynamics, provides a fully quantum description of the electromagnetic interaction. Quantum electrodynamics is, along with general relativity, one of the most accurate physical theories ever devised.[43][44]
 The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treat charged particles as quantum mechanical objects being acted on by a classical electromagnetic field. For example, the elementary quantum model of the hydrogen atom describes the electric field of the hydrogen atom using a classical 




−

e

2



/

(
4
π

ϵ




0




r
)



{\displaystyle \textstyle -e^{2}/(4\pi \epsilon _{_{0}}r)}

 Coulomb potential.[7]: 285  Likewise, in a Stern–Gerlach experiment, a charged particle is modeled as a quantum system, while the background magnetic field is described classically.[40]: 26  This ""semi-classical"" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons by charged particles.
 Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg.[45]
 Even though the predictions of both quantum theory and general relativity have been supported by rigorous and repeated empirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant ""Theory of Everything"" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.[46]
 One proposal for doing so is string theory, which posits that the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force.[47][48]
 Another popular theory is loop quantum gravity (LQG), which describes quantum properties of gravity and is thus a theory of quantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric ""woven"" of finite loops called spin networks. The evolution of a spin network over time is called a spin foam. The characteristic length scale of a spin foam is the Planck length, approximately 1.616×10−35 m, and so lengths shorter than the Planck length are not physically meaningful in LQG.[49]
 Is there a preferred interpretation of quantum mechanics? How does the quantum description of reality, which includes elements such as the ""superposition of states"" and ""wave function collapse"", give rise to the reality we perceive? Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties with wavefunction collapse and the related measurement problem, and quantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus. Richard Feynman once said, ""I think I can safely say that nobody understands quantum mechanics.""[50] According to Steven Weinberg, ""There is now in my opinion no entirely satisfactory interpretation of quantum mechanics.""[51]
 The views of Niels Bohr, Werner Heisenberg and other physicists are often grouped together as the ""Copenhagen interpretation"".[52][53] According to these views, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but is instead a final renunciation of the classical idea of ""causality"". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the complementary nature of evidence obtained under different experimental situations. Copenhagen-type interpretations were adopted by Nobel laureates in quantum physics, including Bohr,[54] Heisenberg,[55] Schrödinger,[56] Feynman,[2] and Zeilinger[57] as well as 21st-century researchers in quantum foundations.[58]
 Albert Einstein, himself one of the founders of quantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such as determinism and locality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as the Bohr–Einstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbids action at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to how thermodynamics is valid, but the fundamental theory behind it is statistical mechanics. In 1935, Einstein and his collaborators Boris Podolsky and Nathan Rosen published an argument that the principle of locality implies the incompleteness of quantum mechanics, a thought experiment later termed the Einstein–Podolsky–Rosen paradox.[note 4] In 1964, John Bell showed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known as Bell inequalities, that can be violated by entangled particles.[63] Since then several experiments have been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.[15][16]
 Bohmian mechanics shows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrödinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.[64]
 Everett's many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[65] This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule,[66][67] with no consensus on whether they have been successful.[68][69][70]
 Relational quantum mechanics appeared in the late 1990s as a modern derivative of Copenhagen-type ideas,[71] and QBism was developed some years later.[72]
 Quantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[73] In 1803 English polymath Thomas Young described the famous double-slit experiment.[74] This experiment played a major role in the general acceptance of the wave theory of light.
 During the early 19th century, chemical research by John Dalton and Amedeo Avogadro lent weight to the atomic theory of matter, an idea that James Clerk Maxwell, Ludwig Boltzmann and others built upon to establish the kinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics.[75] While the early conception of atoms from Greek philosophy had been that they were indivisible units –  the word ""atom"" deriving from the Greek for ""uncuttable"" –  the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard was Michael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure. Julius Plücker, Johann Wilhelm Hittorf and Eugen Goldstein carried on and improved upon Faraday's work, leading to the identification of cathode rays, which J. J. Thomson found to consist of subatomic particles that would be called electrons.[76][77]
 The black-body radiation problem was discovered by Gustav Kirchhoff in 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete ""quanta"" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation.[78] The word quantum derives from the Latin, meaning ""how great"" or ""how much"".[79] According to Planck, quantities of energy could be thought of as divided into ""elements"" whose size (E) would be proportional to their frequency (ν):
 where h is Planck's constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not the physical reality of the radiation.[80] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[81] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into a model of the hydrogen atom that successfully predicted the spectral lines of hydrogen.[82] Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency.[83] In his paper ""On the Quantum Theory of Radiation"", Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation,[84] which became the basis of the laser.[85]
 This phase is known as the old quantum theory. Never complete or self-consistent, the old quantum theory was rather a set of heuristic corrections to classical mechanics.[86][87] The theory is now understood as a semi-classical approximation to modern quantum mechanics.[88][89] Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein and Peter Debye's work on the specific heat of solids, Bohr and Hendrika Johanna van Leeuwen's proof that classical physics cannot account for diamagnetism, and Arnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.[86][90]
 In the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicist Louis de Broglie put forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, and Pascual Jordan[91][92] developed matrix mechanics and the Austrian physicist Erwin Schrödinger invented wave mechanics. Born introduced the probabilistic interpretation of Schrödinger's wave function in July 1926.[93] Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[94]
 By 1930 quantum mechanics had been further unified and formalized by David Hilbert, Paul Dirac and John von Neumann[95] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors[96] and superfluids.[97]
 The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus.
 More technical:
 On Wikibooks
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['quantum information science', 'working physicists', ""philosophical speculation about the 'observer'"", 'quantum chemistry, quantum field theory, quantum technology, and quantum information science', 'small-scale and discrete quantities and interactions which cannot be explained by classical methods'], 'answer_start': [], 'answer_end': []}"
"
 In physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force. Thus, string theory is a theory of quantum gravity.
 String theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has contributed a number of advances to mathematical physics, which have been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of its details.
 String theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in eleven dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the anti-de Sitter/conformal field theory correspondence (AdS/CFT correspondence), which relates string theory to another type of physical theory called a quantum field theory.
 One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, which has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics, and to question the value of continued research on string theory unification.
 In the 20th century, two theoretical frameworks emerged for formulating the laws of physics. The first is Albert Einstein's general theory of relativity, a theory that explains the force of gravity and the structure of spacetime at the macro-level. The other is quantum mechanics, a completely different formulation, which uses known probability principles to describe physical phenomena at the micro-level. By the late 1970s, these two frameworks had proven to be sufficient to explain most of the observed features of the universe, from elementary particles to atoms to the evolution of stars and the universe as a whole.[1]
 In spite of these successes, there are still many problems that remain to be solved. One of the deepest problems in modern physics is the problem of quantum gravity.[1] The general theory of relativity is formulated within the framework of classical physics, whereas the other fundamental forces are described within the framework of quantum mechanics. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity.[2] In addition to the problem of developing a consistent theory of quantum gravity, there are many other fundamental problems in the physics of atomic nuclei, black holes, and the early universe.[a]
 String theory is a theoretical framework that attempts to address these questions and many others. The starting point for string theory is the idea that the point-like particles of particle physics can also be modeled as one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle consistent with non-string models of elementary particles, with its mass, charge, and other properties determined by the vibrational state of the string. String theory's application as a form of quantum gravity proposes a vibrational state responsible for the graviton, a yet unproven quantum particle that is theorized to carry gravitational force.[3]
 One of the main developments of the past several decades in string theory was the discovery of certain 'dualities', mathematical transformations that identify one physical theory with another. Physicists studying string theory have discovered a number of these dualities between different versions of string theory, and this has led to the conjecture that all consistent versions of string theory are subsumed in a single framework known as M-theory.[4]
 Studies of string theory have also yielded a number of results on the nature of black holes and the gravitational interaction. There are certain paradoxes that arise when one attempts to understand the quantum aspects of black holes, and work on string theory has attempted to clarify these issues. In late 1997 this line of work culminated in the discovery of the anti-de Sitter/conformal field theory correspondence or AdS/CFT.[5] This is a theoretical result that relates string theory to other physical theories which are better understood theoretically. The AdS/CFT correspondence has implications for the study of black holes and quantum gravity, and it has been applied to other subjects, including nuclear[6] and condensed matter physics.[7][8]
 Since string theory incorporates all of the fundamental interactions, including gravity, many physicists hope that it will eventually be developed to the point where it fully describes our universe, making it a theory of everything. One of the goals of current research in string theory is to find a solution of the theory that reproduces the observed spectrum of elementary particles, with a small cosmological constant, containing dark matter and a plausible mechanism for cosmic inflation. While there has been progress toward these goals, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of details.[9]
 One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. The scattering of strings is most straightforwardly defined using the techniques of perturbation theory, but it is not known in general how to define string theory nonperturbatively.[10] It is also not clear whether there is any principle by which string theory selects its vacuum state, the physical state that determines the properties of our universe.[11] These problems have led some in the community to criticize these approaches to the unification of physics and question the value of continued research on these problems.[12]
 The application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields.[13]
 In quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory. Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations. One imagines that these diagrams depict the paths of point-like particles and their interactions.[13]
 The starting point for string theory is the idea that the point-like particles of quantum field theory can also be modeled as one-dimensional objects called strings.[14] The interaction of strings is most straightforwardly defined by generalizing the perturbation theory used in ordinary quantum field theory. At the level of Feynman diagrams, this means replacing the one-dimensional diagram representing the path of a point particle by a two-dimensional (2D) surface representing the motion of a string.[15] Unlike in quantum field theory, string theory does not have a full non-perturbative definition, so many of the theoretical questions that physicists would like to answer remain out of reach.[16]
 In theories of particle physics based on string theory, the characteristic length scale of strings is assumed to be on the order of the Planck length, or 10−35 meters, the scale at which the effects of quantum gravity are believed to become significant.[15] On much larger length scales, such as the scales visible in physics laboratories, such objects would be indistinguishable from zero-dimensional point particles, and the vibrational state of the string would determine the type of particle. One of the vibrational states of a string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force.[3]
 The original version of string theory was bosonic string theory, but this version described only bosons, a class of particles that transmit forces between the matter particles, or fermions. Bosonic string theory was eventually superseded by theories called superstring theories. These theories describe both bosons and fermions, and they incorporate a theoretical idea called supersymmetry. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa.[17]
 There are several versions of superstring theory: type I, type IIA, type IIB, and two flavors of heterotic string theory (SO(32) and E8×E8). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA, IIB and heterotic include only closed strings.[18]
 In everyday life, there are three familiar dimensions (3D) of space: height, width and length. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional (4D) spacetime. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.[19]
 In spite of the fact that the Universe is well described by 4D spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily.[b] There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics.[13] Finally, there exist scenarios in which there could actually be more than 4D of spacetime which have nonetheless managed to escape detection.[20]
 String theories require extra dimensions of spacetime for their mathematical consistency. In bosonic string theory, spacetime is 26-dimensional, while in superstring theory it is 10-dimensional, and in M-theory it is 11-dimensional. In order to describe real physical phenomena using string theory, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.[21]
 Compactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to ""close up"" on themselves to form circles.[22] In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.
 Compactification can be used to construct models in which spacetime is effectively four-dimensional. However, not every way of compactifying the extra dimensions produces a model with the right properties to describe nature. In a viable model of particle physics, the compact extra dimensions must be shaped like a Calabi–Yau manifold.[22] A Calabi–Yau manifold is a special space which is typically taken to be six-dimensional in applications to string theory. It is named after mathematicians Eugenio Calabi and Shing-Tung Yau.[23]
 Another approach to reducing the number of dimensions is the so-called brane-world scenario. In this approach, physicists assume that the observable universe is a four-dimensional subspace of a higher dimensional space. In such models, the force-carrying bosons of particle physics arise from open strings with endpoints attached to the four-dimensional subspace, while gravity arises from closed strings propagating through the larger ambient space. This idea plays an important role in attempts to develop models of real-world physics based on string theory, and it provides a natural explanation for the weakness of gravity compared to the other fundamental forces.[24]
 A notable fact about string theory is that the different versions of the theory all turn out to be related in highly nontrivial ways. One of the relationships that can exist between different string theories is called S-duality. This is a relationship that says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the SO(32) heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.[25]
 Another relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius R is equivalent to a string propagating around a circle of radius 1/R in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum p and winding number n in one description, it will have momentum n and winding number p in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.[25]
 In general, the term duality refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. Two theories related by a duality need not be string theories. For example, Montonen–Olive duality is an example of an S-duality relationship between quantum field theories. The AdS/CFT correspondence is an example of a duality that relates string theory to a quantum field theory. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be dual to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.[26]
 In string theory and other related theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For instance, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension p, these are called p-branes. The word brane comes from the word ""membrane"" which refers to a two-dimensional brane.[27]
 Branes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They have mass and can have other attributes such as charge. A p-brane sweeps out a (p+1)-dimensional volume in spacetime called its worldvolume. Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane.[27]
 In string theory, D-branes are an important class of branes that arise when one considers open strings. As an open string propagates through spacetime, its endpoints are required to lie on a D-brane. The letter ""D"" in D-brane refers to a certain mathematical condition on the system known as the Dirichlet boundary condition. The study of D-branes in string theory has led to important results such as the AdS/CFT correspondence, which has shed light on many problems in quantum field theory.[27]
 Branes are frequently studied from a purely mathematical point of view, and they are described as objects of certain categories, such as the derived category of coherent sheaves on a complex algebraic variety, or the Fukaya category of a symplectic manifold.[28] The connection between the physical notion of a brane and the mathematical notion of a category has led to important mathematical insights in the fields of algebraic and symplectic geometry[29] and representation theory.[30]
 Prior to 1995, theorists believed that there were five consistent versions of superstring theory (type I, type IIA, type IIB, and two versions of heterotic string theory). This understanding changed in 1995 when Edward Witten suggested that the five theories were just special limiting cases of an eleven-dimensional theory called M-theory. Witten's conjecture was based on the work of a number of other physicists, including Ashoke Sen, Chris Hull, Paul Townsend, and Michael Duff. His announcement led to a flurry of research activity now known as the second superstring revolution.[31]
 In the 1970s, many physicists became interested in supergravity theories, which combine general relativity with supersymmetry. Whereas general relativity makes sense in any number of dimensions, supergravity places an upper limit on the number of dimensions.[32] In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven.[33] In the same year, Eugene Cremmer, Bernard Julia, and Joël Scherk of the École Normale Supérieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.[34][35]
 Initially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.[35]
 In the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects.[35] Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.[35]
 Although there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation.[35] However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways. They found that a system of strongly interacting strings can, in some cases, be viewed as a system of weakly interacting strings. This phenomenon is known as S-duality. It was studied by Ashoke Sen in the context of heterotic strings in four dimensions[36][37] and by Chris Hull and Paul Townsend in the context of the type IIB theory.[38] Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.[39]
 At around the same time, as many physicists were studying the properties of strings, a small group of physicists were examining the possible applications of higher dimensional objects. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes.[40] Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle.[41] In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.[42]
 Speaking at a string theory conference in 1995, Edward Witten made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of higher-dimensional branes in string theory.[43] In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming different parts of his proposal.[44] Today this flurry of work is known as the second superstring revolution.[45]
 Initially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Hořava and Witten wrote ""As it has been proposed that the eleven-dimensional theory is a supermembrane theory but there are some reasons to doubt that interpretation, we will non-committally call it the M-theory, leaving to the future the relation of M to membranes.""[46] In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the M should stand for ""magic"", ""mystery"", or ""membrane"" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.[47]
 In mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes the behavior of a set of matrices within the framework of quantum mechanics.[48]
 One important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.[48]
 The development of the matrix model formulation of M-theory has led physicists to consider various connections between string theory and a branch of mathematics called noncommutative geometry. This subject is a generalization of ordinary geometry in which mathematicians define new geometric notions using tools from noncommutative algebra.[49] In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which spacetime is described mathematically using noncommutative geometry.[50] This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.[51][52]
 In general relativity, a black hole is defined as a region of spacetime in which the gravitational field is so strong that no particle or radiation can escape. In the currently accepted models of stellar evolution, black holes are thought to arise when massive stars undergo gravitational collapse, and many galaxies are thought to contain supermassive black holes at their centers. Black holes are also important for theoretical reasons, as they present profound challenges for theorists attempting to understand the quantum aspects of gravity. String theory has proved to be an important tool for investigating the theoretical properties of black holes because it provides a framework in which theorists can study their thermodynamics.[53]
 In the branch of physics called statistical mechanics, entropy is a measure of the randomness or disorder of a physical system. This concept was studied in the 1870s by the Austrian physicist Ludwig Boltzmann, who showed that the thermodynamic properties of a gas could be derived from the combined properties of its many constituent molecules. Boltzmann argued that by averaging the behaviors of all the different molecules in a gas, one can understand macroscopic properties such as volume, temperature, and pressure. In addition, this perspective led him to give a precise definition of entropy as the natural logarithm of the number of different states of the molecules (also called microstates) that give rise to the same macroscopic features.[54]
 In the twentieth century, physicists began to apply the same concepts to black holes. In most systems such as gases, the entropy scales with the volume. In the 1970s, the physicist Jacob Bekenstein suggested that the entropy of a black hole is instead proportional to the surface area of its event horizon, the boundary beyond which matter and radiation are lost to its gravitational attraction.[55] When combined with ideas of the physicist Stephen Hawking,[56] Bekenstein's work yielded a precise formula for the entropy of a black hole. The Bekenstein–Hawking formula expresses the entropy S as
 where c is the speed of light, k is the Boltzmann constant, ħ is the reduced Planck constant, G is Newton's constant, and A is the surface area of the event horizon.[57]
 Like any physical system, a black hole has an entropy defined in terms of the number of different microstates that lead to the same macroscopic features. The Bekenstein–Hawking entropy formula gives the expected value of the entropy of a black hole, but by the 1990s, physicists still lacked a derivation of this formula by counting microstates in a theory of quantum gravity. Finding such a derivation of this formula was considered an important test of the viability of any theory of quantum gravity such as string theory.[58]
 In a paper from 1996, Andrew Strominger and Cumrun Vafa showed how to derive the Beckenstein–Hawking formula for certain black holes in string theory.[59] Their calculation was based on the observation that D-branes—which look like fluctuating membranes when they are weakly interacting—become dense, massive objects with event horizons when the interactions are strong. In other words, a system of strongly interacting D-branes in string theory is indistinguishable from a black hole. Strominger and Vafa analyzed such D-brane systems and calculated the number of different ways of placing D-branes in spacetime so that their combined mass and charge is equal to a given mass and charge for the resulting black hole. Their calculation reproduced the Bekenstein–Hawking formula exactly, including the factor of 1/4.[60] Subsequent work by Strominger, Vafa, and others refined the original calculations and gave the precise values of the ""quantum corrections"" needed to describe very small black holes.[61][62]
 The black holes that Strominger and Vafa considered in their original work were quite different from real astrophysical black holes. One difference was that Strominger and Vafa considered only extremal black holes in order to make the calculation tractable. These are defined as black holes with the lowest possible mass compatible with a given charge.[63] Strominger and Vafa also restricted attention to black holes in five-dimensional spacetime with unphysical supersymmetry.[64]
 Although it was originally developed in this very particular and physically unrealistic context in string theory, the entropy calculation of Strominger and Vafa has led to a qualitative understanding of how black hole entropy can be accounted for in any theory of quantum gravity. Indeed, in 1998, Strominger argued that the original result could be generalized to an arbitrary consistent theory of quantum gravity without relying on strings or supersymmetry.[65] In collaboration with several other authors in 2010, he showed that some results on black hole entropy could be extended to non-extremal astrophysical black holes.[66][67]
 One approach to formulating string theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. This is a theoretical result which implies that string theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective.[6] The AdS/CFT correspondence was first proposed by Juan Maldacena in late 1997.[68] Important aspects of the correspondence were elaborated in articles by Steven Gubser, Igor Klebanov, and Alexander Markovich Polyakov,[69] and by Edward Witten.[70] By 2010, Maldacena's article had over 7000 citations, becoming the most highly cited article in the field of high energy physics.[c]
 In the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space.[6] In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left.[71] This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.[72]
 One can imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space.[71] It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.[72]
 This construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can ""stack up"" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.[71]
 An important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics.[73] One can therefore consider an auxiliary theory in which ""spacetime"" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the ""spacetime"" for a quantum field theory. The claim is that this quantum field theory is equivalent to a gravitational theory, such as string theory, in the bulk anti-de Sitter space in the sense that there is a ""dictionary"" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.[74]
 The discovery of the AdS/CFT correspondence was a major advance in physicists' understanding of string theory and quantum gravity. One reason for this is that the correspondence provides a formulation of string theory in terms of quantum field theory, which is well understood by comparison. Another reason is that it provides a general framework in which physicists can study and attempt to resolve the paradoxes of black holes.[53]
 In 1975, Stephen Hawking published a calculation which suggested that black holes are not completely black but emit a dim radiation due to quantum effects near the event horizon.[56] At first, Hawking's result posed a problem for theorists because it suggested that black holes destroy information. More precisely, Hawking's calculation seemed to conflict with one of the basic postulates of quantum mechanics, which states that physical systems evolve in time according to the Schrödinger equation. This property is usually referred to as unitarity of time evolution. The apparent contradiction between Hawking's calculation and the unitarity postulate of quantum mechanics came to be known as the black hole information paradox.[75]
 The AdS/CFT correspondence resolves the black hole information paradox, at least to some extent, because it shows how a black hole can evolve in a manner consistent with quantum mechanics in some contexts. Indeed, one can consider black holes in the context of the AdS/CFT correspondence, and any such black hole corresponds to a configuration of particles on the boundary of anti-de Sitter space.[76] These particles obey the usual rules of quantum mechanics and in particular evolve in a unitary fashion, so the black hole must also evolve in a unitary fashion, respecting the principles of quantum mechanics.[77] In 2005, Hawking announced that the paradox had been settled in favor of information conservation by the AdS/CFT correspondence, and he suggested a concrete mechanism by which black holes might preserve information.[78]
 In addition to its applications to theoretical problems in quantum gravity, the AdS/CFT correspondence has been applied to a variety of problems in quantum field theory. One physical system that has been studied using the AdS/CFT correspondence is the quark–gluon plasma, an exotic state of matter produced in particle accelerators. This state of matter arises for brief instants when heavy ions such as gold or lead nuclei are collided at high energies. Such collisions cause the quarks that make up atomic nuclei to deconfine at temperatures of approximately two trillion kelvin, conditions similar to those present at around 10−11 seconds after the Big Bang.[79]
 The physics of the quark–gluon plasma is governed by a theory called quantum chromodynamics, but this theory is mathematically intractable in problems involving the quark–gluon plasma.[d] In an article appearing in 2005, Đàm Thanh Sơn and his collaborators showed that the AdS/CFT correspondence could be used to understand some aspects of the quark-gluon plasma by describing it in the language of string theory.[80] By applying the AdS/CFT correspondence, Sơn and his collaborators were able to describe the quark-gluon plasma in terms of black holes in five-dimensional spacetime. The calculation showed that the ratio of two quantities associated with the quark-gluon plasma, the shear viscosity and volume density of entropy, should be approximately equal to a certain universal constant. In 2008, the predicted value of this ratio for the quark-gluon plasma was confirmed at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory.[7][81]
 The AdS/CFT correspondence has also been used to study aspects of condensed matter physics. Over the decades, experimental condensed matter physicists have discovered a number of exotic states of matter, including superconductors and superfluids. These states are described using the formalism of quantum field theory, but some phenomena are difficult to explain using standard field theoretic techniques. Some condensed matter theorists including Subir Sachdev hope that the AdS/CFT correspondence will make it possible to describe these systems in the language of string theory and learn more about their behavior.[7]
 So far some success has been achieved in using string theory methods to describe the transition of a superfluid to an insulator. A superfluid is a system of electrically neutral atoms that flows without any friction. Such systems are often produced in the laboratory using liquid helium, but recently experimentalists have developed new ways of producing artificial superfluids by pouring trillions of cold atoms into a lattice of criss-crossing lasers. These atoms initially behave as a superfluid, but as experimentalists increase the intensity of the lasers, they become less mobile and then suddenly transition to an insulating state. During the transition, the atoms behave in an unusual way. For example, the atoms slow to a halt at a rate that depends on the temperature and on the Planck constant, the fundamental parameter of quantum mechanics, which does not enter into the description of the other phases. This behavior has recently been understood by considering a dual description where properties of the fluid are described in terms of a higher dimensional black hole.[8]
 In addition to being an idea of considerable theoretical interest, string theory provides a framework for constructing models of real-world physics that combine general relativity and particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic or semi-realistic models based on string theory.
 Partly because of theoretical and mathematical difficulties and partly because of the extremely high energies needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.[12]
 The currently accepted theory describing elementary particles and their interactions is known as the standard model of particle physics. This theory provides a unified description of three of the fundamental forces of nature: electromagnetism and the strong and weak nuclear forces. Despite its remarkable success in explaining a wide range of physical phenomena, the standard model cannot be a complete description of reality. This is because the standard model fails to incorporate the force of gravity and because of problems such as the hierarchy problem and the inability to explain the structure of fermion masses or dark matter.
 String theory has been used to construct a variety of models of particle physics going beyond the standard model. Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles.[82] One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi–Yau manifold. Such compactifications offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct realistic or semi-realistic models of our four-dimensional world based on M-theory.[83]
 The Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. Despite its success in explaining many observed features of the universe including galactic redshifts, the relative abundance of light elements such as hydrogen and helium, and the existence of a cosmic microwave background, there are several questions that remain unanswered. For example, the standard Big Bang model does not explain why the universe appears to be the same in all directions, why it appears flat on very large distance scales, or why certain hypothesized particles such as magnetic monopoles are not observed in experiments.[84]
 Currently, the leading candidate for a theory going beyond the Big Bang is the theory of cosmic inflation. Developed by Alan Guth and others in the 1980s, inflation postulates a period of extremely rapid accelerated expansion of the universe prior to the expansion described by the standard Big Bang theory. The theory of cosmic inflation preserves the successes of the Big Bang while providing a natural explanation for some of the mysterious features of the universe.[85] The theory has also received striking support from observations of the cosmic microwave background, the radiation that has filled the sky since around 380,000 years after the Big Bang.[86]
 In the theory of inflation, the rapid initial expansion of the universe is caused by a hypothetical particle called the inflaton. The exact properties of this particle are not fixed by the theory but should ultimately be derived from a more fundamental theory such as string theory.[87] Indeed, there have been a number of attempts to identify an inflaton within the spectrum of particles described by string theory and to study inflation using string theory. While these approaches might eventually find support in observational data such as measurements of the cosmic microwave background, the application of string theory to cosmology is still in its early stages.[88]
 In addition to influencing research in theoretical physics, string theory has stimulated a number of major developments in pure mathematics. Like many developing ideas in theoretical physics, string theory does not at present have a mathematically rigorous formulation in which all of its concepts can be defined precisely. As a result, physicists who study string theory are often guided by physical intuition to conjecture relationships between the seemingly different mathematical structures that are used to formalize different parts of the theory. These conjectures are later proved by mathematicians, and in this way, string theory serves as a source of new ideas in pure mathematics.[89]
 After Calabi–Yau manifolds had entered physics as a way to compactify extra dimensions in string theory, many physicists began studying these manifolds. In the late 1980s, several physicists noticed that given such a compactification of string theory, it is not possible to reconstruct uniquely a corresponding Calabi–Yau manifold.[90] Instead, two different versions of string theory, type IIA and type IIB, can be compactified on completely different Calabi–Yau manifolds giving rise to the same physics. In this situation, the manifolds are called mirror manifolds, and the relationship between the two physical theories is called mirror symmetry.[28]
 Regardless of whether Calabi–Yau compactifications of string theory provide a correct description of nature, the existence of the mirror duality between different string theories has significant mathematical consequences. The Calabi–Yau manifolds used in string theory are of interest in pure mathematics, and mirror symmetry allows mathematicians to solve problems in enumerative geometry, a branch of mathematics concerned with counting the numbers of solutions to geometric questions.[28][91]
 Enumerative geometry studies a class of geometric objects called algebraic varieties which are defined by the vanishing of polynomials. For example, the Clebsch cubic illustrated on the right is an algebraic variety defined using a certain polynomial of degree three in four variables. A celebrated result of nineteenth-century mathematicians Arthur Cayley and George Salmon states that there are exactly 27 straight lines that lie entirely on such a surface.[92]
 Generalizing this problem, one can ask how many lines can be drawn on a quintic Calabi–Yau manifold, such as the one illustrated above, which is defined by a polynomial of degree five. This problem was solved by the nineteenth-century German mathematician Hermann Schubert, who found that there are exactly 2,875 such lines. In 1986, geometer Sheldon Katz proved that the number of curves, such as circles, that are defined by polynomials of degree two and lie entirely in the quintic is 609,250.[93]
 By the year 1991, most of the classical problems of enumerative geometry had been solved and interest in enumerative geometry had begun to diminish.[94] The field was reinvigorated in May 1991 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parkes showed that mirror symmetry could be used to translate difficult mathematical questions about one Calabi–Yau manifold into easier questions about its mirror.[95] In particular, they used mirror symmetry to show that a six-dimensional Calabi–Yau manifold can contain exactly 317,206,375 curves of degree three.[94] In addition to counting degree-three curves, Candelas and his collaborators obtained a number of more general results for counting rational curves which went far beyond the results obtained by mathematicians.[96]
 Originally, these results of Candelas were justified on physical grounds. However, mathematicians generally prefer rigorous proofs that do not require an appeal to physical intuition. Inspired by physicists' work on mirror symmetry, mathematicians have therefore constructed their own arguments proving the enumerative predictions of mirror symmetry.[e] Today mirror symmetry is an active area of research in mathematics, and mathematicians are working to develop a more complete mathematical understanding of mirror symmetry based on physicists' intuition.[102] Major approaches to mirror symmetry include the homological mirror symmetry program of Maxim Kontsevich[29] and the SYZ conjecture of Andrew Strominger, Shing-Tung Yau, and Eric Zaslow.[103]
 Group theory is the branch of mathematics that studies the concept of symmetry. For example, one can consider a geometric shape such as an equilateral triangle. There are various operations that one can perform on this triangle without changing its shape. One can rotate it through 120°, 240°, or 360°, or one can reflect in any of the lines labeled S0, S1, or S2 in the picture. Each of these operations is called a symmetry, and the collection of these symmetries satisfies certain technical properties making it into what mathematicians call a group. In this particular example, the group is known as the dihedral group of order 6 because it has six elements. A general group may describe finitely many or infinitely many symmetries; if there are only finitely many symmetries, it is called a finite group.[104]
 Mathematicians often strive for a classification (or list) of all mathematical objects of a given type. It is generally believed that finite groups are too diverse to admit a useful classification. A more modest but still challenging problem is to classify all finite simple groups. These are finite groups that may be used as building blocks for constructing arbitrary finite groups in the same way that prime numbers can be used to construct arbitrary whole numbers by taking products.[f] One of the major achievements of contemporary group theory is the classification of finite simple groups, a mathematical theorem that provides a list of all possible finite simple groups.[104]
 This classification theorem identifies several infinite families of groups as well as 26 additional groups which do not fit into any family. The latter groups are called the ""sporadic"" groups, and each one owes its existence to a remarkable combination of circumstances. The largest sporadic group, the so-called monster group, has over 1053 elements, more than a thousand times the number of atoms in the Earth.[105]
 A seemingly unrelated construction is the j-function of number theory. This object belongs to a special class of functions called modular functions, whose graphs form a certain kind of repeating pattern.[106] Although this function appears in a branch of mathematics that seems very different from the theory of finite groups, the two subjects turn out to be intimately related. In the late 1970s, mathematicians John McKay and John Thompson noticed that certain numbers arising in the analysis of the monster group (namely, the dimensions of its irreducible representations) are related to numbers that appear in a formula for the j-function (namely, the coefficients of its Fourier series).[107] This relationship was further developed by John Horton Conway and Simon Norton[108] who called it monstrous moonshine because it seemed so far fetched.[109]
 In 1992, Richard Borcherds constructed a bridge between the theory of modular functions and finite groups and, in the process, explained the observations of McKay and Thompson.[110][111] Borcherds' work used ideas from string theory in an essential way, extending earlier results of Igor Frenkel, James Lepowsky, and Arne Meurman, who had realized the monster group as the symmetries of a particular[which?] version of string theory.[112] In 1998, Borcherds was awarded the Fields medal for his work.[113]
 Since the 1990s, the connection between string theory and moonshine has led to further results in mathematics and physics.[105] In 2010, physicists Tohru Eguchi, Hirosi Ooguri, and Yuji Tachikawa discovered connections between a different sporadic group, the Mathieu group M24, and a certain version[which?] of string theory.[114] Miranda Cheng, John Duncan, and Jeffrey A. Harvey proposed a generalization of this moonshine phenomenon called umbral moonshine,[115] and their conjecture was proved mathematically by Duncan, Michael Griffin, and Ken Ono.[116] Witten has also speculated that the version of string theory appearing in monstrous moonshine might be related to a certain simplified model of gravity in three spacetime dimensions.[117]
 Some of the structures reintroduced by string theory arose for the first time much earlier as part of the program of classical unification started by Albert Einstein. The first person to add a fifth dimension to a theory of gravity was Gunnar Nordström in 1914, who noted that gravity in five dimensions describes both gravity and electromagnetism in four. Nordström attempted to unify electromagnetism with his theory of gravitation, which was however superseded by Einstein's general relativity in 1919. Thereafter, German mathematician Theodor Kaluza combined the fifth dimension with general relativity, and only Kaluza is usually credited with the idea. In 1926, the Swedish physicist Oskar Klein gave a physical interpretation of the unobservable extra dimension—it is wrapped into a small circle. Einstein introduced a non-symmetric metric tensor, while much later Brans and Dicke added a scalar component to gravity. These ideas would be revived within string theory, where they are demanded by consistency conditions.
 String theory was originally developed during the late 1960s and early 1970s as a never completely successful theory of hadrons, the subatomic particles like the proton and neutron that feel the strong interaction. In the 1960s, Geoffrey Chew and Steven Frautschi discovered that the mesons make families called Regge trajectories with masses related to spins in a way that was later understood by Yoichiro Nambu, Holger Bech Nielsen and Leonard Susskind to be the relationship expected from rotating strings. Chew advocated making a theory for the interactions of these trajectories that did not presume that they were composed of any fundamental particles, but would construct their interactions from self-consistency conditions on the S-matrix. The S-matrix approach was started by Werner Heisenberg in the 1940s as a way of constructing a theory that did not rely on the local notions of space and time, which Heisenberg believed break down at the nuclear scale. While the scale was off by many orders of magnitude, the approach he advocated was ideally suited for a theory of quantum gravity.
 Working with experimental data, R. Dolen, D. Horn and C. Schmid developed some sum rules for hadron exchange. When a particle and antiparticle scatter, virtual particles can be exchanged in two qualitatively different ways. In the s-channel, the two particles annihilate to make temporary intermediate states that fall apart into the final state particles. In the t-channel, the particles exchange intermediate states by emission and absorption. In field theory, the two contributions add together, one giving a continuous background contribution, the other giving peaks at certain energies. In the data, it was clear that the peaks were stealing from the background—the authors interpreted this as saying that the t-channel contribution was dual to the s-channel one, meaning both described the whole amplitude and included the other.
 The result was widely advertised by Murray Gell-Mann, leading Gabriele Veneziano to construct a scattering amplitude that had the property of Dolen–Horn–Schmid duality, later renamed world-sheet duality. The amplitude needed poles where the particles appear, on straight-line trajectories, and there is a special mathematical function whose poles are evenly spaced on half the real line—the gamma function— which was widely used in Regge theory. By manipulating combinations of gamma functions, Veneziano was able to find a consistent scattering amplitude with poles on straight lines, with mostly positive residues, which obeyed duality and had the appropriate Regge scaling at high energy. The amplitude could fit near-beam scattering data as well as other Regge type fits and had a suggestive integral representation that could be used for generalization.
 Over the next years, hundreds of physicists worked to complete the bootstrap program for this model, with many surprises. Veneziano himself discovered that for the scattering amplitude to describe the scattering of a particle that appears in the theory, an obvious self-consistency condition, the lightest particle must be a tachyon. Miguel Virasoro and Joel Shapiro found a different amplitude now understood to be that of closed strings, while Ziro Koba and Holger Nielsen generalized Veneziano's integral representation to multiparticle scattering. Veneziano and Sergio Fubini introduced an operator formalism for computing the scattering amplitudes that was a forerunner of world-sheet conformal theory, while Virasoro understood how to remove the poles with wrong-sign residues using a constraint on the states. Claud Lovelace calculated a loop amplitude, and noted that there is an inconsistency unless the dimension of the theory is 26. Charles Thorn, Peter Goddard and Richard Brower went on to prove that there are no wrong-sign propagating states in dimensions less than or equal to 26.
 In 1969–70, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind recognized that the theory could be given a description in space and time in terms of strings. The scattering amplitudes were derived systematically from the action principle by Peter Goddard, Jeffrey Goldstone, Claudio Rebbi, and Charles Thorn, giving a space-time picture to the vertex operators introduced by Veneziano and Fubini and a geometrical interpretation to the Virasoro conditions.
 In 1971, Pierre Ramond added fermions to the model, which led him to formulate a two-dimensional supersymmetry to cancel the wrong-sign states. John Schwarz and André Neveu added another sector to the fermi theory a short time later. In the fermion theories, the critical dimension was 10. Stanley Mandelstam formulated a world sheet conformal theory for both the bose and fermi case, giving a two-dimensional field theoretic path-integral to generate the operator formalism. Michio Kaku and Keiji Kikkawa gave a different formulation of the bosonic string, as a string field theory, with infinitely many particle types and with fields taking values not on points, but on loops and curves.
 In 1974, Tamiaki Yoneya discovered that all the known string theories included a massless spin-two particle that obeyed the correct Ward identities to be a graviton. John Schwarz and Joël Scherk came to the same conclusion and made the bold leap to suggest that string theory was a theory of gravity, not a theory of hadrons. They reintroduced Kaluza–Klein theory as a way of making sense of the extra dimensions. At the same time, quantum chromodynamics was recognized as the correct theory of hadrons, shifting the attention of physicists and apparently leaving the bootstrap program in the dustbin of history.
 String theory eventually made it out of the dustbin, but for the following decade, all work on the theory was completely ignored. Still, the theory continued to develop at a steady pace thanks to the work of a handful of devotees. Ferdinando Gliozzi, Joël Scherk, and David Olive realized in 1977 that the original Ramond and Neveu Schwarz-strings were separately inconsistent and needed to be combined. The resulting theory did not have a tachyon and was proven to have space-time supersymmetry by John Schwarz and Michael Green in 1984. The same year, Alexander Polyakov gave the theory a modern path integral formulation, and went on to develop conformal field theory extensively. In 1979, Daniel Friedan showed that the equations of motions of string theory, which are generalizations of the Einstein equations of general relativity, emerge from the renormalization group equations for the two-dimensional field theory. Schwarz and Green discovered T-duality, and constructed two superstring theories—IIA and IIB related by T-duality, and type I theories with open strings. The consistency conditions had been so strong, that the entire theory was nearly uniquely determined, with only a few discrete choices.
 In the early 1980s, Edward Witten discovered that most theories of quantum gravity could not accommodate chiral fermions like the neutrino. This led him, in collaboration with Luis Álvarez-Gaumé, to study violations of the conservation laws in gravity theories with anomalies, concluding that type I string theories were inconsistent. Green and Schwarz discovered a contribution to the anomaly that Witten and Alvarez-Gaumé had missed, which restricted the gauge group of the type I string theory to be SO(32). In coming to understand this calculation, Edward Witten became convinced that string theory was truly a consistent theory of gravity, and he became a high-profile advocate. Following Witten's lead, between 1984 and 1986, hundreds of physicists started to work in this field, and this is sometimes called the first superstring revolution.[citation needed]
 During this period, David Gross, Jeffrey Harvey, Emil Martinec, and Ryan Rohm discovered heterotic strings. The gauge group of these closed strings was two copies of E8, and either copy could easily and naturally include the standard model. Philip Candelas, Gary Horowitz, Andrew Strominger and Edward Witten found that the Calabi–Yau manifolds are the compactifications that preserve a realistic amount of supersymmetry, while Lance Dixon and others worked out the physical properties of orbifolds, distinctive geometrical singularities allowed in string theory. Cumrun Vafa generalized T-duality from circles to arbitrary manifolds, creating the mathematical field of mirror symmetry. Daniel Friedan, Emil Martinec and Stephen Shenker further developed the covariant quantization of the superstring using conformal field theory techniques. David Gross and Vipul Periwal discovered that string perturbation theory was divergent. Stephen Shenker showed it diverged much faster than in field theory suggesting that new non-perturbative objects were missing.[citation needed]
 In the 1990s, Joseph Polchinski discovered that the theory requires higher-dimensional objects, called D-branes and identified these with the black-hole solutions of supergravity. These were understood to be the new objects suggested by the perturbative divergences, and they opened up a new field with rich mathematical structure. It quickly became clear that D-branes and other p-branes, not just strings, formed the matter content of the string theories, and the physical interpretation of the strings and branes was revealed—they are a type of black hole. Leonard Susskind had incorporated the holographic principle of Gerardus 't Hooft into string theory, identifying the long highly excited string states with ordinary thermal black hole states. As suggested by 't Hooft, the fluctuations of the black hole horizon, the world-sheet or world-volume theory, describes not only the degrees of freedom of the black hole, but all nearby objects too.
 In 1995, at the annual conference of string theorists at the University of Southern California (USC), Edward Witten gave a speech on string theory that in essence united the five string theories that existed at the time, and giving birth to a new 11-dimensional theory called M-theory. M-theory was also foreshadowed in the work of Paul Townsend at approximately the same time. The flurry of activity that began at this time is sometimes called the second superstring revolution.[31]
 During this period, Tom Banks, Willy Fischler, Stephen Shenker and Leonard Susskind formulated matrix theory, a full holographic description of M-theory using IIA D0 branes.[48] This was the first definition of string theory that was fully non-perturbative and a concrete mathematical realization of the holographic principle. It is an example of a gauge-gravity duality and is now understood to be a special case of the AdS/CFT correspondence. Andrew Strominger and Cumrun Vafa calculated the entropy of certain configurations of D-branes and found agreement with the semi-classical answer for extreme charged black holes.[59] Petr Hořava and Witten found the eleven-dimensional formulation of the heterotic string theories, showing that orbifolds solve the chirality problem. Witten noted that the effective description of the physics of D-branes at low energies is by a supersymmetric gauge theory, and found geometrical interpretations of mathematical structures in gauge theory that he and Nathan Seiberg had earlier discovered in terms of the location of the branes.
 In 1997, Juan Maldacena noted that the low energy excitations of a theory near a black hole consist of objects close to the horizon, which for extreme charged black holes looks like an anti-de Sitter space.[68] He noted that in this limit the gauge theory describes the string excitations near the branes. So he hypothesized that string theory on a near-horizon extreme-charged black-hole geometry, an anti-de Sitter space times a sphere with flux, is equally well described by the low-energy limiting gauge theory, the N = 4 supersymmetric Yang–Mills theory. This hypothesis, which is called the AdS/CFT correspondence, was further developed by Steven Gubser, Igor Klebanov and Alexander Polyakov,[69] and by Edward Witten,[70] and it is now well-accepted. It is a concrete realization of the holographic principle, which has far-reaching implications for black holes, locality and information in physics, as well as the nature of the gravitational interaction.[53] Through this relationship, string theory has been shown to be related to gauge theories like quantum chromodynamics and this has led to a more quantitative understanding of the behavior of hadrons, bringing string theory back to its roots.[citation needed]
 To construct models of particle physics based on string theory, physicists typically begin by specifying a shape for the extra dimensions of spacetime. Each of these different shapes corresponds to a different possible universe, or ""vacuum state"", with a different collection of particles and forces. String theory as it is currently understood has an enormous number of vacuum states, typically estimated to be around 10500, and these might be sufficiently diverse to accommodate almost any phenomenon that might be observed at low energies.[118]
 Many critics of string theory have expressed concerns about the large number of possible universes described by string theory. In his book Not Even Wrong, Peter Woit, a lecturer in the mathematics department at Columbia University, has argued that the large number of different physical scenarios renders string theory vacuous as a framework for constructing models of particle physics. According to Woit,
 The possible existence of, say, 10500 consistent different vacuum states for superstring theory probably destroys the hope of using the theory to predict anything. If one picks among this large set just those states whose properties agree with present experimental observations, it is likely there still will be such a large number of these that one can get just about whatever value one wants for the results of any new observation.[119] Some physicists believe this large number of solutions is actually a virtue because it may allow a natural anthropic explanation of the observed values of physical constants, in particular the small value of the cosmological constant.[119] The anthropic principle is the idea that some of the numbers appearing in the laws of physics are not fixed by any fundamental principle but must be compatible with the evolution of intelligent life. In 1987, Steven Weinberg published an article in which he argued that the cosmological constant could not have been too large, or else galaxies and intelligent life would not have been able to develop.[120] Weinberg suggested that there might be a huge number of possible consistent universes, each with a different value of the cosmological constant, and observations indicate a small value of the cosmological constant only because humans happen to live in a universe that has allowed intelligent life, and hence observers, to exist.[121]
 String theorist Leonard Susskind has argued that string theory provides a natural anthropic explanation of the small value of the cosmological constant.[122] According to Susskind, the different vacuum states of string theory might be realized as different universes within a larger multiverse. The fact that the observed universe has a small cosmological constant is just a tautological consequence of the fact that a small value is required for life to exist.[123] Many prominent theorists and critics have disagreed with Susskind's conclusions.[124] According to Woit, ""in this case [anthropic reasoning] is nothing more than an excuse for failure. Speculative scientific ideas fail not just when they make incorrect predictions, but also when they turn out to be vacuous and incapable of predicting anything.""[125]
 It remains unknown whether string theory is compatible with a metastable, positive cosmological constant.
Some putative examples of such solutions do exist, such as the model described by Kachru et al. in 2003.[126] In 2018, a group of four physicists advanced a controversial conjecture which would imply that no such universe exists. This is contrary to some popular models of dark energy such as Λ-CDM, which requires a positive vacuum energy. However, string theory is likely compatible with certain types of quintessence, where dark energy is caused by a new field with exotic properties.[127]
 One of the fundamental properties of Einstein's general theory of relativity is that it is background independent, meaning that the formulation of the theory does not in any way privilege a particular spacetime geometry.[128]
 One of the main criticisms of string theory from early on is that it is not manifestly background-independent. In string theory, one must typically specify a fixed reference geometry for spacetime, and all other possible geometries are described as perturbations of this fixed one. In his book The Trouble With Physics, physicist Lee Smolin of the Perimeter Institute for Theoretical Physics claims that this is the principal weakness of string theory as a theory of quantum gravity, saying that string theory has failed to incorporate this important insight from general relativity.[129]
 Others have disagreed with Smolin's characterization of string theory. In a review of Smolin's book, string theorist Joseph Polchinski writes
 [Smolin] is mistaking an aspect of the mathematical language being used for one of the physics being described. New physical theories are often discovered using a mathematical language that is not the most suitable for them... In string theory, it has always been clear that the physics is background-independent even if the language being used is not, and the search for a more suitable language continues. Indeed, as Smolin belatedly notes, [AdS/CFT] provides a solution to this problem, one that is unexpected and powerful.[130] Polchinski notes that an important open problem in quantum gravity is to develop holographic descriptions of gravity which do not require the gravitational field to be asymptotically anti-de Sitter.[130] Smolin has responded by saying that the AdS/CFT correspondence, as it is currently understood, may not be strong enough to resolve all concerns about background independence.[131]
 Since the superstring revolutions of the 1980s and 1990s, string theory has been one of the dominant paradigms of high energy theoretical physics.[132] Some string theorists have expressed the view that there does not exist an equally successful alternative theory addressing the deep questions of fundamental physics. In an interview from 1987, Nobel laureate David Gross made the following controversial comments about the reasons for the popularity of string theory:
 The most important [reason] is that there are no other good ideas around. That's what gets most people into it. When people started to get interested in string theory they didn't know anything about it. In fact, the first reaction of most people is that the theory is extremely ugly and unpleasant, at least that was the case a few years ago when the understanding of string theory was much less developed. It was difficult for people to learn about it and to be turned on. So I think the real reason why people have got attracted by it is because there is no other game in town. All other approaches of constructing grand unified theories, which were more conservative to begin with, and only gradually became more and more radical, have failed, and this game hasn't failed yet.[133] Several other high-profile theorists and commentators have expressed similar views, suggesting that there are no viable alternatives to string theory.[134]
 Many critics of string theory have commented on this state of affairs. In his book criticizing string theory, Peter Woit views the status of string theory research as unhealthy and detrimental to the future of fundamental physics. He argues that the extreme popularity of string theory among theoretical physicists is partly a consequence of the financial structure of academia and the fierce competition for scarce resources.[135] In his book The Road to Reality, mathematical physicist Roger Penrose expresses similar views, stating ""The often frantic competitiveness that this ease of communication engenders leads to bandwagon effects, where researchers fear to be left behind if they do not join in.""[136] Penrose also claims that the technical difficulty of modern physics forces young scientists to rely on the preferences of established researchers, rather than forging new paths of their own.[137] Lee Smolin expresses a slightly different position in his critique, claiming that string theory grew out of a tradition of particle physics which discourages speculation about the foundations of physics, while his preferred approach, loop quantum gravity, encourages more radical thinking. According to Smolin,
 String theory is a powerful, well-motivated idea and deserves much of the work that has been devoted to it. If it has so far failed, the principal reason is that its intrinsic flaws are closely tied to its strengths—and, of course, the story is unfinished, since string theory may well turn out to be part of the truth. The real question is not why we have expended so much energy on string theory but why we haven't expended nearly enough on alternative approaches.[138] Smolin goes on to offer a number of prescriptions for how scientists might encourage a greater diversity of approaches to quantum gravity research.[139]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['condensed matter physics', 'Eugenio Calabi and Shing-Tung Yau', 'the second superstring revolution', 'quantum chromodynamics', 'unexpected and powerful'], 'answer_start': [], 'answer_end': []}"
"
 Renewable energy (or green energy) is energy from renewable natural resources that are replenished on a human timescale. Using renewable energy technologies helps with climate change mitigation, energy security, and also has some economic benefits.[1] Commonly used renewable energy types include solar energy, wind power, hydropower, bioenergy and geothermal power. Renewable energy installations can be large or small. They are suited for urban as well as rural areas. Renewable energy is often deployed together with further electrification. This has several benefits: electricity can move heat and vehicles efficiently, and is clean at the point of consumption.[2][3] Variable renewable energy sources are those that have a fluctuating nature, such as wind power and solar power. In contrast, controllable renewable energy sources include dammed hydroelectricity, bioenergy, or geothermal power.   
 Renewable energy systems are rapidly becoming more efficient and cheaper. As a result, their share of global energy consumption is increasing.[4] A large majority of worldwide newly installed electricity capacity is now renewable.[5] In most countries, photovoltaic solar or onshore wind are the cheapest new-build electricity.[6] Renewable energy can help reduce energy poverty in rural and remote areas of developing countries, where lack of energy access is often hindering economic development. Renewable energy resources exist all over the world. This is in contrast to fossil fuels resources which are concentrated in a limited number of countries.   
 There are also other renewable energy technologies that are still under development, for example enhanced geothermal systems, concentrated solar power, cellulosic ethanol, and marine energy.[7][8]
 From 2011 to 2021, renewable energy grew from 20% to 28% of global electricity supply. Use of fossil energy shrank from 68% to 62%, and nuclear from 12% to 10%. The share of hydropower decreased from 16% to 15% while power from sun and wind increased from 2% to 10%. Biomass and geothermal energy grew from 2% to 3%.[9][10] In 2022, renewables accounted for 30% of global electricity generation, up from 21% in 1985.[11]
 Many countries around the world already have renewable energy contributing more than 20% of their total energy supply. Some countries generate over half their electricity from renewables.[12] A few countries generate all their electricity from renewable energy.[13] National renewable energy markets are projected to continue to grow strongly in the 2020s and beyond.[14]
 The deployment of renewable energy is being hindered by massive fossil fuel subsidies.[15] In 2022 the International Energy Agency (IEA) requested all countries to reduce their policy, regulatory, permitting and financing obstacles for renewables.[16] This would increase the chances of the world reaching net zero carbon emissions by 2050.[16] According to the IEA, to achieve net zero emissions by 2050, 90% of global electricity generation will need to be produced from renewable sources.[17]
 
Whether nuclear power is renewable energy or not is still controversial. There are also debates around geopolitics, the metal and mineral extraction needed for solar panels and batteries, possible installations in conservation areas and the need to recycle solar panels. Although most renewable energy sources are sustainable, some are not. For example, some biomass sources are unsustainable at current rates of exploitation.[18] Renewable energy is usually understood as energy harnessed from continuously occurring natural phenomena. The International Energy Agency defines it as ""energy derived from natural processes that are replenished at a faster rate than they are consumed"". Solar power, wind power, hydroelectricity, geothermal energy, and biomass are widely agreed to be the main types of renewable energy.[21] Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water/space heating, transportation, and rural (off-grid) energy services.[22]
 Although almost all forms of renewable energy cause much fewer carbon emissions than fossil fuels, the term is not synonymous with low-carbon energy. Some non-renewable sources of energy, such as nuclear power,[contradictory]generate almost no emissions, while some renewable energy sources can be very carbon-intensive, such as the burning of biomass if it is not offset by planting new plants.[23] Renewable energy is also distinct from sustainable energy, a more abstract concept that seeks to group energy sources based on their overall permanent impact on future generations of humans. For example, biomass is often associated with unsustainable deforestation.[24]
 As part of the global effort to limit climate change, most countries have committed to net zero greenhouse gas emissions.[26] In practice, this means phasing out fossil fuels and replacing them with low-emissions energy sources.[23] At the 2023 United Nations Climate Change Conference, around three-quarters of the world's countries set a goal of tripling renewable energy capacity by 2030.[27] The European Union aims to generate 40% of its electricity from renewables by the same year.[28]
 Renewable energy is also more evenly distributed around the world than fossil fuels, which are concentrated in a limited number of countries.[29] It also brings health benefits by reducing air pollution caused by the burning of fossil fuels. The potential worldwide savings in health care costs have been estimated at trillions of dollars annually.[30]
 Moving to modern renewable energy has very large health benefits due to reducing air pollution from fossil fuels.[31][32][33]
 The two most important forms of renewable energy, solar and wind, are intermittent energy sources: they are not available constantly. In contrast, fossil fuel power plants are usually able to produce precisely the amount of energy an electricity grid requires at a given time. Solar energy can only be captured during the day, and ideally in cloudless conditions. Wind power generation can vary significantly not only day-to-day, but even month-to-month.[34] This poses a challenge when transitioning away from fossil fuels: energy demand will often be higher or lower than what renewables can provide.[35] Both scenarios can cause electricity grids to become overloaded, leading to power outages. 
 Energy storage is an important way of dealing with this variability.[36] Implementation of energy storage, using a wide variety of renewable energy technologies, and implementing a smart grid can reduce risks and costs of renewable energy implementation.[37]
 Sector coupling of the power generation sector with other sectors may increase flexibility: for example the transport sector can be coupled by charging electric vehicles and sending electricity from vehicle to grid.[38] Similarly the industry sector can be coupled by hydrogen produced by electrolysis,[39] and the buildings sector by thermal energy storage for space heating and cooling.[40]
 Electrical energy storage is a collection of methods used to store electrical energy. Electrical energy is stored during times when production (especially from intermittent sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity accounts for more than 85% of all grid power storage.[41] Batteries are increasingly being deployed for storage[42] and grid ancillary services[43] and for domestic storage.[44] Green hydrogen is a more economical means of long-term renewable energy storage, in terms of capital expenditures compared to pumped hydroelectric or batteries.[45][46]
 Solar power produced around 1.3 terrawatt-hours (TWh) worldwide in 2022,[12] representing 4.6% of the world's electricity. Almost all of this growth has happened since 2010.[52] Solar energy can be harnessed anywhere that receives sunlight; however, the amount of solar energy that can be harnessed for electricity generation is influenced by weather conditions, geographic location and time of day.[53]
 There are two mainstream ways of harnessing solar energy: solar thermal, which converts solar energy into heat; and photovoltaics (PV), which converts it into electricity.[23] PV is far more widespread, accounting for around two thirds of the global solar energy capacity as of 2022.[54] It is also growing at a much faster rate, with 170 GW newly installed capacity in 2021,[55] compared to 25 GW of solar thermal.[54]
 Passive solar refers to a range of construction strategies and technologies that aim to optimize the distribution of solar heat in a building. Examples include solar chimneys,[23] orienting a building to the sun, using construction materials that can store heat, and designing spaces that naturally circulate air.[56]
 From 2020 to 2022, solar technology investments almost doubled from USD 162 billion to USD 308 billion, driven by the sector's increasing maturity and cost reductions, particularly in solar photovoltaic (PV), which accounted for 90% of total investments. China and the United States were the main recipients, collectively making up about half of all solar investments since 2013. Despite reductions in Japan and India due to policy changes and COVID-19, growth in China, the United States, and a significant increase from Vietnam's feed-in tariff program offset these declines. Globally, the solar sector added 714 gigawatts (GW) of solar PV and concentrated solar power (CSP) capacity between 2013 and 2021, with a notable rise in large-scale solar heating installations in 2021, especially in China, Europe, Turkey, and Mexico.[57]
 A photovoltaic system, consisting of solar cells assembled into panels, converts light into electrical direct current via the photoelectric effect.[60] PV has several advantages that make it by far the fastest-growing renewable energy technology. It is cheap, low-maintenance and scalable; adding to an existing PV installation as demanded arises is simple. Its main disadvantage is its poor performance in cloudy weather.[23]
 PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station.[61] A household's solar panels can either be used for just that household or, if connected to an electrical grid, can be aggregated with millions of others.[62]
 The first utility-scale solar power plant was built in 1982 in Hesperia, California by ARCO.[63] The plant was not profitable and was sold eight years later.[64] However, over the following decades, PV cells became significantly more efficient and cheaper.[65] As a result, PV adoption has grown exponentially since 2010.[66] Global capacity increased from 230 GW at the end of 2015 to 890 GW in 2021.[67] PV grew fastest in China between 2016 and 2021, adding 560 GW, more than all advanced economies combined.[68] Four of the ten biggest solar power stations are in China, including the biggest, Golmud Solar Park in China.[69]
 Unlike photovoltaic cells that convert sunlight directly into electricity, solar thermal systems convert it into heat. They use mirrors or lenses to concentrate sunlight onto a receiver, which in turn heats a water reservoir. The heated water can then be used in homes. The advantage of solar thermal is that the heated water can be stored until it is needed, eliminating the need for a separate energy storage system.[70] Solar thermal power can also be converted to electricity by using the steam generated from the heated water to drive a turbine connected to a generator. However, because generating electricity this way is much more expensive than photovoltaic power plants, there are very few in use today.[71]
 Humans have harnessed wind energy since at least 3500 BC. Until the 20th century, it was primarily used to power ships, windmills and water pumps. Today, the vast majority of wind power is used to generate electricity using wind turbines.[23] Modern utility-scale wind turbines range from around 600 kW to 9 MW of rated power. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine.[76] Areas where winds are stronger and more constant, such as offshore and high-altitude sites, are preferred locations for wind farms.
 Wind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%.[77]
 Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore, and likely also industrial use of new types of VAWT turbines in addition to the horizontal axis units currently in use. As offshore wind speeds average ~90% greater than that of land, offshore resources can contribute substantially more energy than land-stationed turbines.[78]
 Investments in wind technologies reached USD 161 billion in 2020, with onshore wind dominating at 80% of total investments from 2013 to 2022. Offshore wind investments nearly doubled to USD 41 billion between 2019 and 2020, primarily due to policy incentives in China and expansion in Europe. Global wind capacity increased by 557 GW between 2013 and 2021, with capacity additions increasing by an average of 19% each year.[57]
 Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. Water can generate electricity with a conversion efficiency of about 90%, which is the highest rate in renewable energy.[82] There are many forms of water energy:
 Much hydropower is flexible, thus complementing wind and solar.[86] In 2021, the world renewable hydropower capacity was 1,360 GW.[68] Only a third of the world's estimated hydroelectric potential of 14,000 TWh/year has been developed.[87][88] New hydropower projects face opposition from local communities due to their large impact, including relocation of communities and flooding of wildlife habitats and farming land.[89] High cost and lead times from permission process, including environmental and risk assessments, with lack of environmental and social acceptance are therefore the primary challenges for new developments.[90] It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid.[91] Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with ""pump back"" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Because dispatchable power is more valuable than VRE[92][93] countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro.[94]
 Biomass is biological material derived from living, or recently living organisms. Most commonly, it refers to plants or plant-derived materials. As an energy source, biomass can either be used directly via combustion to produce heat, or converted to a more energy-dense biofuel like ethanol. Wood is the most significant biomass energy source as of 2012[98] and is usually sourced from a trees cleared for silvicultural reasons or fire prevention. Municipal wood waste – for instance, construction materials or sawdust – is also often burned for energy.[99] The biggest per-capita producers of wood-based bioenergy are heavily forested countries like Finland, Sweden, Estonia, Austria, and Denmark.[100]
 
Bioenergy can be environmentally destructive if old-growth forests are cleared to make way for crop production. In particular, demand for palm oil to produce biodiesel has contributed to the deforestation of tropical rainforests in Brazil and Indonesia.[101] In addition, burning biomass still produces carbon emissions, although much less than fossil fuels (39 grams of CO2 per megajoule of energy, compared to 75 g/MJ for fossil fuels).[102] Biofuels are primarily used in transportation, providing 3.5% of the world's transport energy demand in 2022,[103] up from 2.7% in 2010.[104] Biojet is expected to be important for short-term reduction of carbon dioxide emissions from long-haul flights.[105]
 Aside from wood, the major sources of bioenergy are bioethanol and biodiesel.[23] Bioethanol is usually produced by fermenting the sugar components of crops like sugarcane and maize, while biodiesel is mostly made from oils extracted from plans, such as soybean oil and corn oil.[106] Most of the crops used to produce bioethanol and biodiesel are grown specifically for this purpose,[107] although used cooking oil accounted for 14% of the oil used to produce biodiesel as of 2015.[106] The biomass used to produce biofuels varies by region. Maize is the major feedstock in the United States, while sugarcane dominates in Brazil.[108] In the European Union, where biodiesel is more common than bioethanol, rapeseed oil and palm oil are the main feedstocks.[109] China, although it produces comparatively much less biofuel, uses mostly corn and wheat.[110] In many countries, biofuels are either subsidized or mandated to be included in fuel mixtures.[101]
 
There are many other sources of bioenergy that are more niche, or not yet viable at large scales. For instance, bioethanol could be produced from the cellulosic parts of crops, rather than only the seed as is common today.[111] Sweet sorghum may be a promising alternative source of bioethanol, due to its tolerance of a wide range of climates.[112] Cow dung can be converted into methane.[113] There is also a great deal of research involving algal fuel, which is attractive because algae is a non-food resource, grows around 20 times faster than most food crops, and can be grown almost anywhere.[114] Geothermal energy is thermal energy (heat) extracted from the Earth's crust. It originates from several different sources, of which the most significant is slow radioactive decay of minerals contained in the Earth's interior,[23] as well as some leftover heat from the formation of the Earth.[119] Some of the heat is generated near the Earth's surface in the crust, but some also flows from deep within the Earth from the mantle and core.[119] Geothermal energy extraction is viable mostly in countries located on tectonic plate edges, where the Earth's hot mantle is more exposed.[120] As of 2023, the United States has by far the most geothermal capacity (2.7 GW,[121] or less than 0.2% of the country's total energy capacity[122]), followed by Indonesia and the Philippines. Global capacity in 2022 was 15 GW.[121]
 Geothermal energy can be either used directly to heat homes, as is common in Iceland, or to generate electricity. At smaller scales, geothermal power can be generated with geothermal heat pumps, which can extract heat from ground temperatures of under 30 °C (86 °F), allowing them to be used at relatively shallow depths of a few meters.[120] Electricity generation requires large plants and ground temperatures of at least 150 °C (302 °F). In some countries, electricity produced from geothermal energy accounts for a large portion of the total, such as Kenya (43%) and Indonesia (5%).[123]
 Technical advances may eventually make geothermal power more widely available. For example, enhanced geothermal systems involve drilling around 10 kilometres (6.2 mi) into the Earth, breaking apart hot rocks and extracting the heat using water. In theory, this type of geothermal energy extraction could be done anywhere on Earth.[120]
 There are also other renewable energy technologies that are still under development, including enhanced geothermal systems, concentrated solar power, cellulosic ethanol, and marine energy.[7][8] These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.[8]
 There are numerous organizations within the academic, federal,[clarification needed] and commercial sectors conducting large-scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.[124]
Multiple government-supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners.[125]
 Enhanced geothermal systems (EGS) are a new type of geothermal power which does not require natural hot water reservoirs or steam to generate power. Most of the underground heat within drilling reach is trapped in solid rocks, not in water.[126] EGS technologies use hydraulic fracturing to break apart these rocks and release the heat they contain, which is then harvested by pumping water into the ground. The process is sometimes known as ""hot dry rock"" (HDR).[127] Unlike conventional geothermal energy extraction, EGS may be feasible anywhere in the world, depending on the cost of drilling.[128] EGS projects have so far primarily been limited to demonstration plants, as the technology is capital-intensive due to the high cost of drilling.[129]
 Marine energy (also sometimes referred to as ocean energy) is the energy carried by ocean waves, tides, salinity, and ocean temperature differences. Technologies to harness the energy of moving water include wave power, marine current power, and tidal power. Reverse electrodialysis (RED) is a technology for generating electricity by mixing fresh water and salty sea water in large power cells.[130][page needed] Most marine energy harvesting technologies are still at low technology readiness levels and not used at large scales. Tidal energy is generally considered the most mature, but has not seen wide deployment.[131] The world's largest tidal power station is on Sihwa Lake, South Korea,[132] which produces around 550 gigawatt-hours of electricity per year.[133]
 Earth emits roughly 1017 W of infrared thermal radiation that flows toward the cold outer space. Solar energy hits the surface and atmosphere of the earth and produces heat. Using various theorized devices like emissive energy harvester (EEH) or thermoradiative diode, this energy flow can be converted into electricity. In theory, this technology can be used during nighttime.[134][135]
 Producing liquid fuels from oil-rich (fat-rich) varieties of algae is an ongoing research topic. Various microalgae grown in open or closed systems are being tried including some systems that can be set up in brownfield and desert lands.[136]
 Collection of static electricity charges from water droplets on metal surfaces is an experimental technology that would be especially useful in low-income countries with relative air humidity over 60%.[137]
 Breeder reactors could, in principle, extract almost all of the energy contained in uranium or thorium, decreasing fuel requirements by a factor of 100 compared to widely used once-through light water reactors, which extract less than 1% of the energy in the actinide metal (uranium or thorium) mined from the earth.[138] The high fuel-efficiency of breeder reactors could greatly reduce concerns about fuel supply, energy used in mining, and storage of radioactive waste. With seawater uranium extraction (currently too expensive to be economical), there is enough fuel for breeder reactors to satisfy the world's energy needs for 5 billion years at 1983's total energy consumption rate, thus making nuclear energy effectively a renewable energy.[139][140] In addition to seawater the average crustal granite rocks contain significant quantities of uranium and thorium that with breeder reactors can supply abundant energy for the remaining lifespan of the sun on the main sequence of stellar evolution.[141]
 Artificial photosynthesis uses techniques including nanotechnology to store solar electromagnetic energy in chemical bonds by splitting water to produce hydrogen and then using carbon dioxide to make methanol.[142] Researchers in this field strived to design molecular mimics of photosynthesis that use a wider region of the solar spectrum, employ catalytic systems made from abundant, inexpensive materials that are robust, readily repaired, non-toxic, stable in a variety of environmental conditions and perform more efficiently allowing a greater proportion of photon energy to end up in the storage compounds, i.e., carbohydrates (rather than building and sustaining living cells).[143] However, prominent research faces hurdles, Sun Catalytix a MIT spin-off stopped scaling up their prototype fuel-cell in 2012 because it offers few savings over other ways to make hydrogen from sunlight.[144]
 One of the efforts to decarbonize transportation is the increased use of electric vehicles (EVs).[145] Despite that and the use of biofuels, such as biojet, less than 4% of transport energy is from renewables.[146] Occasionally hydrogen fuel cells are used for heavy transport.[147] Meanwhile, in the future electrofuels may also play a greater role in decarbonizing hard-to-abate sectors like aviation and maritime shipping.[148]
 Solar water heating makes an important contribution to renewable heat in many countries, most notably in China, which now has 70% of the global total (180 GWth). Most of these systems are installed on multi-family apartment buildings[149] and meet a portion of the hot water needs of an estimated 50–60 million households in China. Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households.
 Heat pumps provide both heating and cooling, and also flatten the electric demand curve and are thus an increasing priority.[150] Renewable thermal energy is also growing rapidly.[151] About 10% of heating and cooling energy is from renewables.[152]
 Some studies say that a global transition to 100% renewable energy across all sectors – power, heat, transport and industry – is feasible and economically viable.[153][154][155]
 Most new renewables are solar, followed by wind then hydro then bioenergy.[156] Investment in renewables, especially solar, tends to be more effective in creating jobs than coal, gas or oil.[157][158] Worldwide, renewables employ about 12 million people as of 2020, with solar PV being the technology employing the most at almost 4 million.[159] However, as of February 2024, the world's supply of workforce for solar energy is lagging greatly behind demand as universities worldwide still produce more workforce for fossil fuels than for renewable energy industries.[160]
 In 2021, China accounted for almost half of the global increase in renewable electricity.[161]
 There are 3,146 gigawatts installed in 135 countries, while 156 countries have laws regulating the renewable energy sector.[9][10]
 Globally in 2020 there are over 10 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer.[162] The clean energy sectors added about 4.7 million jobs globally between 2019 and 2022, totaling 35 million jobs by 2022.[163]: 5 
 The International Renewable Energy Agency (IRENA) stated that ~86% (187 GW) of renewable capacity added in 2022 had lower costs than electricity generated from fossil fuels.[164] IRENA also stated that capacity added since 2000 reduced electricity bills in 2022 by at least $520 billion, and that in non-OECD countries, the lifetime savings of 2022 capacity additions will reduce costs by up to $580 billion.[164]
 * = 2018. All other values for 2019.
 The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies.[179]
 In the decade of 2010–2019, worldwide investment in renewable energy capacity excluding large hydropower amounted to US$2.7 trillion, of which the top countries China contributed US$818 billion, the United States contributed US$392.3 billion, Japan contributed US$210.9 billion, Germany contributed US$183.4 billion, and the United Kingdom contributed US$126.5 billion.[180] This was an increase of over three and possibly four times the equivalent amount invested in the decade of 2000–2009 (no data is available for 2000–2003).[180]
 As of 2022, an estimated 28% of the world's electricity was generated by renewables. This is up from 19% in 1990.[181]
 A December 2022 report by the IEA forecasts that over 2022-2027, renewables are seen growing by almost 2 400 GW in its main forecast, equal to the entire installed power capacity of China in 2021. This is an 85% acceleration from the previous five years, and almost 30% higher than what the IEA forecast in its 2021 report, making its largest ever upward revision. Renewables are set to account for over 90% of global electricity capacity expansion over the forecast period.[68] To achieve net zero emissions by 2050, IEA believes that 90% of global electricity generation will need to be produced from renewable sources.[17]
 In June 2022 IEA Executive Director Fatih Birol said that countries should invest more in renewables to ""ease the pressure on consumers from high fossil fuel prices, make our energy systems more secure, and get the world on track to reach our climate goals.""[183]
 China's five year plan to 2025 includes increasing direct heating by renewables such as geothermal and solar thermal.[184]
 REPowerEU, the EU plan to escape dependence on fossil Russian gas, is expected to call for much more green hydrogen.[185]
 After a transitional period,[186] renewable energy production is expected to make up most of the world's energy production. In 2018, the risk management firm, DNV GL, forecasts that the world's primary energy mix will be split equally between fossil and non-fossil sources by 2050.[187]
 In July 2014, WWF and the World Resources Institute convened a discussion among a number of major US companies who had declared their intention to increase their use of renewable energy. These discussions identified a number of ""principles"" which companies seeking greater access to renewable energy considered important market deliverables. These principles included choice (between suppliers and between products), cost competitiveness, longer term fixed price supplies, access to third-party financing vehicles, and collaboration.[188]
 UK statistics released in September 2020 noted that ""the proportion of demand met from renewables varies from a low of 3.4 per cent (for transport, mainly from biofuels) to highs of over 20 per cent for 'other final users', which is largely the service and commercial sectors that consume relatively large quantities of electricity, and industry"".[189]
 In some locations, individual households can opt to purchase renewable energy through a consumer green energy program.
 Renewable energy in developing countries is an increasingly used alternative to fossil fuel energy, as these countries scale up their energy supplies and address energy poverty. Renewable energy technology was once seen as unaffordable for developing countries.[190] However, since 2015, investment in non-hydro renewable energy has been higher in developing countries than in developed countries, and comprised 54% of global renewable energy investment in 2019.[191] The International Energy Agency forecasts that renewable energy will provide the majority of energy supply growth through 2030 in Africa and Central and South America, and 42% of supply growth in China.[192]
 In Kenya, the Olkaria V Geothermal Power Station is one of the largest in the world.[194] The Grand Ethiopia Renaissance Dam project incorporates wind turbines.[195] Once completed, Morocco's Ouarzazate Solar Power Station is projected to provide power to over a million people.[196]
 Policies to support renewable energy have been vital in their expansion. Where Europe dominated in establishing energy policy in the early 2000s, most countries around the world now have some form of energy policy.[197]
 The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed in 2009, with 75 countries signing the charter of IRENA.[198] As of April 2019, IRENA has 160 member states.[199] The then United Nations Secretary-General Ban Ki-moon has said that renewable energy can lift the poorest nations to new levels of prosperity,[200] and in September 2011 he launched the UN Sustainable Energy for All initiative to improve energy access, efficiency and the deployment of renewable energy.[201]
 The 2015 Paris Agreement on climate change motivated many countries to develop or improve renewable energy policies.[14] In 2017, a total of 121 countries adopted some form of renewable energy policy.[197] National targets that year existed in 176 countries.[14] In addition, there is also a wide range of policies at the state/provincial, and local levels.[104] Some public utilities help plan or install residential energy upgrades.
 Many national, state and local governments have created green banks. A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies.[202] Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy.
 Climate neutrality by the year 2050 is the main goal of the European Green Deal.[203] For the European Union to reach their target of climate neutrality, one goal is to decarbonise its energy system by aiming to achieve ""net-zero greenhouse gas emissions by 2050.""[204]
 The International Renewable Energy Agency's (IRENA) 2023 report on renewable energy finance highlights steady investment growth since 2018: USD 348 billion in 2020 (a 5.6% increase from 2019), USD 430 billion in 2021 (24% up from 2020), and USD 499 billion in 2022 (16% higher). This trend is driven by increasing recognition of renewable energy's role in mitigating climate change and enhancing energy security, along with investor interest in alternatives to fossil fuels. Policies such as feed-in tariffs in China and Vietnam have significantly increased renewable adoption. Furthermore, from 2013 to 2022, installation costs for solar photovoltaic (PV), onshore wind, and offshore wind fell by 69%, 33%, and 45%, respectively, making renewables more cost-effective.[205][57]
 Between 2013 and 2022, the renewable energy sector underwent a significant realignment of investment priorities. Investment in solar and wind energy technologies markedly increased. In contrast, other renewable technologies such as hydropower (including pumped storage hydropower), biomass, biofuels, geothermal, and marine energy experienced a substantial decrease in financial investment. Notably, from 2017 to 2022, investment in these alternative renewable technologies declined by 45%, falling from USD 35 billion to USD 17 billion.[57]
 In 2023, the renewable energy sector experienced a significant surge in investments, particularly in solar and wind technologies, totaling approximately USD 200 billion—a 75% increase from the previous year. The increased investments in 2023 contributed between 1% and 4% to the GDP in key regions including the United States, China, the European Union, and India.[206]
 Renewable electricity generation by wind and solar is variable. This results in reduced capacity factor and may require keeping some gas-fired power plants or other dispatchable generation on standby[209][210][211] until there is enough energy storage, demand response, grid improvement, and/or base load power from non-intermittent sources like hydropower, nuclear power or bioenergy.
 The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization.[212][better source needed]
 The International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.[213]
 Whether nuclear power should be considered a form of renewable energy is an ongoing subject of debate. Statutory definitions of renewable energy usually exclude many present nuclear energy technologies, with the notable exception of the state of Utah.[214] Dictionary-sourced definitions of renewable energy technologies often omit or explicitly exclude mention of nuclear energy sources, with an exception made for the natural nuclear decay heat generated within the Earth.[215][216]
 The most common fuel used in conventional nuclear fission power stations, uranium-235 is ""non-renewable"" according to the Energy Information Administration, the organization however is silent on the recycled MOX fuel.[216] The National Renewable Energy Laboratory does not mention nuclear power in its ""energy basics"" definition.[217]
 The geopolitical impact of the growing use of renewable energy is a subject of ongoing debate and research.[220] Many fossil-fuel producing countries, such as Qatar, Russia, Saudi Arabia and Norway, are currently able to exert diplomatic or geopolitical influence as a result of their oil wealth. Most of these countries are expected to be among the geopolitical ""losers"" of the energy transition, although some, like Norway, are also significant producers and exporters of renewable energy. Fossil fuels and the infrastructure to extract them may, in the long term, become stranded assets.[221] It has been speculated that countries dependent on fossil fuel revenue may one day find it in their interests to quickly sell off their remaining fossil fuels.[222]
 Conversely, nations abundant in renewable resources, and the minerals required for renewables technology, are expected to gain influence.[223][224] In particular, China has become the world's dominant manufacturer of the technology needed to produce or store renewable energy, especially solar panels, wind turbines, and lithium-ion batteries.[225] Nations rich in solar and wind energy could become major energy exporters.[226] Some may produce and export green hydrogen,[227][226] although electricity is projected to be the dominant energy carrier in 2050, accounting for almost 50% of total energy consumption (up from 22% in 2015).[228] Countries with large uninhabited areas such as Australia, China, and many African and Middle Eastern countries have a potential for huge installations of renewable energy. The production of renewable energy technologies requires rare-earth elements with new supply chains.[229]
 Countries with already weak governments that rely on fossil fuel revenue may face even higher political instability or popular unrest. Analysts consider Nigeria, Angola, Chad, Gabon, and Sudan, all countries with a history of military coups, to be at risk of instability due to dwindling oil income.[230]
 A study found that transition from fossil fuels to renewable energy systems reduces risks from mining, trade and political dependence because renewable energy systems don't need fuel – they depend on trade only for the acquisition of materials and components during construction.[231]
 In October 2021, European Commissioner for Climate Action Frans Timmermans suggested ""the best answer"" to the 2021 global energy crisis is ""to reduce our reliance on fossil fuels.""[232] He said those blaming the European Green Deal were doing so ""for perhaps ideological reasons or sometimes economic reasons in protecting their vested interests.""[232] Some critics blamed the European Union Emissions Trading System (EU ETS) and closure of nuclear plants for contributing to the energy crisis.[233][234][235] European Commission President Ursula von der Leyen said that Europe is ""too reliant"" on natural gas and too dependent on natural gas imports. According to Von der Leyen, ""The answer has to do with diversifying our suppliers ... and, crucially, with speeding up the transition to clean energy.""[236]
 The renewable energy transition requires increased extraction of certain metals and minerals.[237] Solar power panels require large amounts of aluminum.[238] This impacts the environment and can lead to environmental conflict.[239]
 The International Energy Agency does not recognise shortages of resources but states that supply could struggle to keep pace with the world's climate ambitions. Electric vehicles (EV) and battery storage are expected to cause the most demand. Wind farms and solar PV are less consuming. The extension of electrical grids requires large amounts of copper and aluminium. The IEA recommends to scale up recycling. By 2040, quantities of copper, lithium, cobalt, and nickel from spent batteries could reduce combined primary supply requirements for these minerals by around 10%.[237]
 The demand for lithium by 2040 is expected to grow by the factor of 42. Graphite and nickel exploration is predicted to grow about 20-fold. For each of the most relevant minerals and metals, a significant share of resources are concentrated in only one country: copper in Chile, nickel in Indonesia, rare earths in China, cobalt in the Democratic Republic of the Congo (DRC), and lithium in Australia. China dominates processing of them all.[237]
 A controversial approach is deep sea mining. Minerals can be collected from new sources like polymetallic nodules lying on the seabed,[240] but this could damage biodiversity.[241]
 The transition to renewable energy depends on non-renewable resources, such as mined metals.[242] Manufacturing of photovoltaic panels, wind turbines and batteries requires significant amounts of rare-earth elements[243] which has significant social and environmental impact if mined in forests and protected areas.[244] Due to co-occurrence of rare-earth and radioactive elements (thorium, uranium and radium), rare-earth mining results in production of low-level radioactive waste.[245] In Africa, the green energy transition created a mining boom, causing deforestation and creating possibility to zoonotic spillover. To mitigate climate change and prevent epidemics some territories should stay intact.[246]
 Installations used to produce wind, solar and hydropower are an increasing threat to key conservation areas, with facilities built in areas set aside for nature conservation and other environmentally sensitive areas. They are often much larger than fossil fuel power plants, needing areas of land up to 10 times greater than coal or gas to produce equivalent energy amounts.[247] More than 2000 renewable energy facilities are built, and more are under construction, in areas of environmental importance and threaten the habitats of plant and animal species across the globe. The authors' team emphasized that their work should not be interpreted as anti-renewables because renewable energy is crucial for reducing carbon emissions. The key is ensuring that renewable energy facilities are built in places where they do not damage biodiversity.[248]
 In 2020 scientists published a world map of areas that contain renewable energy materials as well as estimations of their overlaps with ""Key Biodiversity Areas"", ""Remaining Wilderness"" and ""Protected Areas"". The authors assessed that careful strategic planning is needed.[249][250][251]
 Solar panels are recycled to reduce electronic waste and create a source for materials that would otherwise need to be mined,[252] but such business is still small and work is ongoing to improve and scale-up the process.[253][254][255]
 Solar power plants may compete with arable land,[242][257] while on-shore wind farms face opposition due to aesthetic concerns and noise, which is impacting both humans and wildlife.[258][259][260][need quotation to verify]In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.[261] These concerns, when directed against renewable energy, are sometimes described as ""not in my back yard"" attitude (NIMBY).
 A 2011 UK Government document states that ""projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake.""[262] In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.[263][264]
 In international public opinion surveys there is strong support for renewables such as solar power and wind power.[212][265]
 Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. The oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from more than a million years ago. The use of biomass for fire did not become commonplace until many hundreds of thousands of years later.[266] Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile.[267] From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times.[268] Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass.
 In 1885, Werner Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:
 In conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten.[269] Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus (The Protestant Ethic and the Spirit of Capitalism), published in 1905.[270] Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 Scientific American article: ""in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race"".[271]
 The theory of peak oil was published in 1956.[272] In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity-generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.[273]
 New government spending, regulation and policies helped the renewables industry weather the 2009 global financial crisis better than many other sectors.[274] In 2022, renewables accounted for 30% of global electricity generation, up from 21% in 1985.[11]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['race', 'China, Europe, Turkey, and Mexico', 'Producing liquid fuels', 'climate change mitigation, energy security', 'improving efficiency and increasing overall energy yields'], 'answer_start': [], 'answer_end': []}"
"Environmental issues are disruptions in the usual function of ecosystems.[1] Further, these issues can be caused by humans (human impact on the environment)[2] or they can be natural. These issues are considered serious when the ecosystem cannot recover in the present situation, and catastrophic if the ecosystem is projected to certainly collapse. 
 Environmental protection is the practice of protecting the natural environment on the individual, organizational or governmental levels, for the benefit of both the environment and humans. Environmentalism is a social and environmental movement that addresses environmental issues through advocacy, legislation education, and activism.[3]
 Environment destruction caused by humans is a global, ongoing problem.[4] Water pollution also cause problems to marine life.[5] Most scholars think that the project peak global world population of between 9-10 billion people, could live sustainably within the earth's ecosystems if human society worked to live sustainably within planetary boundaries.[6][7][8] The bulk of environmental impacts are caused by excessive consumption of industrial goods by the world's wealthiest populations.[9][10][11] The UN Environmental Program, in its ""Making Peace With Nature"" Report in 2021, found addressing key planetary crises, like pollution, climate change and biodiversity loss, was achievable if parties work to address the Sustainable Development Goals.[12]
 Major current environmental issues may include climate change, pollution, environmental degradation, and resource depletion. The conservation movement lobbies for protection of endangered species and protection of any ecologically valuable 
natural areas, genetically modified foods and global warming. The UN system has adopted international frameworks for environmental issues in three key issues, which has been encoded as the ""triple planetary crises"": climate change, pollution, and biodiversity loss.[13]
 Human impact on the environment (or anthropogenic environmental impact) refers to changes to biophysical environments[14] and to ecosystems, biodiversity, and natural resources[15] caused directly or indirectly by humans. Modifying the environment to fit the needs of society (as in the built environment) is causing severe effects[16][17] including global warming,[14][18][19] environmental degradation[14] (such as ocean acidification[14][20]), mass extinction and biodiversity loss,[21][22][23] ecological crisis, and ecological collapse. Some human activities that cause damage (either directly or indirectly) to the environment on a global scale include population growth,[24][25][26] neoliberal economic policies[27][28][29] and rapid economic growth,[30] overconsumption, overexploitation, pollution, and deforestation. Some of the problems, including global warming and biodiversity loss, have been proposed as representing catastrophic risks to the survival of the human species.[31][32]
 Environmental degradation is the deterioration of the environment through depletion of resources such as quality of air, water and soil; the destruction of ecosystems; habitat destruction; the extinction of wildlife; and pollution. It is defined as any change or disturbance to the environment perceived to be deleterious or undesirable.[39][40] The environmental degradation process amplifies the impact of environmental issues which leave lasting impacts on the environment.[41]
 Environmental degradation is one of the ten threats officially cautioned by the High-level Panel on Threats, Challenges and Change of the United Nations. The United Nations International Strategy for Disaster Reduction defines environmental degradation as ""the reduction of the capacity of the environment to meet social and ecological objectives, and needs"".[42]
 Environmental conflicts, socio-environmental conflict or ecological distribution conflicts (EDCs) are social conflicts caused by environmental degradation or by unequal distribution of environmental resources.[43][44][45] The Environmental Justice Atlas documented 3,100 environmental conflicts worldwide as of April 2020 and emphasised that many more conflicts remained undocumented.[43]
 Parties involved in these conflicts include locally affected communities, states, companies and investors, and social or environmental movements;[46][47] typically environmental defenders are protecting their homelands from resource extraction or hazardous waste disposal.[43] Resource extraction and hazardous waste activities often create resource scarcities (such as by overfishing or deforestation), pollute the environment, and degrade the living space for humans and nature, resulting in conflict.[48] A particular case of environmental conflicts are forestry conflicts, or forest conflicts which ""are broadly viewed as struggles of varying intensity between interest groups, over values and issues related to forest policy and the use of forest resources"".[49] In the last decades, a growing number of these have been identified globally.[50]
 Frequently environmental conflicts focus on environmental justice issues, the rights of indigenous people, the rights of peasants, or threats to communities whose livelihoods are dependent on the ocean.[43] Outcomes of local conflicts are increasingly influenced by trans-national environmental justice networks that comprise the global environmental justice movement.[43][51]
 Environmental justice or eco-justice, is a social movement to address environmental injustice, which occurs when poor or marginalized communities are harmed by hazardous waste, resource extraction, and other land uses from which they do not benefit.[53][54] The movement has generated hundreds of studies showing that exposure to environmental harm is inequitably distributed.[55]
 The movement began in the United States in the 1980s. It was heavily influenced by the American civil rights movement and focused on environmental racism within rich countries. The movement was later expanded to consider gender, international environmental injustice, and inequalities within marginalised groups. As the movement achieved some success in rich countries, environmental burdens were shifted to the Global South (as for example through extractivism or the global waste trade). The movement for environmental justice has thus become more global, with some of its aims now being articulated by the United Nations. The movement overlaps with movements for Indigenous land rights and for the human right to a healthy environment.[56]
 The goal of the environmental justice movement is to achieve agency for marginalised communities in making environmental decisions that affect their lives. The global environmental justice movement arises from local environmental conflicts in which environmental defenders frequently confront multi-national corporations in resource extraction or other industries. Local outcomes of these conflicts are increasingly influenced by trans-national environmental justice networks.[57][58]
 The 2023 IPCC report highlighted the disproportionate effects of climate change on vulnerable populations. The report's findings make it clear that every increment of global warming exacerbates challenges such as extreme heatwaves, heavy rainfall, and other weather extremes, which in turn amplify risks for human health and ecosystems. With nearly half of the world's population residing in regions highly susceptible to climate change, the urgency for global actions that are both rapid and sustained is underscored. The importance of integrating diverse knowledge systems, including scientific, Indigenous, and local knowledge, into climate action is highlighted as a means to foster inclusive solutions that address the complexities of climate impacts across different communities.[61]
 In addition, the report points out the critical gap in adaptation finance, noting that developing countries require significantly more resources to effectively adapt to climate challenges than what is currently available. This financial disparity raises questions about the global commitment to equitable climate action and underscores the need for a substantial increase in support and resources. The IPCC's analysis suggests that with adequate financial investment and international cooperation, it is possible to embark on a pathway towards resilience and sustainability that benefits all sections of society.[61]
 Environmental Impact assessment (EIA) is the assessment of the environmental consequences of a plan, policy, program, or actual projects prior to the decision to move forward with the proposed action. In this context, the term ""environmental impact assessment"" is usually used when applied to actual projects by individuals or companies and the term ""strategic environmental assessment"" (SEA) applies to policies, plans and programmes most often proposed by organs of state.[65][66] It is a tool of environmental management forming a part of project approval and decision-making.[67]  Environmental assessments may be governed by rules of administrative procedure regarding public participation and documentation of decision making, and may be subject to judicial review.
 The environmental movement (sometimes referred to as the ecology movement) is a social movement that aims to protect the natural world from harmful environmental practices in order to create sustainable living.[70] Environmentalists advocate the just and sustainable management of resources and stewardship of the environment through changes in public policy and individual behavior.[71] In its recognition of humanity as a participant in (not an enemy of) ecosystems, the movement is centered on ecology, health, as well as human rights.
 Environmental issues are addressed at a regional, national or international level by government organizations.
 The largest international agency, set up in 1972, is the United Nations Environment Programme. The International Union for Conservation of Nature brings together 83 states, 108 government agencies, 766 Non-governmental organizations and 81 international organizations and about 10,000 experts, scientists from countries around the world.[72] International non-governmental organizations include Greenpeace, Friends of the Earth and World Wide Fund for Nature. Governments enact environmental policy and enforce environmental law and this is done to differing degrees around the world.
 There are an increasing number of films being produced on environmental issues, especially on climate change and global warming. Al Gore's 2006 film An Inconvenient Truth gained commercial success and a high media profile.
 Issues
 Specific issues
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Sustainable Development Goals', 'locally affected communities, states, companies and investors, and social or environmental movements', 'protection of endangered species', 'climate change, pollution, environmental degradation, and resource depletion', 'exposure to environmental harm is inequitably distributed'], 'answer_start': [], 'answer_end': []}"
"
 
 In common usage, climate change describes global warming—the ongoing increase in global average temperature—and its effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global average temperature is primarily caused by humans burning fossil fuels.[3][4] Fossil fuel use, deforestation, and some agricultural and industrial practices add to greenhouse gases, notably carbon dioxide and methane.[5] Greenhouse gases absorb some of the heat that the Earth radiates after it warms from sunlight. Larger amounts of these gases trap more heat in Earth's lower atmosphere, causing global warming.
 Climate change has an increasingly large impact on the environment. Deserts are expanding, while heat waves and wildfires are becoming more common.[6] Amplified warming in the Arctic has contributed to thawing permafrost, retreat of glaciers and sea ice decline.[7] Higher temperatures are also causing more intense storms, droughts, and other weather extremes.[8] Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct.[9] Even if efforts to minimise future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.[10]
 Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss. Human migration and conflict can also be a result.[11] The World Health Organization (WHO) calls climate change the greatest threat to global health in the 21st century.[12] Societies and ecosystems will experience more severe risks without action to limit warming.[13] Adapting to climate change through efforts like flood control measures or drought-resistant crops partially reduces climate change risks, although some limits to adaptation have already been reached.[14] Poorer communities are responsible for a small share of global emissions, yet have the least ability to adapt and are most vulnerable to climate change.[15][16]
 Many climate change impacts have been felt in recent years, with 2023 the warmest on record at +1.48 °C (2.66 °F) since regular tracking began in 1850.[18][19] Additional warming will increase these impacts and can trigger tipping points, such as melting all of the Greenland ice sheet.[20] Under the 2015 Paris Agreement, nations collectively agreed to keep warming ""well under 2 °C"". However, with pledges made under the Agreement, global warming would still reach about 2.7 °C (4.9 °F) by the end of the century.[21] Limiting warming to 1.5 °C will require halving emissions by 2030 and achieving net-zero emissions by 2050.[22]
 Fossil fuel use can be phased out by conserving energy and switching to energy sources that do not produce significant carbon pollution. These energy sources include wind, solar, hydro, and nuclear power.[23][24] Cleanly generated electricity can replace fossil fuels for powering transportation, heating buildings, and running industrial processes.[25] Carbon can also be removed from the atmosphere, for instance by increasing forest cover and farming with methods that capture carbon in soil.[26][27]
 Before the 1980s it was unclear whether the warming effect of increased greenhouse gases was stronger than the cooling effect of airborne particulates in air pollution. Scientists used the term inadvertent climate modification to refer to human impacts on the climate at this time.[28] In the 1980s, the terms global warming and climate change became more common, often being used interchangeably.[29] Scientifically, global warming refers only to increased surface warming, while climate change describes both global warming and its effects on Earth's climate system, such as precipitation changes.[28]
 Climate change can also be used more broadly to include changes to the climate that have happened throughout Earth's history.[30] Global warming—used as early as 1975[31]—became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate.[32] Since the 2000s, climate change has increased usage.[33] Various scientists, politicians and media now use the terms climate crisis or climate emergency to talk about climate change, and global heating instead of global warming.[34]
 Over the last few million years human beings evolved in a climate that cycled through ice ages, with global average temperature ranging between 1 °C warmer and 5–6 °C colder than current levels.[37][38] One of the hotter periods was the Last Interglacial between 115,000 and 130,000 years ago, when sea levels were 6 to 9 meters higher than today.[39]
 The most recent glacial maximum 20,000 years ago had sea levels that were about 125 meters (410 ft) lower than today.[40] Temperatures stabilized in the current interglacial period beginning 11,700 years ago.[41] Historical patterns of warming and cooling, like the Medieval Warm Period and the Little Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions.[42] Climate information for that period comes from climate proxies, such as trees and ice cores.[43]
 Around 1850 thermometer records began to provide global coverage.[46] 
Between the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling from sulfur dioxide emissions. Sulfur dioxide causes acid rain, but it also produces sulfate aerosols in the atmosphere, which reflect sunlight and cause so-called global dimming. After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature.[47][48][49]
 Multiple independent datasets all show worldwide increases in surface temperature,[50] at a rate of around 0.2 °C per decade.[51] The 2013–2022 decade warmed to an average 1.15 °C [1.00–1.25 °C] compared to the pre-industrial baseline (1850–1900).[52] Not every single year was warmer than the last: internal climate variability processes can make any year 0.2 °C warmer or colder than the average.[53] From 1998 to 2013, negative phases of two such processes, Pacific Decadal Oscillation (PDO)[54] and Atlantic Multidecadal Oscillation (AMO)[55] caused a so-called ""global warming hiatus"".[56] After the hiatus, the opposite occurred, with years like 2023 exhibiting temperatures well above even the recent average.[57] This is why the temperature change is defined in terms of a 20-year average, which reduces the noise of hot and cold years and decadal climate patterns, and detects the long-term signal.[58]: 5 [59]
 A wide range of other observations reinforce the evidence of warming.[60][61] The upper atmosphere is cooling, because greenhouse gases are trapping heat near the Earth's surface, and so less heat is radiating into space.[62] Warming reduces average snow cover and forces the retreat of glaciers. At the same time, warming also causes greater evaporation from the oceans, leading to more atmospheric humidity, more and heavier precipitation.[63] Plants are flowering earlier in spring, and thousands of animal species have been permanently moving to cooler areas.[64]
 Different regions of the world warm at different rates. The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature.[65] This is because oceans lose more heat by evaporation and oceans can store a lot of heat.[66] The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean.[67][68] The rest has heated the atmosphere, melted ice, and warmed the continents.[69]
 The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat.[70] Local black carbon deposits on snow and ice also contribute to Arctic warming.[71] Arctic surface temperatures are increasing between three and four times faster than in the rest of the world.[72][73][74] Melting of ice sheets near the poles weakens both the Atlantic and the Antarctic limb of thermohaline circulation, which further changes the distribution of heat and precipitation around the globe.[75][76][77][78]
 The World Meteorological Organization estimates a 66% chance of global temperatures exceeding 1.5 °C warming from the preindustrial baseline for at least one year between 2023 and 2027.[81][82] Because the IPCC uses a 20-year average to define global temperature changes, a single year exceeding 1.5 °C does not break the limit. 
 The IPCC expects the 20-year average global temperature to exceed +1.5 °C in the early 2030s.[83] The IPCC Sixth Assessment Report (2023) included projections that by 2100 global warming is very likely to reach 1.0-1.8 °C under a scenario with very low emissions of greenhouse gases, 2.1-3.5 °C under an intermediate emissions scenario, 
or 3.3-5.7 °C under a very high emissions scenario.[84] In the intermediate and high emission scenarios, the warming will continue past 2100.[85][86]
 The remaining carbon budget for staying beneath certain temperature increases is determined by modelling the carbon cycle and climate sensitivity to greenhouse gases.[87] According to the IPCC, global warming can be kept below 1.5 °C with a two-thirds chance if emissions after 2018 do not exceed 420 or 570 gigatonnes of CO2. This corresponds to 10 to 13 years of current emissions.  There are high uncertainties about the budget. For instance, it may be 100 gigatonnes of CO2 equivalent smaller due to CO2 and methane release from permafrost and wetlands.[88] However, it is clear that fossil fuel resources need to be proactively kept in the ground to prevent substantial warming. Otherwise, their shortages would not occur until the emissions have already locked in significant long-term impacts.[89]
 The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example, El Niño events cause short-term spikes in surface temperature while La Niña events cause short term cooling.[90] Their relative frequency can affect global temperature trends on a decadal timescale.[91] Other changes are caused by an imbalance of energy from external forcings.[92] Examples of these include changes in the concentrations of greenhouse gases, solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun.[93]
 To determine the human contribution to climate change, unique ""fingerprints"" for all potential causes are developed and compared with both observed patterns and known internal climate variability.[94] For example, solar forcing—whose fingerprint involves warming the entire atmosphere—is ruled out because only the lower atmosphere has warmed.[95] Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes in albedo, are less impactful.[96]
 Greenhouse gases are transparent to sunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time.[102]
 While water vapour (≈50%) and clouds (≈25%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to be feedbacks that change climate sensitivity. On the other hand, concentrations of gases such as CO2 (≈20%), tropospheric ozone,[103] CFCs and nitrous oxide are added or removed independently from temperature, and are therefore considered to be external forcings that change global temperatures.[104]
 Before the Industrial Revolution, naturally-occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C warmer than it would have been in their absence.[105][106] Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas),[107] has increased the amount of greenhouse gases in the atmosphere, resulting in a radiative imbalance. In 2019, the concentrations of CO2 and methane had increased by about 48% and 160%, respectively, since 1750.[108] These CO2 levels are higher than they have been at any time during the last 2 million years. Concentrations of methane are far higher than they were over the last 800,000 years.[109]
 Global anthropogenic greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases.[110] CO2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity.[5] Additional CO2 emissions come from deforestation and industrial processes, which include the CO2 released by the chemical reactions for making cement, steel, aluminum, and fertiliser.[111] Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, and coal mining, as well as oil and gas extraction.[112] Nitrous oxide emissions largely come from the microbial decomposition of fertiliser.[113]
 While methane only lasts in the atmosphere for an average of 12 years,[114] CO2 lasts much longer. The Earth's surface absorbs CO2 as part of the carbon cycle. While plants on land and in the ocean absorb most excess emissions of CO2 every year, that CO2 is returned to the atmosphere when biological matter is digested, burns, or decays.[115] Land-surface carbon sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions.[116] The ocean has absorbed 20 to 30% of emitted CO2 over the last 2 decades.[117] CO2 is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete.[115]
 According to Food and Agriculture Organization, around 30% of Earth's land area is largely unusable for humans (glaciers, deserts, etc.), 26% is forests, 10% is shrubland and 34% is agricultural land.[119] Deforestation is the main land use change contributor to global warming,[120] as the destroyed trees release CO2, and are not replaced by new trees, removing that carbon sink.[26] Between 2001 and 2018, 27% of deforestation was from permanent clearing to enable agricultural expansion for crops and livestock. Another 24% has been lost to temporary clearing under the shifting cultivation agricultural systems. 26% was due to logging for wood and derived products, and wildfires have accounted for the remaining 23%.[121] Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink.[122]
 Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns.[123] In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler.[122] At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains.[123] Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect.[124]
 Air pollution, in the form of aerosols, affects the climate on a large scale.[125] Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming,[126] and is primarily attributed to sulfate aerosols produced by the combustion of fossil fuels with heavy sulfur concentrations like coal and bunker fuel.[49] Smaller contributions come from black carbon, organic carbon from combustion of fossil fuels and biofuels, and from anthropogenic dust.[127][48][128][129][130] Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much.[131][49]
 Aerosols also have indirect effects on the Earth's energy budget. Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets.[132] They also reduce the growth of raindrops, which makes clouds more reflective to incoming sunlight.[133] Indirect effects of aerosols are the largest uncertainty in radiative forcing.[134]
 While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise.[135] Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050.[136] The effect of decreasing sulfur content of fuel oil for ships since 2020[137] is estimated to cause an additional 0.05 °C increase in global mean temperature by 2050.[138]
 As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system.[134] Solar irradiance has been measured directly by satellites,[141] and indirect measurements are available from the early 1600s onwards.[134] Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (the troposphere).[142] The upper atmosphere (the stratosphere) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling.[95] 
This is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere.[143]
 Explosive volcanic eruptions can release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapor into the atmosphere, which adds to greenhouse gases and increases temperatures.[144] These impacts on temperature only last for several years, because both water vapor and volcanic material have low persistence in the atmosphere.[145] volcanic CO2 emissions are more persistent, but they are equivalent to less than 1% of current human-caused CO2 emissions.[146] Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution.[145]
 The response of the climate system to an initial forcing is modified by feedbacks: increased by ""self-reinforcing"" or ""positive"" feedbacks and reduced by ""balancing"" or ""negative"" feedbacks.[148] The main reinforcing feedbacks are the water-vapour feedback, the ice–albedo feedback, and the net effect of clouds.[149][150] The primary balancing mechanism is radiative cooling, as Earth's surface gives off more heat to space in response to rising temperature.[151] In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilising effect of CO2 on plant growth.[152]
 Uncertainty over feedbacks, particularly cloud cover,[153] is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.[154] As air warms, it can hold more moisture. Water vapour, as a potent greenhouse gas, holds heat in the atmosphere.[149] If cloud cover increases, more sunlight will be reflected back into space, cooling the planet. If clouds become higher and thinner, they act as an insulator, reflecting heat from below back downwards and warming the planet.[155]
 Another major feedback is the reduction of snow cover and sea ice in the Arctic, which reduces the reflectivity of the Earth's surface.[156]
More of the Sun's energy is now absorbed in these regions, contributing to amplification of Arctic temperature changes.[157] Arctic amplification is also thawing permafrost, which releases methane and CO2 into the atmosphere.[158] Climate change can also cause methane releases from wetlands, marine systems, and freshwater systems.[159] Overall, climate feedbacks are expected to become increasingly positive.[160]
 Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans.[161] This fraction is not static and if future CO2 emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%.[162] This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plants when they are warmer.[163][164] The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes in thermohaline circulation and phytoplankton distribution.[165][166][76]
 A climate model is a representation of the physical, chemical and biological processes that affect the climate system.[167] Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing.[168] Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks.[169][170] Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere.[171]
 The physical realism of models is tested by examining their ability to simulate contemporary or past climates.[172] Past models have underestimated the rate of Arctic shrinkage[173] and underestimated the rate of precipitation increase.[174] Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations.[175] The 2017 United States-published National Climate Assessment notes that ""climate models may still be underestimating or missing relevant feedback processes"".[176] Additionally, climate models may be unable to adequately predict short-term regional climatic shifts.[177]
 A subset of climate models add societal factors to a physical climate model. These models simulate how population, economic growth, and energy use affect—and interact with—the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change.[178][179] Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm.[180]
 The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations.[181] Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency.[182] Extremely wet or dry events within the monsoon period have increased in India and East Asia.[183] Monsoonal precipitation over the Northern Hemisphere has increased since 1980.[184] The rainfall rate and intensity of hurricanes and typhoons is likely increasing,[185] and the geographic range likely expanding poleward in response to climate warming.[186] Frequency of tropical cyclones has not increased as a result of climate change.[187]
 Global sea level is rising as a consequence of thermal expansion and the melting of glaciers and ice sheets. Between 1993 and 2020, the rise increased over time, averaging 3.3 ± 0.3 mm per year.[189] Over the 21st century, the IPCC projects 32–62 cm of sea level rise under a low emission scenario, 44–76 cm under an intermediate one and 65–101 cm under a very high emission scenario.[190] Marine ice sheet instability processes in Antarctica may add substantially to these values,[191] including the possibility of a 2-meter sea level rise by 2100 under high emissions.[192]
 Climate change has led to decades of shrinking and thinning of the Arctic sea ice.[193] While ice-free summers are expected to be rare at 1.5 °C degrees of warming, they are set to occur once every three to ten years at a warming level of 2 °C.[194] Higher atmospheric CO2 concentrations cause more CO2 to dissolve in the oceans, which is making them more acidic.[195] Because oxygen is less soluble in warmer water,[196] its concentrations in the ocean are decreasing, and dead zones are expanding.[197]
 Greater degrees of global warming increase the risk of passing through 'tipping points'—thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state.[200][201] For instance, the Greenland ice sheet is already melting, but if global warming reaches levels between 1.7 °C and 2.3 °C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5 °C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place.[202] While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of major ocean currents like the Atlantic meridional overturning circulation (AMOC), and irreversible damage to key ecosystems like the Amazon rainforest and coral reefs can unfold in a matter of decades.[199]
 The long-term effects of climate change on oceans include further ice melt, ocean warming, sea level rise, ocean acidification and ocean deoxygenation.[203] The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime.[204] When net emissions stabilise surface air temperatures will also stabilise, but oceans and ice caps will continue to absorb excess heat from the atmosphere. The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years.[205] Oceanic CO2 uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years.[206] Deep oceans (below 2,000 metres (6,600 ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date.[207] Further, West Antarctic ice sheet appears committed to practically irreversible melting, which would increase the sea levels by at least 3.3 m (10 ft 10 in) over approximately 2000 years.[199][208][209][210][211][212][213][214]
 Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes.[215] For instance, the range of hundreds of North American birds has shifted northward at an average rate of 1.5 km/year over the past 55 years.[216] Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear.[217] A related phenomenon driven by climate change is woody plant encroachment, affecting up to 500 million hectares globally.[218] Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics.[219] The size and speed of global warming is making abrupt changes in ecosystems more likely.[220] Overall, it is expected that climate change will result in the extinction of many species.[221]
 The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land.[222] Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp, and seabirds.[223] Ocean acidification makes it harder for marine calcifying organisms such as mussels, barnacles and corals to produce shells and skeletons; and heatwaves have bleached coral reefs.[224] Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life.[225] Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts.[226]
 The effects of climate change are impacting humans everywhere in the world.[232] Impacts can be observed on all continents and ocean regions,[233] with low-latitude, less developed areas facing the greatest risk.[234] Continued warming has potentially ""severe, pervasive and irreversible impacts"" for people and ecosystems.[235] The risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.[236]
 The World Health Organization (WHO) calls climate change the greatest threat to global health in the 21st century.[237] Extreme weather leads to injury and loss of life.[238] Various infectious diseases are more easily transmitted in a warmer climate, such as dengue fever and malaria.[239] Crop failures can lead to food shortages and malnutrition, particularly effecting children.[240] Both children and older people are vulnerable to extreme heat.[241] The WHO has estimated that between 2030 and 2050, climate change would cause around 250,000 additional deaths per year. They assessed deaths from heat exposure in elderly people, increases in diarrhea, malaria, dengue, coastal flooding, and childhood malnutrition.[242] By 2100, 50% to 75% of the global population may face climate conditions that are life-threatening due to combined effects of extreme heat and humidity.[243]
 Climate change is affecting food security. It has caused reduction in global yields of maize, wheat, and soybeans between 1981 and 2010.[244] Future warming could further reduce global yields of major crops.[245] Crop production will probably be negatively affected in low-latitude countries, while effects at northern latitudes may be positive or negative.[246] Up to an additional 183 million people worldwide, particularly those with lower incomes, are at risk of hunger as a consequence of these impacts.[247] Climate change also impacts fish populations. Globally, less will be available to be fished.[248] Regions dependent on glacier water, regions that are already dry, and small islands have a higher risk of water stress due to climate change.[249]
 Economic damages due to climate change may be severe and there is a chance of disastrous consequences.[250] Severe impacts are expected in South-East Asia and sub-Saharan Africa, where most of the local inhabitants are dependent upon natural and agricultural resources.[251][252] Heat stress can prevent outdoor labourers from working. If warming reaches 4 °C then labour capacity in those regions could be reduced by 30 to 50%.[253] The World Bank estimates that between 2016 and 2030, climate change could drive over 120 million people into extreme poverty without adaptation.[254]
 Inequalities based on wealth and social status have worsened due to climate change.[255] Major difficulties in mitigating, adapting, and recovering to climate shocks are faced by marginalised people who have less control over resources.[256][251] Indigenous people, who are subsistent on their land and ecosystems, will face endangerment to their wellness and lifestyles due to climate change.[257] An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities.[258]
 While women are not inherently more at risk from climate change and shocks, limits on women's resources and discriminatory gender norms constrain their adaptive capacity and resilience.[259] For example, women's work burdens, including hours worked in agriculture, tend to decline less than men's during climate shocks such as heat stress.[259]
 Low-lying islands and coastal communities are threatened by sea level rise, which makes urban flooding more common. Sometimes, land is permanently lost to the sea.[260] This could lead to statelessness for people in island nations, such as the Maldives and Tuvalu.[261] In some regions, the rise in temperature and humidity may be too severe for humans to adapt to.[262] With worst-case climate change, models project that almost one-third of humanity might live in Sahara-like uninhabitable and extremely hot climates.[263]
 These factors can drive climate or environmental migration, within and between countries.[11] More people are expected to be displaced because of sea level rise, extreme weather and conflict from increased competition over natural resources. Climate change may also increase vulnerability, leading to ""trapped populations"" who are not able to move due to a lack of resources.[264]
 Climate change can be mitigated by reducing the rate at which greenhouse gases are emitted into the atmosphere, and by increasing the rate at which carbon dioxide is removed from the atmosphere.[270] In order to limit global warming to less than 1.5 °C global greenhouse gas emissions needs to be net-zero by 2050, or by 2070 with a 2 °C target.[88] This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry.[271]
 The United Nations Environment Programme estimates that countries need to triple their pledges under the Paris Agreement within the next decade to limit global warming to 2 °C. An even greater level of reduction is required to meet the 1.5 °C goal.[272] With pledges made under the Paris Agreement as of October 2021, global warming would still have a 66% chance of reaching about 2.7 °C (range: 2.2–3.2 °C) by the end of the century.[21] Globally, limiting warming to 2 °C may result in higher economic benefits than economic costs.[273]
 Although there is no single pathway to limit global warming to 1.5 or 2 °C,[274] most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions.[275] To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in agriculture and forestry,[276] such as preventing deforestation and restoring natural ecosystems by reforestation.[277]
 Other approaches to mitigating climate change have a higher level of risk. Scenarios that limit global warming to 1.5 °C typically project the large-scale use of carbon dioxide removal methods over the 21st century.[278] There are concerns, though, about over-reliance on these technologies, and environmental impacts.[279] Solar radiation modification (SRM) is also a possible supplement to deep reductions in emissions. However, SRM raises significant ethical and legal concerns, and the risks are imperfectly understood.[280]
 Renewable energy is key to limiting climate change.[282] For decades, fossil fuels have accounted for roughly 80% of the world's energy use.[283] The remaining share has been split between nuclear power and renewables (including hydropower, bioenergy, wind and solar power and geothermal energy).[284] Fossil fuel use is expected to peak in absolute terms prior to 2030 and then to decline, with coal use experiencing the sharpest reductions.[285] Renewables represented 75% of all new electricity generation installed in 2019, nearly all solar and wind.[286] Other forms of clean energy, such as nuclear and hydropower, currently have a larger share of the energy supply. However, their future growth forecasts appear limited in comparison.[287]
 While solar panels and onshore wind are now among the cheapest forms of adding new power generation capacity in many locations,[288] green energy policies are needed to achieve a rapid transition from fossil fuels to renewables.[289] To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. Investment in coal would be eliminated and coal use nearly phased out by 2050.[290][291]
 Electricity generated from renewable sources would also need to become the main energy source for heating and transport.[292] Transport can switch away from internal combustion engine vehicles and towards electric vehicles, public transit, and active transport (cycling and walking).[293][294] For shipping and flying, low-carbon fuels would reduce emissions.[293] Heating could be increasingly decarbonised with technologies like heat pumps.[295]
 There are obstacles to the continued rapid growth of clean energy, including renewables. For wind and solar, there are environmental and land use concerns for new projects.[296] Wind and solar also produce energy intermittently and with seasonal variability. Traditionally, hydro dams with reservoirs and conventional power plants have been used when variable energy production is low. Going forward, battery storage can be expanded, energy demand and supply can be matched, and long-distance transmission can smooth variability of renewable outputs.[282] Bioenergy is often not carbon-neutral and may have negative consequences for food security.[297] The growth of nuclear power is constrained by controversy around radioactive waste, nuclear weapon proliferation, and accidents.[298][299] Hydropower growth is limited by the fact that the best sites have been developed, and new projects are confronting increased social and environmental concerns.[300]
 Low-carbon energy improves human health by minimising climate change as well as reducing air pollution deaths,[301] which were estimated at 7 million annually in 2016.[302] Meeting the Paris Agreement goals that limit warming to a 2 °C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5 °C could save millions and simultaneously increase energy security and reduce poverty.[303] Improving air quality also has economic benefits which may be larger than mitigation costs.[304]
 Reducing energy demand is another major aspect of reducing emissions.[305] If less energy is needed, there is more flexibility for clean energy development. It also makes it easier to manage the electricity grid, and minimises carbon-intensive infrastructure development.[306] Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy.[307] Several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.[308]
 Strategies to reduce energy demand vary by sector. In the transport sector, passengers and freight can switch to more efficient travel modes, such as buses and trains, or use electric vehicles.[309] Industrial strategies to reduce energy demand include improving heating systems and motors, designing less energy-intensive products, and increasing product lifetimes.[310] In the building sector the focus is on better design of new buildings, and higher levels of energy efficiency in retrofitting.[311] The use of technologies like heat pumps can also increase building energy efficiency.[312]
  Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand.[313] A set of actions could reduce agriculture and forestry-based emissions by two thirds from 2010 levels. These include reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.[314]
 On the demand side, a key component of reducing emissions is shifting people towards plant-based diets.[315] Eliminating the production of livestock for meat and dairy would eliminate about 3/4ths of all emissions from agriculture and other land use.[316] Livestock also occupy 37% of ice-free land area on Earth and consume feed from the 12% of land area used for crops, driving deforestation and land degradation.[317]
 Steel and cement production are responsible for about 13% of industrial CO2 emissions. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production, so that reducing CO2 emissions requires research into alternative chemistries.[318]
 Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO2 beyond naturally occurring levels.[319] Reforestation and afforestation (planting forests where there were none before) are among the most mature sequestration techniques, although the latter raises food security concerns.[320] Farmers can promote sequestration of carbon in soils through practices such as use of winter cover crops, reducing the intensity and frequency of tillage, and using compost and manure as soil amendments.[321] Forest and landscape restoration yields many benefits for the climate, including greenhouse gas emissions sequestration and reduction.[122] Restoration/recreation of coastal wetlands, prairie plots and seagrass meadows increases the uptake of carbon into organic matter.[322][323] When carbon is sequestered in soils and in organic matter such as trees, there is a risk of the carbon being re-released into the atmosphere later through changes in land use, fire, or other changes in ecosystems.[324]
 Where energy production or CO2-intensive heavy industries continue to produce waste CO2, the gas can be captured and stored instead of released to the atmosphere. Although its current use is limited in scale and expensive,[325] carbon capture and storage (CCS) may be able to play a significant role in limiting CO2 emissions by mid-century.[326] This technique, in combination with bioenergy (BECCS) can result in net negative emissions as CO2 is drawn from the atmosphere.[327] It remains highly uncertain whether carbon dioxide removal techniques will be able to play a large role in limiting warming to 1.5 °C. Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals.[328]
 Adaptation is ""the process of adjustment to current or expected changes in climate and its effects"".[329]: 5  Without additional mitigation, adaptation cannot avert the risk of ""severe, widespread and irreversible"" impacts.[330] More severe climate change requires more transformative adaptation, which can be prohibitively expensive.[331] The capacity and potential for humans to adapt is unevenly distributed across different regions and populations, and developing countries generally have less.[332] The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.[333]
 Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, and building flood controls. If that fails, managed retreat may be needed.[334] There are economic barriers for tackling dangerous heat impact. Avoiding strenuous work or having air conditioning is not possible for everybody.[335] In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control, and genetic improvements for increased tolerance to a changing climate.[336] Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes.[337] Education, migration and early warning systems can reduce climate vulnerability.[338] Planting mangroves or encouraging other coastal vegetation can buffer storms.[339][340]
 Ecosystems adapt to climate change, a process that can be supported by human intervention. By increasing connectivity between ecosystems, species can migrate to more favourable climate conditions. Species can also be introduced to areas acquiring a favorable climate. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest acts as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.[341]
 There are synergies but also trade-offs between adaptation and mitigation.[342] An example for synergy is increased food productivity, which has large benefits for both adaptation and mitigation.[343] An example of a trade-off is that increased use of air conditioning allows people to better cope with heat, but increases energy demand. Another trade-off example is that more compact urban development may reduce emissions from transport and construction, but may also increase the urban heat island effect, exposing people to heat-related health risks.[344]
 Countries that are most vulnerable to climate change have typically been responsible for a small share of global emissions. This raises questions about justice and fairness.[345] Limiting global warming makes it much easier to achieve the UN's Sustainable Development Goals, such as eradicating poverty and reducing inequalities. The connection is recognised in Sustainable Development Goal 13 which is to ""take urgent action to combat climate change and its impacts"".[346] The goals on food, clean water and ecosystem protection have synergies with climate mitigation.[347]
 The geopolitics of climate change is complex. It has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from switching to a low-carbon economy themselves. Sometimes mitigation also has localised benefits though. For instance, the benefits of a coal phase-out to public health and local environments exceed the costs in almost all regions.[348] Furthermore, net importers of fossil fuels win economically from switching to clean energy, causing net exporters to face stranded assets: fossil fuels they cannot sell.[349]
 A wide range of policies, regulations, and laws are being used to reduce emissions. As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions.[350] Carbon can be priced with carbon taxes and emissions trading systems.[351] Direct global fossil fuel subsidies reached $319 billion in 2017, and $5.2 trillion when indirect costs such as air pollution are priced in.[352] Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths.[353] Money saved on fossil subsidies could be used to support the transition to clean energy instead.[354] More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry.[355] Several countries require utilities to increase the share of renewables in power production.[356]
 Policy designed through the lens of climate justice tries to address human rights issues and social inequality. According to proponents of climate justice, the costs of climate adaptation should be paid by those most responsible for climate change, while the beneficiaries of payments should be those suffering impacts. One way this can be addressed in practice is to have wealthy nations pay poorer countries to adapt.[357]
 Oxfam found that in 2023 the wealthiest 10% of people were responsible for 50% of global emissions, while the bottom 50% were responsible for just 8%.[358] Production of emissions is another way to look at responsibility: under that approach, the top 21 fossil fuel companies would owe cumulative climate reparations of $5.4 trillion over the period 2025–2050.[359] To achieve a just transition, people working in the fossil fuel sector would also need other jobs, and their communities would need investments.[360]
 Nearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC).[362] The goal of the UNFCCC is to prevent dangerous human interference with the climate system.[363] As stated in the convention, this requires that greenhouse gas concentrations are stabilised in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained.[364] The UNFCCC does not itself restrict emissions but rather provides a framework for protocols that do. Global emissions have risen since the UNFCCC was signed.[365] Its yearly conferences are the stage of global negotiations.[366]
 The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions.[367] During the negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to ""[take] the lead"" in reducing their emissions,[368] since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere. Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs.[369]
 The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77.[370] Associated parties aimed to limit the global temperature rise to below 2 °C.[371] The Accord set the goal of sending $100 billion per year to developing countries for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund.[372] As of 2020[update], only 83.3 billion were delivered. Only in 2023 the target is expected to be achieved.[373]
 In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 2.0 °C and contains an aspirational goal of keeping warming under 1.5 °C.[374] The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years.[375] The Paris Agreement restated that developing countries must be financially supported.[376] As of October 2021[update], 194 states and the European Union have signed the treaty and 191 states and the EU have ratified or acceded to the agreement.[377]
 The 1987 Montreal Protocol, an international agreement to stop emitting ozone-depleting gases, may have been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so.[378] The 2016 Kigali Amendment to the Montreal Protocol aims to reduce the emissions of hydrofluorocarbons, a group of powerful greenhouse gases which served as a replacement for banned ozone-depleting gases. This made the Montreal Protocol a stronger agreement against climate change.[379]
 In 2019, the United Kingdom parliament became the first national government to declare a climate emergency.[380] Other countries and jurisdictions followed suit.[381] That same year, the European Parliament declared a ""climate and environmental emergency"".[382] The European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050.[383] In 2021, the European Commission released its ""Fit for 55"" legislation package, which contains guidelines for the car industry; all new cars on the European market must be zero-emission vehicles from 2035.[384]
 Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon-neutral by 2050, and China by 2060.[385] While India has strong incentives for renewables, it also plans a significant expansion of coal in the country.[386] Vietnam is among very few coal-dependent, fast-developing countries that pledged to phase out unabated coal power by the 2040s or as soon as possible thereafter.[387]
 As of 2021, based on information from 48 national climate plans, which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5 °C or 2 °C, respectively.[388]
 Public debate about climate change has been strongly affected by climate change denial and misinformation, which originated in the United States and has since spread to other countries, particularly Canada and Australia. Climate change denial has originated from fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists.[390] Like the tobacco industry, the main strategy of these groups has been to manufacture doubt about climate-change related scientific data and results.[391] People who hold unwarranted doubt about climate change are called climate change ""skeptics"", although ""contrarians"" or ""deniers"" are more appropriate terms.[392]
 There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimise the negative impacts of climate change.[393] Manufacturing uncertainty about the science later developed into a manufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community in order to delay policy changes.[394] Strategies to promote these ideas include criticism of scientific institutions,[395] and questioning the motives of individual scientists.[393] An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change.[396]
 Climate change came to international public attention in the late 1980s.[400] Due to media coverage in the early 1990s, people often confused climate change with other environmental issues like ozone depletion.[401] In popular culture, the climate fiction movie The Day After Tomorrow (2004) and the Al Gore documentary An Inconvenient Truth (2006) focused on climate change.[400]
 Significant regional, gender, age and political differences exist in both public concern for, and understanding of, climate change. More highly educated people, and in some countries, women and younger people, were more likely to see climate change as a serious threat.[402] Partisan gaps also exist in many countries,[403] and countries with high CO2 emissions tend to be less concerned.[404] Views on causes of climate change vary widely between countries.[405] Concern has increased over time,[403] to the point where in 2021 a majority of citizens in many countries express a high level of worry about climate change, or view it as a global emergency.[406] Higher levels of worry are associated with stronger public support for policies that address climate change.[407]
 Climate protests demand that political leaders take action to prevent climate change. They can take the form of public demonstrations, fossil fuel divestment, lawsuits and other activities.[408] Prominent demonstrations include the School Strike for Climate. In this initiative, young people across the globe have been protesting since 2018 by skipping school on Fridays, inspired by Swedish teenager Greta Thunberg.[409] Mass civil disobedience actions by groups like Extinction Rebellion have protested by disrupting roads and public transport.[410]
 Litigation is increasingly used as a tool to strengthen climate action from public institutions and companies. Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change.[411] Lawsuits against fossil-fuel companies generally seek compensation for loss and damage.[412]
 Scientists in the 19th century such as Alexander von Humboldt began to foresee the effects of climate change.[414][415][416][417] In the 1820s, Joseph Fourier proposed the greenhouse effect to explain why Earth's temperature was higher than the Sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so sunlight reaches the surface where it is converted to heat. However, the atmosphere is not transparent to heat radiating from the surface, and captures some of that heat, which in turn warms the planet.[418]
 In 1856 Eunice Newton Foote demonstrated that the warming effect of the Sun is greater for air with water vapour than for dry air, and that the effect is even greater with carbon dioxide (CO2). She concluded that ""An atmosphere of that gas would give to our earth a high temperature...""[419][420]
 Starting in 1859,[421] John Tyndall established that nitrogen and oxygen—together totaling 99% of dry air—are transparent to radiated heat. However, water vapour and gases such as methane and carbon dioxide absorb radiated heat and re-radiate that heat into the atmosphere. Tyndall proposed that changes in the concentrations of these gases may have caused climatic changes in the past, including ice ages.[422]
 Svante Arrhenius noted that water vapour in air continuously varied, but the CO2 concentration in air was influenced by long-term geological processes. Warming from increased CO2 levels would increase the amount of water vapour, amplifying warming in a positive feedback loop. In 1896, he published the first climate model of its kind, projecting that halving CO2 levels could have produced a drop in temperature initiating an ice age. Arrhenius calculated the temperature increase expected from doubling CO2 to be around 5–6 °C.[423] Other scientists were initially skeptical and believed that the greenhouse effect was saturated so that adding more CO2 would make no difference, and that the climate would be self-regulating.[424] Beginning in 1938, Guy Stewart Callendar published evidence that climate was warming and CO2 levels were rising,[425] but his calculations met the same objections.[424]
 In the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum. This model predicted that increasing CO2 levels would cause warming. Around the same time, Hans Suess found evidence that CO2 levels had been rising, and Roger Revelle showed that the oceans would not absorb the increase. The two scientists subsequently helped Charles Keeling to begin a record of continued increase, which has been termed the ""Keeling Curve"".[424] Scientists alerted the public,[430] and the dangers were highlighted at James Hansen's 1988 Congressional testimony.[32] The Intergovernmental Panel on Climate Change (IPCC), set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research.[431] As part of the IPCC reports, scientists assess the scientific discussion that takes place in peer-reviewed journal articles.[432]
 There is a near-complete scientific consensus that the climate is warming and that this is caused by human activities. As of 2019, agreement in recent literature reached over 99%.[427][428] No scientific body of national or international standing disagrees with this view.[433] Consensus has further developed that some form of action should be taken to protect people against the impacts of climate change. National science academies have called on world leaders to cut global emissions.[434] The 2021 IPCC Assessment Report stated that it is ""unequivocal"" that climate change is caused by humans.[428]
  This article incorporates text from a free content work.  Licensed under CC BY-SA 3.0 (license statement/permission). Text taken from The status of women in agrifood systems – Overview​,  FAO, FAO.  
 Fourth Assessment Report
 Fifth Assessment report
 
 Special Report: Global Warming of 1.5 °C
 Special Report: Climate change and Land
 Special Report: The Ocean and Cryosphere in a Changing Climate
 Sixth Assessment Report
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['climate change', 'The United Nations Environment Programme', 'climate change is caused by humans', 'woody plant encroachment', 'unclear'], 'answer_start': [], 'answer_end': []}"
"
 Sustainable development is an approach to growth and human development that aims to meet the needs of the present without compromising the ability of future generations to meet their own needs.[1][2] The aim is to have a society where living conditions and resources meet human needs without undermining planetary integrity.[3][4] Sustainable development aims to balance the needs of the economy, environment, and social well-being. The Brundtland Report in 1987 helped to make the concept of sustainable development better known. 
 Sustainable development overlaps with the idea of sustainability which is a normative concept.[5] UNESCO formulated a distinction between the two concepts as follows: ""Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it.""[6] There are some problems with the concept of sustainable development. Some scholars say it is an oxymoron because according to them, development is inherently unsustainable. Others commentators are disappointed in the lack of progress that has been achieved so far.[7][8] Part of the problem is that development itself is not consistently defined.[9]: 16 
 
The Rio Process that began at the 1992 Earth Summit in Rio de Janeiro has placed the concept of sustainable development on the international agenda. In 2015 the United Nations General Assembly (UNGA) adopted the Sustainable Development Goals for the year 2030. These development goals address the global challenges, including for example poverty, climate change, biodiversity loss, and peace. In 1987, the United Nations World Commission on Environment and Development released the report Our Common Future, commonly called the Brundtland Report.[1] The report included a definition of ""sustainable development"" which is now widely used:[1][10]
 Sustainable development is a development that meets the needs of the present without compromising the ability of future generations to meet their own needs. It contains two key concepts within it:
 Sustainable development thus tries to find a balance between economic development, environmental protection, and social well-being.
 Sustainable development has its roots in ideas regarding sustainable forest management, which were developed in Europe during the 17th and 18th centuries.[18][19][20] In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued, in his 1662 essay Sylva, that ""sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over- exploitation of natural resources."" In 1713, Hans Carl von Carlowitz, a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published Sylvicultura economics, a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert, von Carlowitz developed the concept of managing forests for sustained yield.[18] His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig, eventually leading to the development of the science of forestry. This, in turn, influenced people like Gifford Pinchot, the first head of the US Forest Service, whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s.[18][19]
 Following the publication of Rachel Carson's Silent Spring in 1962, the developing environmental movement drew attention to the relationship between economic growth and environmental degradation. Kenneth E. Boulding, in his influential 1966 essay The Economics of the Coming Spaceship Earth, identified the need for the economic system to fit itself to the ecological system with its limited pools of resources.[19] Another milestone was the 1968 article by Garrett Hardin that popularized the term ""tragedy of the commons"".[21]
 The direct linking of sustainability and development in a contemporary sense can be traced to the early 1970s. ""Strategy of Progress"", a 1972 book (in German) by Ernst Basler, explained how the long-acknowledged sustainability concept of preserving forests for future wood production can be directly transferred to the broader importance of preserving environmental resources to sustain the world for future generations.[22] That same year, the interrelationship of environment and development was formally demonstrated in a systems dynamic simulation model reported in the classic report on Limits to Growth. This was commissioned by the Club of Rome and written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology. Describing the desirable ""state of global equilibrium"", the authors wrote: ""We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people.""[23] The year 1972 also saw the publication of the influential book, A Blueprint for Survival.[24][25]
 In 1975, an MIT research group prepared ten days of hearings on ""Growth and Its Implication for the Future"" for the US Congress, the first hearings ever held on sustainable development.[26]
 In 1980, the International Union for Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority[27] and introduced the term ""sustainable development"".[28]: 4  Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged.[29]
 Since the Brundtland Report, the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of ""socially inclusive and environmentally sustainable economic growth"".[28]: 5  In 1992, the UN Conference on Environment and Development published the Earth Charter, which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognizes these interdependent pillars. Furthermore, Agenda 21 emphasizes that broad public participation in decision-making is a fundamental prerequisite for achieving sustainable development.[30]
 The Rio Protocol was a huge leap forward: for the first time, the world agreed on a sustainability agenda. In fact, a global consensus was facilitated by neglecting concrete goals and operational details. The Sustainable Development Goals (SDGs) now have concrete targets (unlike the results from the Rio Process) but no methods for sanctions.[31][9]: 137 
 Sustainable development, like sustainability, is regarded to have three dimensions: the environment, economy and society. The idea is that a good balance between the three dimensions should be achieved. Instead of calling them dimensions, other terms commonly used are pillars, domains, aspects, spheres.
 Scholars usually distinguish three different areas of sustainability. These are the environmental, the social, and the economic. Several terms are in use for this concept. Authors may speak of three pillars, dimensions, components, aspects,[32] perspectives, factors, or goals. All mean the same thing in this context.[11] The three dimensions paradigm has few theoretical foundations. It emerged without a single point of origin.[11][33] Scholars rarely question the distinction itself. The idea of sustainability with three dimensions is a dominant interpretation in the literature.[11]
 Countries could develop systems for monitoring and evaluation of progress towards achieving sustainable development by adopting indicators that measure changes across economic, social and environmental dimensions. Six interdependent capacities are deemed to be necessary for the successful pursuit of sustainable development.[35] These are the capacities to measure progress towards sustainable development; promote equity within and between generations; adapt to shocks and surprises; transform the system onto more sustainable development pathways; link knowledge with action for sustainability; and to devise governance arrangements that allow people to work together.
 Environmental sustainability concerns the natural environment and how it endures and remains diverse and productive. Since natural resources are derived from the environment, the state of air, water, and climate is of particular concern. Environmental sustainability requires society to design activities to meet human needs while preserving the life support systems of the planet. This, for example, entails using water sustainably, using renewable energy and sustainable material supplies (e.g. harvesting wood from forests at a rate that maintains the biomass and biodiversity).[36]
 An unsustainable situation occurs when natural capital (the total of nature's resources) is used up faster than it can be replenished.[37]: 58  Sustainability requires that human activity only uses nature's resources at a rate at which they can be replenished naturally. The concept of sustainable development is intertwined with the concept of carrying capacity. Theoretically, the long-term result of environmental degradation is the inability to sustain human life.[37]
 Important operational principles of sustainable development were published by Herman Daly in 1990: renewable resources should provide a sustainable yield (the rate of harvest should not exceed the rate of regeneration); for non-renewable resources there should be equivalent development of renewable substitutes; waste generation should not exceed the assimilative capacity of the environment.[38]
 Environmental problems associated with industrial agriculture and agribusiness are now being addressed through approaches such as sustainable agriculture, organic farming and more sustainable business practices.[39] The most cost-effective climate change mitigation options include afforestation, sustainable forest management, and reducing deforestation.[40] At the local level there are various movements working towards sustainable food systems which may include less meat consumption, local food production, slow food, sustainable gardening, and organic gardening.[41] The environmental effects of different dietary patterns depend on many factors, including the proportion of animal and plant foods consumed and the method of food production.[42][43]
 As global population and affluence have increased, so has the use of various materials increased in volume, diversity, and distance transported. Included here are raw materials, minerals, synthetic chemicals (including hazardous substances), manufactured products, food, living organisms, and waste.[44] By 2050, humanity could consume an estimated 140 billion tons of minerals, ores, fossil fuels and biomass per year (three times its current amount) unless the economic growth rate is decoupled from the rate of natural resource consumption. Developed countries' citizens consume an average of 16 tons of those four key resources per capita per year, ranging up to 40 or more tons per person in some developed countries with resource consumption levels far beyond what is likely sustainable. By comparison, the average person in India today consumes four tons per year.[45]
 Sustainable use of materials has targeted the idea of dematerialization, converting the linear path of materials (extraction, use, disposal in landfill) to a circular material flow that reuses materials as much as possible, much like the cycling and reuse of waste in nature.[46] Dematerialization is being encouraged through the ideas of industrial ecology, eco design[47] and ecolabelling.
 This way of thinking is expressed in the concept of circular economy, which employs reuse, sharing, repair, refurbishment, remanufacturing and recycling to create a closed-loop system, minimizing the use of resource inputs and the creation of waste, pollution and carbon emissions.[48] Building electric vehicles has been one of the most popular ways in the field of sustainable development, the potential of using reusable energy and reducing waste offered a perspective in sustainable development.[49] The European Commission has adopted an ambitious Circular Economy Action Plan in 2020, which aims at making sustainable products the norm in the EU.[50][51]
 There is a connection between ecosystems and biodiversity. Ecosystems are made up of various living things interacting with one another and their surroundings. Along with this, biodiversity lays the groundwork for ecosystems to function well by defining the kinds of species that can coexist in an environment, as well as their functions and interactions with other species.[52][53] In 2019, a summary for policymakers of the largest, most comprehensive study to date of biodiversity and ecosystem services was published by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services. It recommended that human civilization will need a transformative change, including sustainable agriculture, reductions in consumption and waste, fishing quotas and collaborative water management.[54][55] Biodiversity is not only crucial for the well-being of animals and wildlife but also plays a positive role in the lives of human beings in the way in which it aids development of human life.[56]
 The environmental impact of a community or humankind as a whole depends both on population and impact per person, which in turn depends in complex ways on what resources are being used, whether or not those resources are renewable, and the scale of the human activity relative to the carrying capacity of the ecosystems involved.[57] Careful resource management can be applied at many scales, from economic sectors like agriculture, manufacturing and industry, to work organizations, the consumption patterns of households and individuals, and the resource demands of individual goods and services.[58][59]
 The underlying driver of direct human impacts on the environment is human consumption.[60] This impact is reduced by not only consuming less but also making the full cycle of production, use, and disposal more sustainable. Consumption of goods and services can be analyzed and managed at all scales through the chain of consumption, starting with the effects of individual lifestyle choices and spending patterns, through to the resource demands of specific goods and services, the impacts of economic sectors, through national economies to the global economy.[61] Key resource categories relating to human needs are food, energy, raw materials and water.
 It has been suggested that because of rural poverty and overexploitation, environmental resources should be treated as important economic assets, called natural capital.[62] Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption.[63] ""Growth"" generally ignores the direct effect that the environment may have on social welfare, whereas ""development"" takes it into account.[64]
 As early as the 1970s, the concept of sustainability was used to describe an economy ""in equilibrium with basic ecological support systems"".[65] Scientists in many fields have highlighted The Limits to Growth,[66][67] and economists have presented alternatives, for example a 'steady-state economy', to address concerns over the impacts of expanding human development on the planet.[68] In 1987, the economist Edward Barbier published the study The Concept of Sustainable Economic Development, where he recognized that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other.[69]
 A World Bank study from 1999 concluded that based on the theory of genuine savings (defined as ""traditional net savings less the value of resource depletion and environmental degradation plus the value of investment in human capital""), policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental.[70] Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule[clarification needed] steady state.[71][72][73][74]
 A meta review in 2002 looked at environmental and economic valuations and found a ""lack of concrete understanding of what ""sustainability policies"" might entail in practice"".[75] A study concluded in 2007 that knowledge, manufactured and human capital (health and education) has not compensated for the degradation of natural capital in many parts of the world.[76] It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics.[77]
 The World Business Council for Sustainable Development published a Vision 2050 document in 2021 to show ""How business can lead the transformations the world needs"". The vision states that ""we envision a world in which 9+billion people can live well, within planetary boundaries, by 2050.""[78] This report was highlighted by The Guardian as ""the largest concerted corporate sustainability action plan to date – include reversing the damage done to ecosystems, addressing rising greenhouse gas emissions and ensuring societies move to sustainable agriculture.""[79]
 There are many reasons why sustainability is so difficult to achieve. These reasons have the name sustainability barriers.[15][80] Before addressing these barriers it is important to analyze and understand them.[15]: 34  Some barriers arise from nature and its complexity (""everything is related"").[81] Others arise from the human condition. One example is the value-action gap. This reflects the fact that people often do not act according to their convictions. Experts describe these barriers as intrinsic to the concept of sustainability.[82]: 81 
 Other barriers are extrinsic to the concept of sustainability. This means it is possible to overcome them. One way would be to put a price tag on the consumption of public goods.[82]: 84  Some extrinsic barriers relate to the nature of dominant institutional frameworks. Examples would be where market mechanisms fail for public goods. Existing societies, economies, and cultures encourage increased consumption. There is a structural imperative for growth in competitive market economies. This inhibits necessary societal change.[83]
 Furthermore, there are several barriers related to the difficulties of implementing sustainability policies. There are trade-offs between the goals of environmental policies and economic development. Environmental goals include nature conservation. Development may focus on poverty reduction.[80][15]: 65  There are also trade-offs between short-term profit and long-term viability.[82]: 65  Political pressures generally favor the short term over the long term. So they form a barrier to actions oriented toward improving sustainability.[82]: 86 
 The concept of sustainable development has been and still is, subject to criticism, including the question of what is to be sustained in sustainable development. It has been argued that there is no such thing as sustainable use of a non-renewable resource, since any positive rate of exploitation will eventually lead to the exhaustion of earth's finite stock;[84]: 13  this perspective renders the Industrial Revolution as a whole unsustainable.[85]: 20f [86]: 61–67 [68]: 22f 
 The sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible.[87] Natural capital can not necessarily be substituted by economic capital.[68] While it is possible that we can find ways to replace some natural resources, it is much less likely that they will ever be able to replace ecosystem services, such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest.
 The concept of sustainable development has been criticized from different angles. While some see it as paradoxical (or an oxymoron) and regard development as inherently unsustainable, others are disappointed in the lack of progress that has been achieved so far.[7][8] Part of the problem is that ""development"" itself is not consistently defined.[9]: 16 [88] Such a viewpoint contradicts the mainstream academic community, which frequently concedes that the processes of capitalism are incompatible with the long-term sustainability of human life.
 The vagueness of the Brundtland definition of sustainable development has been criticized as follows:[9]: 17  The definition has ""opened up the possibility of downplaying sustainability. Hence, governments spread the message that we can have it all at the same time, i.e. economic growth, prospering societies and a healthy environment. No new ethic is required. This so-called weak version of sustainability is popular among governments, and businesses, but profoundly wrong and not even weak, as there is no alternative to preserving the earth's ecological integrity.""[89]: 2 
 The 2030 Agenda for Sustainable Development, adopted by all United Nations members in 2015, created 17 world Sustainable Development Goals (SDGs). They were created with the aim of ""peace and prosperity for people and the planet...""[90][91][92]  – while tackling climate change and working to preserve oceans and forests. The SDGs highlight the connections between the environmental, social and economic aspects of sustainable development. Sustainability is at the center of the SDGs.[93][94]
 The short titles of the 17 SDGs are: No poverty (SDG 1), Zero hunger (SDG 2), Good health and well-being (SDG 3), Quality education (SDG 4), Gender equality (SDG 5), Clean water and sanitation (SDG 6), Affordable and clean energy (SDG 7), Decent work and economic growth (SDG 8), Industry, innovation and infrastructure (SDG 9), Reduced inequalities (SDG 10), Sustainable cities and communities (SDG 11), Responsible consumption and production (SDG 12), Climate action (SDG 13), Life below water (SDG 14), Life on land (SDG 15), Peace, justice, and strong institutions (SDG 16), and Partnerships for the goals (SDG 17).
 Education for sustainable development (ESD) is a term officially used by the United Nations and is defined as education practices that encourage changes in knowledge, skills, values and attitudes to enable a more sustainable and just society for humanity. ESD aims to empower and equip current and future generations to meet their needs using a balanced and integrated approach to the economic, social and environmental dimensions of sustainable development.[96][97]
 Agenda 21 was the first international document that identified education as an essential tool for achieving sustainable development and highlighted areas of action for education.[98][99] ESD is a component of measurement in an indicator for Sustainable Development Goal 12 (SDG) for ""responsible consumption and production"". SDG 12 has 11 targets and target 12.8 is ""By 2030, ensure that people everywhere have the relevant information and awareness for sustainable development and lifestyles in harmony with nature.""[100] 20 years after the Agenda 21 document was declared, the 'Future we want' document was declared in the Rio+20 UN Conference on Sustainable Development, stating that ""We resolve to promote education for sustainable development and to integrate sustainable development more actively into education beyond the Decade of Education for Sustainable Development.""[101]
 One version of education for Sustainable Development recognizes modern-day environmental challenges and seeks to define new ways to adjust to a changing biosphere, as well as engage individuals to address societal issues that come with them [102] In the International Encyclopedia of Education, this approach to education is seen as an attempt to ""shift consciousness toward an ethics of life-giving relationships that respects the interconnectedness of man to his natural world"" in order to equip future members of society with environmental awareness and a sense of responsibility to sustainability.[103]
 For UNESCO, education for sustainable development involves:
 integrating key sustainable development issues into teaching and learning. This may include, for example, instruction about climate change, disaster risk reduction, biodiversity, and poverty reduction and sustainable consumption. It also requires participatory teaching and learning methods that motivate and empower learners to change their behaviours and take action for sustainable development. ESD consequently promotes competencies like critical thinking, imagining future scenarios and making decisions in a collaborative way.[104][105] The Thessaloniki Declaration, presented at the ""International Conference on Environment and Society: Education and Public Awareness for Sustainability"" by UNESCO and the Government of Greece (December 1997), highlights the importance of sustainability not only with regards to the natural environment, but also with ""poverty, health, food security, democracy, human rights, and peace"".[106]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['biodiversity', 'UNESCO and the Government of Greece', 'sustainability', 'disaster risk reduction, biodiversity, and poverty reduction and sustainable consumption', 'adapt to shocks and surprises'], 'answer_start': [], 'answer_end': []}"
"
 Urban planning, also known as town planning, city planning, regional planning, or rural planning in specific contexts, is a technical and political process that is focused on the development and design of land use and the built environment, including air, water, and the infrastructure passing into and out of urban areas, such as transportation, communications, and distribution networks, and their accessibility.[1] Traditionally, urban planning followed a top-down approach in master planning the physical layout of human settlements.[2] The primary concern was the public welfare,[1][2] which included considerations of efficiency, sanitation, protection and use of the environment,[1] as well as effects of the master plans on the social and economic activities.[3] Over time, urban planning has adopted a focus on the social and environmental bottom lines that focus on planning as a tool to improve the health and well-being of people, maintaining sustainability standards. Similarly, in the early 21st century, Jane Jacobs's writings on legal and political perspectives to emphasize the interests of residents, businesses and communities effectively influenced urban planners to take into broader consideration of resident experiences and needs while planning.
 Urban planning answers questions about how people will live, work, and play in a given area and thus, guides orderly development in urban, suburban and rural areas.[4] Although predominantly concerned with the planning of settlements and communities, urban planners are also responsible for planning the efficient transportation of goods, resources, people, and waste; the distribution of basic necessities such as water and electricity; a sense of inclusion and opportunity for people of all kinds, culture and needs; economic growth or business development; improving health and conserving areas of natural environmental significance that actively contributes to reduction in CO2 emissions[5] as well as protecting heritage structures and built environments. Since most urban planning teams consist of highly educated individuals that work for city governments,[6] recent debates focus on how to involve more community members in city planning processes.
 Urban planning is an interdisciplinary field that includes civil engineering, architecture, human geography, politics, social science and design sciences. Practitioners of urban planning are concerned with research and analysis, strategic thinking, engineering architecture, urban design, public consultation, policy recommendations, implementation and management.[2] It is closely related to the field of urban design and some urban planners provide designs for streets, parks, buildings and other urban areas.[7] Urban planners work with the cognate fields of civil engineering, landscape architecture, architecture, and public administration to achieve strategic, policy and sustainability goals. Early urban planners were often members of these cognate fields though today, urban planning is a separate, independent professional discipline. The discipline of urban planning is the broader category that includes different sub-fields such as land-use planning, zoning, economic development, environmental planning, and transportation planning.[8] Creating the plans requires a thorough understanding of penal codes and zonal codes of planning.
 Another important aspect of urban planning is that the range of urban planning projects include the large-scale master planning of empty sites or Greenfield projects as well as small-scale interventions and refurbishments of existing structures, buildings and public spaces. Pierre Charles L'Enfant in Washington, D.C., Daniel Burnham in Chicago, Lúcio Costa in Brasília and Georges-Eugene Haussmann in Paris planned cities from scratch, and Robert Moses and Le Corbusier refurbished and transformed cities and neighborhoods to meet their ideas of urban planning.[9]
 There is evidence of urban planning and designed communities dating back to the Mesopotamian, Indus Valley, Minoan, and Egyptian civilizations in the third millennium BCE. Archaeologists studying the ruins of cities in these areas find paved streets that were laid out at right angles in a grid pattern.[11] The idea of a planned out urban area evolved as different civilizations adopted it. Beginning in the 8th century BCE, Greek city states primarily used orthogonal (or grid-like) plans.[12] Hippodamus of Miletus (498–408 BC), the ancient Greek architect and urban planner, is considered to be ""the father of European urban planning"", and the namesake of the ""Hippodamian plan"" (grid plan) of city layout.[13]
 The ancient Romans also used orthogonal plans for their cities. City planning in the Roman world was developed for military defense and public convenience. The spread of the Roman Empire subsequently spread the ideas of urban planning. As the Roman Empire declined, these ideas slowly disappeared. However, many cities in Europe still held onto the planned Roman city center. Cities in Europe from the 9th to 14th centuries, often grew organically and sometimes chaotically. But in the following centuries with the coming of the Renaissance many new cities were enlarged with newly planned extensions.[14] From the 15th century on, much more is recorded of urban design and the people that were involved. In this period, theoretical treatises on architecture and urban planning start to appear in which theoretical questions around planning the main lines, ensuring plans meet the needs of the given population and so forth are addressed and designs of towns and cities are described and depicted. During the Enlightenment period, several European rulers ambitiously attempted to redesign capital cities. During the Second French Empire, Baron Georges-Eugène Haussmann, under the direction of Napoleon III, redesigned the city of Paris into a more modern capital, with long, straight, wide boulevards.[15]
 Planning and architecture went through a paradigm shift at the turn of the 20th century. The industrialized cities of the 19th century grew at a tremendous rate. The evils of urban life for the working poor were becoming increasingly evident as a matter of public concern. The laissez-faire style of government management of the economy, in fashion for most of the Victorian era, was starting to give way to a New Liberalism that championed intervention on the part of the poor and disadvantaged. Around 1900, theorists began developing urban planning models to mitigate the consequences of the industrial age, by providing citizens, especially factory workers, with healthier environments. The following century would therefore be globally dominated by a central planning approach to urban planning, not representing an increment in the overall quality of the urban realm.
 At the beginning of the 20th century, urban planning began to be recognized as a separate profession. The Town and Country Planning Association was founded in 1899 and the first academic course in Great Britain on urban planning was offered by the University of Liverpool in 1909.[16] In the 1920s, the ideas of modernism and uniformity began to surface in urban planning, and lasted until the 1970s. In 1933, Le Corbusier presented the Radiant City, a city that grows up in the form of towers, as a solution to the problem of pollution and over-crowding. But many planners started to believe that the ideas of modernism in urban planning led to higher crime rates and social problems.[3][17]
 In the second half of the 20th century, urban planners gradually shifted their focus to individualism and diversity in urban centers.[18]
 Urban planners studying the effects of increasing congestion in urban areas began to address the externalities, the negative impacts caused by induced demand from larger highway systems in western countries such as in the United States. The United Nations Department of Economic and Social Affairs predicted in 2018 that around 2.5 billion more people occupy urban areas by 2050 according to population elements of global migration. New planning theories have adopted non-traditional concepts such as Blue Zones and Innovation Districts to incorporate geographic areas within the city that allow for novel business development and the prioritization of infrastructure that would assist with improving the quality of life of citizens by extending their potential lifespan.
 Planning practices have incorporated policy changes to help address anthropocentric global climate change. London began to charge a congestion charge for cars trying to access already crowded places in the city.[19] Cities nowadays stress the importance of public transit and cycling by adopting such policies.
 Planning theory is the body of scientific concepts, definitions, behavioral relationships, and assumptions that define the body of knowledge of urban planning. There are eight procedural theories of planning that remain the principal theories of planning procedure today: the rational-comprehensive approach, the incremental approach, the transactive approach, the communicative approach, the advocacy approach, the equity approach, the radical approach, and the humanist or phenomenological approach.[20] Some other conceptual planning theories include Ebenezer Howard's The Three Magnets theory that he envisioned for the future of British settlement, also his Garden Cities, the Concentric Model Zone also called the Burgess Model by sociologist Ernest Burgess, the Radburn Superblock that encourages pedestrian movement, the Sector Model and the Multiple Nuclei Model among others.[21]
 Participatory planning is an urban planning approach that involves the entire community in the planning process. Participatory planning in the United States emerged during the 1960s and 1970s.[22]
 Technical aspects of urban planning involve the application of scientific, technical processes, considerations and features that are involved in planning for land use, urban design, natural resources, transportation, and infrastructure. Urban planning includes techniques such as: predicting population growth, zoning, geographic mapping and analysis, analyzing park space, surveying the water supply, identifying transportation patterns, recognizing food supply demands, allocating healthcare and social services, and analyzing the impact of land use.
 In order to predict how cities will develop and estimate the effects of their interventions, planners use various models. These models can be used to indicate relationships and patterns in demographic, geographic, and economic data. They might deal with short-term issues such as how people move through cities, or long-term issues such as land use and growth.[23] One such model is the Geographic Information System (GIS) that is used to create a model of the existing planning and then to project future impacts on the society, economy and environment.
 Building codes and other regulations dovetail with urban planning by governing how cities are constructed and used from the individual level.[24] Enforcement methodologies include governmental zoning, planning permissions, and building codes,[1] as well as private easements and restrictive covenants.[25]
 With recent advances in information and communication technologies and the Internet of Things, an increasing number of cities are adopting technologies such as crowdsorced mobile phone sensing and machine learning to collect data and extract useful information to help make informed urban planning decisions. [26]
 An urban planner is a professional who works in the field of urban planning for the purpose of optimizing the effectiveness of a community's land use and infrastructure. They formulate plans for the development and management of urban and suburban areas. They typically analyze land use compatibility as well as economic, environmental, and social trends. In developing any plan for a community (whether commercial, residential, agricultural, natural or recreational), urban planners must consider a wide array of issues including sustainability, existing and potential pollution, transport including potential congestion, crime, land values, economic development, social equity, zoning codes, and other legislation.
 The importance of the urban planner is increasing in the 21st century, as modern society begins to face issues of increased population growth, climate change and unsustainable development.[27][28] An urban planner could be considered a green collar professional.[29]
 Some researchers suggest that urban planners, globally, work in different ""planning cultures"", adapted to their cities and cultures.[30] However, professionals have identified skills, abilities, and basic knowledge sets that are common to urban planners across regional and national boundaries.[31][32][33]
 The school of neoclassical economics argues that planning is unnecessary, or even harmful, as it market efficiency allows for effective land use.[34] A pluralist strain of political thinking argues in a similar vein that the government should not intrude in the political competition between different interest groups which decides how land is used.[34] The traditional justification for urban planning has in response been that the planner does to the city what the engineer or architect does to the home, that is, make it more amenable to the needs and preferences of its inhabitants.[34]
 The widely adopted consensus-building model of planning, which seeks to accommodate different preferences within the community has been criticized for being based upon, rather than challenging, the power structures of the community.[35] Instead, agonism has been proposed as a framework for urban planning decision-making.[35]
 Another debate within the urban planning field is about who is included and excluded in the urban planning decision-making process. Most urban planning processes use a top-down approach which fails to include the residents of the places where urban planners and city officials are working. Sherry Arnstein's ""ladder of citizen participation"" is oftentimes used by many urban planners and city governments to determine the degree of inclusivity or exclusivity of their urban planning.[36] One main source of engagement between city officials and residents are city council meetings that are open to the residents and that welcome public comments. Additionally, in USA there are some federal requirements for citizen participation in government-funded infrastructure projects.[6]
 Participatory urban planning has been criticized for contributing to the housing crisis in parts of the world.[37]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['public welfare', 'designs of towns and cities', 'improving health and conserving areas of natural environmental significance', 'city council meetings', 'considerations of efficiency, sanitation, protection and use of the environment'], 'answer_start': [], 'answer_end': []}"
"Other reasons this message may be displayed:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['this message may be displayed:', 'this message may be displayed:', 'this message may be displayed', 'this message may be displayed:', 'reasons'], 'answer_start': [], 'answer_end': []}"
"
 Space exploration is the use of astronomy and space technology to explore outer space.[1] While the exploration of space is currently carried out mainly by astronomers with telescopes, its physical exploration is conducted both by uncrewed robotic space probes and human spaceflight. Space exploration, like its classical form astronomy, is one of the main sources for space science.
 While the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical space exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries.[2]
 The early era of space exploration was driven by a ""Space Race"" between the Soviet Union and the United States. A driving force of the start of space exploration was during the Cold War. After the ability to create nuclear weapons, the narrative of defense/offense left land and the power to control the air became the focus. Both the Soviet and the U.S. were fighting to prove their superiority in technology through exploring the unknown: space. In fact, the reason NASA was made was due to the response of Sputnik I.[3] The launch of the first human-made object to orbit Earth, the Soviet Union's Sputnik 1, on 4 October 1957, and the first Moon landing by the American Apollo 11 mission on 20 July 1969 are often taken as landmarks for this initial period. The Soviet space program achieved many of the first milestones, including the first living being in orbit in 1957, the first human spaceflight (Yuri Gagarin aboard Vostok 1) in 1961, the first spacewalk (by Alexei Leonov) on 18 March 1965, the first automatic landing on another celestial body in 1966, and the launch of the first space station (Salyut 1) in 1971. 
After the first 20 years of exploration, focus shifted from one-off flights to renewable hardware, such as the Space Shuttle program, and from competition to cooperation as with the International Space Station (ISS).
 With the substantial completion of the ISS[4] following STS-133 in March 2011, plans for space exploration by the U.S. remained in flux. Constellation, a Bush administration program for a return to the Moon by 2020[5] was judged unrealistic by an expert review panel reporting in 2009.[6] 
The Obama administration proposed a revised Constellation in 2010 to focus on crewed missions beyond low Earth orbit (LEO), extending the operation of the ISS beyond 2020, transferring development of crewed launch vehicles to the private sector, and developing technology for missions  beyond LEO.[7] Constellation ultimately was replaced with the Artemis Program, of which the first mission occurred in 2022, with a planned crewed landing to occur with Artemis 3.[8]
 In the 2000s, China initiated a successful crewed spaceflight program while India launched the Chandrayaan programme, while the European Union and Japan have also planned future crewed space missions. The two primary global programs gaining traction in the 2020s are the Chinese-led International Lunar Research Station and the US-led Artemis Program, with its plan to build the Lunar Gateway, each having its own set of international partners.
 The first telescope is said to have been invented in 1608 in the Netherlands by an eyeglass maker named Hans Lippershey, but their first recorded use in astronomy was by Galileo Galilei in 1609.[9] In 1668 Isaac Newton built his own reflecting telescope, the first fully functional telescope of this kind, and a landmark for future developments due to its superior features over the previous Galilean telescope.[10]
 A string of discoveries in the Solar System (and beyond) followed, then and in the next centuries: the mountains of the Moon, the phases of Venus, the main satellites of Jupiter and Saturn, the rings of Saturn, many comets, the asteroids, the new planets Uranus and Neptune, and many more satellites.
 The Orbiting Astronomical Observatory 2 was the first space telescope launched 1968,[11] but the launching of Hubble Space Telescope in 1990[12] set a milestone. As of 1 December 2022, there were 5,284 confirmed exoplanets discovered. The Milky Way is estimated to contain 100–400 billion stars[13] and more than 100 billion planets.[14] There are at least 2 trillion galaxies in the observable universe.[15][16] HD1 is the most distant known object from Earth, reported as 33.4 billion light-years away.[17][18][19][20][21][22]
 MW 18014 was a German V-2 rocket test launch that took place on 20 June 1944, at the Peenemünde Army Research Center in Peenemünde. It was the first human-made object to reach outer space, attaining an apogee of 176 kilometers,[23] which is well above the Kármán line.[24] It was a vertical test launch. Although the rocket reached space, it did not reach orbital velocity, and therefore returned to Earth in an impact, becoming the first sub-orbital spaceflight.[25]
 The first successful orbital launch was of the Soviet uncrewed Sputnik 1 (""Satellite 1"") mission on 4 October 1957. The satellite weighed about 83 kg (183 lb), and is believed to have orbited Earth at a height of about 250 km (160 mi). It had two radio transmitters (20 and 40 MHz), which emitted ""beeps"" that could be heard by radios around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data was encoded in the duration of radio beeps. The results indicated that the satellite was not punctured by a meteoroid. Sputnik 1 was launched by an R-7 rocket. It burned up upon re-entry on 3 January 1958.
 The first successful human spaceflight was Vostok 1 (""East 1""), carrying the 27-year-old Russian cosmonaut, Yuri Gagarin, on 12 April 1961. The spacecraft completed one orbit around the globe, lasting about 1 hour and 48 minutes. Gagarin's flight resonated around the world; it was a demonstration of the advanced Soviet space program and it opened an entirely new era in space exploration: human spaceflight.
 The first artificial object to reach another celestial body was Luna 2 reaching the Moon in 1959.[26] The first soft landing on another celestial body was performed by Luna 9 landing on the Moon on 3 February 1966.[27] Luna 10 became the first artificial satellite of the Moon, entering in a lunar orbit on 3 April 1966.[28]
 The first crewed landing on another celestial body was performed by Apollo 11 on 20 July 1969, landing on the Moon. There have been a total of six spacecraft with humans landing on the Moon starting from 1969 to the last human landing in 1972.
 The first interplanetary flyby was the 1961 Venera 1 flyby of Venus, though the 1962 Mariner 2 was the first flyby of Venus to return data (closest approach 34,773 kilometers). Pioneer 6 was the first satellite to orbit the Sun, launched on 16 December 1965. The other planets were first flown by in 1965 for Mars by Mariner 4, 1973 for Jupiter by Pioneer 10, 1974 for Mercury by Mariner 10, 1979 for Saturn by Pioneer 11, 1986 for Uranus by Voyager 2, 1989 for Neptune by Voyager 2. In 2015, the dwarf planets Ceres and Pluto were orbited by Dawn and passed by New Horizons, respectively. This accounts for flybys of each of the eight planets in the Solar System, the Sun, the Moon, and Ceres and Pluto (two of the five recognized dwarf planets).
 The first interplanetary surface mission to return at least limited surface data from another planet was the 1970 landing of Venera 7, which returned data to Earth for 23 minutes from Venus. In 1975 the Venera 9 was the first to return images from the surface of another planet, returning images from Venus. In 1971 the Mars 3 mission achieved the first soft landing on Mars returning data for almost 20 seconds. Later much longer duration surface missions were achieved, including over six years of Mars surface operation by Viking 1 from 1975 to 1982 and over two hours of transmission from the surface of Venus by Venera 13 in 1982, the longest ever Soviet planetary surface mission. Venus and Mars are the two planets outside of Earth on which humans have conducted surface missions with uncrewed robotic spacecraft.
 Salyut 1 was the first space station of any kind, launched into low Earth orbit by the Soviet Union on 19 April 1971. The International Space Station is currently the largest and oldest of the 2 current fully functional space stations, inhabited continuously since the year 2000. The other, Tiangong space station built by China, is now fully crewed and operational.
 Voyager 1 became the first human-made object to leave the Solar System into interstellar space on 25 August 2012. The probe passed the heliopause at 121 AU to enter interstellar space.[29]
 The Apollo 13 flight passed the far side of the Moon at an altitude of 254 kilometers (158 miles; 137 nautical miles) above the lunar surface, and 400,171 km (248,655 mi) from Earth, marking the record for the farthest humans have ever traveled from Earth in 1970.
 As of 26 November 2022[update] Voyager 1 was at a distance of 159 AU (23.8 billion km; 14.8 billion mi) from Earth.[30] It is the most distant human-made object from Earth.[31]
 Starting in the mid-20th century probes and then human mission were sent into Earth orbit, and then on to the Moon. Also, probes were sent throughout the known Solar System, and into Solar orbit. Uncrewed spacecraft have been sent into orbit around Saturn, Jupiter, Mars, Venus, and Mercury by the 21st century, and the most distance active spacecraft, Voyager 1 and 2 traveled beyond 100 times the Earth-Sun distance. The instruments were enough though that it is thought they have left the Sun's heliosphere, a sort of bubble of particles made in the Galaxy by the Sun's solar wind.
 The Sun is a major focus of space exploration. Being above the atmosphere in particular and Earth's magnetic field gives access to the solar wind and infrared and ultraviolet radiations that cannot reach Earth's surface. The Sun generates most space weather, which can affect power generation and transmission systems on Earth and interfere with, and even damage, satellites and space probes. Numerous spacecraft dedicated to observing the Sun, beginning with the Apollo Telescope Mount, have been launched and still others have had solar observation as a secondary objective. Parker Solar Probe, launched in 2018, will approach the Sun to within 1/9th the orbit of Mercury.
 Mercury remains the least explored of the Terrestrial planets. As of May 2013, the Mariner 10 and MESSENGER missions have been the only missions that have made close observations of Mercury. MESSENGER entered orbit around Mercury in March 2011, to further investigate the observations made by Mariner 10 in 1975 (Munsell, 2006b).
A third mission to Mercury, scheduled to arrive in 2025, BepiColombo is to include two probes. BepiColombo is a joint mission between Japan and the European Space Agency. MESSENGER and BepiColombo are intended to gather complementary data to help scientists understand many of the mysteries discovered by Mariner 10's flybys.
 Flights to other planets within the Solar System are accomplished at a cost in energy, which is described by the net change in velocity of the spacecraft, or delta-v. Due to the relatively high delta-v to reach Mercury and its proximity to the Sun, it is difficult to explore and orbits around it are rather unstable.
 Venus was the first target of interplanetary flyby and lander missions and, despite one of the most hostile surface environments in the Solar System, has had more landers sent to it (nearly all from the Soviet Union) than any other planet in the Solar System. The first flyby was the 1961 Venera 1, though the 1962 Mariner 2 was the first flyby to successfully return data. Mariner 2 has been followed by several other flybys by multiple space agencies often as part of missions using a Venus flyby to provide a gravitational assist en route to other celestial bodies. In 1967 Venera 4 became the first probe to enter and directly examine the atmosphere of Venus. In 1970, Venera 7 became the first successful lander to reach the surface of Venus and by 1985 it had been followed by eight additional successful Soviet Venus landers which provided images and other direct surface data. Starting in 1975 with the Soviet orbiter Venera 9 some ten successful orbiter missions have been sent to Venus, including later missions which were able to map the surface of Venus using radar to pierce the obscuring atmosphere.
 Space exploration has been used as a tool to understand Earth as a celestial object. Orbital missions can provide data for Earth that can be difficult or impossible to obtain from a purely ground-based point of reference.
 For example, the existence of the Van Allen radiation belts was unknown until their discovery by the United States' first artificial satellite, Explorer 1. These belts contain radiation trapped by Earth's magnetic fields, which currently renders construction of habitable space stations above 1000 km impractical.
Following this early unexpected discovery, a large number of Earth observation satellites have been deployed specifically to explore Earth from a space-based perspective. These satellites have significantly contributed to the understanding of a variety of Earth-based phenomena. For instance, the hole in the ozone layer was found by an artificial satellite that was exploring Earth's atmosphere, and satellites have allowed for the discovery of archeological sites or geological formations that were difficult or impossible to otherwise identify.
 The Moon was the first celestial body to be the object of space exploration. It holds the distinctions of being the first remote celestial object to be flown by, orbited, and landed upon by spacecraft, and the only remote celestial object ever to be visited by humans.
 In 1959 the Soviets obtained the first images of the far side of the Moon, never previously visible to humans. The U.S. exploration of the Moon began with the Ranger 4 impactor in 1962. Starting in 1966 the Soviets successfully deployed a number of landers to the Moon which were able to obtain data directly from the Moon's surface; just four months later, Surveyor 1 marked the debut of a successful series of U.S. landers. The Soviet uncrewed missions culminated in the Lunokhod program in the early 1970s, which included the first uncrewed rovers and also successfully brought lunar soil samples to Earth for study. This marked the first (and to date the only) automated return of extraterrestrial soil samples to Earth. Uncrewed exploration of the Moon continues with various nations periodically deploying lunar orbiters, and in 2008 the Indian Moon Impact Probe and in 2023 the Chandrayaan-3 of India became the first spacecraft to land on the lunar south pole.
 Crewed exploration of the Moon began in 1968 with the Apollo 8 mission that successfully orbited the Moon, the first time any extraterrestrial object was orbited by humans. In 1969, the Apollo 11 mission marked the first time humans set foot upon another world. Crewed exploration of the Moon did not continue for long. The Apollo 17 mission in 1972 marked the sixth landing and the most recent human visit. Artemis 2 is scheduled to complete a crewed flyby of the Moon in 2025, and Artemis 3 will perform the first lunar landing since Apollo 17 with it scheduled for launch no earlier than 2026. Robotic missions are still pursued vigorously.
 The exploration of Mars has been an important part of the space exploration programs of the Soviet Union (later Russia), the United States, Europe, Japan and India. Dozens of robotic spacecraft, including orbiters, landers, and rovers, have been launched toward Mars since the 1960s. These missions were aimed at gathering data about current conditions and answering questions about the history of Mars. The questions raised by the scientific community are expected to not only give a better appreciation of the Red Planet but also yield further insight into the past, and possible future, of Earth.
 The exploration of Mars has come at a considerable financial cost with roughly two-thirds of all spacecraft destined for Mars failing before completing their missions, with some failing before they even began. Such a high failure rate can be attributed to the complexity and large number of variables involved in an interplanetary journey, and has led researchers to jokingly speak of The Great Galactic Ghoul[32] which subsists on a diet of Mars probes. This phenomenon is also informally known as the ""Mars Curse"".[33]  In contrast to overall high failure rates in the exploration of Mars, India has become the first country to achieve success of its maiden attempt.  India's Mars Orbiter Mission (MOM)[34][35][36] is one of the least expensive interplanetary missions ever undertaken with an approximate total cost of ₹ 450 Crore (US$73 million).[37][38] The first mission to Mars by any Arab country has been taken up by the United Arab Emirates. Called the Emirates Mars Mission, it was launched on 19 July 2020 and went into orbit around Mars on 9 February 2021. The uncrewed exploratory probe was named ""Hope Probe"" and was sent to Mars to study its atmosphere in detail.[39]
 The Russian space mission Fobos-Grunt, which launched on 9 November 2011 experienced a failure leaving it stranded in low Earth orbit.[40] It was to begin exploration of the Phobos and Martian circumterrestrial orbit, and study whether the moons of Mars, or at least Phobos, could be a ""trans-shipment point"" for spaceships traveling to Mars.[41]
 Until the advent of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes, their shapes and terrain remaining a mystery.
Several asteroids have now been visited by probes, the first of which was Galileo, which flew past two: 951 Gaspra in 1991, followed by 243 Ida in 1993. Both of these lay near enough to Galileo's planned trajectory to Jupiter that they could be visited at acceptable cost. The first landing on an asteroid was performed by the NEAR Shoemaker probe in 2000, following an orbital survey of the object, 433 Eros. The dwarf planet Ceres and the asteroid 4 Vesta, two of the three largest asteroids, were visited by NASA's Dawn spacecraft, launched in 2007.
 Hayabusa was a robotic spacecraft developed by the Japan Aerospace Exploration Agency to return a sample of material from the small near-Earth asteroid 25143 Itokawa to Earth for further analysis. Hayabusa was launched on 9 May 2003 and rendezvoused with Itokawa in mid-September 2005. After arriving at Itokawa, Hayabusa studied the asteroid's shape, spin, topography, color, composition, density, and history. In November 2005, it landed on the asteroid twice to collect samples. The spacecraft returned to Earth on 13 June 2010.
 The exploration of Jupiter has consisted solely of a number of automated NASA spacecraft visiting the planet since 1973. A large majority of the missions have been ""flybys"", in which detailed observations are taken without the probe landing or entering orbit; such as in Pioneer and Voyager programs. The Galileo and Juno spacecraft are the only spacecraft to have entered the planet's orbit. As Jupiter is believed to have only a relatively small rocky core and no real solid surface, a landing mission is precluded.
 Reaching Jupiter from Earth requires a delta-v of 9.2 km/s,[42] which is comparable to the 9.7 km/s delta-v needed to reach low Earth orbit.[43] Fortunately, gravity assists through planetary flybys can be used to reduce the energy required at launch to reach Jupiter, albeit at the cost of a significantly longer flight duration.[42]
 Jupiter has 95 known moons, many of which have relatively little known information about them.
 Saturn has been explored only through uncrewed spacecraft launched by NASA, including one mission (Cassini–Huygens) planned and executed in cooperation with other space agencies. These missions consist of flybys in 1979 by Pioneer 11, in 1980 by Voyager 1, in 1982 by Voyager 2 and an orbital mission by the Cassini spacecraft, which lasted from 2004 until 2017.
 Saturn has at least 62 known moons, although the exact number is debatable since Saturn's rings are made up of vast numbers of independently orbiting objects of varying sizes. The largest of the moons is Titan, which holds the distinction of being the only moon in the Solar System with an atmosphere denser and thicker than that of Earth.  Titan holds the distinction of being the only object in the Outer Solar System that has been explored with a lander, the Huygens probe deployed by the Cassini spacecraft.
 The exploration of Uranus has been entirely through the Voyager 2 spacecraft, with no other visits currently planned. Given its axial tilt of 97.77°, with its polar regions exposed to sunlight or darkness for long periods, scientists were not sure what to expect at Uranus. The closest approach to Uranus occurred on 24 January 1986. Voyager 2 studied the planet's unique atmosphere and magnetosphere. Voyager 2 also examined its ring system and the moons of Uranus including all five of the previously known moons, while discovering an additional ten previously unknown moons.
 Images of Uranus proved to have a very uniform appearance, with no evidence of the dramatic storms or atmospheric banding evident on Jupiter and Saturn. Great effort was required to even identify a few clouds in the images of the planet. The magnetosphere of Uranus, however, proved to be unique, being profoundly affected by the planet's unusual axial tilt. In contrast to the bland appearance of Uranus itself, striking images were obtained of the Moons of Uranus, including evidence that Miranda had been unusually geologically active.
 The exploration of Neptune began with 25 August 1989 Voyager 2 flyby, the sole visit to the system as of 2024. The possibility of a Neptune Orbiter has been discussed, but no other missions have been given serious thought.
 Although the extremely uniform appearance of Uranus during Voyager 2's visit in 1986 had led to expectations that Neptune would also have few visible atmospheric phenomena, the spacecraft found that Neptune had obvious banding, visible clouds, auroras, and even a conspicuous anticyclone storm system rivaled in size only by Jupiter's Great Red Spot. Neptune also proved to have the fastest winds of any planet in the Solar System, measured as high as 2,100 km/h.[44] Voyager 2 also examined Neptune's ring and moon system. It discovered 900 complete rings and additional partial ring ""arcs"" around Neptune. In addition to examining Neptune's three previously known moons, Voyager 2 also discovered five previously unknown moons, one of which, Proteus, proved to be the last largest moon in the system. Data from Voyager 2 supported the view that Neptune's largest moon, Triton, is a captured Kuiper belt object.[45]
 The dwarf planet Pluto presents significant challenges for spacecraft because of its great distance from Earth (requiring high velocity for reasonable trip times) and small mass (making capture into orbit very difficult at present). Voyager 1 could have visited Pluto, but controllers opted instead for a close flyby of Saturn's moon Titan, resulting in a trajectory incompatible with a Pluto flyby. Voyager 2 never had a plausible trajectory for reaching Pluto.[46]
 After an intense political battle, a mission to Pluto dubbed New Horizons was granted funding from the United States government in 2003.[47] New Horizons was launched successfully on 19 January 2006. In early 2007 the craft made use of a gravity assist from Jupiter. Its closest approach to Pluto was on 14 July 2015; scientific observations of Pluto began five months prior to closest approach and continued for 16 days after the encounter.
 The New Horizons mission also did a flyby of the small planetesimal Arrokoth, in the Kuiper belt, in 2019. This was its first extended mission.[48]
 Although many comets have been studied from Earth sometimes with centuries-worth of observations, only a few comets have been closely visited. In 1985, the International Cometary Explorer conducted the first comet fly-by (21P/Giacobini-Zinner) before joining the Halley Armada studying the famous comet. The Deep Impact probe smashed into 9P/Tempel to learn more about its structure and composition and the Stardust mission returned samples of another comet's tail. The Philae lander successfully landed on Comet Churyumov–Gerasimenko in 2014 as part of the broader Rosetta mission.
 Deep space exploration is the branch of astronomy, astronautics and space technology that is involved with the exploration of distant regions of outer space.[49] Physical exploration of space is conducted both by human spaceflights (deep-space astronautics) and by robotic spacecraft.
 Some of the best candidates for future deep space engine technologies include anti-matter, nuclear power and beamed propulsion.[50] The latter, beamed propulsion, appears to be the best candidate for deep space exploration presently available, since it uses known physics and known technology that is being developed for other purposes.[51]
 Breakthrough Starshot is a research and engineering project by the Breakthrough Initiatives to develop a proof-of-concept fleet of light sail spacecraft named StarChip,[52] to be capable of making the journey to the Alpha Centauri star system 4.37 light-years away. It was founded in 2016 by Yuri Milner, Stephen Hawking, and Mark Zuckerberg.[53][54]
 An article in science magazine Nature suggested the use of asteroids as a gateway for space exploration, with the ultimate destination being Mars. In order to make such an approach viable, three requirements need to be fulfilled: first, ""a thorough asteroid survey to find thousands of nearby bodies suitable for astronauts to visit""; second, ""extending flight duration and distance capability to ever-increasing ranges out to Mars""; and finally, ""developing better robotic vehicles and tools to enable astronauts to explore an asteroid regardless of its size, shape or spin"". Furthermore, using asteroids would provide astronauts with protection from galactic cosmic rays, with mission crews being able to land on them without great risk to radiation exposure.
 The James Webb Space Telescope (JWST or ""Webb"") is a space telescope that is the successor to the Hubble Space Telescope.[55][56] The JWST will provide greatly improved resolution and sensitivity over the Hubble, and will enable a broad range of investigations across the fields of astronomy and cosmology, including observing some of the most distant events and objects in the universe, such as the formation of the first galaxies. Other goals include understanding the formation of stars and planets, and direct imaging of exoplanets and novas.[57]
 The primary mirror of the James Webb Space Telescope, the Optical Telescope Element, is composed of 18 hexagonal mirror segments made of gold-plated beryllium which combine to create a 6.5-meter (21 ft; 260 in) diameter mirror that is much larger than the Hubble's 2.4-meter (7.9 ft; 94 in) mirror.  Unlike the Hubble, which observes in the near ultraviolet, visible, and near infrared (0.1 to 1 μm) spectra, the JWST will observe in a lower frequency range, from long-wavelength visible light through mid-infrared (0.6 to 27 μm), which will allow it to observe high redshift objects that are too old and too distant for the Hubble to observe.[58] The telescope must be kept very cold in order to observe in the infrared without interference, so it will be deployed in space near the Earth–Sun L2 Lagrangian point, and a large sunshield made of silicon- and aluminum-coated Kapton will keep its mirror and instruments below 50 K (−220 °C; −370 °F).[59]
 The Artemis program is an ongoing crewed spaceflight program carried out by NASA, U.S. commercial spaceflight companies, and international partners such as ESA,[60] with the goal of landing ""the first woman and the next man"" on the Moon, specifically at the lunar south pole region by 2024. Artemis would be the next step towards the long-term goal of establishing a sustainable presence on the Moon, laying the foundation for private companies to build a lunar economy, and eventually sending humans to Mars.
 In 2017, the lunar campaign was authorized by Space Policy Directive 1, utilizing various ongoing spacecraft programs such as Orion, the Lunar Gateway, Commercial Lunar Payload Services, and adding an undeveloped crewed lander. The Space Launch System will serve as the primary launch vehicle for Orion, while commercial launch vehicles are planned for use to launch various other elements of the campaign.[61] NASA requested $1.6 billion in additional funding for Artemis for fiscal year 2020,[62] while the Senate Appropriations Committee requested from NASA a five-year budget profile[63] which is needed for evaluation and approval by Congress.[64][65] As of 2024, the first Artemis mission was launched in 2022 with the second mission, a crewed lunar flyby planned for 2025.[66] Construction on the Lunar Gateway is underway with initial capabilities set for the 2025-2027 timeframe.[67] The first CLPS lander landed in 2024, marking the first US spacecraft to land since Apollo 17.[68]
 The research that is conducted by national space exploration agencies, such as NASA and Roscosmos, is one of the reasons supporters cite to justify government expenses. Economic analyses of the NASA programs often showed ongoing economic benefits (such as NASA spin-offs), generating many times the revenue of the cost of the program.[69] It is also argued that space exploration would lead to the extraction of resources on other planets and especially asteroids, which contain billions of dollars worth of minerals and metals. Such expeditions could generate a lot of revenue.[70] In addition, it has been argued that space exploration programs help inspire youth to study in science and engineering.[71] Space exploration also gives scientists the ability to perform experiments in other settings and expand humanity's knowledge.[72]
 Another claim is that space exploration is a necessity to humankind and that staying on Earth will lead to extinction. Some of the reasons are lack of natural resources, comets, nuclear war, and worldwide epidemic. Stephen Hawking, renowned British theoretical physicist, said that ""I don't think the human race will survive the next thousand years, unless we spread into space. There are too many accidents that can befall life on a single planet. But I'm an optimist. We will reach out to the stars.""[73] Arthur C. Clarke (1950) presented a summary of motivations for the human exploration of space in his non-fiction semi-technical monograph Interplanetary Flight.[74] He argued that humanity's choice is essentially between expansion off Earth into space, versus cultural (and eventually biological) stagnation and death.
These motivations could be attributed to one of the first rocket scientists in NASA, Wernher von Braun, and his vision of humans moving beyond Earth. The basis of this plan was to:
 Develop multi-stage rockets capable of placing satellites, animals, and humans in space.
 Development of large, winged reusable spacecraft capable of carrying humans and equipment into Earth orbit in a way that made space access routine and cost-effective.
 Construction of a large, permanently occupied space station to be used as a platform both to observe Earth and from which to launch deep space expeditions.
 Launching the first human flights around the Moon, leading to the first landings of humans on the Moon, with the intent of exploring that body and establishing permanent lunar bases.
 
Assembly and fueling of spaceships in Earth orbit for the purpose of sending humans to Mars with the intent of eventually colonizing that planet.[75] Known as the Von Braun Paradigm, the plan was formulated to lead humans in the exploration of space. Von Braun's vision of human space exploration served as the model for efforts in space exploration well into the twenty-first century, with NASA incorporating this approach into the majority of their projects.[75] The steps were followed out of order, as seen by the Apollo program reaching the moon before the space shuttle program was started, which in turn was used to complete the International Space Station. Von Braun's Paradigm formed NASA's drive for human exploration, in the hopes that humans discover the far reaches of the universe.
 NASA has produced a series of public service announcement videos supporting the concept of space exploration.[76]
 Overall, the public remains largely supportive of both crewed and uncrewed space exploration. According to an Associated Press Poll conducted in July 2003, 71% of U.S. citizens agreed with the statement that the space program is ""a good investment"", compared to 21% who did not.[77]
 Space advocacy and space policy[78] regularly invokes exploration as a human nature.[79]
 Spaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space.
 Spaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.
 A spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of Earth. Once in space, the motion of a spacecraft—both when unpropelled and when under propulsion—is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.
 Satellites are used for a large number of purposes. Common types include military (spy) and civilian Earth observation satellites, communication satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites.
 The commercialization of space first started out with the launching of private satellites by NASA or other space agencies. Current examples of the commercial satellite use of space include satellite navigation systems, satellite television and satellite radio. The next step of commercialization of space was seen as human spaceflight. Flying humans safely to and from space had become routine to NASA.[80] Reusable spacecraft were an entirely new engineering challenge, something only seen in novels and films like Star Trek and War of the Worlds. Great names like Buzz Aldrin supported the use of making a reusable vehicle like the space shuttle. Aldrin held that reusable spacecraft were the key in making space travel affordable, stating that the use of ""passenger space travel is a huge potential market big enough to justify the creation of reusable launch vehicles"".[81] How can the public go against the words of one of America's best known heroes in space exploration? After all exploring space is the next great expedition, following the example of Lewis and Clark.Space tourism is the next step reusable vehicles in the commercialization of space. The purpose of this form of space travel is used by individuals for the purpose of personal pleasure.
 Private spaceflight companies such as SpaceX and Blue Origin, and commercial space stations such as the Axiom Space and the Bigelow Commercial Space Station have dramatically changed the landscape of space exploration, and will continue to do so in the near future.
 Astrobiology is the interdisciplinary study of life in the universe, combining aspects of astronomy, biology and geology.[82] It is focused primarily on the study of the origin, distribution and evolution of life. It is also known as exobiology (from Greek: έξω, exo, ""outside"").[83][84][85] The term ""Xenobiology"" has been used as well, but this is technically incorrect because its terminology means ""biology of the foreigners"".[86] Astrobiologists must also consider the possibility of life that is chemically entirely distinct from any life found on Earth.[87] In the Solar System some of the prime locations for current or past astrobiology are on Enceladus, Europa, Mars, and Titan.[88]
 To date, the longest human occupation of space is the International Space Station which has been in continuous use for 23 years, 198 days. Valeri Polyakov's record single spaceflight of almost 438 days aboard the Mir space station has not been surpassed. The health effects of space have been well documented through years of research conducted in the field of aerospace medicine. Analog environments similar to those one may experience in space travel (like deep sea submarines) have been used in this research to further explore the relationship between isolation and extreme environments.[89] It is imperative that the health of the crew be maintained as any deviation from baseline may compromise the integrity of the mission as well as the safety of the crew, hence the reason why astronauts must endure rigorous medical screenings and tests prior to embarking on any missions. However, it does not take long for the environmental dynamics of spaceflight to commence its toll on the human body; for example, space motion sickness (SMS) – a condition which affects the neurovestibular system and culminates in mild to severe signs and symptoms such as vertigo, dizziness, fatigue, nausea, and disorientation – plagues almost all space travelers within their first few days in orbit.[89] Space travel can also have a profound impact on the psyche of the crew members as delineated in anecdotal writings composed after their retirement. Space travel can adversely affect the body's natural biological clock (circadian rhythm); sleep patterns causing sleep deprivation and fatigue; and social interaction; consequently, residing in a Low Earth Orbit (LEO) environment for a prolonged amount of time can result in both mental and physical exhaustion.[89] Long-term stays in space reveal issues with bone and muscle loss in low gravity, immune system suppression, and radiation exposure. The lack of gravity causes fluid to rise upward which can cause pressure to build up in the eye, resulting in vision problems; the loss of bone minerals and densities; cardiovascular deconditioning; and decreased endurance and muscle mass.[90]
 Radiation is an insidious health hazard to space travelers as it is invisible and can cause cancer. When above the Earth's magnetic field spacecraft are no longer protected from the sun's radiation; the danger of radiation is even more potent in deep space. The hazards of radiation can be ameliorated through protective shielding on the spacecraft, alerts, and dosimetry.[91]
 Fortunately, with new and rapidly evolving technological advancements, those in Mission Control are able to monitor the health of their astronauts more closely utilizing telemedicine. One may not be able to completely evade the physiological effects of space flight, but they can be mitigated. For example, medical systems aboard space vessels such as the International Space Station (ISS) are well equipped and designed to counteract the effects of lack of gravity and weightlessness; on-board treadmills can help prevent muscle loss and reduce the risk of developing premature osteoporosis.[89][91] Additionally, a crew medical officer is appointed for each ISS mission and a flight surgeon is available 24/7 via the ISS Mission Control Center located in Houston, Texas.[91] Although the interactions are intended to take place in real time, communications between the space and terrestrial crew may become delayed – sometimes by as much as 20 minutes[91] – as their distance from each other increases when the spacecraft moves further out of LEO; because of this the crew are trained and need to be prepared to respond to any medical emergencies that may arise on the vessel as the ground crew are hundreds of miles away. As one can see, travelling and possibly living in space poses many challenges. Many past and current concepts for the continued exploration and colonization of space focus on a return to the Moon as a ""stepping stone"" to the other planets, especially Mars. At the end of 2006 NASA announced they were planning to build a permanent Moon base with continual presence by 2024.[92]
 Beyond the technical factors that could make living in space more widespread, it has been suggested that the lack of private property, the inability or difficulty in establishing property rights in space, has been an impediment to the development of space for human habitation. Since the advent of space technology in the latter half of the twentieth century, the ownership of property in space has been murky, with strong arguments both for and against. In particular, the making of national territorial claims in outer space and on celestial bodies has been specifically proscribed by the Outer Space Treaty, which had been, as of 2012[update], ratified by all spacefaring nations.[93]
Space colonization, also called space settlement and space humanization, would be the permanent autonomous (self-sufficient) human habitation of locations outside Earth, especially of natural satellites or planets such as the Moon or Mars, using significant amounts of in-situ resource utilization.
 Participation and representation of humanity in space is an issue ever since the first phase of space exploration.[94] Some rights of non-spacefaring countries have been mostly secured through international space law, declaring space the ""province of all mankind"", understanding spaceflight as its resource, though sharing of space for all humanity is still criticized as imperialist and lacking.[94] Additionally to international inclusion, the inclusion of women and people of colour has also been lacking. To reach a more inclusive spaceflight some organizations like the Justspace Alliance[94] and IAU featured Inclusive Astronomy[95] have been formed in recent years.
 The first woman to go to space was Valentina Tereshkova. She flew in 1963 but it was not until the 1980s that another woman entered space again. All astronauts were required to be military test pilots at the time and women were not able to join this career, this is one reason for the delay in allowing women to join space crews.[citation needed] After the rule changed, Svetlana Savitskaya became the second woman to go to space, she was also from the Soviet Union. Sally Ride became the next woman in space and the first woman to fly to space through the United States program.
 Since then, eleven other countries have allowed women astronauts. The first all-female space walk occurred in 2018, including Christina Koch and Jessica Meir. They had both previously participated in space walks with NASA. The first woman to go to the Moon is planned for 2024.
 Despite these developments women are still underrepresented among astronauts and especially cosmonauts. Issues that block potential applicants from the programs, and limit the space missions they are able to go on, include:
 Artistry in and from space ranges from signals, capturing and arranging material like Yuri Gagarin's selfie in space or the image The Blue Marble, over drawings like the first one in space by cosmonaut and artist Alexei Leonov, music videos like Chris Hadfield's cover of Space Oddity on board the ISS, to permanent installations on celestial bodies like on the Moon.
 Solar System → Local Interstellar Cloud → Local Bubble → Gould Belt → Orion Arm → Milky Way → Milky Way subgroup → Local Group → Local Sheet → Virgo Supercluster → Laniakea Supercluster → Local Hole → Observable universe → UniverseEach arrow (→) may be read as ""within"" or ""part of"".
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the origin, distribution and evolution of life', 'astronomers with telescopes', 'current conditions and answering questions about the history of Mars', 'Lewis and Clark', 'detailed observations are taken without the probe landing or entering orbit'], 'answer_start': [], 'answer_end': []}"
"
 Astrobiology is a scientific field within the life and environmental sciences that studies the origins, early evolution, distribution, and future of life in the universe by investigating its deterministic conditions and contingent events.[2] As a discipline, astrobiology is founded on the premise that life may exist beyond Earth.[3]
 Research in astrobiology comprises three main areas: the study of habitable environments in the Solar System and beyond, the search for planetary biosignatures of past or present extraterrestrial life, and the study of the origin and early evolution of life on Earth.
 The field of astrobiology has its origins in the 20th century with the advent of space exploration and the discovery of exoplanets. Early astrobiology research focused on the search for extraterrestrial life and the study of the potential for life to exist on other planets.[2] In the 1960s and 1970s, NASA began its astrobiology pursuits within  the Viking program, which was the first US mission to land on Mars and search for signs of life.[4] This mission, along with other early space exploration missions, laid the foundation for the development of astrobiology as a discipline.
 Regarding habitable environments, astrobiology investigates potential locations beyond Earth that could support life, such as Mars, Europa, and exoplanets, through research into the extremophiles populating austere environments on Earth, like volcanic and deep sea environments. Research within this topic is conducted utilising the methodology of the geosciences, especially geobiology, for astrobiological applications.
 The search for biosignatures involves the identification of signs of past or present life in the form of organic compounds, isotopic ratios, or microbial fossils. Research within this topic is conducted utilising the methodology of planetary and environmental science, especially atmospheric science, for astrobiological applications, and is often conducted through remote sensing and in situ missions.
 Astrobiology also concerns the study of the origin and early evolution of life on Earth to try to understand the conditions that are necessary for life to form on other planets.[5] This research seeks to understand how life emerged from non-living matter and how it evolved to become the diverse array of organisms we see today. Research within this topic is conducted utilising the methodology of paleosciences, especially paleobiology, for astrobiological applications.
 Astrobiology is a rapidly developing field with a strong interdisciplinary aspect that holds many challenges and opportunities for scientists. Astrobiology programs and research centres are present in many universities and research institutions around the world, and space agencies like NASA and ESA have dedicated departments and programs for astrobiology research.
 The term astrobiology was first proposed by the Russian astronomer Gavriil Tikhov in 1953.[6] It is etymologically derived from the Greek ἄστρον, ""star""; βίος, ""life""; and -λογία, -logia, ""study"". A close synonym is exobiology from the Greek Έξω, ""external""; βίος, ""life""; and -λογία, -logia, ""study"", coined by American molecular biologist Joshua Lederberg; exobiology is considered to have a narrow scope limited to search of life external to Earth.[7] Another associated term is xenobiology, from the Greek ξένος, ""foreign""; βίος, ""life""; and -λογία, ""study"", coined by American science fiction writer Robert Heinlein in his work The Star Beast;[8] xenobiology is now used in a more specialised sense, referring to 'biology based on foreign chemistry', whether of extraterrestrial or terrestrial (typically synthetic) origin.[9]
 While the potential for extraterrestrial life, especially intelligent life, has been explored throughout human history within philosophy and narrative, the question is a verifiable hypothesis and thus a valid line of scientific inquiry;[10][11] planetary scientist David Grinspoon calls it a field of natural philosophy, grounding speculation on the unknown in known scientific theory.[12]
 The modern field of astrobiology can be traced back to the 1950s and 1960s with the advent of space exploration, when scientists began to seriously consider the possibility of life on other planets. In 1957, the Soviet Union launched Sputnik 1, the first artificial satellite, which marked the beginning of the Space Age. This event led to an increase in the study of the potential for life on other planets, as scientists began to consider the possibilities opened up by the new technology of space exploration. In 1959, NASA funded its first exobiology project, and in 1960, NASA founded the Exobiology Program, now one of four main elements of NASA's current Astrobiology Program.[13] In 1971, NASA funded Project Cyclops,[14] part of the search for extraterrestrial intelligence, to search radio frequencies of the electromagnetic spectrum for interstellar communications transmitted by extraterrestrial life outside the Solar System. In the 1960s-1970s, NASA established the Viking program, which was the first US mission to land on Mars and search for metabolic signs of present life; the results were inconclusive.
 In the 1980s and 1990s, the field began to expand and diversify as new discoveries and technologies emerged. The discovery of microbial life in extreme environments on Earth, such as deep-sea hydrothermal vents, helped to clarify the feasibility of potential life existing in harsh conditions. The development of new techniques for the detection of biosignatures, such as the use of stable isotopes, also played a significant role in the evolution of the field.
 The contemporary landscape of astrobiology emerged in the early 21st century, focused on utilising Earth and environmental science for applications within comparate space environments. Missions included the ESA's Beagle 2, which failed minutes after landing on Mars, NASA's Phoenix lander, which probed the environment for past and present planetary habitability of microbial life on Mars and researched the history of water, and NASA's Curiosity rover, currently probing the environment for past and present planetary habitability of microbial life on Mars.
 Astrobiological research makes a number of simplifying assumptions when studying the necessary components for planetary habitability.
 Carbon and Organic Compounds: Carbon is the fourth most abundant element in the universe and the energy required to make or break a bond is at just the appropriate level for building molecules which are not only stable, but also reactive. The fact that carbon atoms bond readily to other carbon atoms allows for the building of extremely long and complex molecules. As such, astrobiological research presumes that the vast majority of life forms in the Milky Way galaxy are based on carbon chemistries, as are all life forms on Earth.[15][16] However, theoretical astrobiology entertains the potential for other organic molecular bases for life, thus astrobiological research often focuses on identifying environments that have the potential to support life based on the presence of organic compounds.
 Liquid water: Liquid water is a common molecule that provides an excellent environment for the formation of complicated carbon-based molecules, and is generally considered necessary for life as we know it to exist. Thus, astrobiological research presumes that extraterrestrial life similarly depends upon access to liquid water, and often focuses on identifying environments that have the potential to support liquid water.[17][18] Some researchers posit environments of water-ammonia mixtures as possible solvents for hypothetical types of biochemistry.[19]
 Environmental Stability: Where organisms adaptively evolve to the conditions of the environments in which they reside, environmental stability is considered necessary for life to exist. This presupposes the necessity of a stable temperature, pressure, and radiation levels; resultantly, astrobiological research focuses on planets orbiting Sun-like red dwarf stars.[20][16] This is because very large stars have relatively short lifetimes, meaning that life might not have time to emerge on planets orbiting them; very small stars provide so little heat and warmth that only planets in very close orbits around them would not be frozen solid, and in such close orbits these planets would be tidally locked to the star;[21] whereas the long lifetimes of red dwarfs could allow the development of habitable environments on planets with thick atmospheres.[22] This is significant as red dwarfs are extremely common. (See also: Habitability of red dwarf systems).
 Energy source: It is assumed that any life elsewhere in the universe would also require an energy source. Previously, it was assumed that this would necessarily be from a sun-like star, however with developments within extremophile research contemporary astrobiological research often focuses on identifying environments that have the potential to support life based on the availability of an energy source, such as the presence of volcanic activity on a planet or moon that could provide a source of heat and energy.
 It is important to note that these assumptions are based on our current understanding of life on Earth and the conditions under which it can exist. As our understanding of life and the potential for it to exist in different environments evolves, these assumptions may change.
 Astrobiological research concerning the study of habitable environments in our solar system and beyond utilises methods within the geosciences. Research within this branch primarily concerns the geobiology of organisms that can survive in extreme environments on Earth, such as in volcanic or deep sea environments, to understand the limits of life, and the conditions under which life might be able to survive on other planets. This includes, but is not limited to;
 Deep-sea extremophiles: Researchers are studying organisms that live in the extreme environments of deep-sea hydrothermal vents and cold seeps.[23] These organisms survive in the absence of sunlight, and some are able to survive in high temperatures and pressures, and use chemical energy instead of sunlight to produce food.
 Desert extremophiles: Researchers are studying organisms that can survive in extreme dry, high temperature conditions, such as in deserts.[24]
 Microbes in extreme environments: Researchers are investigating the diversity and activity of microorganisms in environments such as deep mines, subsurface soil, cold glaciers[25] and polar ice,[26] and high-altitude environments.
 Research also regards the long-term survival of life on Earth, and the possibilities and hazards of life on other planets, including;
 Biodiversity and ecosystem resilience: Scientists are studying how the diversity of life and the interactions between different species contribute to the resilience of ecosystems and their ability to recover from disturbances.[27]
 Climate change and extinction: Researchers are investigating the impacts of climate change on different species and ecosystems, and how they may lead to extinction or adaptation.[28] This includes the evolution of Earth's climate and geology, and their potential impact on the habitability of the planet in the future, especially for humans.
 Human impact on the biosphere: Scientists are studying the ways in which human activities, such as deforestation, pollution, and the introduction of invasive species, are affecting the biosphere and the long-term survival of life on Earth.[29]
 Long-term preservation of life: Researchers are exploring ways to preserve samples of life on Earth for long periods of time, such as cryopreservation and genomic preservation, in the event of a catastrophic event that could wipe out most of life on Earth.[30]
 Emerging astrobiological research concerning the search for planetary biosignatures of past or present extraterrestrial life utilise methodologies within planetary sciences. These include; 
 The study of microbial life in the subsurface of Mars:  Scientists are using data from Mars rover missions to study the composition of the subsurface of Mars, searching for biosignatures of past or present microbial life.[31]
The study of subsurface oceans on icy moons:   Discoveries of subsurface oceans on moons such as Europa[32][33][34] and Enceladus[35][36] showed habitability zones, making them viable targets for the search for extraterrestrial life. Currently,[when?] missions like the Europa Clipper were planned for searching for biosignatures within these environments.  The study of the atmospheres of planets:   Scientists are studying the potential for life to exist in the atmospheres of planets, with a focus on the study of the physical and chemical conditions necessary for such life to exist, namely the detection of organic molecules and biosignature gases; for example, the study of the possibility of life in the atmospheres of exoplanets that orbit red dwarfs and the study of the potential for microbial life in the upper atmosphere of Venus.[37]
 Telescopes and remote sensing of exoplanets: The discovery of thousands of exoplanets has opened up new opportunities for the search for biosignatures. Scientists are using telescopes such as the James Webb Space Telescope and the Transiting Exoplanet Survey Satellite to search for biosignatures on exoplanets. They are also developing new techniques for the detection of biosignatures, such as the use of remote sensing to search for biosignatures in the atmosphere of exoplanets.[38]
 SETI and CETI:  Scientists search for signals from intelligent extraterrestrial civilizations using radio and optical telescopes within the discipline of extraterrestrial intelligence communications (CETI). CETI focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. Communication attempts by humans have included broadcasting mathematical languages, pictorial systems such as the Arecibo message, and computational approaches to detecting and deciphering 'natural' language communication. While some high-profile scientists, such as Carl Sagan, have advocated the transmission of messages,[39][40] theoretical physicist Stephen Hawking warned against it, suggesting that aliens may raid Earth for its resources.[41]
 Emerging astrobiological research concerning the study of the origin and early evolution of life on Earth utilises methodologies within the palaeosciences. These include;
 The study of the early atmosphere: Researchers are investigating the role of the early atmosphere in providing the right conditions for the emergence of life, such as the presence of gases that could have helped to stabilise the climate and the formation of organic molecules.[42]
 The study of the early magnetic field: Researchers are investigating the role of the early magnetic field in protecting the Earth from harmful radiation and helping to stabilise the climate.[43] This research has immense astrobiological implications where the subjects of current astrobiological research like Mars lack such a field.
 The study of prebiotic chemistry: Scientists are studying the chemical reactions that could have occurred on the early Earth that led to the formation of the building blocks of life- amino acids, nucleotides, and lipids- and how these molecules could have formed spontaneously under early Earth conditions.[44]  The study of impact events: Scientists are investigating the potential role of impact events- especially meteorites- in the delivery of water and organic molecules to early Earth.[45]
 The study of the primordial soup:  Researchers are investigating the conditions and ingredients that were present on the early Earth that could have led to the formation of the first living organisms, such as the presence of water and organic molecules, and how these ingredients could have led to the formation of the first living organisms.[46] This includes the role of water in the formation of the first cells and in catalysing chemical reactions.
 The study of the role of minerals: Scientists are investigating the role of minerals like clay in catalysing the formation of organic molecules, thus playing a role in the emergence of life on Earth.[47]
 The study of the role of energy and electricity: Scientists are investigating the potential sources of energy and electricity that could have been available on the early Earth, and their role in the formation of organic molecules, thus the emergence of life.[48]
 The study of the early oceans: Scientists are investigating the composition and chemistry of the early oceans and how it may have played a role in the emergence of life, such as the presence of dissolved minerals that could have helped to catalyse the formation of organic molecules.[49]
 The study of hydrothermal vents: Scientists are investigating the potential role of hydrothermal vents in the origin of life, as these environments may have provided the energy and chemical building blocks needed for its emergence.[50]
 The study of plate tectonics: Scientists are investigating the role of plate tectonics in creating a diverse range of environments on the early Earth.[51]
 The study of the early biosphere: Researchers are investigating the diversity and activity of microorganisms in the early Earth, and how these organisms may have played a role in the emergence of life.[52]
 The study of microbial fossils: Scientists are investigating the presence of microbial fossils in ancient rocks, which can provide clues about the early evolution of life on Earth and the emergence of the first organisms.[53]
 The systematic search for possible life outside Earth is a valid multidisciplinary scientific endeavor.[54] However, hypotheses and predictions as to its existence and origin vary widely, and at the present, the development of hypotheses firmly grounded on science may be considered astrobiology's most concrete practical application. It has been proposed that viruses are likely to be encountered on other life-bearing planets,[55][56] and may be present even if there are no biological cells.[57]
 As of 2019[update], no evidence of extraterrestrial life has been identified.[58] Examination of the Allan Hills 84001 meteorite, which was recovered in Antarctica in 1984 and originated from Mars, is thought by David McKay, as well as few other scientists, to contain microfossils of extraterrestrial origin; this interpretation is controversial.[59][60][61]
 Yamato 000593, the second largest meteorite from Mars, was found on Earth in 2000. At a microscopic level, spheres are found in the meteorite that are rich in carbon compared to surrounding areas that lack such spheres. The carbon-rich spheres may have been formed by biotic activity according to some NASA scientists.[62][63][64]
 On 5 March 2011, Richard B. Hoover, a scientist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites in the fringe Journal of Cosmology, a story widely reported on by mainstream media.[65][66] However, NASA formally distanced itself from Hoover's claim.[67] According to American astrophysicist Neil deGrasse Tyson: ""At the moment, life on Earth is the only known life in the universe, but there are compelling arguments to suggest we are not alone.""[68]
 Most astronomy-related astrobiology research falls into the category of extrasolar planet (exoplanet) detection, the hypothesis being that if life arose on Earth, then it could also arise on other planets with similar characteristics. To that end, a number of instruments designed to detect Earth-sized exoplanets have been considered, most notably NASA's Terrestrial Planet Finder (TPF) and ESA's Darwin programs, both of which have been cancelled. NASA launched the Kepler mission in March 2009, and the French Space Agency launched the COROT space mission in 2006.[69][70] There are also several less ambitious ground-based efforts underway.
 The goal of these missions is not only to detect Earth-sized planets but also to directly detect light from the planet so that it may be studied spectroscopically. By examining planetary spectra, it would be possible to determine the basic composition of an extrasolar planet's atmosphere and/or surface.[71] Given this knowledge, it may be possible to assess the likelihood of life being found on that planet. A NASA research group, the Virtual Planet Laboratory,[72] is using computer modeling to generate a wide variety of virtual planets to see what they would look like if viewed by TPF or Darwin. It is hoped that once these missions come online, their spectra can be cross-checked with these virtual planetary spectra for features that might indicate the presence of life.
 An estimate for the number of planets with intelligent communicative extraterrestrial life can be gleaned from the Drake equation, essentially an equation expressing the probability of intelligent life as the product of factors such as the fraction of planets that might be habitable and the fraction of planets on which life might arise:[73]
 where:
 However, whilst the rationale behind the equation is sound, it is unlikely that the equation will be constrained to reasonable limits of error any time soon. The problem with the formula is that it is not used to generate or support hypotheses because it contains factors that can never be verified. The first term, R*, number of stars, is generally constrained within a few orders of magnitude. The second and third terms, fp, stars with planets and fe, planets with habitable conditions, are being evaluated for the star's neighborhood. Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference,[74] but some applications of the formula had been taken literally and related to simplistic or pseudoscientific arguments.[75] Another associated topic is the Fermi paradox, which suggests that if intelligent life is common in the universe, then there should be obvious signs of it.
 Another active research area in astrobiology is planetary system formation. It has been suggested that the peculiarities of the Solar System (for example, the presence of Jupiter as a protective shield)[76] may have greatly increased the probability of intelligent life arising on Earth.[77][78]
 Biology cannot state that a process or phenomenon, by being mathematically possible, has to exist forcibly in an extraterrestrial body. Biologists specify what is speculative and what is not.[75] The discovery of extremophiles, organisms able to survive in extreme environments, became a core research element for astrobiologists, as they are important to understand four areas in the limits of life in planetary context: the potential for panspermia, forward contamination due to human exploration ventures, planetary colonization by humans, and the exploration of extinct and extant extraterrestrial life.[79]
 Until the 1970s, life was thought to be entirely dependent on energy from the Sun. Plants on Earth's surface capture energy from sunlight to photosynthesize sugars from carbon dioxide and water, releasing oxygen in the process that is then consumed by oxygen-respiring organisms, passing their energy up the food chain. Even life in the ocean depths, where sunlight cannot reach, was thought to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did.[80] The world's ability to support life was thought to depend on its access to sunlight. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible Alvin, scientists discovered colonies of giant tube worms, clams, crustaceans, mussels, and other assorted creatures clustered around undersea volcanic features known as black smokers.[80] These creatures thrive despite having no access to sunlight, and it was soon discovered that they comprise an entirely independent ecosystem. Although most of these multicellular lifeforms need dissolved oxygen (produced by oxygenic photosynthesis) for their aerobic cellular respiration and thus are not completely independent from sunlight by themselves, the basis for their food chain is a form of bacterium that derives its energy from oxidization of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. Other lifeforms entirely decoupled from the energy from sunlight are green sulfur bacteria which are capturing geothermal light for anoxygenic photosynthesis or bacteria running chemolithoautotrophy based on the radioactive decay of uranium.[81] This chemosynthesis revolutionized the study of biology and astrobiology by revealing that life need not be sunlight-dependent; it only requires water and an energy gradient in order to exist.
 Biologists have found extremophiles that thrive in ice, boiling water, acid, alkali, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life.[82][83] This opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats. Characterization of these organisms, their environments and their evolutionary pathways, is considered a crucial component to understanding how life might evolve elsewhere in the universe. For example, some organisms able to withstand exposure to the vacuum and radiation of outer space include the lichen fungi Rhizocarpon geographicum and Rusavskia elegans,[84] the bacterium Bacillus safensis,[85] Deinococcus radiodurans,[85] Bacillus subtilis,[85] yeast Saccharomyces cerevisiae,[85] seeds from Arabidopsis thaliana ('mouse-ear cress'),[85] as well as the invertebrate animal Tardigrade.[85] While tardigrades are not considered true extremophiles, they are considered extremotolerant microorganisms that have contributed to the field of astrobiology. Their extreme radiation tolerance and presence of DNA protection proteins may provide answers as to whether life can survive away from the protection of the Earth's atmosphere.[86]
 Jupiter's moon, Europa,[83][87][88][89][90] and Saturn's moon, Enceladus,[91][35] are now considered the most likely locations for extant extraterrestrial life in the Solar System due to their subsurface water oceans where radiogenic and tidal heating enables liquid water to exist.[81]
 The origin of life, known as abiogenesis, distinct from the evolution of life, is another ongoing field of research. Oparin and Haldane postulated that the conditions on the early Earth were conducive to the formation of organic compounds from inorganic elements and thus to the formation of many of the chemicals common to all forms of life we see today. The study of this process, known as prebiotic chemistry, has made some progress, but it is still unclear whether or not life could have formed in such a manner on Earth. The alternative hypothesis of panspermia is that the first elements of life may have formed on another planet with even more favorable conditions (or even in interstellar space, asteroids, etc.) and then have been carried over to Earth.
 The cosmic dust permeating the universe contains complex organic compounds (""amorphous organic solids with a mixed aromatic-aliphatic structure"") that could be created naturally, and rapidly, by stars.[92][93][94] Further, a scientist suggested that these compounds may have been related to the development of life on Earth and said that, ""If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.""[92]
 More than 20% of the carbon in the universe may be associated with polycyclic aromatic hydrocarbons (PAHs), possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.[95] PAHs are subjected to interstellar medium conditions and are transformed through hydrogenation, oxygenation and hydroxylation, to more complex organics—""a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively"".[96][97]
 In October 2020, astronomers proposed the idea of detecting life on distant planets by studying the shadows of trees at certain times of the day to find patterns that could be detected through observation of exoplanets.[98][99]
 The Rare Earth hypothesis postulates that multicellular life forms found on Earth may actually be more of a rarity than scientists assume. According to this hypothesis, life on Earth (and more, multi-cellular life) is possible because of a conjunction of the right circumstances (galaxy and location within it, planetary system, star, orbit, planetary size, atmosphere, etc.); and the chance for all those circumstances to repeat elsewhere may be rare. It provides a possible answer to the Fermi paradox which suggests, ""If extraterrestrial aliens are common, why aren't they obvious?"" It is apparently in opposition to the principle of mediocrity, assumed by famed astronomers Frank Drake, Carl Sagan, and others. The principle of mediocrity suggests that life on Earth is not exceptional, and it is more than likely to be found on innumerable other worlds.
 Research into the environmental limits of life and the workings of extreme ecosystems is ongoing, enabling researchers to better predict what planetary environments might be most likely to harbor life. Missions such as the Phoenix lander, Mars Science Laboratory, ExoMars, Mars 2020 rover to Mars, and the Cassini probe to Saturn's moons aim to further explore the possibilities of life on other planets in the Solar System.
 The two Viking landers each carried four types of biological experiments to the surface of Mars in the late 1970s. These were the only Mars landers to carry out experiments looking specifically for metabolism by current microbial life on Mars. The landers used a robotic arm to collect soil samples into sealed test containers on the craft. The two landers were identical, so the same tests were carried out at two places on Mars' surface; Viking 1 near the equator and Viking 2 further north.[100] The result was inconclusive,[101] and is still disputed by some scientists.[102][103][104][105]
 Norman Horowitz was the chief of the Jet Propulsion Laboratory bioscience section for the Mariner and Viking missions from 1965 to 1976.  Horowitz considered that the great versatility of the carbon atom makes it the element most likely to provide solutions, even exotic solutions, to the problems of survival of life on other planets.[106]  However, he also considered that the conditions found on Mars were incompatible with carbon based life.
 Beagle 2 was an unsuccessful British Mars lander that formed part of the European Space Agency's 2003 Mars Express mission. Its primary purpose was to search for signs of life on Mars, past or present. Although it landed safely, it was unable to correctly deploy its solar panels and telecom antenna.[107]
 EXPOSE is a multi-user facility mounted in 2008 outside the International Space Station dedicated to astrobiology.[108][109] EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights that allow exposure of organic chemicals and biological samples to outer space in low Earth orbit.[110]
 The Mars Science Laboratory (MSL) mission landed the Curiosity rover that is currently in operation on Mars.[111] It was launched 26 November 2011, and landed at Gale Crater on 6 August 2012. Mission objectives are to help assess Mars' habitability and in doing so, determine whether Mars is or has ever been able to support life,[112] collect data for a future human mission, study Martian geology, its climate, and further assess the role that water, an essential ingredient for life as we know it, played in forming minerals on Mars.
 The Tanpopo mission is an orbital astrobiology experiment investigating the potential interplanetary transfer of life, organic compounds, and possible terrestrial particles in the low Earth orbit. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of microbial life as well as prebiotic organic compounds. Early mission results show evidence that some clumps of microorganism can survive for at least one year in space.[113] This may support the idea that clumps greater than 0.5 millimeters of microorganisms could be one way for life to spread from planet to planet.[113]
 ExoMars is a robotic mission to Mars to search for possible biosignatures of Martian life, past or present. This astrobiological mission is currently under development by the European Space Agency (ESA) in partnership with the Russian Federal Space Agency (Roscosmos); it is planned for a 2022 launch.[114][115][116]
 Mars 2020 successfully landed its rover Perseverance in Jezero Crater on 18 February 2021. It will investigate environments on Mars relevant to astrobiology, investigate its surface geological processes and history, including the assessment of its past habitability and potential for preservation of biosignatures and biomolecules within accessible geological materials.[117] The Science Definition Team is proposing the rover collect and package at least 31 samples of rock cores and soil for a later mission to bring back for more definitive analysis in laboratories on Earth. The rover could make measurements and technology demonstrations to help designers of a human expedition understand any hazards posed by Martian dust and demonstrate how to collect carbon dioxide (CO2), which could be a resource for making molecular oxygen (O2) and rocket fuel.[118][119]
 Europa Clipper is a mission planned by NASA for a 2025 launch that will conduct detailed reconnaissance of Jupiter's moon Europa and will investigate whether its internal ocean could harbor conditions suitable for life.[120][121] It will also aid in the selection of future landing sites.[122][123]
 Dragonfly is a NASA mission scheduled to land on Titan in 2036 to assess its microbial habitability and study its prebiotic chemistry. Dragonfly is a rotorcraft lander that will perform controlled flights between multiple locations on the surface, which allows sampling of diverse regions and geological contexts.[124]
 Icebreaker Life is a lander mission that was proposed for NASA's Discovery Program for the 2021 launch opportunity,[125] but it was not selected for development. It would have had a stationary lander that would be a near copy of the successful 2008 Phoenix and it would have carried an upgraded astrobiology scientific payload, including a 1-meter-long core drill to sample ice-cemented ground in the northern plains to conduct a search for organic molecules and evidence of current or past life on Mars.[126][127] One of the key goals of the Icebreaker Life mission is to test the hypothesis that the ice-rich ground in the polar regions has significant concentrations of organics due to protection by the ice from oxidants and radiation.
 Journey to Enceladus and Titan (JET) is an astrobiology mission concept to assess the habitability potential of Saturn's moons Enceladus and Titan by means of an orbiter.[128][129][130]
 Enceladus Life Finder (ELF) is a proposed astrobiology mission concept for a space probe intended to assess the habitability of the internal aquatic ocean of Enceladus, Saturn's sixth-largest moon.[131][132]
 Life Investigation For Enceladus (LIFE) is a proposed astrobiology sample-return mission concept. The spacecraft would enter into Saturn orbit and enable multiple flybys through Enceladus' icy plumes to collect icy plume particles and volatiles and return them to Earth on a capsule. The spacecraft may sample Enceladus' plumes, the E ring of Saturn, and the upper atmosphere of Titan.[133][134][135]
 Oceanus is an orbiter proposed in 2017 for the New Frontiers mission No. 4. It would travel to the moon of Saturn, Titan, to assess its habitability.[136] Oceanus' objectives are to reveal Titan's organic chemistry, geology, gravity, topography, collect 3D reconnaissance data, catalog the organics and determine where they may interact with liquid water.[137]
 Explorer of Enceladus and Titan (E2T) is an orbiter mission concept that would investigate the evolution and habitability of the Saturnian satellites Enceladus and Titan. The mission concept was proposed in 2017 by the European Space Agency.[138]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['geobiology of organisms that can survive in extreme environments on Earth', 'Enceladus and Titan', 'water', 'deterministic conditions and contingent events', 'astrobiological applications'], 'answer_start': [], 'answer_end': []}"
"
 An exoplanet or extrasolar planet is a planet outside the Solar System. The first possible evidence of an exoplanet was noted in 1917 but was not then recognized as such. The first confirmation of the detection occurred in 1992. A different planet, first detected in 1988, was confirmed in 2003. As of 1 May 2024, there are 5,662 confirmed exoplanets in 4,169 planetary systems, with 896 systems having more than one planet.[3][4] The James Webb Space Telescope (JWST) is expected to discover more exoplanets, and to give more insight into their traits, such as their composition, environmental conditions, and potential for life.[5]
 There are many methods of detecting exoplanets. Transit photometry and Doppler spectroscopy have found the most, but these methods suffer from a clear observational bias favoring the detection of planets near the star; thus, 85% of the exoplanets detected are inside the tidal locking zone.[6] In several cases, multiple planets have been observed around a star.[7] About 1 in 5 Sun-like stars[a] have an ""Earth-sized""[b] planet in the habitable zone.[c][8][9] Assuming there are 200 billion stars in the Milky Way,[d] it can be hypothesized that there are 11 billion potentially habitable Earth-sized planets in the Milky Way, rising to 40 billion if planets orbiting the numerous red dwarfs are included.[10]
 The least massive exoplanet known is Draugr (also known as PSR B1257+12 A or PSR B1257+12 b), which is about twice the mass of the Moon. The most massive exoplanet listed on the NASA Exoplanet Archive is HR 2562 b,[11][12][13] about 30 times the mass of Jupiter. However, according to some definitions of a planet (based on the nuclear fusion of deuterium[14]), it is too massive to be a planet and might be a brown dwarf instead. Known orbital times for exoplanets vary from less than an hour (for those closest to their star) to thousands of years. Some exoplanets are so far away from the star that it is difficult to tell whether they are gravitationally bound to it.
 Almost all planets detected so far are within the Milky Way. However, there is evidence that extragalactic planets, exoplanets located in other galaxies, may exist.[15][16] The nearest exoplanets are located 4.2 light-years (1.3 parsecs) from Earth and orbit Proxima Centauri, the closest star to the Sun.[17]
 The discovery of exoplanets has intensified interest in the search for extraterrestrial life. There is special interest in planets that orbit in a star's habitable zone (sometimes called ""goldilocks zone""), where it is possible for liquid water, a prerequisite for life as we know it, to exist on the surface. However, the study of planetary habitability also considers a wide range of other factors in determining the suitability of a planet for hosting life.[18]
 Rogue planets are those that do not orbit any star. Such objects are considered a separate category of planets, especially if they are gas giants, often counted as sub-brown dwarfs.[19] The rogue planets in the Milky Way possibly number in the billions or more.[20][21]
 The official definition of the term planet used by the International Astronomical Union (IAU) only covers the Solar System and thus does not apply to exoplanets.[22][23] The IAU Working Group on Extrasolar Planets issued a position statement containing a working definition of ""planet"" in 2001 and which was modified in 2003.[24] An exoplanet was defined by the following criteria:
 This working definition was amended by the IAU's Commission F2: Exoplanets and the Solar System in August 2018.[25][26] The official working definition of an exoplanet is now as follows:
 The IAU noted that this definition could be expected to evolve as knowledge improves.
 The IAU's working definition is not always used. One alternate suggestion is that planets should be distinguished from brown dwarfs on the basis of their formation. It is widely thought that giant planets form through core accretion, which may sometimes produce planets with masses above the deuterium fusion threshold;[27][28][14] massive planets of that sort may have already been observed.[29] Brown dwarfs form like stars from the direct gravitational collapse of clouds of gas, and this formation mechanism also produces objects that are below the 13 MJup limit and can be as low as 1 MJup.[30] Objects in this mass range that orbit their stars with wide separations of hundreds or thousands of AU and have large star/object mass ratios likely formed as brown dwarfs; their atmospheres would likely have a composition more similar to their host star than accretion-formed planets, which would contain increased abundances of heavier elements. Most directly imaged planets as of April 2014 are massive and have wide orbits so probably represent the low-mass end of a brown dwarf formation.[31]
One study suggests that objects above 10 MJup formed through gravitational instability and should not be thought of as planets.[32]
 Also, the 13-Jupiter-mass cutoff does not have a precise physical significance. Deuterium fusion can occur in some objects with a mass below that cutoff.[14] The amount of deuterium fused depends to some extent on the composition of the object.[33] As of 2011, the Extrasolar Planets Encyclopaedia included objects up to 25 Jupiter masses, saying, ""The fact that there is no special feature around 13 MJup in the observed mass spectrum reinforces the choice to forget this mass limit"".[34] 
As of 2016, this limit was increased to 60 Jupiter masses[35] based on a study of mass–density relationships.[36]
The Exoplanet Data Explorer includes objects up to 24 Jupiter masses with the advisory: ""The 13 Jupiter-mass distinction by the IAU Working Group is physically unmotivated for planets with rocky cores, and observationally problematic due to the sin i ambiguity.""[37]
The NASA Exoplanet Archive includes objects with a mass (or minimum mass) equal to or less than 30 Jupiter masses.[38]
Another criterion for separating planets and brown dwarfs, rather than deuterium fusion, formation process or location, is whether the core pressure is dominated by Coulomb pressure or electron degeneracy pressure with the dividing line at around 5 Jupiter masses.[39][40]
 The convention for naming exoplanets is an extension of the system used for designating multiple-star systems as adopted by the International Astronomical Union (IAU). For exoplanets orbiting a single star, the IAU designation is formed by taking the designated or proper name of its parent star, and adding a lower case letter.[42] Letters are given in order of each planet's discovery around the parent star, so that the first planet discovered in a system is designated ""b"" (the parent star is considered ""a"") and later planets are given subsequent letters. If several planets in the same system are discovered at the same time, the closest one to the star gets the next letter, followed by the other planets in order of orbital size. A provisional IAU-sanctioned standard exists to accommodate the designation of circumbinary planets. A limited number of exoplanets have IAU-sanctioned proper names. Other naming systems exist.
 For centuries scientists, philosophers, and science fiction writers suspected that extrasolar planets existed, but there was no way of knowing whether they were real in fact, how common they were, or how similar they might be to the planets of the Solar System. Various detection claims made in the nineteenth century were rejected by astronomers.
 The first evidence of a possible exoplanet, orbiting Van Maanen 2, was noted in 1917, but was not recognized as such. The astronomer Walter Sydney Adams, who later became director of the Mount Wilson Observatory, produced a spectrum of the star using Mount Wilson's 60-inch telescope. He interpreted the spectrum to be of an F-type main-sequence star, but it is now thought that such a spectrum could be caused by the residue of a nearby exoplanet that had been pulverized by the gravity of the star, the resulting dust then falling onto the star.[43]
 The first suspected scientific detection of an exoplanet occurred in 1988. Shortly afterwards, the first confirmation of detection came in 1992 from the Arecibo Observatory, with the discovery of several terrestrial-mass planets orbiting the pulsar PSR B1257+12.[44] The first confirmation of an exoplanet orbiting a main-sequence star was made in 1995, when a giant planet was found in a four-day orbit around the nearby star 51 Pegasi. Some exoplanets have been imaged directly by telescopes, but the vast majority have been detected through indirect methods, such as the transit method and the radial-velocity method. In February 2018, researchers using the Chandra X-ray Observatory, combined with a planet detection technique called microlensing, found evidence of planets in a distant galaxy, stating, ""Some of these exoplanets are as (relatively) small as the moon, while others are as massive as Jupiter. Unlike Earth, most of the exoplanets are not tightly bound to stars, so they're actually wandering through space or loosely orbiting between stars. We can estimate that the number of planets in this [faraway] galaxy is more than a trillion.""[45]
 On 21 March 2022, the 5000th exoplanet beyond the Solar System was confirmed.[46]
 On 11 January 2023, NASA scientists reported the detection of LHS 475 b, an Earth-like exoplanet – and the first exoplanet discovered by the James Webb Space Telescope.[47]
 This space we declare to be infinite... In it are an infinity of worlds of the same kind as our own. In the sixteenth century, the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun (heliocentrism), put forward the view that fixed stars are similar to the Sun and are likewise accompanied by planets.
 In the eighteenth century, the same possibility was mentioned by Isaac Newton in the ""General Scholium"" that concludes his Principia. Making a comparison to the Sun's planets, he wrote ""And if the fixed stars are the centres of similar systems, they will all be constructed according to a similar design and subject to the dominion of One.""[49]
 In 1952, more than 40 years before the first hot Jupiter was discovered, Otto Struve wrote that there is no compelling reason that planets could not be much closer to their parent star than is the case in the Solar System, and proposed that Doppler spectroscopy and the transit method could detect super-Jupiters in short orbits.[50]
 Claims of exoplanet detections have been made since the nineteenth century. Some of the earliest involve the binary star 70 Ophiuchi. In 1855, William Stephen Jacob at the East India Company's Madras Observatory reported that orbital anomalies made it ""highly probable"" that there was a ""planetary body"" in this system.[51] In the 1890s, Thomas J. J. See of the University of Chicago and the United States Naval Observatory stated that the orbital anomalies proved the existence of a dark body in the 70 Ophiuchi system with a 36-year period around one of the stars.[52] However, Forest Ray Moulton published a paper proving that a three-body system with those orbital parameters would be highly unstable.[53]
 During the 1950s and 1960s, Peter van de Kamp of Swarthmore College made another prominent series of detection claims, this time for planets orbiting Barnard's Star.[54] Astronomers now generally regard all early reports of detection as erroneous.[55]
 In 1991, Andrew Lyne, M. Bailes and S. L. Shemar claimed to have discovered a pulsar planet in orbit around PSR 1829-10, using pulsar timing variations.[56] The claim briefly received intense attention, but Lyne and his team soon retracted it.[57]
 As of 1 May 2024, a total of 5,662 confirmed exoplanets are listed in the Extrasolar Planets Encyclopaedia, including a few that were confirmations of controversial claims from the late 1980s.[3]  The first published discovery to receive subsequent confirmation was made in 1988 by the Canadian astronomers Bruce Campbell, G. A. H. Walker, and Stephenson Yang of the University of Victoria and the University of British Columbia.[58] Although they were cautious about claiming a planetary detection, their radial-velocity observations suggested that a planet orbits the star Gamma Cephei. Partly because the observations were at the very limits of instrumental capabilities at the time, astronomers remained skeptical for several years about this and other similar observations. It was thought some of the apparent planets might instead have been brown dwarfs, objects intermediate in mass between planets and stars. In 1990, additional observations were published that supported the existence of the planet orbiting Gamma Cephei,[59] but subsequent work in 1992 again raised serious doubts.[60] Finally, in 2003, improved techniques allowed the planet's existence to be confirmed.[61]
 On 9 January 1992, radio astronomers Aleksander Wolszczan and Dale Frail announced the discovery of two planets orbiting the pulsar PSR 1257+12.[44] This discovery was confirmed, and is generally considered to be the first definitive detection of exoplanets. Follow-up observations solidified these results, and confirmation of a third planet in 1994 revived the topic in the popular press.[62] These pulsar planets are thought to have formed from the unusual remnants of the supernova that produced the pulsar, in a second round of planet formation, or else to be the remaining rocky cores of gas giants that somehow survived the supernova and then decayed into their current orbits. As pulsars are aggressive stars, it was considered unlikely at the time that a planet may be able to be formed in their orbit.[63]
 In the early 1990s, a group of astronomers led by Donald Backer, who were studying what they thought was a binary pulsar (PSR B1620−26 b), determined that a third object was needed to explain the observed Doppler shifts. Within a few years, the gravitational effects of the planet on the orbit of the pulsar and white dwarf had been measured, giving an estimate of the mass of the third object that was too small for it to be a star. The conclusion that the third object was a planet was announced by Stephen Thorsett and his collaborators in 1993.[64]
 On 6 October 1995, Michel Mayor and Didier Queloz of the University of Geneva announced the first definitive detection of an exoplanet orbiting a main-sequence star, nearby G-type star 51 Pegasi.[65][66][67] This discovery, made at the Observatoire de Haute-Provence, ushered in the modern era of exoplanetary discovery, and was recognized by a share of the 2019 Nobel Prize in Physics. Technological advances, most notably in high-resolution spectroscopy, led to the rapid detection of many new exoplanets: astronomers could detect exoplanets indirectly by measuring their gravitational influence on the motion of their host stars. More extrasolar planets were later detected by observing the variation in a star's apparent luminosity as an orbiting planet transited in front of it.[65]
 Initially, the most known exoplanets were massive planets that orbited very close to their parent stars. Astronomers were surprised by these ""hot Jupiters"", because theories of planetary formation had indicated that giant planets should only form at large distances from stars. But eventually more planets of other sorts were found, and it is now clear that hot Jupiters make up the minority of exoplanets.[65] In 1999, Upsilon Andromedae became the first main-sequence star known to have multiple planets.[68] Kepler-16 contains the first discovered planet that orbits a binary main-sequence star system.[69]
 On 26 February 2014, NASA announced the discovery of 715 newly verified exoplanets around 305 stars by the Kepler Space Telescope. These exoplanets were checked using a statistical technique called ""verification by multiplicity"".[70][71][72] Before these results, most confirmed planets were gas giants comparable in size to Jupiter or larger because they were more easily detected, but the Kepler planets are mostly between the size of Neptune and the size of Earth.[70]
 On 23 July 2015, NASA announced Kepler-452b, a near-Earth-size planet orbiting the habitable zone of a G2-type star.[73]
 On 6 September 2018, NASA discovered an exoplanet about 145 light years away from Earth in the constellation Virgo.[74] This exoplanet, Wolf 503b, is twice the size of Earth and was discovered orbiting a type of star known as an ""Orange Dwarf"". Wolf 503b completes one orbit in as few as six days because it is very close to the star. Wolf 503b is the only exoplanet that large that can be found near the so-called small planet radius gap. The gap, sometimes called the Fulton gap,[74][75] is the observation that it is unusual to find exoplanets with sizes between 1.5 and 2 times the radius of the Earth.[76]
 In January 2020, scientists announced the discovery of TOI 700 d, the first Earth-sized planet in the habitable zone detected by TESS.[77]
 As of January 2020, NASA's Kepler and TESS missions had identified 4374 planetary candidates yet to be confirmed,[78] several of them being nearly Earth-sized and located in the habitable zone, some around Sun-like stars.[79][80][81]
 In September 2020, astronomers reported evidence, for the first time, of an extragalactic planet, M51-ULS-1b, detected by eclipsing a bright X-ray source (XRS), in the Whirlpool Galaxy (M51a).[84][85]
 Also in September 2020, astronomers using microlensing techniques reported the detection, for the first time, of an Earth-mass rogue planet unbounded by any star, and free floating in the Milky Way galaxy.[86][87]
 Planets are extremely faint compared to their parent stars. For example, a Sun-like star is about a billion times brighter than the reflected light from any exoplanet orbiting it. It is difficult to detect such a faint light source, and furthermore, the parent star causes a glare that tends to wash it out. It is necessary to block the light from the parent star to reduce the glare while leaving the light from the planet detectable; doing so is a major technical challenge which requires extreme optothermal stability.[88] All exoplanets that have been directly imaged are both large (more massive than Jupiter) and widely separated from their parent stars.
 Specially designed direct-imaging instruments such as Gemini Planet Imager, VLT-SPHERE, and SCExAO will image dozens of gas giants, but the vast majority of known extrasolar planets have only been detected through indirect methods.
 Planets may form within a few to tens (or more) of millions of years of their star forming.[102][103][104][105][106] 
The planets of the Solar System can only be observed in their current state, but observations of different planetary systems of varying ages allows us to observe planets at different stages of evolution. Available observations range from young proto-planetary disks where planets are still forming[107] to planetary systems of over 10 Gyr old.[108] When planets form in a gaseous protoplanetary disk,[109] they accrete hydrogen/helium envelopes.[110][111] These envelopes cool and contract over time and, depending on the mass of the planet, some or all of the hydrogen/helium is eventually lost to space.[109] This means that even terrestrial planets may start off with large radii if they form early enough.[112][113][114] An example is Kepler-51b which has only about twice the mass of Earth but is almost the size of Saturn, which is a hundred times the mass of Earth. Kepler-51b is quite young at a few hundred million years old.[115]
 There is at least one planet on average per star.[7]
About 1 in 5 Sun-like stars[a] have an ""Earth-sized""[b] planet in the habitable zone.[117]
 Most known exoplanets orbit stars roughly similar to the Sun, i.e. main-sequence stars of spectral categories F, G, or K. Lower-mass stars (red dwarfs, of spectral category M) are less likely to have planets massive enough to be detected by the radial-velocity method.[118][119] Despite this, several tens of planets around red dwarfs have been discovered by the Kepler telescope, which uses the transit method to detect smaller planets.
 Using data from Kepler, a correlation has been found between the metallicity of a star and the probability that the star hosts a giant planet, similar to the size of Jupiter. Stars with higher metallicity are more likely to have planets, especially giant planets, than stars with lower metallicity.[120]
 Some planets orbit one member of a binary star system,[121] and several circumbinary planets have been discovered which orbit both members of a binary star. A few planets in triple star systems are known[122] and one in the quadruple system Kepler-64.
 In 2013, the color of an exoplanet was determined for the first time. The best-fit albedo measurements of HD 189733b suggest that it is deep dark blue.[123][124] Later that same year, the colors of several other exoplanets were determined, including GJ 504 b which visually has a magenta color,[125] and Kappa Andromedae b, which if seen up close would appear reddish in color.[126] Helium planets are expected to be white or grey in appearance.[127]
 The apparent brightness (apparent magnitude) of a planet depends on how far away the observer is, how reflective the planet is (albedo), and how much light the planet receives from its star, which depends on how far the planet is from the star and how bright the star is. So, a planet with a low albedo that is close to its star can appear brighter than a planet with a high albedo that is far from the star.[128]
 The darkest known planet in terms of geometric albedo is TrES-2b, a hot Jupiter that reflects less than 1% of the light from its star, making it less reflective than coal or black acrylic paint. Hot Jupiters are expected to be quite dark due to sodium and potassium in their atmospheres, but it is not known why TrES-2b is so dark—it could be due to an unknown chemical compound.[129][130][131]
 For gas giants, geometric albedo generally decreases with increasing metallicity or atmospheric temperature unless there are clouds to modify this effect. Increased cloud-column depth increases the albedo at optical wavelengths, but decreases it at some infrared wavelengths. Optical albedo increases with age, because older planets have higher cloud-column depths. Optical albedo decreases with increasing mass, because higher-mass giant planets have higher surface gravities, which produces lower cloud-column depths. Also, elliptical orbits can cause major fluctuations in atmospheric composition, which can have a significant effect.[132]
 There is more thermal emission than reflection at some near-infrared wavelengths for massive and/or young gas giants. So, although optical brightness is fully phase-dependent, this is not always the case in the near infrared.[132]
 Temperatures of gas giants reduce over time and with distance from their stars. Lowering the temperature increases optical albedo even without clouds. At a sufficiently low temperature, water clouds form, which further increase optical albedo. At even lower temperatures, ammonia clouds form, resulting in the highest albedos at most optical and near-infrared wavelengths.[132]
 In 2014, a magnetic field around HD 209458 b was inferred from the way hydrogen was evaporating from the planet. It is the first (indirect) detection of a magnetic field on an exoplanet. The magnetic field is estimated to be about one-tenth as strong as Jupiter's.[133][134]
 The magnetic fields of exoplanets may be detectable by their auroral radio emissions with sensitive enough radio telescopes such as LOFAR.[135][136] The radio emissions could enable determination of the rotation rate of the interior of an exoplanet, and may yield a more accurate way to measure exoplanet rotation than by examining the motion of clouds.[137]
 Earth's magnetic field results from its flowing liquid metallic core, but on massive super-Earths with high pressure, different compounds may form which do not match those created under terrestrial conditions. Compounds may form with greater viscosities and high melting temperatures, which could prevent the interiors from separating into different layers and so result in undifferentiated coreless mantles. Forms of magnesium oxide such as MgSi3O12 could be a liquid metal at the pressures and temperatures found in super-Earths and could generate a magnetic field in the mantles of super-Earths.[138][139]
 Hot Jupiters have been observed to have a larger radius than expected. This could be caused by the interaction between the stellar wind and the planet's magnetosphere creating an electric current through the planet that heats it up (Joule heating) causing it to expand. The more magnetically active a star is, the greater the stellar wind and the larger the electric current leading to more heating and expansion of the planet. This theory matches the observation that stellar activity is correlated with inflated planetary radii.[140]
 In August 2018, scientists announced the transformation of gaseous deuterium into a liquid metallic hydrogen form. This may help researchers better understand giant gas planets, such as Jupiter, Saturn and related exoplanets, since such planets are thought to contain a lot of liquid metallic hydrogen, which may be responsible for their observed powerful magnetic fields.[141][142]
 Although scientists previously announced that the magnetic fields of close-in exoplanets may cause increased stellar flares and starspots on their host stars, in 2019 this claim was demonstrated to be false in the HD 189733 system.  The failure to detect ""star-planet interactions"" in the well-studied HD 189733 system calls other related claims of the effect into question.[143]
 In 2019, the strength of the surface magnetic fields of 4 hot Jupiters were estimated and ranged between 20 and 120 gauss compared to Jupiter's surface magnetic field of 4.3 gauss.[144][145]
 In 2007, two independent teams of researchers came to opposing conclusions about the likelihood of plate tectonics on larger super-Earths[146][147] with one team saying that plate tectonics would be episodic or stagnant[148] and the other team saying that plate tectonics is very likely on super-Earths even if the planet is dry.[149]
 If super-Earths have more than 80 times as much water as Earth, then they become ocean planets with all land completely submerged. However, if there is less water than this limit, then the deep water cycle will move enough water between the oceans and mantle to allow continents to exist.[150][151]
 Large surface temperature variations on 55 Cancri e have been attributed to possible volcanic activity releasing large clouds of dust which blanket the planet and block thermal emissions.[152][153]
 The star 1SWASP J140747.93-394542.6 is orbited by an object that is circled by a ring system much larger than Saturn's rings. However, the mass of the object is not known; it could be a brown dwarf or low-mass star instead of a planet.[154][155]
 The brightness of optical images of Fomalhaut b could be due to starlight reflecting off a circumplanetary ring system with a radius between 20 and 40 times that of Jupiter's radius, about the size of the orbits of the Galilean moons.[156]
 The rings of the Solar System's gas giants are aligned with their planet's equator. However, for exoplanets that orbit close to their star, tidal forces from the star would lead to the outermost rings of a planet being aligned with the planet's orbital plane around the star. A planet's innermost rings would still be aligned with the planet's equator so that if the planet has a tilted rotational axis, then the different alignments between the inner and outer rings would create a warped ring system.[157]
 In December 2013 a candidate exomoon of a rogue planet was announced.[158] On 3 October 2018, evidence suggesting a large exomoon orbiting Kepler-1625b was reported.[159]
 Atmospheres have been detected around several exoplanets. The first to be observed was HD 209458 b in 2001.[161]
 As of February 2014, more than fifty transiting and five directly imaged exoplanet atmospheres have been observed,[162] resulting in detection of molecular spectral features; observation of day–night temperature gradients; and constraints on vertical atmospheric structure.[163] Also, an atmosphere has been detected on the non-transiting hot Jupiter Tau Boötis b.[164][165]
 In May 2017, glints of light from Earth, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.[166][167] The technology used to determine this may be useful in studying the atmospheres of distant worlds, including those of exoplanets.
 KIC 12557548 b is a small rocky planet, very close to its star, that is evaporating and leaving a trailing tail of cloud and dust like a comet.[168] The dust could be ash erupting from volcanos and escaping due to the small planet's low surface-gravity, or it could be from metals that are vaporized by the high temperatures of being so close to the star with the metal vapor then condensing into dust.[169]
 In June 2015, scientists reported that the atmosphere of GJ 436 b was evaporating, resulting in a giant cloud around the planet and, due to radiation from the host star, a long trailing tail 14 million km (9 million mi) long.[170]
 Tidally locked planets in a 1:1 spin-orbit resonance would have their star always shining directly overhead on one spot, which would be hot with the opposite hemisphere receiving no light and being freezing cold. Such a planet could resemble an eyeball, with the hotspot being the pupil.[171] Planets with an eccentric orbit could be locked in other resonances. 3:2 and 5:2 resonances would result in a double-eyeball pattern with hotspots in both eastern and western hemispheres.[172] Planets with both an eccentric orbit and a tilted axis of rotation would have more complicated insolation patterns.[173]
 Surface features can be distinguished from atmospheric features by comparing emission and reflection spectroscopy with transmission spectroscopy. Mid-infrared spectroscopy of exoplanets may detect rocky surfaces, and near-infrared may identify magma oceans or high-temperature lavas, hydrated silicate surfaces and water ice, giving an unambiguous method to distinguish between rocky and gaseous exoplanets.[174]
 Measuring the intensity of the light it receives from its parent star can estimate the temperature of an exoplanet. For example, the planet OGLE-2005-BLG-390Lb is estimated to have a surface temperature of roughly −220 °C (50 K). However, such estimates may be substantially in error because they depend on the planet's usually unknown albedo, and because factors such as the greenhouse effect may introduce unknown complications. A few planets have had their temperature measured by observing the variation in infrared radiation as the planet moves around in its orbit and is eclipsed by its parent star. For example, the planet HD 189733b has been estimated to have an average temperature of 1,205 K (932 °C) on its dayside and 973 K (700 °C) on its nightside.[176]
 As more planets are discovered, the field of exoplanetology continues to grow into a deeper study of extrasolar worlds, and will ultimately tackle the prospect of life on planets beyond the Solar System.[177] At cosmic distances, life can only be detected if it is developed at a planetary scale and strongly modified the planetary environment, in such a way that the modifications cannot be explained by classical physico-chemical processes (out of equilibrium processes).[177] For example, molecular oxygen (O2) in the atmosphere of Earth is a result of photosynthesis by living plants and many kinds of microorganisms, so it can be used as an indication of life on exoplanets, although small amounts of oxygen could also be produced by non-biological means.[178] Furthermore, a potentially habitable planet must orbit a stable star at a distance within which planetary-mass objects with sufficient atmospheric pressure can support liquid water at their surfaces.[179][180]
 The habitable zone around a star is the region where the temperature is just right to allow liquid water to exist on the surface of a planet; that is, not too close to the star for the water to evaporate and not too far away from the star for the water to freeze. The heat produced by stars varies depending on the size and age of the star, so that the habitable zone can be at different distances for different stars. Also, the atmospheric conditions on the planet influence the planet's ability to retain heat so that the location of the habitable zone is also specific to each type of planet: desert planets (also known as dry planets), with very little water, will have less water vapor in the atmosphere than Earth and so have a reduced greenhouse effect, meaning that a desert planet could maintain oases of water closer to its star than Earth is to the Sun. The lack of water also means there is less ice to reflect heat into space, so the outer edge of desert-planet habitable zones is further out.[181][182] Rocky planets with a thick hydrogen atmosphere could maintain surface water much further out than the Earth–Sun distance.[183] Planets with larger mass have wider habitable zones because gravity reduces the water cloud column depth which reduces the greenhouse effect of water vapor, thus moving the inner edge of the habitable zone closer to the star.[184]
 Planetary rotation rate is one of the major factors determining the circulation of the atmosphere and hence the pattern of clouds: slowly rotating planets create thick clouds that reflect more and so can be habitable much closer to their star. Earth with its current atmosphere would be habitable in Venus's orbit, if it had Venus's slow rotation. If Venus lost its water ocean due to a runaway greenhouse effect, it is likely to have had a higher rotation rate in the past. Alternatively, Venus never had an ocean because water vapor was lost to space during its formation [185] and could have had its slow rotation throughout its history.[186]
 Tidally locked planets (a.k.a. ""eyeball"" planets[187]) can be habitable closer to their star than previously thought due to the effect of clouds: at high stellar flux, strong convection produces thick water clouds near the substellar point that greatly increase the planetary albedo and reduce surface temperatures.[188]
 Planets in the habitable zones of stars with low metallicity are more habitable for complex life on land than high metallicity stars because the stellar spectrum of high metallicity stars is less likely to cause the formation of ozone thus enabling more ultraviolet rays to reach the planet's surface.[189][190]
 Habitable zones have usually been defined in terms of surface temperature, however over half of Earth's biomass is from subsurface microbes,[191] and the temperature increases with depth, so the subsurface can be conducive for microbial life when the surface is frozen and if this is considered, the habitable zone extends much further from the star,[192] even rogue planets could have liquid water at sufficient depths underground.[193] In an earlier era of the universe the temperature of the cosmic microwave background would have allowed any rocky planets that existed to have liquid water on their surface regardless of their distance from a star.[194] Jupiter-like planets might not be habitable, but they could have habitable moons.[195]
 The outer edge of the habitable zone is where planets are completely frozen, but  planets well inside the habitable zone can periodically become frozen. If orbital fluctuations or other causes produce cooling, then this creates more ice, but ice reflects sunlight causing even more cooling, creating a feedback loop until the planet is completely or nearly completely frozen. When the surface is frozen, this stops carbon dioxide weathering, resulting in a build-up of carbon dioxide in the atmosphere from volcanic emissions. This creates a greenhouse effect which thaws the planet again. Planets with a large axial tilt[196] are less likely to enter snowball states and can retain liquid water further from their star. Large fluctuations of axial tilt can have even more of a warming effect than a fixed large tilt.[197][198] Paradoxically, planets orbiting cooler stars, such as red dwarfs, are less likely to enter snowball states because the infrared radiation emitted by cooler stars is mostly at wavelengths that are absorbed by ice which heats it up.[199][200]
 If a planet has an eccentric orbit, then tidal heating can provide another source of energy besides stellar radiation. This means that eccentric planets in the radiative habitable zone can be too hot for liquid water. Tides also circularize orbits over time, so there could be planets in the habitable zone with circular orbits that have no water because they used to have eccentric orbits.[201] Eccentric planets further out than the habitable zone would still have frozen surfaces, but the tidal heating could create a subsurface ocean similar to Europa's.[202] In some planetary systems, such as in the Upsilon Andromedae system, the eccentricity of orbits is maintained or even periodically varied by perturbations from other planets in the system. Tidal heating can cause outgassing from the mantle, contributing to the formation and replenishment of an atmosphere.[203]
 A review in 2015 identified exoplanets Kepler-62f, Kepler-186f and Kepler-442b as the best candidates for being potentially habitable.[204] These are at a distance of 1200, 490 and 1,120 light-years away, respectively. Of these, Kepler-186f is in similar size to Earth with its 1.2-Earth-radius measure, and it is located towards the outer edge of the habitable zone around its red dwarf star.
 When looking at the nearest terrestrial exoplanet candidates, Proxima Centauri b is about 4.2 light-years away. Its equilibrium temperature is estimated to be −39 °C (234 K).[205]
 Exoplanets are often members of planetary systems of multiple planets around a star. The planets interact with each other gravitationally and sometimes form resonant systems where the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.[209]
 Some hot Jupiters orbit their stars in the opposite direction to their stars' rotation.[210] One proposed explanation is that hot Jupiters tend to form in dense clusters, where perturbations are more common and gravitational capture of planets by neighboring stars is possible.[211]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['hot Jupiters tend to form in dense clusters', 'Aleksander Wolszczan and Dale Frail', 'physically unmotivated for planets with rocky cores', 'the formation and replenishment of an atmosphere', 'unknown complications'], 'answer_start': [], 'answer_end': []}"
"Astrophysics is a science that employs the methods and principles of physics and chemistry in the study of astronomical objects and phenomena.[1][2] As one of  the founders of the discipline, James Keeler, said, Astrophysics ""seeks to ascertain the nature of the heavenly bodies, rather than their positions or motions in space–what they are, rather than where they are.""[3] Among the subjects studied are the Sun (solar physics), other stars, galaxies, extrasolar planets, the interstellar medium and the cosmic microwave background.[4][5] Emissions from these objects are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, astrophysicists apply concepts and methods from many disciplines of physics, including classical mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.
 In practice, modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter, dark energy, black holes, and other celestial bodies; and the origin and ultimate fate of the universe.[4] Topics also studied by theoretical astrophysicists include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity, special relativity, quantum and physical cosmology, including string cosmology and astroparticle physics.
 Astronomy is an ancient science, long separated from the study of terrestrial physics. In the Aristotelian worldview, bodies in the sky appeared to be unchanging spheres whose only motion was uniform motion in a circle, while the earthly world was the realm which underwent growth and decay and in which natural motion was in a straight line and ended when the moving object reached its goal.  Consequently, it was held that the celestial region was made of a fundamentally different kind of matter from that found in the terrestrial sphere; either Fire as maintained by Plato, or Aether as maintained by Aristotle.[6][7]
During the 17th century, natural philosophers such as Galileo,[8] Descartes,[9] and Newton[10] began to maintain that the celestial and terrestrial regions were made of similar kinds of material and were subject to the same natural laws.[11]  Their challenge was that the tools had not yet been invented with which to prove these assertions.[12]
 For much of the nineteenth century, astronomical research was focused on the routine work of measuring the positions and computing the motions of astronomical objects.[13][14]  A new astronomy, soon to be called astrophysics, began to emerge when William Hyde Wollaston and Joseph von Fraunhofer independently discovered that, when decomposing the light from the Sun, a multitude of dark lines (regions where there was less or no light) were observed in the spectrum.[15] By 1860 the physicist, Gustav Kirchhoff, and the chemist, Robert Bunsen, had demonstrated that the dark lines in the solar spectrum corresponded to bright lines in the spectra of known gases, specific lines corresponding to unique chemical elements.[16] Kirchhoff deduced that the dark lines in the solar spectrum are caused by absorption by chemical elements in the Solar atmosphere.[17] In this way it was proved that the chemical elements found in the Sun and stars were also found on Earth.
 Among those who extended the study of solar and stellar spectra was Norman Lockyer, who in 1868 detected radiant, as well as dark lines in solar spectra. Working with chemist Edward Frankland to investigate the spectra of elements at various temperatures and pressures, he could not associate a yellow line in the solar spectrum with any known elements.  He thus claimed the line represented a new element, which was called helium, after the Greek Helios, the Sun personified.[18][19]
 In 1885, Edward C. Pickering undertook an ambitious program of stellar spectral classification at Harvard College Observatory, in which a team of woman computers, notably Williamina Fleming, Antonia Maury, and Annie Jump Cannon, classified the spectra recorded on photographic plates. By 1890, a catalog of over 10,000 stars had been prepared that grouped them into thirteen spectral types. Following Pickering's vision, by 1924 Cannon expanded the catalog to nine volumes and over a quarter of a million stars, developing the Harvard Classification Scheme which was accepted for worldwide use in 1922.[20]
 In 1895, George Ellery Hale and James E. Keeler, along with a group of ten associate editors from Europe and the United States,[21]  established The Astrophysical Journal: An International Review of Spectroscopy and Astronomical Physics.[22]  It was intended that the journal would fill the gap between journals in astronomy and physics, providing a venue for publication of articles on astronomical applications of the spectroscope; on laboratory research closely allied to astronomical physics, including wavelength determinations of metallic and gaseous spectra and experiments on radiation and absorption; on theories of the Sun, Moon, planets, comets, meteors, and nebulae; and on instrumentation for telescopes and laboratories.[21]
 Around 1920, following the discovery of the Hertzsprung–Russell diagram still used as the basis for classifying stars and their evolution, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars.[23][24] At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered.[25]
 In 1925 Cecilia Helena Payne (later Cecilia Payne-Gaposchkin) wrote an influential doctoral dissertation at Radcliffe College, in which she applied Saha's ionization theory to stellar atmospheres to relate the spectral classes to the temperature of stars.[26]  Most significantly, she discovered that hydrogen and helium were the principal components of stars, not the composition of Earth. Despite Eddington's suggestion, discovery was so unexpected that her dissertation readers (including Russell) convinced her to modify the conclusion before publication. However, later research confirmed her discovery.[27][28]
 By the end of the 20th century, studies of astronomical spectra had expanded to cover wavelengths extending from radio waves through optical, x-ray, and gamma wavelengths.[29] In the 21st century, it further expanded to include observations based on gravitational waves.
 Observational astronomy is a division of the astronomical science that is concerned with recording and interpreting data, in contrast with theoretical astrophysics, which is mainly concerned with finding out the measurable implications of physical models. It is the practice of observing celestial objects by using telescopes and other astronomical apparatus.
 The majority of astrophysical observations are made using the electromagnetic spectrum.
 Other than electromagnetic radiation, few things may be observed from the Earth that originate from great distances. A few gravitational wave observatories have been constructed, but gravitational waves are extremely difficult to detect. Neutrino observatories have also been built, primarily to study the Sun. Cosmic rays consisting of very high-energy particles can be observed hitting the Earth's atmosphere.
 Observations can also vary in their time scale. Most optical observations take minutes to hours, so phenomena that change faster than this cannot readily be observed. However, historical data on some objects is available, spanning centuries or millennia. On the other hand, radio observations may look at events on a millisecond timescale (millisecond pulsars) or combine years of data (pulsar deceleration studies). The information obtained from these different timescales is very different.
 The study of the Sun has a special place in observational astrophysics. Due to the tremendous distance of all other stars, the Sun can be observed in a kind of detail unparalleled by any other star. Understanding the Sun serves as a guide to understanding of other stars.
 The topic of how stars change, or stellar evolution, is often modeled by placing the varieties of star types in their respective positions on the Hertzsprung–Russell diagram, which can be viewed as representing the state of a stellar object, from birth to destruction.
 Theoretical astrophysicists use a wide variety of tools which include analytical models (for example, polytropes to approximate the behaviors of a star) and computational numerical simulations. Each has some advantages. Analytical models of a process are generally better for giving insight into the heart of what is going on. Numerical models can reveal the existence of phenomena and effects that would otherwise not be seen.[30][31]
 Theorists in astrophysics endeavor to create theoretical models and figure out the observational consequences of those models. This helps allow observers to look for data that can refute a model or help in choosing between several alternate or conflicting models.
 Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.
 Topics studied by theoretical astrophysicists include stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Relativistic astrophysics serves as a tool to gauge the properties of large-scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves.
 Some widely accepted and studied theories and models in astrophysics, now included in the Lambda-CDM model, are the Big Bang, cosmic inflation, dark matter, dark energy and fundamental theories of physics.
 The roots of astrophysics can be found in the seventeenth century emergence of a unified physics, in which the same laws applied to the celestial and terrestrial realms.[11]  There were scientists who were qualified in both physics and astronomy who laid the firm foundation for the current science of astrophysics.  In modern times, students continue to be drawn to astrophysics due to its popularization by the Royal Astronomical Society and notable educators such as prominent professors Lawrence Krauss, Subrahmanyan Chandrasekhar, Stephen Hawking, Hubert Reeves, Carl Sagan and Patrick Moore. The efforts of the early, late, and present scientists continue to attract young people to study the history and science of astrophysics.[32][33][34]
The television sitcom show The Big Bang Theory popularized the field of astrophysics with the general public, and featured some well known scientists like Stephen Hawking and Neil deGrasse Tyson.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['how stars change, or stellar evolution', 'Galileo,[8] Descartes,[9] and Newton', 'the Big Bang', 'cosmic inflation, dark matter, dark energy and fundamental theories of physics', 'the tools had not yet been invented with which to prove these assertions'], 'answer_start': [], 'answer_end': []}"
"
 Particle physics or high-energy physics is the study of fundamental particles and forces that constitute matter and radiation. The field also studies combinations of elementary particles up to the scale of protons and neutrons, while the study of combination of protons and neutrons is called nuclear physics.
 The fundamental particles in the universe are classified in the Standard Model as fermions (matter particles) and bosons (force-carrying particles). There are three generations of fermions,  although ordinary matter is made only from the first fermion generation. The first generation consists of up and down quarks which form  protons and neutrons,  and electrons and electron neutrinos. The three fundamental interactions known to be mediated by bosons are electromagnetism, the weak interaction, and the strong interaction.
 Quarks cannot exist on their own but form hadrons.  Hadrons that contain an odd number of quarks are called baryons and those that contain an even number are called mesons. Two baryons, the proton and the neutron, make up most of the mass of ordinary matter. Mesons are unstable and the longest-lived last for only a few hundredths of a microsecond. They occur after collisions between particles made of quarks, such as fast-moving protons and neutrons in cosmic rays. Mesons are also produced in cyclotrons or other particle accelerators.
 Particles have corresponding  antiparticles with the same mass but with opposite electric charges. For example, the antiparticle of the electron is the positron. The electron has a negative electric charge, the positron has a positive charge. These antiparticles can theoretically form a corresponding form of matter called antimatter. Some particles, such as the photon, are their own antiparticle.
 These elementary particles are excitations of the quantum fields that also govern their interactions. The dominant theory explaining these fundamental particles and fields, along with their dynamics, is called the Standard Model. The reconciliation of gravity to the current particle physics theory is not solved; many theories have addressed this problem, such as loop quantum gravity, string theory and supersymmetry theory.
 Practical particle physics is the study of these particles in radioactive processes and in particle accelerators such as the Large Hadron Collider. Theoretical particle physics is the study of these particles in the context of cosmology and quantum theory. The two are closely interrelated: the Higgs boson was postulated by theoretical particle physicists and its presence confirmed by practical experiments.
 The idea that all matter is fundamentally composed of elementary particles dates from at least the 6th century BC.[1] In the 19th century, John Dalton, through his work on stoichiometry, concluded that each element of nature was composed of a single, unique type of particle.[2] The word atom, after the Greek word atomos meaning ""indivisible"", has since then denoted the smallest particle of a chemical element, but physicists later discovered that atoms are not, in fact, the fundamental particles of nature, but are conglomerates of even smaller particles, such as the electron. The early 20th century explorations of nuclear physics and quantum physics led to proofs of nuclear fission in 1939 by Lise Meitner (based on experiments by Otto Hahn), and nuclear fusion by Hans Bethe in that same year; both discoveries also led to the development of nuclear weapons.
 Throughout the 1950s and 1960s, a bewildering variety of particles was found in collisions of particles from beams of increasingly high energy. It was referred to informally as the ""particle zoo"". Important discoveries such as the CP violation by James Cronin and Val Fitch brought new questions to matter-antimatter imbalance.[3]  After the formulation of the Standard Model during the 1970s, physicists clarified the origin of the particle zoo. The large number of particles was explained as combinations of a (relatively) small number of more fundamental particles and framed in the context of quantum field theories. This reclassification marked the beginning of modern particle physics.[4][5]
 The current state of the classification of all elementary particles is explained by the Standard Model, which gained widespread acceptance in the mid-1970s after experimental confirmation of the existence of quarks. It describes the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons. The species of gauge bosons are eight gluons, W−, W+ and Z bosons, and the photon.[6] The Standard Model also contains 24 fundamental fermions (12 particles and their associated anti-particles), which are the constituents of all matter.[7] Finally, the Standard Model also predicted the existence of a type of boson known as the Higgs boson. On 4 July 2012, physicists with the Large Hadron Collider at CERN announced they had found a new particle that behaves similarly to what is expected from the Higgs boson.[8]
 The Standard Model, as currently formulated, has 61 elementary particles.[9] Those elementary particles can combine to form composite particles, accounting for the hundreds of other species of particles that have been discovered since the 1960s. The Standard Model has been found to agree with almost all the experimental tests conducted to date. However, most particle physicists believe that it is an incomplete description of nature and that a more fundamental theory awaits discovery (See Theory of Everything). In recent years, measurements of neutrino mass have provided the first experimental deviations from the Standard Model, since neutrinos do not have mass in the Standard Model.[10]
 Modern particle physics research is focused on subatomic particles, including atomic constituents, such as electrons, protons, and neutrons (protons and neutrons are composite particles called baryons, made of quarks), that are produced by radioactive and scattering processes; such particles are photons, neutrinos, and muons, as well as a wide range of exotic particles.[11] All particles and their interactions observed to date can be described almost entirely by the Standard Model.[6]
 Dynamics of particles are also governed by quantum mechanics; they exhibit wave–particle duality, displaying particle-like behaviour under certain experimental conditions and wave-like behaviour in others. In more technical terms, they are described by quantum state vectors in a Hilbert space, which is also treated in quantum field theory. Following the convention of particle physicists, the term elementary particles is applied to those particles that are, according to current understanding, presumed to be indivisible and not composed of other particles.[9]
 Ordinary matter is made from first-generation quarks (up, down) and leptons (electron, electron neutrino).[12] Collectively, quarks and leptons are called fermions, because they have a quantum spin of half-integers (-1/2, 1/2, 3/2, etc.). This causes the fermions to obey the Pauli exclusion principle, where no two particles may occupy the same quantum state.[13] Quarks have fractional elementary electric charge (-1/3 or 2/3)[14] and leptons have whole-numbered electric charge (0 or 1).[15] Quarks also have color charge, which is labeled arbitrarily with no correlation to actual light color as red, green and blue.[16] Because the interactions between the quarks stores energy which can convert to other particles when the quarks are far apart enough, quarks cannot be observed independently. This is called color confinement.[16]
 There are three known generations of quarks (up and down, strange and charm, top and bottom) and leptons (electron and its neutrino, muon and its neutrino, tau and its neutrino), with strong indirect evidence that the fourth generation of fermions does not exist.[17]
 Bosons are the mediators or carriers of fundamental interactions, such as electromagnetism, the weak interaction, and the strong interaction.[18] Electromagnetism is mediated by the photon, the quanta of light.[19]: 29–30  The weak interaction is mediated by the W and Z bosons.[20] The strong interaction is mediated by the gluon, which can link quarks together to form composite particles.[21] Due to the aforementioned color confinement, gluons are never observed independently.[22] The Higgs boson gives mass to the W and Z bosons via the Higgs mechanism[23] – the gluon and photon are expected to be massless.[22] All bosons have an integer quantum spin (0 and 1) and can have the same quantum state.[18]
 Most aforementioned particles have corresponding antiparticles, which compose antimatter. Normal particles have positive lepton or baryon number, and antiparticles have these numbers negative.[24] Most properties of corresponding antiparticles and particles are the same, with a few gets reversed; the electron's antiparticle, positron, has an opposite charge. To differentiate between antiparticles and particles, a plus or negative sign is added in superscript. For example, the electron and the positron are denoted e− and e+.[25] When a particle and an antiparticle interact with each other, they are annihilated and convert to other particles.[26] Some particles, such as the photon or gluon, have no antiparticles.[citation needed]
 Quarks and gluons additionally have color charges, which influences the strong interaction. Quark's color charges are called red, green and blue (though the particle itself have no physical color), and in antiquarks are called antired, antigreen and antiblue.[16] The gluon can have eight color charges, which are the result of quarks' interactions to form composite particles (gauge symmetry SU(3)).[27]
 The neutrons and protons in the atomic nuclei are baryons – the neutron is composed of two down quarks and one up quark, and the proton is composed of two up quarks and one down quark.[28] A baryon is composed of three quarks, and a meson is composed of two quarks (one normal, one anti). Baryons and mesons are collectively called hadrons. Quarks inside hadrons are governed by the strong interaction, thus are subjected to quantum chromodynamics (color charges). The bounded quarks must have their color charge to be neutral, or ""white"" for analogy with mixing the primary colors.[29] More exotic hadrons can have other types, arrangement or number of quarks (tetraquark, pentaquark).[30]
 A normal atom is made from protons, neutrons and electrons.[citation needed] By modifying the particles inside a normal atom, exotic atoms can be formed.[31] A simple example would be the hydrogen-4.1, which has one of its electrons replaced with a muon.[32]
 The graviton is a hypothetical particle that can mediate the gravitational interaction, but it has not been detected or completely reconciled with current theories.[33]
 The world's major particle physics laboratories are:
 
 Theoretical particle physics attempts to develop the models, theoretical framework, and mathematical tools to understand current experiments and make predictions for future experiments (see also theoretical physics). There are several major interrelated efforts being made in theoretical particle physics today.
 One important branch attempts to better understand the Standard Model and its tests. Theorists make quantitative predictions of observables at collider and astronomical experiments, which along with experimental measurements is used to extract the parameters of the Standard Model with less uncertainty. This work probes the limits of the Standard Model and therefore expands scientific understanding of nature's building blocks. Those efforts are made challenging by the difficulty of calculating high precision quantities in quantum chromodynamics. Some theorists working in this area use the tools of perturbative quantum field theory and effective field theory, referring to themselves as phenomenologists.[citation needed] Others make use of lattice field theory and call themselves lattice theorists.
 Another major effort is in model building where model builders develop ideas for what physics may lie beyond the Standard Model (at higher energies or smaller distances). This work is often motivated by the hierarchy problem and is constrained by existing experimental data.[46][47] It may involve work on supersymmetry, alternatives to the Higgs mechanism, extra spatial dimensions (such as the Randall–Sundrum models), Preon theory, combinations of these, or other ideas.
 A third major effort in theoretical particle physics is string theory. String theorists attempt to construct a unified description of quantum mechanics and general relativity by building a theory based on small strings, and branes rather than particles. If the theory is successful, it may be considered a ""Theory of Everything"", or ""TOE"".[48]
 There are also other areas of work in theoretical particle physics ranging from particle cosmology to loop quantum gravity.[citation needed]
 In principle, all physics (and practical applications developed therefrom) can be derived from the study of fundamental particles. In practice, even if ""particle physics"" is taken to mean only ""high-energy atom smashers"", many technologies have been developed during these pioneering investigations that later find wide uses in society. Particle accelerators are used to produce medical isotopes for research and treatment (for example, isotopes used in PET imaging), or used directly in external beam radiotherapy. The development of superconductors has been pushed forward by their use in particle physics. The World Wide Web and touchscreen technology were initially developed at CERN. Additional applications are found in medicine, national security, industry, computing, science, and workforce development, illustrating a long and growing list of beneficial practical applications with contributions from particle physics.[49]
 Major efforts to look for physics beyond the Standard Model include the Future Circular Collider proposed for CERN[50] and the Particle Physics Project Prioritization Panel (P5) in the US that will update the 2014 P5 study that recommended the Deep Underground Neutrino Experiment, among other experiments.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['therapy', 'James Cronin and Val Fitch', 'The development of superconductors', 'electromagnetism, the weak interaction, and the strong interaction', 'beneficial practical applications'], 'answer_start': [], 'answer_end': []}"
"
 Nanotechnology was defined by the National Nanotechnology Initiative as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers (nm). At this scale, commonly known as the nanoscale, surface area and quantum mechanical effects become important in describing properties of matter. The definition of nanotechnology is inclusive of all types of research and technologies that deal with these special properties. It is therefore common to see the plural form ""nanotechnologies"" as well as ""nanoscale technologies"" to refer to the broad range of research and applications whose common trait is size.[1] An earlier description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology.[2]
 Nanotechnology as defined by size is naturally broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage,[3][4] engineering,[5] microfabrication,[6] and molecular engineering.[7] The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly,[8] from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.
 Scientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials,[9] and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.
 The concepts that seeded nanotechnology were first discussed in 1959 by physicist Richard Feynman in his talk There's Plenty of Room at the Bottom, in which he described the possibility of synthesis via direct manipulation of atoms.
 The term ""nano-technology"" was first used by Norio Taniguchi in 1974, though it was not widely known. Inspired by Feynman's concepts, K. Eric Drexler used the term ""nanotechnology"" in his 1986 book Engines of Creation: The Coming Era of Nanotechnology, which proposed the idea of a nanoscale ""assembler"" that would be able to build a copy of itself and of other items of arbitrary complexity with atomic control. Also in 1986, Drexler co-founded The Foresight Institute (with which he is no longer affiliated) to help increase public awareness and understanding of nanotechnology concepts and implications.
 The emergence of nanotechnology as a field in the 1980s occurred through the convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework for nanotechnology, and high-visibility experimental advances that drew additional wide-scale attention to the prospects of atomic control of matter. In the 1980s, two major breakthroughs sparked the growth of nanotechnology in the modern era. First, the invention of the scanning tunneling microscope in 1981 which enabled visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986.[10][11] Binnig, Quate and Gerber also invented the analogous atomic force microscope that year.
 Second, fullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry.[12][13] C60 was not initially described as nanotechnology; the term was used regarding subsequent work with related carbon nanotubes (sometimes called graphene tubes or Bucky tubes) which suggested potential applications for nanoscale electronics and devices. The discovery of carbon nanotubes is largely attributed to Sumio Iijima of NEC in 1991,[14] for which Iijima won the inaugural 2008 Kavli Prize in Nanoscience.
 In the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology.[15] Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003.[16]
 Meanwhile, commercialization of products based on advancements in nanoscale technologies began emerging. These products are limited to bulk applications of nanomaterials and do not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based transparent sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles.[17][18]
 Governments moved to promote and fund research into nanotechnology, such as in the U.S. with the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established funding for research on the nanoscale, and in Europe via the European Framework Programmes for Research and Technological Development.
 By the mid-2000s new and serious scientific attention began to flourish.  Projects emerged to produce nanotechnology roadmaps[19][20]  which center on atomically precise manipulation of matter and discuss existing and projected capabilities, goals, and applications.
 Nanotechnology is the science and engineering of functional systems at the molecular scale. This covers both current work and concepts that are more advanced. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up, using techniques and tools being developed today to make complete, high-performance products.
 One nanometer (nm) is one billionth, or 10−9, of a meter. By comparison, typical carbon–carbon bond lengths, or the spacing between these atoms in a molecule, are in the range 0.12–0.15 nm, and a DNA double-helix has a diameter around 2 nm. On the other hand, the smallest cellular life forms, the bacteria of the genus Mycoplasma, are around 200 nm in length. By convention, nanotechnology is taken as the scale range 1 to 100 nm following the definition used by the National Nanotechnology Initiative in the US. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which are approximately a quarter of a nm kinetic diameter) since nanotechnology must build its devices from atoms and molecules. The upper limit is more or less arbitrary but is around the size below which the phenomena not observed in larger structures start to become apparent and can be made use of in the nano device.[21] These new phenomena make nanotechnology distinct from devices that are merely miniaturized versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology.[22]
 To put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth.[23] Or another way of putting it: a nanometer is the amount an average man's beard grows in the time it takes him to raise the razor to his face.[23]
 Two main approaches are used in nanotechnology. In the ""bottom-up"" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition.[24] In the ""top-down"" approach, nano-objects are constructed from larger entities without atomic-level control.[25]
 Areas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved during the last few decades to provide a basic scientific foundation of nanotechnology.
 Several phenomena become pronounced as the size of the system decreases. These include statistical mechanical effects, as well as quantum mechanical effects, for example, the ""quantum size effect"" where the electronic properties of solids are altered with great reductions in particle size. This effect does not come into play by going from macro to micro dimensions. However, quantum effects can become significant when the nanometer size range is reached, typically at distances of 100 nanometers or less, the so-called quantum realm. Additionally, a number of physical (mechanical, electrical, optical, etc.) properties change when compared to macroscopic systems. One example is the increase in surface area to volume ratio altering mechanical, thermal, and catalytic properties of materials. Diffusion and reactions at the nanoscale can be different, for instance in nanostructured materials and nanodevices. Systems with fast ion transport are referred to as nanoionics. The mechanical properties of nanosystems are of interest in the nanomechanics research. The catalytic activity of nanomaterials also opens potential risks in their interaction with biomaterials.
 Materials reduced to the nanoscale can show different properties compared to what they exhibit on a macroscale, enabling unique applications. For instance, opaque substances can become transparent (copper); stable materials can turn combustible (aluminium); insoluble materials may become soluble (gold). A material such as gold, which is chemically inert at normal scales, can serve as a potent chemical catalyst at nanoscales. Much of the fascination with nanotechnology stems from these quantum and surface phenomena that matter exhibits at the nanoscale.[26]
 Modern synthetic chemistry has reached the point where it is possible to prepare small molecules to almost any structure. These methods are used today to manufacture a wide variety of useful chemicals such as pharmaceuticals or commercial polymers. This ability raises the question of extending this kind of control to the next-larger level, seeking methods to assemble these single molecules into supramolecular assemblies consisting of many molecules arranged in a well-defined manner.
 These approaches utilize the concepts of molecular self-assembly and/or supramolecular chemistry to automatically arrange themselves into some useful conformation through a bottom-up approach. The concept of molecular recognition is especially important: molecules can be designed so that a specific configuration or arrangement is favored due to non-covalent intermolecular forces. The Watson–Crick basepairing rules are a direct result of this, as is the specificity of an enzyme being targeted to a single substrate, or the specific folding of the protein itself. Thus, two or more components can be designed to be complementary and mutually attractive so that they make a more complex and useful whole.
 Such bottom-up approaches should be capable of producing devices in parallel and be much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases. Most useful structures require complex and thermodynamically unlikely arrangements of atoms. Nevertheless, there are many examples of self-assembly based on molecular recognition in biology, most notably Watson–Crick basepairing and enzyme-substrate interactions. The challenge for nanotechnology is whether these principles can be used to engineer new constructs in addition to natural ones.
 Molecular nanotechnology, sometimes called molecular manufacturing, describes engineered nanosystems (nanoscale machines) operating on the molecular scale. Molecular nanotechnology is especially associated with the molecular assembler, a machine that can produce a desired structure or device atom-by-atom using the principles of mechanosynthesis. Manufacturing in the context of productive nanosystems is not related to and should be clearly distinguished from, the conventional technologies used to manufacture nanomaterials such as carbon nanotubes and nanoparticles.
 When the term ""nanotechnology"" was independently coined and popularized by Eric Drexler (who at the time was unaware of an earlier usage by Norio Taniguchi) it referred to a future manufacturing technology based on molecular machine systems. The premise was that molecular-scale biological analogies of traditional machine components demonstrated molecular machines were possible: by the countless examples found in biology, it is known that sophisticated, stochastically optimized biological machines can be produced.
 It is hoped that developments in nanotechnology will make possible their construction by some other means, perhaps using biomimetic principles. However, Drexler and other researchers[27] have proposed that advanced nanotechnology, although perhaps initially implemented by biomimetic means, ultimately could be based on mechanical engineering principles, namely, a manufacturing technology based on the mechanical functionality of these components (such as gears, bearings, motors, and structural members) that would enable programmable, positional assembly to atomic specification.[28] The physics and engineering performance of exemplar designs were analyzed in Drexler's book Nanosystems: Molecular Machinery, Manufacturing, and Computation.[2]
 In general, it is very difficult to assemble devices on the atomic scale, as one has to position atoms on other atoms of comparable size and stickiness. Another view, put forth by Carlo Montemagno,[29] is that future nanosystems will be hybrids of silicon technology and biological molecular machines. Richard Smalley argued that mechanosynthesis are impossible due to the difficulties in mechanically manipulating individual molecules.
 This led to an exchange of letters in the ACS publication Chemical & Engineering News in 2003.[30] Though biology clearly demonstrates that molecular machine systems are possible, non-biological molecular machines are today only in their infancy. Leaders in research on non-biological molecular machines are Alex Zettl and his colleagues at Lawrence Berkeley Laboratories and UC Berkeley.[31] They have constructed at least three distinct molecular devices whose motion is controlled from the desktop with changing voltage: a nanotube nanomotor, a molecular actuator,[32] and a nanoelectromechanical relaxation oscillator.[33] See nanotube nanomotor for more examples.
 An experiment indicating that positional molecular assembly is possible was performed by Ho and Lee at Cornell University in 1999. They used a scanning tunneling microscope to move an individual carbon monoxide molecule (CO) to an individual iron atom (Fe) sitting on a flat silver crystal and chemically bound the CO to the Fe by applying a voltage.
 The nanomaterials field includes subfields that develop or study materials having unique properties arising from their nanoscale dimensions.[36]
 These seek to arrange smaller components into more complex assemblies.
 These seek to create smaller devices by using larger ones to direct their assembly.
 These seek to develop components of a desired functionality without regard to how they might be assembled.
 These subfields seek to anticipate what inventions nanotechnology might yield, or attempt to propose an agenda along which inquiry might progress. These often take a big-picture view of nanotechnology, with more emphasis on its societal implications than the details of how such inventions could actually be created.
 Nanomaterials can be classified in 0D, 1D, 2D and 3D nanomaterials. The dimensionality plays a major role in determining the characteristic of nanomaterials including physical, chemical, and biological characteristics. With the decrease in dimensionality, an increase in surface-to-volume ratio is observed. This indicates that smaller dimensional nanomaterials have higher surface area compared to 3D nanomaterials. Recently, two dimensional (2D) nanomaterials are extensively investigated for electronic, biomedical, drug delivery and biosensor applications.
 There are several important modern developments. The atomic force microscope (AFM) and the Scanning Tunneling Microscope (STM) are two early versions of scanning probes that launched nanotechnology. There are other types of scanning probe microscopy. Although conceptually similar to the scanning confocal microscope developed by Marvin Minsky in 1961 and the scanning acoustic microscope (SAM) developed by Calvin Quate and coworkers in the 1970s, newer scanning probe microscopes have much higher resolution, since they are not limited by the wavelength of sound or light.
 The tip of a scanning probe can also be used to manipulate nanostructures (a process called positional assembly). Feature-oriented scanning methodology may be a promising way to implement these nanomanipulations in automatic mode.[54][55] However, this is still a slow process because of low scanning velocity of the microscope.
 Various techniques of nanolithography such as optical lithography, X-ray lithography, dip pen nanolithography, electron beam lithography or nanoimprint lithography were also developed. Lithography is a top-down fabrication technique where a bulk material is reduced in size to a nanoscale pattern.
 Another group of nanotechnological techniques include those used for fabrication of nanotubes and nanowires, those used in semiconductor fabrication such as deep ultraviolet lithography, electron beam lithography, focused ion beam machining, nanoimprint lithography, atomic layer deposition, and molecular vapor deposition, and further including molecular self-assembly techniques such as those employing di-block copolymers. The precursors of these techniques preceded the nanotech era, and are extensions in the development of scientific advancements rather than techniques that were devised with the sole purpose of creating nanotechnology and which were results of nanotechnology research.[56]
 The top-down approach anticipates nanodevices that must be built piece by piece in stages, much as manufactured items are made. Scanning probe microscopy is an important technique both for characterization and synthesis of nanomaterials. Atomic force microscopes and scanning tunneling microscopes can be used to look at surfaces and to move atoms around. By designing different tips for these microscopes, they can be used for carving out structures on surfaces and to help guide self-assembling structures. By using, for example, feature-oriented scanning approach, atoms or molecules can be moved around on a surface with scanning probe microscopy techniques.[54][55] At present, it is expensive and time-consuming for mass production but very suitable for laboratory experimentation.
 In contrast, bottom-up techniques build or grow larger structures atom by atom or molecule by molecule. These techniques include chemical synthesis, self-assembly and positional assembly. Dual polarisation interferometry is one tool suitable for characterisation of self-assembled thin films. Another variation of the bottom-up approach is molecular beam epitaxy or MBE. Researchers at Bell Telephone Laboratories like John R. Arthur. Alfred Y. Cho, and Art C. Gossard developed and implemented MBE as a research tool in the late 1960s and 1970s. Samples made by MBE were key to the discovery of the fractional quantum Hall effect for which the 1998 Nobel Prize in Physics was awarded. MBE allows scientists to lay down atomically precise layers of atoms and, in the process, build up complex structures. Important for research on semiconductors, MBE is also widely used to make samples and devices for the newly emerging field of spintronics.
 However, new therapeutic products, based on responsive nanomaterials, such as the ultradeformable, stress-sensitive Transfersome vesicles, are under development and already approved for human use in some countries.[57]
 As of August 21, 2008, the Project on Emerging Nanotechnologies estimates that over 800 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week.[18] The project lists all of the products in a publicly accessible online database. Most applications are limited to the use of ""firstgeneration"" passive nanomaterials which includes titanium dioxide in sunscreen, cosmetics, surface coatings,[58] and some food products; Carbon allotropes used to produce gecko tape; silver in food packaging, clothing, disinfectants, and household appliances; zinc oxide in sunscreens and cosmetics, surface coatings, paints and outdoor furniture varnishes; and cerium oxide as a fuel catalyst.[17]
 Further applications allow tennis balls to last longer, golf balls to fly straighter, and even bowling balls to become more durable and have a harder surface. Trousers and socks have been infused with nanotechnology so that they will last longer and keep people cool in the summer. Bandages are being infused with silver nanoparticles to heal cuts faster.[59] Video game consoles and personal computers may become cheaper, faster, and contain more memory thanks to nanotechnology.[60] Also, to build structures for on chip computing with light, for example on chip optical quantum information processing, and picosecond transmission of information.[61]
 Nanotechnology may have the ability to make existing medical applications cheaper and easier to use in places like the general practitioners' offices and at homes.[62] Cars are being manufactured using nanomaterials in such ways that car parts require fewer metals during manufacturing and less fuel to operate in the future.[63]
 Nanoencapsulation is a technology involving the enclosure of active substances within carriers or particles of nanometer size. Typically, these carriers are nanoparticles that offer various advantages, such as enhanced bioavailability, controlled release, targeted delivery, and protection of the encapsulated substances. In the medical field, nanoencapsulation plays a significant role in drug delivery and therapeutic strategies. It facilitates more efficient drug administration, minimizing side effects, and increasing treatment effectiveness by encapsulating drugs within nanoparticles. Nanoencapsulation is particularly useful for improving the bioavailability of poorly water-soluble drugs, enabling controlled and sustained drug release, and supporting the development of targeted therapies. These features collectively contribute to advancements in medical treatments and patient care.[64][65]
 Scientists are now turning to nanotechnology in an attempt to develop diesel engines with cleaner exhaust fumes. Platinum is currently used as the diesel engine catalyst in these engines. The catalyst is what cleans the exhaust fume particles. First, a reduction catalyst is employed to take nitrogen atoms from NOx molecules in order to free oxygen. Next, the oxidation catalyst oxidizes the hydrocarbons and carbon monoxide to form carbon dioxide and water.[citation needed] Platinum is used in both the reduction and the oxidation catalysts.[66] Using platinum though, is inefficient in that it is expensive and unsustainable. Danish company InnovationsFonden invested DKK 15 million in a search for new catalyst substitutes using nanotechnology. The goal of the project, launched in the autumn of 2014, is to maximize surface area and minimize the amount of material required. Objects tend to minimize their surface energy; two drops of water, for example, will join to form one drop and decrease surface area. If the catalyst's surface area that is exposed to the exhaust fumes is maximized, efficiency of the catalyst is maximized. The team working on this project aims to create nanoparticles that will not merge. Every time the surface is optimized, material is saved. Thus, creating these nanoparticles will increase the effectiveness of the resulting diesel engine catalyst—in turn leading to cleaner exhaust fumes—and will decrease cost. If successful, the team hopes to reduce platinum use by 25%.[67]
 Nanotechnology also has a prominent role in the fast-developing field of Tissue Engineering. When designing scaffolds, researchers attempt to mimic the nanoscale features of a cell's microenvironment to direct its differentiation down a suitable lineage.[68] For example, when creating scaffolds to support the growth of bone, researchers may mimic osteoclast resorption pits.[69]
 Researchers have successfully used DNA origami-based nanobots capable of carrying out logic functions to achieve targeted drug delivery in cockroaches. It is said that the computational power of these nanobots can be scaled up to that of a Commodore 64.[70]
 An area of concern is the effect that industrial-scale manufacturing and use of nanomaterials would have on human health and the environment, as suggested by nanotoxicology research. For these reasons, some groups advocate that nanotechnology be regulated by governments. Others counter that overregulation would stifle scientific research and the development of beneficial innovations. Public health research agencies, such as the National Institute for Occupational Safety and Health are actively conducting research on potential health effects stemming from exposures to nanoparticles.[71][72]
 Some nanoparticle products may have unintended consequences. Researchers have discovered that bacteriostatic silver nanoparticles used in socks to reduce foot odor are being released in the wash.[73] These particles are then flushed into the wastewater stream and may destroy bacteria which are critical components of natural ecosystems, farms, and waste treatment processes.[74]
 Public deliberations on risk perception in the US and UK carried out by the Center for Nanotechnology in Society found that participants were more positive about nanotechnologies for energy applications than for health applications, with health applications raising moral and ethical dilemmas such as cost and availability.[75]
 Experts, including director of the Woodrow Wilson Center's Project on Emerging Nanotechnologies David Rejeski, have testified[76] that successful commercialization depends on adequate oversight, risk research strategy, and public engagement. Berkeley, California is currently the only city in the United States to regulate nanotechnology;[77] In 2008, Cambridge, Massachusetts considered enacting a similar law,[78] but ultimately rejected it.[79]
 Nanofibers are used in several areas and in different products, in everything from aircraft wings to tennis rackets. Inhaling airborne nanoparticles and nanofibers may lead to a number of pulmonary diseases, e.g. fibrosis.[80] Researchers have found that when rats breathed in nanoparticles, the particles settled in the brain and lungs, which led to significant increases in biomarkers for inflammation and stress response[81] and that nanoparticles induce skin aging through oxidative stress in hairless mice.[82][83]
 A two-year study at UCLA's School of Public Health found lab mice consuming nano-titanium dioxide showed DNA and chromosome damage to a degree ""linked to all the big killers of man, namely cancer, heart disease, neurological disease and aging"".[84]
 A Nature Nanotechnology study suggests some forms of carbon nanotubes – a poster child for the ""nanotechnology revolution"" – could be as harmful as asbestos if inhaled in sufficient quantities. Anthony Seaton of the Institute of Occupational Medicine in Edinburgh, Scotland, who contributed to the article on carbon nanotubes said ""We know that some of them probably have the potential to cause mesothelioma. So those sorts of materials need to be handled very carefully.""[85] In the absence of specific regulation forthcoming from governments, Paull and Lyons (2008) have called for an exclusion of engineered nanoparticles in food.[86] A newspaper article reports that workers in a paint factory developed serious lung disease and nanoparticles were found in their lungs.[87][88][89][90]
 Calls for tighter regulation of nanotechnology have occurred alongside a growing debate related to the human health and safety risks of nanotechnology.[91] There is significant debate about who is responsible for the regulation of nanotechnology. Some regulatory agencies currently cover some nanotechnology products and processes (to varying degrees) – by ""bolting on"" nanotechnology to existing regulations – there are clear gaps in these regimes.[92] Davies (2008) has proposed a regulatory road map describing steps to deal with these shortcomings.[93]
 Stakeholders concerned by the lack of a regulatory framework to assess and control risks associated with the release of nanoparticles and nanotubes have drawn parallels with bovine spongiform encephalopathy (""mad cow"" disease), thalidomide, genetically modified food,[94] nuclear energy, reproductive technologies, biotechnology, and asbestosis. Andrew Maynard, chief science advisor to the Woodrow Wilson Center's Project on Emerging Nanotechnologies, concludes that there is insufficient funding for human health and safety research, and as a result there is currently limited understanding of the human health and safety risks associated with nanotechnology.[95] As a result, some academics have called for stricter application of the precautionary principle, with delayed marketing approval, enhanced labelling and additional safety data development requirements in relation to certain forms of nanotechnology.[96]
 The Royal Society report[15] identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that ""manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure"" (p. xiii).
 The Center for Nanotechnology in Society has found that people respond to nanotechnologies differently, depending on application – with participants in public deliberations more positive about nanotechnologies for energy than health applications – suggesting that any public calls for nano regulations may differ by technology sector.[75]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['existing and projected capabilities, goals, and applications', 'Binnig, Quate and Gerber', 'manipulation of matter', 'the nanoscale, surface area and quantum mechanical effects', 'unique applications'], 'answer_start': [], 'answer_end': []}"
"Biomedical engineering (BME) or medical engineering is the application of engineering principles and design concepts to medicine and biology for healthcare applications (e.g., diagnostic or therapeutic purposes). BME is also traditionally logical sciences to advance health care treatment, including diagnosis, monitoring, and therapy.[1][2] Also included under the scope of a biomedical engineer is the management of current medical equipment in hospitals while adhering to relevant industry standards. This involves procurement, routine testing, preventive maintenance, and making equipment recommendations, a role also known as a Biomedical Equipment Technician (BMET) or as a clinical engineer.
 Biomedical engineering has recently emerged as its own field of study, as compared to many other engineering fields.[citation needed] Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, imaging technologies such as MRI and EKG/ECG, regenerative tissue growth, and the development of pharmaceutical drugs including biopharmaceuticals.
 Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data.
 Bioinformatics is considered both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organizational principles within nucleic acid and protein sequences.
 Biomechanics is the study of the structure and function of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles,[3] using the methods of mechanics.[4]
 A biomaterial is any matter, surface, or construct that interacts with living systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.
 Biomedical optics combines the principles of physics, engineering, and biology to study the interaction of biological tissue and light, and how this can be exploited for sensing, imaging, and treatment.[5] It has a wide range of applications, including optical imaging, microscopy, ophthalmoscopy, spectroscopy, and therapy. Examples of biomedical optics techniques and technologies include optical coherence tomography (OCT), fluorescence microscopy, confocal microscopy, and photodynamic therapy (PDT). OCT, for example, uses light to create high-resolution, three-dimensional images of internal structures, such as the retina in the eye or the coronary arteries in the heart. Fluorescence microscopy involves labeling specific molecules with fluorescent dyes and visualizing them using light, providing insights into biological processes and disease mechanisms. More recently, adaptive optics is helping imaging by correcting aberrations in biological tissue, enabling higher resolution imaging and improved accuracy in procedures such as laser surgery and retinal imaging.
 Tissue engineering, like genetic engineering (see below), is a major segment of biotechnology – which overlaps significantly with BME.
 One of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones[6] and tracheas[7] from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients.[8] Bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.[9]
 Genetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology (not a medical application, but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.[citation needed]
 Neural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.
 Pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment.
 This is an extremely broad category—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.
 A medical device is intended for use in:
 Some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.
 Stereolithography is a practical example of medical modeling being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies,[10] treatments,[11] patient monitoring,[12] of complex diseases.
 Medical devices are regulated and classified (in the US) as follows (see also Regulation):
 Medical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly ""view"" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, radiology, and other means.
 Alternatively, navigation-guided equipment utilizes electromagnetic tracking technology, such as catheter placement into the brain or feeding tube placement systems. For example, ENvizion Medical's ENvue, an electromagnetic navigation system for enteral feeding tube placement. The system uses an external field generator and several EM passive sensors enabling scaling of the display to the patient's body contour, and a real-time view of the feeding tube tip location and direction, which helps the medical staff ensure the correct placement in the GI tract.[13]
 Imaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including: fluoroscopy, magnetic resonance imaging (MRI), nuclear medicine, positron emission tomography (PET), PET-CT scans, projection radiography such as X-rays and CT scans, tomography, ultrasound, optical microscopy, and electron microscopy.
 An implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases, implants contain electronics, e.g. artificial pacemakers and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.
 Artificial body part replacements are one of the many applications of bionics. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different functions and processes of the eyes, ears, and other organs paved the way for improved cameras, television, radio transmitters and receivers, and many other tools.
 In recent years biomedical sensors based in microwave technology have gained more attention. Different sensors can be manufactured for specific uses in both diagnosing and monitoring disease conditions, for example microwave sensors can be used as a complementary technique to X-ray to monitor lower extremity trauma.[14] The sensor monitor the dielectric properties and can thus notice change in tissue (bone, muscle, fat etc.) under the skin so when measuring at different times during the healing process the response from the sensor will change as the trauma heals.
 Clinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.
 Their inherent focus on practical implementation of technology has tended to keep them oriented more towards incremental-level redesigns and reconfigurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a ""bridge"" between the primary designers and the end-users, by combining the perspectives of being both close to the point-of-use, while also trained in product and process engineering. Clinical engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also, see safety engineering for a discussion of the procedures used to design safe systems. The clinical engineering department is constructed with a manager, supervisor, engineer, and technician. One engineer per eighty beds in the hospital is the ratio. Clinical engineers are also authorized to audit pharmaceutical and associated stores to monitor FDA recalls of invasive items.
 Rehabilitation engineering is the systematic application of engineering sciences to design, develop, adapt, test, evaluate, apply, and distribute technological solutions to problems confronted by individuals with disabilities. Functional areas addressed through rehabilitation engineering may include mobility, communications, hearing, vision, and cognition, and activities associated with employment, independent living, education, and integration into the community.[1]
 While some rehabilitation engineers have master's degrees in rehabilitation engineering, usually a subspecialty of Biomedical engineering, most rehabilitation engineers have an undergraduate or graduate degrees in biomedical engineering, mechanical engineering, or electrical engineering. A Portuguese university provides an undergraduate degree and a master's degree in Rehabilitation Engineering and Accessibility.[6][8] Qualification to become a Rehab' Engineer in the UK is possible via a University BSc Honours Degree course such as Health Design & Technology Institute, Coventry University.[9]
 The rehabilitation process for people with disabilities often entails the design of assistive devices such as Walking aids intended to promote the inclusion of their users into the mainstream of society, commerce, and recreation.
 Regulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to ""a situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death""[15]
 Regardless of the country-specific legislation, the main regulatory objectives coincide worldwide.[16] For example, in the medical device regulations, a product must be: 1) safe and 2) effective and 3) for all the manufactured devices (why is this part deleted?)
 A product is safe if patients, users, and third parties do not run unacceptable risks of physical hazards (death, injuries, ...) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at an acceptable level if compared with the benefit derived from the use of it.
 A product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.
 The previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impact safety and effectiveness over the whole medical device lifecycle.
 The medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical devices, drugs, biologics, and combination products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for ""consumer"" use, such as physical therapy devices (which are also ""medical"" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K ""clearance"" (typically for Class 2 devices) or pre-market ""approval"" (typically for drugs and class 3 devices).
 In the European context, safety effectiveness and quality is ensured through the ""Conformity Assessment"" which is defined as ""the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive"". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file, and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. The risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in a different structure. The Quality System deliverables usually include procedures that ensure quality throughout all product life cycles. The same standard (ISO EN 13485) is usually applied for quality management systems in the US and worldwide.
 In the European Union, there are certifying entities named ""Notified Bodies"", accredited by the European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.
 The different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the optimal extent of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.
 Directive 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation ""Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices"" (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2.
RoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.
 The scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS complies with the hazardous substances limits and have a CE mark on their products.
 The new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.
 The mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.
 AS/ANS 3551:2012 is the Australian and New Zealand standards for the management of medical devices. The standard specifies the procedures required to maintain a wide range of medical assets in a clinical setting (e.g. Hospital).[17] The standards are based on the IEC 606101 standards.
 The standard covers a wide range of medical equipment management elements including, procurement, acceptance testing, maintenance (electrical safety and preventive maintenance testing) and decommissioning.
 Biomedical engineers require considerable knowledge of both engineering and biology, and typically have a Bachelor's (B.Sc., B.S., B.Eng. or B.S.E.) or Master's (M.S., M.Sc., M.S.E., or M.Eng.) or a doctoral (Ph.D., or MD-PhD[18][19][20]) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Sc., B.S., B.Eng. or B.S.E.) to doctoral levels. Biomedical engineering has only recently been emerging as its own discipline rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which includes enough biological science content that many students use it as a ""pre-med"" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.[21]
 In the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. As of 2023, 155 programs are currently accredited by ABET.[22]
 In Canada and Australia, accredited graduate programs in biomedical engineering are common.[23] For example, McMaster University offers an M.A.Sc, an MD/PhD, and a PhD in Biomedical engineering.[24] The first Canadian undergraduate BME program was offered at University of Guelph as a four-year B.Eng. program.[25] The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering[26] as is Flinders University.[27]
 As with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees is also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.
 Graduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them.[28] Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a master's degree or apply to medical school afterwards.
 Graduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the university's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or another engineering discipline (plus certain life science coursework), or life science (plus certain engineering coursework).
 Education in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards.[29] Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education.[30] Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.
 As with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but, in US, in industry such a license is not required to be an employee as an engineer in the majority of situations (due to an exception known as the industrial exemption, which effectively applies to the vast majority of American engineers). The US model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed. This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.
 Biomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.[31]
 In the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division.[32] The Institute of Physics and Engineering in Medicine (IPEM) has a panel for the accreditation of MSc courses in Biomedical Engineering and Chartered Engineering status can also be sought through IPEM.
 The Fundamentals of Engineering exam – the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.
 Beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.
 In 2012 there were about 19,400 biomedical engineers employed in the US, and the field was predicted to grow by 5% (faster than average) from 2012 to 2022.[33] Biomedical engineering has the highest percentage of female engineers compared to other common engineering professions.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Graduate education', 'The Food and Drug Administration', 'It has experienced steady and strong growth', 'decommissioning', 'different procedures'], 'answer_start': [], 'answer_end': []}"
"
 
 
 Genetic engineering, also called genetic modification or genetic manipulation, is the modification and  manipulation of an organism's genes using technology. It is a set of technologies used to change the genetic makeup of cells, including the transfer of genes within and across species boundaries to produce improved or novel organisms. 
 New DNA is obtained by either isolating and copying the genetic material of interest using recombinant DNA methods or by artificially synthesising the DNA. A construct is usually created and used to insert this DNA into the host organism. The first recombinant DNA molecule was made by Paul Berg in 1972 by combining DNA from the monkey virus SV40 with the lambda virus. 
 As well as inserting genes, the process can be used to remove, or ""knock out"", genes. The new DNA can be inserted randomly, or targeted to a specific part of the genome.[1]
 An organism that is generated through genetic engineering is considered to be genetically modified (GM) and the resulting entity is a genetically modified organism (GMO). The first GMO was a bacterium generated by Herbert Boyer and Stanley Cohen in 1973. Rudolf Jaenisch created the first GM animal when he inserted foreign DNA into a mouse in 1974. The first company to focus on genetic engineering, Genentech, was founded in 1976 and started the production of human proteins. Genetically engineered human insulin was produced in 1978 and insulin-producing bacteria were commercialised in 1982. Genetically modified food has been sold since 1994, with the release of the Flavr Savr tomato. The Flavr Savr was engineered to have a longer shelf life, but most current GM crops are modified to increase resistance to insects and herbicides. GloFish, the first GMO designed as a pet, was sold in the United States in December 2003. In 2016 salmon modified with a growth hormone were sold.
 Genetic engineering has been applied in numerous fields including research, medicine, industrial biotechnology and agriculture. In research, GMOs are used to study gene function and expression through loss of function, gain of function, tracking and expression experiments. By knocking out genes responsible for certain conditions it is possible to create animal model organisms of human diseases. As well as producing hormones, vaccines and other drugs, genetic engineering has the potential to cure genetic diseases through gene therapy. Chinese hamster ovary (CHO) cells are used in industrial genetic engineering. Additionally mRNA vaccines are made through genetic engineering to treat viruses such as COVID-19. The same techniques that are used to produce drugs can also have industrial applications such as producing enzymes for laundry detergent, cheeses and other products.
 The rise of commercialised genetically modified crops has provided economic benefit to farmers in many different countries, but has also been the source of most of the controversy surrounding the technology. This has been present since its early use; the first field trials were destroyed by anti-GM activists. Although there is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, critics consider GM food safety a leading concern. Gene flow, impact on non-target organisms, control of the food supply and intellectual property rights have also been raised as potential issues. These concerns have led to the development of a regulatory framework, which started in 1975. It has led to an international treaty, the Cartagena Protocol on Biosafety, that was adopted in 2000. Individual countries have developed their own regulatory systems regarding GMOs, with the most marked differences occurring between the United States and Europe.
 Genetic engineering: Process of inserting new genetic information into existing cells in order to modify a specific organism for the purpose of changing its characteristics.
 Note: Adapted from ref.[2][3]
  Genetic engineering is a process that alters the genetic structure of an organism by either removing or introducing DNA, or modifying existing genetic material in situ. Unlike traditional animal and plant breeding, which involves doing multiple crosses and then selecting for the organism with the desired phenotype, genetic engineering takes the gene directly from one organism and delivers it to the other. This is much faster, can be used to insert any genes from any organism (even ones from different domains) and prevents other undesirable genes from also being added.[4]
 Genetic engineering could potentially fix severe genetic disorders in humans by replacing the defective gene with a functioning one.[5] It is an important tool in research that allows the function of specific genes to be studied.[6] Drugs, vaccines and other products have been harvested from organisms engineered to produce them.[7] Crops have been developed that aid food security by increasing yield, nutritional value and tolerance to environmental stresses.[8]
 The DNA can be introduced directly into the host organism or into a cell that is then fused or hybridised with the host.[9] This relies on recombinant nucleic acid techniques to form new combinations of heritable genetic material followed by the incorporation of that material either indirectly through a vector system or directly through micro-injection, macro-injection or micro-encapsulation. 
 Genetic engineering does not normally include traditional breeding, in vitro fertilisation, induction of polyploidy, mutagenesis and cell fusion techniques that do not use recombinant nucleic acids or a genetically modified organism in the process.[9] However, some broad definitions of genetic engineering include selective breeding.[10] Cloning and stem cell research, although not considered genetic engineering,[11] are closely related and genetic engineering can be used within them.[12] Synthetic biology is an emerging discipline that takes genetic engineering a step further by introducing artificially synthesised material into an organism.[13]
 Plants, animals or microorganisms that have been changed through genetic engineering are termed genetically modified organisms or GMOs.[14] If genetic material from another species is added to the host, the resulting organism is called transgenic. If genetic material from the same species or a species that can naturally breed with the host is used the resulting organism is called cisgenic.[15] If genetic engineering is used to remove genetic material from the target organism the resulting organism is termed a knockout organism.[16] In Europe genetic modification is synonymous with genetic engineering while within the United States of America and Canada genetic modification can also be used to refer to more conventional breeding methods.[17][18][19]
 Humans have altered the genomes of species for thousands of years through selective breeding, or artificial selection[20]: 1 [21]: 1  as contrasted with natural selection. More recently, mutation breeding has used exposure to chemicals or radiation to produce a high frequency of random mutations, for selective breeding purposes. Genetic engineering as the direct manipulation of DNA by humans outside breeding and mutations has only existed since the 1970s. The term ""genetic engineering"" was coined by the Russian-born geneticist Nikolay Timofeev-Ressovsky in his 1934 paper ""The Experimental Production of Mutations"", published in the British journal Biological Reviews.[22] Jack Williamson used the term in his science fiction novel Dragon's Island, published in 1951[23] – one year before DNA's role in heredity was confirmed by Alfred Hershey and Martha Chase,[24] and two years before James Watson and Francis Crick showed that the DNA molecule has a double-helix structure – though the general concept of direct genetic manipulation was explored in rudimentary form in Stanley G. Weinbaum's 1936 science fiction story Proteus Island.[25][26]
 In 1972, Paul Berg created the first recombinant DNA molecules by combining DNA from the monkey virus SV40 with that of the lambda virus.[27] In 1973 Herbert Boyer and Stanley Cohen created the first transgenic organism by inserting antibiotic resistance genes into the plasmid of an Escherichia coli bacterium.[28][29] A year later Rudolf Jaenisch created a transgenic mouse by introducing foreign DNA into its embryo, making it the world's first transgenic animal[30] These achievements led to concerns in the scientific community about potential risks from genetic engineering, which were first discussed in depth at the Asilomar Conference in 1975. One of the main recommendations from this meeting was that government oversight of recombinant DNA research should be established until the technology was deemed safe.[31][32]
 In 1976 Genentech, the first genetic engineering company, was founded by Herbert Boyer and Robert Swanson and a year later the company produced a human protein (somatostatin) in E. coli. Genentech announced the production of genetically engineered human insulin in 1978.[33] In 1980, the U.S. Supreme Court in the Diamond v. Chakrabarty case ruled that genetically altered life could be patented.[34] The insulin produced by bacteria was approved for release by the Food and Drug Administration (FDA) in 1982.[35]
 In 1983, a biotech company, Advanced Genetic Sciences (AGS) applied for U.S. government authorisation to perform field tests with the ice-minus strain of Pseudomonas syringae to protect crops from frost, but environmental groups and protestors delayed the field tests for four years with legal challenges.[36] In 1987, the ice-minus strain of P. syringae became the first genetically modified organism (GMO) to be released into the environment[37] when a strawberry field and a potato field in California were sprayed with it.[38] Both test fields were attacked by activist groups the night before the tests occurred: ""The world's first trial site attracted the world's first field trasher"".[37]
 The first field trials of genetically engineered plants occurred in France and the US in 1986, tobacco plants were engineered to be resistant to herbicides.[39] The People's Republic of China was the first country to commercialise transgenic plants, introducing a virus-resistant tobacco in 1992.[40] In 1994 Calgene attained approval to commercially release the first genetically modified food, the Flavr Savr, a tomato engineered to have a longer shelf life.[41] In 1994, the European Union approved tobacco engineered to be resistant to the herbicide bromoxynil, making it the first genetically engineered crop commercialised in Europe.[42] In 1995, Bt potato was approved safe by the Environmental Protection Agency, after having been approved by the FDA, making it the first pesticide producing crop to be approved in the US.[43] In 2009 11 transgenic crops were grown commercially in 25 countries, the largest of which by area grown were the US, Brazil, Argentina, India, Canada, China, Paraguay and South Africa.[44]
 In 2010, scientists at the J. Craig Venter Institute created the first synthetic genome and inserted it into an empty bacterial cell. The resulting bacterium, named Mycoplasma laboratorium, could replicate and produce proteins.[45][46] Four years later this was taken a step further when a bacterium was developed that replicated a plasmid containing a unique base pair, creating the first organism engineered to use an expanded genetic alphabet.[47][48] In 2012, Jennifer Doudna and Emmanuelle Charpentier collaborated to develop the CRISPR/Cas9 system,[49][50] a technique which can be used to easily and specifically alter the genome of almost any organism.[51]
 Creating a GMO is a multi-step process. Genetic engineers must first choose what gene they wish to insert into the organism. This is driven by what the aim is for the resultant organism and is built on earlier research. Genetic screens can be carried out to determine potential genes and further tests then used to identify the best candidates. The development of microarrays, transcriptomics and genome sequencing has made it much easier to find suitable genes.[52] Luck also plays its part; the Roundup Ready gene was discovered after scientists noticed a bacterium thriving in the presence of the herbicide.[53]
 The next step is to isolate the candidate gene. The cell containing the gene is opened and the DNA is purified.[54] The gene is separated by using restriction enzymes to cut the DNA into fragments[55] or polymerase chain reaction (PCR) to amplify up the gene segment.[56] These segments can then be extracted through gel electrophoresis. If the chosen gene or the donor organism's genome has been well studied it may already be accessible from a genetic library. If the DNA sequence is known, but no copies of the gene are available, it can also be artificially synthesised.[57] Once isolated the gene is ligated into a plasmid that is then inserted into a bacterium. The plasmid is replicated when the bacteria divide, ensuring unlimited copies of the gene are available.[58] The RK2 plasmid is notable for its ability to replicate in a wide variety of single-celled organisms, which makes it suitable as a genetic engineering tool.[59]
 Before the gene is inserted into the target organism it must be combined with other genetic elements. These include a promoter and terminator region, which initiate and end transcription. A selectable marker gene is added, which in most cases confers antibiotic resistance, so researchers can easily determine which cells have been successfully transformed. The gene can also be modified at this stage for better expression or effectiveness. These manipulations are carried out using recombinant DNA techniques, such as restriction digests, ligations and molecular cloning.[60]
 There are a number of techniques used to insert genetic material into the host genome. Some bacteria can naturally take up foreign DNA. This ability can be induced in other bacteria via stress (e.g. thermal or electric shock), which increases the cell membrane's permeability to DNA; up-taken DNA can either integrate with the genome or exist as extrachromosomal DNA. DNA is generally inserted into animal cells using microinjection, where it can be injected through the cell's nuclear envelope directly into the nucleus, or through the use of viral vectors.[61]
 Plant genomes can be engineered by physical methods or by use of Agrobacterium for the delivery of sequences hosted in T-DNA binary vectors. In plants the DNA is often inserted using Agrobacterium-mediated transformation,[62] taking advantage of the Agrobacteriums T-DNA sequence that allows natural insertion of genetic material into plant cells.[63] Other methods include biolistics, where particles of gold or tungsten are coated with DNA and then shot into young plant cells,[64] and electroporation, which involves using an electric shock to make the cell membrane permeable to plasmid DNA.
 As only a single cell is transformed with genetic material, the organism must be regenerated from that single cell. In plants this is accomplished through the use of tissue culture.[65][66] In animals it is necessary to ensure that the inserted DNA is present in the embryonic stem cells.[67] Bacteria consist of a single cell and reproduce clonally so regeneration is not necessary. Selectable markers are used to easily differentiate transformed from untransformed cells. These markers are usually present in the transgenic organism, although a number of strategies have been developed that can remove the selectable marker from the mature transgenic plant.[68]
 Further testing using PCR, Southern hybridization, and DNA sequencing is conducted to confirm that an organism contains the new gene.[69] These tests can also confirm the chromosomal location and copy number of the inserted gene. The presence of the gene does not guarantee it will be expressed at appropriate levels in the target tissue so methods that look for and measure the gene products (RNA and protein) are also used. These include northern hybridisation, quantitative RT-PCR, Western blot, immunofluorescence, ELISA and phenotypic analysis.[70]
 The new genetic material can be inserted randomly within the host genome or targeted to a specific location. The technique of gene targeting uses homologous recombination to make desired changes to a specific endogenous gene.  This tends to occur at a relatively low frequency in plants and animals and generally requires the use of selectable markers. The frequency of gene targeting can be greatly enhanced through genome editing. Genome editing uses artificially engineered nucleases that create specific double-stranded breaks at desired locations in the genome, and use the cell's endogenous mechanisms to repair the induced break by the natural processes of homologous recombination and nonhomologous end-joining. There are four families of engineered nucleases: meganucleases,[71][72] zinc finger nucleases,[73][74] transcription activator-like effector nucleases (TALENs),[75][76] and the Cas9-guideRNA system (adapted from CRISPR).[77][78] TALEN and CRISPR are the two most commonly used and each has its own advantages.[79] TALENs have greater target specificity, while CRISPR is easier to design and more efficient.[79] In addition to enhancing gene targeting, engineered nucleases can be used to introduce mutations at endogenous genes that generate a gene knockout.[80][81]
 Genetic engineering has applications in medicine, research, industry and agriculture and can be used on a wide range of plants, animals and microorganisms. Bacteria, the first organisms to be genetically modified, can have plasmid DNA inserted containing new genes that code for medicines or enzymes that process food and other substrates.[82][83] Plants have been modified for insect protection, herbicide resistance, virus resistance, enhanced nutrition, tolerance to environmental pressures and the production of edible vaccines.[84] Most commercialised GMOs are insect resistant or herbicide tolerant crop plants.[85] Genetically modified animals have been used for research, model animals and the production of agricultural or pharmaceutical products. The genetically modified animals include animals with genes knocked out, increased susceptibility to disease, hormones for extra growth and the ability to express proteins in their milk.[86]
 Genetic engineering has many applications to medicine that include the manufacturing of drugs, creation of model animals that mimic human conditions and gene therapy. One of the earliest uses of genetic engineering was to mass-produce human insulin in bacteria.[33] This application has now been applied to human growth hormones, follicle stimulating hormones (for treating infertility), human albumin, monoclonal antibodies, antihemophilic factors, vaccines and many other drugs.[87][88] Mouse hybridomas, cells fused together to create monoclonal antibodies, have been adapted through genetic engineering to create human monoclonal antibodies.[89] Genetically engineered viruses are being developed that can still confer immunity, but lack the infectious sequences.[90]
 Genetic engineering is also used to create animal models of human diseases. Genetically modified mice are the most common genetically engineered animal model.[91] They have been used to study and model cancer (the oncomouse), obesity, heart disease, diabetes, arthritis, substance abuse, anxiety, aging and Parkinson disease.[92] Potential cures can be tested against these mouse models. 
 Gene therapy is the genetic engineering of humans, generally by replacing defective genes with effective ones. Clinical research using somatic gene therapy has been conducted with several diseases, including X-linked SCID,[93] chronic lymphocytic leukemia (CLL),[94][95] and Parkinson's disease.[96] In 2012, Alipogene tiparvovec became the first gene therapy treatment to be approved for clinical use.[97][98] In 2015 a virus was used to insert a healthy gene into the skin cells of a boy suffering from a rare skin disease, epidermolysis bullosa, in order to grow, and then graft healthy skin onto 80 percent of the boy's body which was affected by the illness.[99]
 Germline gene therapy would result in any change being inheritable, which has raised concerns within the scientific community.[100][101] In 2015, CRISPR was used to edit the DNA of non-viable human embryos,[102][103] leading scientists of major world academies to call for a moratorium on inheritable human genome edits.[104] There are also concerns that the technology could be used not just for treatment, but for enhancement, modification or alteration of a human beings' appearance, adaptability, intelligence, character or behavior.[105] The distinction between cure and enhancement can also be difficult to establish.[106] In November 2018, He Jiankui announced that he had edited the genomes of two human embryos, to attempt to disable the CCR5 gene, which codes for a receptor that HIV uses to enter cells. The work was widely condemned as unethical, dangerous, and premature.[107] Currently, germline modification is banned in 40 countries. Scientists that do this type of research will often let embryos grow for a few days without allowing it to develop into a baby.[108]
 Researchers are altering the genome of pigs to induce the growth of human organs, with the aim of increasing the success of pig to human organ transplantation.[109] Scientists are creating ""gene drives"", changing the genomes of mosquitoes to make them immune to malaria, and then looking to spread the genetically altered mosquitoes throughout the mosquito population in the hopes of eliminating the disease.[110]
 Genetic engineering is an important tool for natural scientists, with the creation of transgenic organisms one of the most important tools for analysis of gene function.[111] Genes and other genetic information from a wide range of organisms can be inserted into bacteria for storage and modification, creating genetically modified bacteria in the process. Bacteria are cheap, easy to grow, clonal, multiply quickly, relatively easy to transform and can be stored at -80 °C almost indefinitely. Once a gene is isolated it can be stored inside the bacteria providing an unlimited supply for research.[112]
 Organisms are genetically engineered to discover the functions of certain genes. This could be the effect on the phenotype of the organism, where the gene is expressed or what other genes it interacts with. These experiments generally involve loss of function, gain of function, tracking and expression.
 Organisms can have their cells transformed with a gene coding for a useful protein, such as an enzyme, so that they will overexpress the desired protein. Mass quantities of the protein can then be manufactured by growing the transformed organism in bioreactor equipment using industrial fermentation, and then purifying the protein.[116] Some genes do not work well in bacteria, so yeast, insect cells or mammalian cells can also be used.[117] These techniques are used to produce medicines such as insulin, human growth hormone, and vaccines, supplements such as tryptophan, aid in the production of food (chymosin in cheese making) and fuels.[118] Other applications with genetically engineered bacteria could involve making them perform tasks outside their natural cycle, such as making biofuels,[119] cleaning up oil spills, carbon and other toxic waste[120] and detecting arsenic in drinking water.[121] Certain genetically modified microbes can also be used in biomining and bioremediation, due to their ability to extract heavy metals from their environment and incorporate them into compounds that are more easily recoverable.[122]
 In materials science, a genetically modified virus has been used in a research laboratory as a scaffold for assembling a more environmentally friendly lithium-ion battery.[123][124] Bacteria have also been engineered to function as sensors by expressing a fluorescent protein under certain environmental conditions.[125]
 One of the best-known and controversial applications of genetic engineering is the creation and use of genetically modified crops or genetically modified livestock to produce genetically modified food. Crops have been developed to increase production, increase tolerance to abiotic stresses, alter the composition of the food, or to produce novel products.[127]
 The first crops to be released commercially on a large scale provided protection from insect pests or tolerance to herbicides. Fungal and virus resistant crops have also been developed or are in development.[128][129] This makes the insect and weed management of crops easier and can indirectly increase crop yield.[130][131] GM crops that directly improve yield by accelerating growth or making the plant more hardy (by improving salt, cold or drought tolerance) are also under development.[132] In 2016 Salmon have been genetically modified with growth hormones to reach normal adult size much faster.[133]
 GMOs have been developed that modify the quality of produce by increasing the nutritional value or providing more industrially useful qualities or quantities.[132] The Amflora potato produces a more industrially useful blend of starches. Soybeans and canola have been genetically modified to produce more healthy oils.[134][135] The first commercialised GM food was a tomato that had delayed ripening, increasing its shelf life.[136]
 Plants and animals have been engineered to produce materials they do not normally make.  Pharming uses crops and animals as bioreactors to produce vaccines, drug intermediates, or the drugs themselves; the useful product is purified from the harvest and then used in the standard pharmaceutical production process.[137] Cows and goats have been engineered to express drugs and other proteins in their milk, and in 2009 the FDA approved a drug produced in goat milk.[138][139]
 Genetic engineering has potential applications in conservation and natural area management. Gene transfer through viral vectors has been proposed as a means of controlling invasive species as well as vaccinating threatened fauna from disease.[140] Transgenic trees have been suggested as a way to confer resistance to pathogens in wild populations.[141] With the increasing risks of maladaptation in organisms as a result of climate change and other perturbations, facilitated adaptation through gene tweaking could be one solution to reducing extinction risks.[142] Applications of genetic engineering in conservation are thus far mostly theoretical and have yet to be put into practice.
 Genetic engineering is also being used to create microbial art.[143] Some bacteria have been genetically engineered to create black and white photographs.[144] Novelty items such as lavender-colored carnations,[145] blue roses,[146] and glowing fish[147][148] have also been produced through genetic engineering.
 The regulation of genetic engineering concerns the approaches taken by governments to assess and manage the risks associated with the development and release of GMOs. The development of a regulatory framework began in 1975, at Asilomar, California.[149] The Asilomar meeting recommended a set of voluntary guidelines regarding the use of recombinant technology.[31] As the technology improved the US established a committee at the Office of Science and Technology,[150] which assigned regulatory approval of GM food to the USDA, FDA and EPA.[151] The Cartagena Protocol on Biosafety, an international treaty that governs the transfer, handling, and use of GMOs,[152] was adopted on 29 January 2000.[153] One hundred and fifty-seven countries are members of the Protocol, and many use it as a reference point for their own regulations.[154]
 The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.[155][156][157][158] Some countries allow the import of GM food with authorisation, but either do not allow its cultivation (Russia, Norway, Israel) or have provisions for cultivation even though no GM products are yet produced (Japan, South Korea).  Most countries that do not allow GMO cultivation do permit research.[159] Some of the most marked differences occur between the US and Europe. The US policy focuses on the product (not the process), only looks at verifiable scientific risks and uses the concept of substantial equivalence.[160] The European Union by contrast has possibly the most stringent GMO regulations in the world.[161] All GMOs, along with irradiated food, are considered ""new food"" and subject to extensive, case-by-case, science-based food evaluation by the European Food Safety Authority.  The criteria for authorisation fall in four broad categories: ""safety"", ""freedom of choice"", ""labelling"", and ""traceability"".[162] The level of regulation in other countries that cultivate GMOs lie in between Europe and the United States.
 One of the key issues concerning regulators is whether GM products should be labeled.  The European Commission says that mandatory labeling and traceability are needed to allow for informed choice, avoid potential false advertising[173] and facilitate the withdrawal of products if adverse effects on health or the environment are discovered.[174] The American Medical Association[175] and the American Association for the Advancement of Science[176] say that absent scientific evidence of harm even voluntary labeling is misleading and will falsely alarm consumers. Labeling of GMO products in the marketplace is required in 64 countries.[177] Labeling can be mandatory up to a threshold GM content level (which varies between countries) or voluntary. In Canada and the US labeling of GM food is voluntary,[178] while in Europe all food (including processed food) or feed which contains greater than 0.9% of approved GMOs must be labelled.[161]
 Critics have objected to the use of genetic engineering on several grounds, including ethical, ecological and economic concerns. Many of these concerns involve GM crops and whether food produced from them is safe and what impact growing them will have on the environment. These controversies have led to litigation, international trade disputes, and protests, and to restrictive regulation of commercial products in some countries.[179]
 Accusations that scientists are ""playing God"" and other religious issues have been ascribed to the technology from the beginning.[180] Other ethical issues raised include the patenting of life,[181] the use of intellectual property rights,[182] the level of labeling on products,[183][184] control of the food supply[185] and the objectivity of the regulatory process.[186] Although doubts have been raised,[187] economically most studies have found growing GM crops to be beneficial to farmers.[188][189][190]
 Gene flow between GM crops and compatible plants, along with increased use of selective herbicides, can increase the risk of ""superweeds"" developing.[191] Other environmental concerns involve potential impacts on non-target organisms, including soil microbes,[192] and an increase in secondary and resistant insect pests.[193][194] Many of the environmental impacts regarding GM crops may take many years to be understood and are also evident in conventional agriculture practices.[192][195] With the commercialisation of genetically modified fish there are concerns over what the environmental consequences will be if they escape.[196]
 There are three main concerns over the safety of genetically modified food: whether they may provoke an allergic reaction; whether the genes could transfer from the food into human cells; and whether the genes not approved for human consumption could outcross to other crops.[197] There is a scientific consensus[198][199][200][201] that currently available food derived from GM crops poses no greater risk to human health than conventional food,[202][203][204][205][206] but that each GM food needs to be tested on a case-by-case basis before introduction.[207][208][209] Nonetheless, members of the public are less likely than scientists to perceive GM foods as safe.[210][211][212][213]
 Genetic engineering features in many science fiction stories.[214] Frank Herbert's novel The White Plague describes the deliberate use of genetic engineering to create a pathogen which specifically kills women.[214] Another of Herbert's creations, the Dune series of novels, uses genetic engineering to create the powerful Tleilaxu.[215] Few films have informed audiences about genetic engineering, with the exception of the 1978 The Boys from Brazil and the 1993 Jurassic Park, both of which make use of a lesson, a demonstration, and a clip of scientific film.[216][217] Genetic engineering methods are weakly represented in film; Michael Clark, writing for the Wellcome Trust, calls the portrayal of genetic engineering and biotechnology ""seriously distorted""[217] in films such as The 6th Day. In Clark's view, the biotechnology is typically ""given fantastic but visually arresting forms"" while the science is either relegated to the background or fictionalised to suit a young audience.[217]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Genetic engineering', 'Alfred Hershey and Martha Chase', 'its ability to replicate in a wide variety of single-celled organisms', '1978 The Boys from Brazil and the 1993 Jurassic Park', 'potential risks from genetic engineering'], 'answer_start': [], 'answer_end': []}"
"
 Biotechnology is a multidisciplinary field that involves the integration of natural sciences and engineering sciences in order to achieve the application of organisms and parts thereof for products and services.[1]
 The term biotechnology was first used by Károly Ereky in 1919[2] to refer to the production of products from raw materials with the aid of living organisms. The core principle of biotechnology involves harnessing biological systems and organisms, such as bacteria, yeast, and plants, to perform specific tasks or produce valuable substances.
 Biotechnology had a significant impact on many areas of society, from medicine to agriculture to environmental science. One of the key techniques used in biotechnology is genetic engineering, which allows scientists to modify the genetic makeup of organisms to achieve desired outcomes. This can involve inserting genes from one organism into another, and consequently, create new traits or modifying existing ones.[3]
 Other important techniques used in biotechnology include tissue culture, which allows researchers to grow cells and tissues in the lab for research and medical purposes, and fermentation, which is used to produce a wide range of products such as beer, wine, and cheese.
 The applications of biotechnology are diverse and have led to the development of essential products like life-saving drugs, biofuels, genetically modified crops, and innovative materials.[4] It has also been used to address environmental challenges, such as developing biodegradable plastics and using microorganisms to clean up contaminated sites.
 Biotechnology is a rapidly evolving field with significant potential to address pressing global challenges and improve the quality of life for people around the world; however, despite its numerous benefits, it also poses ethical and societal challenges, such as questions around genetic modification and intellectual property rights. As a result, there is ongoing debate and regulation surrounding the use and application of biotechnology in various industries and fields.[5]
 The concept of biotechnology encompasses a wide range of procedures for modifying living organisms for human purposes, going back to domestication of animals, cultivation of the plants, and ""improvements"" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering, as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms, such as pharmaceuticals, crops, and livestock.[6] As per the European Federation of Biotechnology, biotechnology is the integration of natural science and organisms, cells, parts thereof, and molecular analogues for products and services.[7] Biotechnology is based on the basic biological sciences (e.g., molecular biology, biochemistry, cell biology, embryology, genetics, microbiology) and conversely provides methods to support and perform basic research in biology.
 Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation, and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured, and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).[8][9][10] The utilization of biological processes, organisms or systems to produce products that are anticipated to improve human lives is termed biotechnology.[11]
 By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials directly) for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells, and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals.[12] Relatedly, biomedical engineering is an overlapping field that often draws upon and applies biotechnology (by various definitions), especially in certain sub-fields of biomedical or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.
 Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of ""utilizing a biotechnological system to make products"". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.
 Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best-suited crops (e.g., those with the highest yields) to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.[clarification needed]
 These processes also were included in early fermentation of beer.[13] These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods. In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains broke down into alcohols, such as ethanol. Later, other cultures produced the process of lactic acid fermentation, which produced other preserved foods, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.
 Before the time of Charles Darwin's work and life, animal and plant scientists had already used selective breeding. Darwin added to that body of work with his scientific observations about the ability of science to change species. These accounts contributed to Darwin's theory of natural selection.[14]
 For thousands of years, humans have used selective breeding to improve the production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.[15]
 In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using Clostridium acetobutylicum, to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.[16]
 Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold Penicillium. His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley – to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.[15]
 The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of Diamond v. Chakrabarty.[17] Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the genus Pseudomonas) capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the Pseudomonas bacterium).
 The MOSFET (metal–oxide–semiconductor field-effect transistor) was invented by Mohamed M. Atalla and Dawon Kahng in 1959.[18] Two years later, Leland C. Clark and Champ Lyons invented the first biosensor in 1962.[19][20] Biosensor MOSFETs were later developed, and they have since been widely used to measure physical, chemical, biological and environmental parameters.[21] The first BioFET was the ion-sensitive field-effect transistor (ISFET), invented by Piet Bergveld in 1970.[22][23] It is a special type of MOSFET,[21] where the metal gate is replaced by an ion-sensitive membrane, electrolyte solution and reference electrode.[24] The ISFET is widely used in biomedical applications, such as the detection of DNA hybridization, biomarker detection from blood, antibody detection, glucose measurement, pH sensing, and genetic technology.[24]
 By the mid-1980s, other BioFETs had been developed, including the gas sensor FET (GASFET), pressure sensor FET (PRESSFET), chemical field-effect transistor (ChemFET), reference ISFET (REFET), enzyme-modified FET (ENFET) and immunologically modified FET (IMFET).[21] By the early 2000s, BioFETs such as the DNA field-effect transistor (DNAFET), gene-modified FET (GenFET) and cell-potential BioFET (CPFET) had been developed.[24]
 A factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.[25]
 Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds that resist pests and drought. By increasing farm productivity, biotechnology boosts biofuel production.[26]
 Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non-food (industrial) uses of crops and other products (e.g., biodegradable plastics, vegetable oil, biofuels), and environmental uses.
 For example, one application of biotechnology is the directed use of microorganisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, clean up sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.
 A series of derived terms have been coined to identify several branches of biotechnology, for example:
 In medicine, modern biotechnology has many applications in areas such as pharmaceutical drug discoveries and production, pharmacogenomics, and genetic testing (or genetic screening). In 2021, nearly 40% of the total company value of pharmaceutical biotech companies worldwide were active in Oncology with Neurology and Rare Diseases being the other two big applications.[37]
 Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs.[38] Researchers in the field investigate the influence of genetic variation on drug responses in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity.[39] The purpose of pharmacogenomics is to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects.[40] Such approaches promise the advent of ""personalized medicine""; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.[41][42]
 Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology – biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium Escherichia coli. Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle or pigs). The genetically engineered bacteria are able to produce large quantities of synthetic human insulin at relatively low cost.[43][44] Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well.[44]
 Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins.[45] Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use.[46][47] Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.
 Genetically modified crops (""GM crops"", or ""biotech crops"") are plants used in agriculture, the DNA of which has been modified with genetic engineering techniques. In most cases, the main aim is to introduce a new trait that does not occur naturally in the species. Biotechnology firms can contribute to future food security by improving the nutrition and viability of urban agriculture. Furthermore, the protection of intellectual property rights encourages private sector investment in agrobiotechnology.
 Examples in food crops include resistance to certain pests,[48] diseases,[49] stressful environmental conditions,[50] resistance to chemical treatments (e.g. resistance to a herbicide[51]), reduction of spoilage,[52] or improving the nutrient profile of the crop.[53] Examples in non-food crops include production of pharmaceutical agents,[54] biofuels,[55] and other industrially useful goods,[56] as well as for bioremediation.[57][58]
 Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from 17,000 to 1,600,000 square kilometers (4,200,000 to 395,400,000 acres).[59] 10% of the world's crop lands were planted with GM crops in 2010.[59] As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the US, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.[59]
 Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA with the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding.[60] Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato.[61] To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed; in November 2013 none were available on the market,[62] but in 2015 the FDA approved the first GM salmon for commercial production and consumption.[63]
 There is a scientific consensus[64][65][66][67] that currently available food derived from GM crops poses no greater risk to human health than conventional food,[68][69][70][71][72] but that each GM food needs to be tested on a case-by-case basis before introduction.[73][74][75] Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe.[76][77][78][79] The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.[80][81][82][83]
 GM crops also provide a number of ecological benefits, if not used in excess.[84] Insect-resistant crops have proven to lower pesticide usage, therefore reducing the environmental impact of pesticides as a whole.[85] However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.
 Biotechnology has several applications in the realm of food security. Crops like Golden rice are engineered to have higher nutritional content, and there is potential for food products with longer shelf lives.[86] Though not a form of agricultural biotechnology, vaccines can help prevent diseases found in animal agriculture. Additionally, agricultural biotechnology can expedite breeding processes in order to yield faster results and provide greater quantities of food.[87] Transgenic biofortification in cereals has been considered as a promising method to combat malnutrition in India and other countries.[88]
 Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as microorganisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels.[89] In the current decades, significant progress has been done in creating genetically modified organisms (GMOs) that enhance the diversity of applications and economical viability of industrial biotechnology. By using renewable raw materials to produce a variety of chemicals and fuels, industrial biotechnology is actively advancing towards lowering greenhouse gas emissions and moving away from a petrochemical-based economy.[90]
 Synthetic biology is considered one of the essential cornerstones in industrial biotechnology due to its financial and sustainable contribution to the manufacturing sector. Jointly biotechnology and synthetic biology play a crucial role in generating cost-effective products with nature-friendly features by using bio-based production instead of fossil-based.[91] Synthetic biology can be used to engineer model microorganisms, such as Escherichia coli, by genome editing tools to enhance their ability to produce bio-based products, such as bioproduction of medicines and biofuels.[92] For instance, E. coli and Saccharomyces cerevisiae in a consortium could be used as industrial microbes to produce precursors of the chemotherapeutic agent paclitaxel by applying the metabolic engineering in a co-culture approach to exploit the benefits from the two microbes.[93]
 Another example of synthetic biology applications in industrial biotechnology is the re-engineering of the metabolic pathways of E. coli by CRISPR and CRISPRi systems toward the production of a chemical known as 1,4-butanediol, which is used in fiber manufacturing. In order to produce 1,4-butanediol, the authors alter the metabolic regulation of the Escherichia coli by CRISPR to induce point mutation in the gltA gene, knockout of the sad gene, and knock-in six genes (cat1, sucD, 4hbd, cat2, bld, and bdh). Whereas CRISPRi system used to knockdown the three competing genes (gabD, ybgC, and tesB) that affect the biosynthesis pathway of 1,4-butanediol. Consequently, the yield of 1,4-butanediol significantly increased from 0.9 to 1.8 g/L.[94]
 Environmental biotechnology includes various disciplines that play an essential role in reducing environmental waste and providing environmentally safe processes, such as biofiltration and biodegradation.[95][96] The environment can be affected by biotechnologies, both positively and adversely. Vallero and others have argued that the difference between beneficial biotechnology (e.g., bioremediation is to clean up an oil spill or hazard chemical leak) versus the adverse effects stemming from biotechnological enterprises (e.g., flow of genetic material from transgenic organisms into wild strains) can be seen as applications and implications, respectively.[97] Cleaning up environmental wastes is an example of an application of environmental biotechnology; whereas loss of biodiversity or loss of containment of a harmful microbe are examples of environmental implications of biotechnology.
 Many cities have installed CityTrees, which use biotechnology to filter pollutants from urban atmospheres.[98]
 The regulation of genetic engineering concerns approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology, and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the US and Europe.[99] Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety.[100] The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing.[101] The cultivation of GMOs has triggered a debate about the coexistence of GM and non-GM crops. Depending on the coexistence regulations, incentives for the cultivation of GM crops differ.[102]
 The EUginius (European GMO Initiative for a Unified Database System) database is intended to help companies, interested private users and competent authorities to find precise information on the presence, detection and identification of GMOs used in the European Union. The information is provided in English.
 In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support are provided for two or three years during the course of their PhD thesis work. Nineteen institutions offer NIGMS supported BTPs.[103] Biotechnology training is also offered at the undergraduate level and in community colleges.
 But see also: Domingo, José L.; Bordonaba, Jordi Giné (2011). ""A literature review on the safety assessment of genetically modified plants"" (PDF). Environment International. 37 (4): 734–742. doi:10.1016/j.envint.2011.01.003. PMID 21296423. Archived (PDF) from the original on October 9, 2022. In spite of this, the number of studies specifically focused on safety assessment of GM plants is still limited. However, it is important to remark that for the first time, a certain equilibrium in the number of research groups suggesting, on the basis of their studies, that a number of varieties of GM products (mainly maize and soybeans) are as safe and nutritious as the respective conventional non-GM plant, and those raising still serious concerns, was observed. Moreover, it is worth mentioning that most of the studies demonstrating that GM foods are as nutritional and safe as those obtained by conventional breeding, have been performed by biotechnology companies or associates, which are also responsible of commercializing these GM plants. Anyhow, this represents a notable advance in comparison with the lack of studies published in recent years in scientific journals by those companies. Krimsky, Sheldon (2015). ""An Illusory Consensus behind GMO Health Assessment"". Science, Technology, & Human Values. 40 (6): 883–914. doi:10.1177/0162243915598381. S2CID 40855100. I began this article with the testimonials from respected scientists that there is literally no scientific controversy over the health effects of GMOs. My investigation into the scientific literature tells another story. And contrast: Panchin, Alexander Y.; Tuzhikov, Alexander I. (January 14, 2016). ""Published GMO studies find no evidence of harm when corrected for multiple comparisons"". Critical Reviews in Biotechnology. 37 (2): 213–217. doi:10.3109/07388551.2015.1130684. ISSN 0738-8551. PMID 26767435. S2CID 11786594. Here, we show that a number of articles some of which have strongly and negatively influenced the public opinion on GM crops and even provoked political actions, such as GMO embargo, share common flaws in the statistical evaluation of the data. Having accounted for these flaws, we conclude that the data presented in these articles does not provide any substantial evidence of GMO harm.  The presented articles suggesting possible harm of GMOs received high public attention. However, despite their claims, they actually weaken the evidence for the harm and lack of substantial equivalency of studied GMOs. We emphasize that with over 1783 published articles on GMOs over the last 10 years it is expected that some of them should have reported undesired differences between GMOs and conventional crops even if no such differences exist in reality. and","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['environmental science', 'Panchin, Alexander Y.; Tuzhikov, Alexander I.', 'significant impact on many areas of society, from medicine to agriculture to environmental science', 'share common flaws in the statistical evaluation of the data', 'biodiversity or loss of containment of a harmful microbe'], 'answer_start': [], 'answer_end': []}"
"
 A medication (also called medicament, medicine, pharmaceutical drug, medicinal drug or simply drug) is a drug used to diagnose, cure, treat, or prevent disease.[1][2] Drug therapy (pharmacotherapy) is an important part of the medical field and relies on the science of pharmacology for continual advancement and on pharmacy for appropriate management.
 Drugs are classified in many ways. One of the key divisions is by level of control, which distinguishes prescription drugs (those that a pharmacist dispenses only on the order of a physician, physician assistant, or qualified nurse) from over-the-counter drugs (those that consumers can order for themselves). Another key distinction is between traditional small molecule drugs, usually derived from chemical synthesis, and biopharmaceuticals, which include recombinant proteins, vaccines, blood products used therapeutically (such as IVIG), gene therapy, monoclonal antibodies and cell therapy (for instance, stem cell therapies). Other ways to classify medicines are by mode of action, route of administration, biological system affected, or therapeutic effects. An elaborate and widely used classification system is the Anatomical Therapeutic Chemical Classification System. The World Health Organization keeps a list of essential medicines.
 Drug discovery and drug development are complex and expensive endeavors undertaken by pharmaceutical companies, academic scientists, and governments. As a result of this complex path from discovery to commercialization, partnering has become a standard practice for advancing drug candidates through development pipelines.  Governments generally regulate what drugs can be marketed, how drugs are marketed, and in some jurisdictions, drug pricing. Controversies have arisen over drug pricing and disposal of used drugs.
 Medication is a medicine or a chemical compound used to treat or cure illness. According to Encyclopædia Britannica, medication is ""a substance used in treating a disease or relieving pain"".[3]
 As defined by the National Cancer Institute, dosage forms of medication can include tablets, capsules, liquids, creams, and patches. Medications can be given in different ways, such as by mouth, by infusion into a vein, or by drops put into the ear or eye. A medication that does not contain an active ingredient and is used in research studies is called a placebo.[4]
 In Europe, the term is ""medicinal product"", and it is defined by EU law as:
 In the US, a ""drug"" is:
 Drug use among elderly Americans has been studied; in a group of 2377 people with an average age of 71 surveyed between 2005 and 2006, 84% took at least one prescription drug, 44% took at least one over-the-counter (OTC) drug, and 52% took at least one dietary supplement; in a group of 2245 elderly Americans (average age of 71) surveyed over the period 2010 – 2011, those percentages were 88%, 38%, and 64%.[7]
 One of the key classifications is between traditional small molecule drugs; usually derived from chemical synthesis and biological medical products; which include recombinant proteins, vaccines, blood products used therapeutically (such as IVIG), gene therapy, and cell therapy (for instance, stem cell therapies).[citation needed]
 Pharmaceuticals or drugs or medicines are classified into various other groups besides their origin on the basis of pharmacological properties like mode of action and their pharmacological action or activity,[8] such as by chemical properties, mode or route of administration, biological system affected, or therapeutic effects. An elaborate and widely used classification system is the Anatomical Therapeutic Chemical Classification System (ATC system). The World Health Organization keeps a list of essential medicines.
 A sampling of classes of medicine includes:
 Pharmaceuticals may also be described as ""specialty"", independent of other classifications, which is an ill-defined class of drugs that might be difficult to administer, require special handling during administration, require patient monitoring during and immediately after administration, have particular regulatory requirements restricting their use, and are generally expensive relative to other drugs.[9]
 Drugs affecting the central nervous system include psychedelics, hypnotics, anaesthetics, antipsychotics, eugeroics, antidepressants (including tricyclic antidepressants, monoamine oxidase inhibitors, lithium salts, and selective serotonin reuptake inhibitors (SSRIs)), antiemetics, anticonvulsants/antiepileptics, anxiolytics, barbiturates, movement disorder (e.g., Parkinson's disease) drugs, nootropics, stimulants (including amphetamines), benzodiazepines, cyclopyrrolones, dopamine antagonists, antihistamines, cholinergics, anticholinergics, emetics, cannabinoids, and 5-HT (serotonin) antagonists.
 The main classes of painkillers are NSAIDs, opioids, and local anesthetics.
 For consciousness (anesthetic drugs)
 Some anesthetics include benzodiazepines and barbiturates.
 The main categories of drugs for musculoskeletal disorders are: NSAIDs (including COX-2 selective inhibitors), muscle relaxants, neuromuscular drugs, and anticholinesterases.
 Antibiotics, sympathomimetics, antihistamines, anticholinergics, NSAIDs, corticosteroids, antiseptics, local anesthetics, antifungals, and cerumenolytics.
 Bronchodilators, antitussives, mucolytics, decongestants, inhaled and systemic corticosteroids, beta2-adrenergic agonists, anticholinergics, mast cell stabilizers, leukotriene antagonists.
 Androgens, antiandrogens, estrogens, gonadotropin, corticosteroids, human growth hormone, insulin, antidiabetics (sulfonylureas, biguanides/metformin, thiazolidinediones, insulin), thyroid hormones, antithyroid drugs, calcitonin, diphosphonate, vasopressin analogues.
 Antifungal, alkalinizing agents, quinolones, antibiotics, cholinergics, anticholinergics, antispasmodics, 5-alpha reductase inhibitor, selective alpha-1 blockers, sildenafils, fertility medications.
 NSAIDs, anticholinergics, haemostatic drugs, antifibrinolytics, Hormone Replacement Therapy (HRT), bone regulators, beta-receptor agonists, follicle stimulating hormone, luteinising hormone, LHRH, gamolenic acid, gonadotropin release inhibitor, progestogen, dopamine agonists, oestrogen, prostaglandins, gonadorelin, clomiphene, tamoxifen, diethylstilbestrol.
 Emollients, anti-pruritics, antifungals, antiseptics, scabicides, pediculicides, tar products, vitamin A derivatives, vitamin D analogues, keratolytics, abrasives, systemic antibiotics, topical antibiotics, hormones, desloughing agents, exudate absorbents, fibrinolytics, proteolytics, sunscreens, antiperspirants, corticosteroids, immune modulators.
 Antibiotics, antifungals, antileprotics, antituberculous drugs, antimalarials, anthelmintics, amoebicides, antivirals, antiprotozoals, probiotics, prebiotics, antitoxins, and antivenoms.
 Vaccines, immunoglobulins, immunosuppressants, interferons, and monoclonal antibodies.
 Anti-allergics, antihistamines, NSAIDs,[medical citation needed] corticosteroids.
 Tonics, electrolytes and mineral preparations (including iron preparations and magnesium preparations), parenteral nutrition, vitamins, anti-obesity drugs, anabolic drugs, haematopoietic drugs, food product drugs.
 Cytotoxic drugs, therapeutic antibodies, sex hormones, aromatase inhibitors, somatostatin inhibitors, recombinant interleukins, G-CSF, erythropoietin.
 Contrast media.
 A euthanaticum is used for euthanasia and physician-assisted suicide. Euthanasia is not permitted by law in many countries, and consequently, medicines will not be licensed for this use in those countries.
 A single drug may contain single or multiple active ingredients.
 The administration is the process by which a patient takes medicine. There are three major categories of drug administration: enteral (via the human gastrointestinal tract), injection into the body, and by other routes (dermal, nasal, ophthalmic, otologic, and urogenital).[10]
 Oral administration, the most common form of enteral administration, can be performed using various dosage forms including tablets or capsules and liquid such as syrup or suspension. Other ways to take the medication include buccally (placed inside the cheek), sublingually (placed underneath the tongue), eye and ear drops (dropped into the eye or ear), and transdermally (applied to the skin).[11]
 They can be administered in one dose, as a bolus. Administration frequencies[12] are often abbreviated from Latin, such as every 8 hours reading Q8H from Quaque VIII Hora. The drug frequencies are often expressed as the number of times a drug is used per day (e.g., four times a day). It[specify] may include event-related information (e.g., 1 hour before meals, in the morning, at bedtime), or complimentary to an interval, although equivalent expressions may have different implications (e.g., every 8 hours versus 3 times a day).[citation needed]
 In the fields of medicine, biotechnology, and pharmacology, drug discovery is the process by which new drugs are discovered.[citation needed]
 Historically, drugs were discovered by identifying the active ingredient from traditional remedies or by serendipitous discovery. Later chemical libraries of synthetic small molecules, natural products, or extracts were screened in intact cells or whole organisms to identify substances that have a desirable therapeutic effect in a process known as classical pharmacology.  Since sequencing of the human genome which allowed rapid cloning and synthesis of large quantities of purified proteins, it has become common practice to use high throughput screening of large compound libraries against isolated biological targets which are hypothesized to be disease-modifying in a process known as reverse pharmacology. Hits from these screens are then tested in cells and then in animals for efficacy. Even more recently, scientists have been able to understand the shape of biological molecules at the atomic level and to use that knowledge to design (see drug design) drug candidates.[citation needed]
 Modern drug discovery involves the identification of screening hits, medicinal chemistry, and optimization of those hits to increase the affinity, selectivity (to reduce the potential of side effects), efficacy/potency, metabolic stability (to increase the half-life), and oral bioavailability. Once a compound that fulfills all of these requirements has been identified, it will begin the process of drug development prior to clinical trials. One or more of these steps may, but not necessarily, involve computer-aided drug design.
 Despite advances in technology and understanding of biological systems, drug discovery is still a lengthy, ""expensive, difficult, and inefficient process"" with a low rate of new therapeutic discovery.[13] In 2010, the research and development cost of each new molecular entity (NME) was approximately US$1.8 billion.[14] Drug discovery is done by pharmaceutical companies, sometimes with research assistance from universities. The ""final product"" of drug discovery is a patent on the potential drug. The drug requires very expensive Phase I, II, and III clinical trials, and most of them fail. Small companies have a critical role, often then selling the rights to larger companies that have the resources to run the clinical trials.
 Drug discovery is different from Drug Development. Drug Discovery is often considered the process of identifying new medicine. At the same time, Drug development is delivering a new drug molecule into clinical practice. In its broad definition, this encompasses all steps from the basic research process of finding a suitable molecular target to supporting the drug's commercial launch.
 Drug development is the process of bringing a new drug to the market once a lead compound has been identified through the process of drug discovery. It includes pre-clinical research (microorganisms/animals) and clinical trials (on humans) and may include the step of obtaining regulatory approval to market the drug.[15][16]
 Drug Development Process
 Discovery: The Drug Development process starts with Discovery, a process of identifying a new medicine.
 Development: Chemicals extracted from natural products are used to make pills, capsules, or syrups for oral use. Injections for direct infusion into the blood drops for eyes or ears.
 Preclinical research: Drugs go under laboratory or animal testing, to ensure that they can be used on Humans.
 Clinical testing: The drug is used on people to confirm that it is safe to use.
 FDA Review: drug is sent to FDA before launching the drug into the market.
 FDA post-Market Review: The drug is reviewed and monitored by FDA for the safety once it is available to the public.
 The regulation of drugs varies by jurisdiction. In some countries, such as the United States, they are regulated at the national level by a single agency. In other jurisdictions, they are regulated at the state level, or at both state and national levels by various bodies, as is the case in Australia.  The role of therapeutic goods regulation is designed mainly to protect the health and safety of the population. Regulation is aimed at ensuring the safety, quality, and efficacy of the therapeutic goods which are covered under the scope of the regulation. In most jurisdictions, therapeutic goods must be registered before they are allowed to be marketed. There is usually some degree of restriction on the availability of certain therapeutic goods depending on their risk to consumers.[citation needed]
 Depending upon the jurisdiction, drugs may be divided into over-the-counter drugs (OTC) which may be available without special restrictions, and prescription drugs, which must be prescribed by a licensed medical practitioner in accordance with medical guidelines due to the risk of adverse effects and contraindications. The precise distinction between OTC and prescription depends on the legal jurisdiction. A third category, ""behind-the-counter"" drugs, is implemented in some jurisdictions. These do not require a prescription, but must be kept in the dispensary, not visible to the public, and be sold only by a pharmacist or pharmacy technician. Doctors may also prescribe prescription drugs for off-label use – purposes which the drugs were not originally approved for by the regulatory agency. The Classification of Pharmaco-Therapeutic Referrals helps guide the referral process between pharmacists and doctors.
 The International Narcotics Control Board of the United Nations imposes a world law of prohibition of certain drugs. They publish a lengthy list of chemicals and plants whose trade and consumption (where applicable) are forbidden. OTC drugs are sold without restriction as they are considered safe enough that most people will not hurt themselves accidentally by taking it as instructed.[17] Many countries, such as the United Kingdom have a third category of ""pharmacy medicines"", which can be sold only in registered pharmacies by or under the supervision of a pharmacist.
 Medical errors include over-prescription and polypharmacy, mis-prescription, contraindication and lack of detail in dosage and administration instructions. In 2000 the definition of a prescription error was studied using a Delphi method conference; the conference was motivated by ambiguity in what a prescription error is and a need to use a uniform definition in studies.[18]
 In many jurisdictions, drug prices are regulated.
 In the UK, the Pharmaceutical Price Regulation Scheme is intended to ensure that the National Health Service is able to purchase drugs at reasonable prices. The prices are negotiated between the Department of Health, acting with the authority of Northern Ireland and the UK Government, and the representatives of the Pharmaceutical industry brands, the Association of the British Pharmaceutical Industry (ABPI). For 2017 this payment percentage set by the PPRS will be 4,75%.[19]
 In Canada, the Patented Medicine Prices Review Board examines drug pricing and determines if a price is excessive or not. In these circumstances, drug manufacturers must submit a proposed price to the appropriate regulatory agency. Furthermore, ""the International Therapeutic Class Comparison Test is responsible for comparing the National Average Transaction Price of the patented drug product under review""[20] different countries that the prices are being compared to are the following: France, Germany, Italy, Sweden, Switzerland, the United Kingdom, and the United States[20]
 In Brazil, the prices are regulated through legislation under the name of Medicamento Genérico (generic drugs) since 1999.[21]
 In India, drug prices are regulated by the National Pharmaceutical Pricing Authority.
 In the United States, drug costs are partially unregulated, but instead are the result of negotiations between drug companies and insurance companies.[22]
 High prices have been attributed to monopolies given to manufacturers by the government.[23] New drug development costs continue to rise as well. Despite the enormous advances in science and technology, the number of new blockbuster drugs approved by the government per billion dollars spent has halved every 9 years since 1950.[24]
 A blockbuster drug is a drug that generates more than $1 billion in revenue for a pharmaceutical company in a single year.[25] Cimetidine was the first drug ever to reach more than $1 billion a year in sales, thus making it the first blockbuster drug.[26]
 In the pharmaceutical industry, a blockbuster drug is one that achieves acceptance by prescribing physicians as a therapeutic standard for, most commonly, a highly prevalent chronic (rather than acute) condition. Patients often take the medicines for long periods.[27] Antibiotics first arrived on the medical scene in 1932 thanks to Gerhard Domagk;[28] and were coined the ""wonder drugs"". The introduction of the sulfa drugs led to the mortality rate from pneumonia in the U.S. to drop from 0.2% each year to 0.05% by 1939.[29] Antibiotics inhibit the growth or the metabolic activities of bacteria and other microorganisms by a chemical substance of microbial origin. Penicillin, introduced a few years later, provided a broader spectrum of activity compared to sulfa drugs and reduced side effects. Streptomycin, found in 1942, proved to be the first drug effective against the cause of tuberculosis and also came to be the best known of a long series of important antibiotics. A second generation of antibiotics was introduced in the 1940s: aureomycin and chloramphenicol. Aureomycin was the best known of the second generation.[citation needed]
 Lithium was discovered in the 19th century for nervous disorders and its possible mood-stabilizing or prophylactic effect; it was cheap and easily produced. As lithium fell out of favor in France, valpromide came into play. This antibiotic was the origin of the drug that eventually created the mood stabilizer category. Valpromide had distinct psychotrophic effects that were of benefit in both the treatment of acute manic states and in the maintenance treatment of manic depression illness. Psychotropics can either be sedative or stimulant; sedatives aim at damping down the extremes of behavior. Stimulants aim at restoring normality by increasing tone. Soon arose the notion of a tranquilizer which was quite different from any sedative or stimulant. The term tranquilizer took over the notions of sedatives and became the dominant term in the West through the 1980s. In Japan, during this time, the term tranquilizer produced the notion of a psyche-stabilizer and the term mood stabilizer vanished.[30]
 Premarin (conjugated estrogens, introduced in 1942) and Prempro (a combination estrogen-progestin pill, introduced in 1995) dominated the hormone replacement therapy (HRT) during the 1990s. HRT is not a life-saving drug, nor does it cure any disease. HRT has been prescribed to improve one's quality of life. Doctors prescribe estrogen for their older female patients both to treat short-term menopausal symptoms and to prevent long-term diseases. In the 1960s and early 1970s, more and more physicians began to prescribe estrogen for their female patients. between 1991 and 1999, Premarin was listed as the most popular prescription and best-selling drug in America.[30]
 The first oral contraceptive, Enovid, was approved by FDA in 1960. Oral contraceptives inhibit ovulation and so prevent conception. Enovid was known to be much more effective than alternatives including the condom and the diaphragm. As early as 1960, oral contraceptives were available in several different strengths by every manufacturer. In the 1980s and 1990s, an increasing number of options arose including, most recently, a new delivery system for the oral contraceptive via a transdermal patch. In 1982, a new version of the Pill was introduced, known as the ""biphasic"" pill. By 1985, a new triphasic pill was approved. Physicians began to think of the Pill as an excellent means of birth control for young women.[30]
 Stimulants such as Ritalin (methylphenidate) came to be pervasive tools for behavior management and modification in young children. Ritalin was first marketed in 1955 for narcolepsy; its potential users were middle-aged and the elderly. It was not until some time in the 1980s along with hyperactivity in children that Ritalin came onto the market. Medical use of methylphenidate is predominantly for symptoms of attention deficit/hyperactivity disorder (ADHD). Consumption of methylphenidate in the U.S. out-paced all other countries between 1991 and 1999. Significant growth in consumption was also evident in Canada, New Zealand, Australia, and Norway. Currently, 85% of the world's methylphenidate is consumed in America.[30]
 The first minor tranquilizer was Meprobamate. Only fourteen months after it was made available, meprobamate had become the country's largest-selling prescription drug. By 1957, meprobamate had become the fastest-growing drug in history. The popularity of meprobamate paved the way for Librium and Valium, two minor tranquilizers that belonged to a new chemical class of drugs called the benzodiazepines. These were drugs that worked chiefly as anti-anxiety agents and muscle relaxants. The first benzodiazepine was Librium. Three months after it was approved, Librium had become the most prescribed tranquilizer in the nation. Three years later, Valium hit the shelves and was ten times more effective as a muscle relaxant and anti-convulsant. Valium was the most versatile of the minor tranquilizers. Later came the widespread adoption of major tranquilizers such as chlorpromazine and the drug reserpine. In 1970, sales began to decline for Valium and Librium, but sales of new and improved tranquilizers, such as Xanax, introduced in 1981 for the newly created diagnosis of panic disorder, soared.[30]
 Mevacor (lovastatin) is the first and most influential statin in the American market. The 1991 launch of Pravachol (pravastatin), the second available in the United States, and the release of Zocor (simvastatin) made Mevacor no longer the only statin on the market.
In 1998, Viagra was released as a treatment for erectile dysfunction.[30]
 Using plants and plant substances to treat all kinds of diseases and medical conditions is believed to date back to prehistoric medicine.[citation needed]
 The Kahun Gynaecological Papyrus, the oldest known medical text of any kind, dates to about 1800 BC and represents the first documented use of any kind of drug.[31][32] It and other medical papyri describe Ancient Egyptian medical practices, such as using honey to treat infections and the legs of bee-eaters to treat neck pains.
 Ancient Babylonian medicine demonstrated the use of medication in the first half of the 2nd millennium BC. Medicinal creams and pills were employed as treatments.[33]
 On the Indian subcontinent, the Atharvaveda, a sacred text of Hinduism whose core dates from the second millennium BC, although the hymns recorded in it are believed to be older, is the first Indic text dealing with medicine. It describes plant-based drugs to counter diseases.[34] The earliest foundations of ayurveda were built on a synthesis of selected ancient herbal practices, together with a massive addition of theoretical conceptualizations, new nosologies and new therapies dating from about 400 BC onwards.[35] The student of Āyurveda was expected to know ten arts that were indispensable in the preparation and application of his medicines: distillation, operative skills, cooking, horticulture, metallurgy, sugar manufacture, pharmacy, analysis and separation of minerals, compounding of metals, and preparation of alkalis.
 The Hippocratic Oath for physicians, attributed to fifth century BC Greece, refers to the existence of ""deadly drugs"", and ancient Greek physicians imported drugs from Egypt and elsewhere.[36] The pharmacopoeia De materia medica, written between 50 and 70 CE by the Greek physician Pedanius Dioscorides, was widely read for more than 1,500 years.[37]
 Al-Kindi's ninth century AD book, De Gradibus and Ibn Sina (Avicenna)'s The Canon of Medicine, covers a range of drugs known to the practice of medicine in the medieval Islamic world.
 Medieval medicine of Western Europe saw advances in surgery compared to previously, but few truly effective drugs existed, beyond opium (found in such extremely popular drugs as the ""Great Rest"" of the Antidotarium Nicolai at the time)[38] and quinine. Folklore cures and potentially poisonous metal-based compounds were popular treatments. Theodoric Borgognoni, (1205–1296), one of the most significant surgeons of the medieval period, responsible for introducing and promoting important surgical advances including basic antiseptic practice and the use of anaesthetics. Garcia de Orta described some herbal treatments that were used.[vague]
 For most of the 19th century, drugs were not highly effective, leading Oliver Wendell Holmes Sr. to famously comment in 1842 that ""if all medicines in the world were thrown into the sea, it would be all the better for mankind and all the worse for the fishes"".[27]: 21 
 During the First World War, Alexis Carrel and Henry Dakin developed the Carrel-Dakin method of treating wounds with an irrigation, Dakin's solution, a germicide which helped prevent gangrene.
 In the inter-war period, the first anti-bacterial agents such as the sulpha antibiotics were developed. The Second World War saw the introduction of widespread and effective antimicrobial therapy with the development and mass production of penicillin antibiotics, made possible by the pressures of the war and the collaboration of British scientists with the American pharmaceutical industry.
 Medicines commonly used by the late 1920s included aspirin, codeine, and morphine for pain; digitalis, nitroglycerin, and quinine for heart disorders, and insulin for diabetes. Other drugs included antitoxins, a few biological vaccines, and a few synthetic drugs. In the 1930s, antibiotics emerged: first sulfa drugs, then penicillin and other antibiotics. Drugs increasingly became ""the center of medical practice"".[27]: 22  In the 1950s, other drugs emerged including corticosteroids for inflammation, rauvolfia alkaloids as tranquilizers and antihypertensives, antihistamines for nasal allergies, xanthines for asthma, and typical antipsychotics for psychosis.[27]: 23–24  As of 2007, thousands of approved drugs have been developed. Increasingly, biotechnology is used to discover biopharmaceuticals.[27] Recently, multi-disciplinary approaches have yielded a wealth of new data on the development of novel antibiotics and antibacterials and on the use of biological agents for antibacterial therapy.[39]
 In the 1950s, new psychiatric drugs, notably the antipsychotic chlorpromazine, were designed in laboratories and slowly came into preferred use. Although often accepted as an advance in some ways, there was some opposition, due to serious adverse effects such as tardive dyskinesia. Patients often opposed psychiatry and refused or stopped taking the drugs when not subject to psychiatric control.
 Governments have been heavily involved in the regulation of drug development and drug sales. In the U.S., the Elixir Sulfanilamide disaster led to the establishment of the Food and Drug Administration, and the 1938 Federal Food, Drug, and Cosmetic Act required manufacturers to file new drugs with the FDA. The 1951 Humphrey-Durham Amendment required certain drugs to be sold by prescription. In 1962, a subsequent amendment required new drugs to be tested for efficacy and safety in clinical trials.[27]: 24–26 
 Until the 1970s, drug prices were not a major concern for doctors and patients. As more drugs became prescribed for chronic illnesses, however, costs became burdensome, and by the 1970s nearly every U.S. state required or encouraged the substitution of generic drugs for higher-priced brand names. This also led to the 2006 U.S. law, Medicare Part D, which offers Medicare coverage for drugs.[27]: 28–29 
 As of 2008, the United States is the leader in medical research, including pharmaceutical development. U.S. drug prices are among the highest in the world, and drug innovation is correspondingly high. In 2000, U.S.-based firms developed 29 of the 75 top-selling drugs; firms from the second-largest market, Japan, developed eight, and the United Kingdom contributed 10. France, which imposes price controls, developed three. Throughout the 1990s, outcomes were similar.[27]: 30–31 
 Controversies concerning pharmaceutical drugs include patient access to drugs under development and not yet approved, pricing, and environmental issues.
 Governments worldwide have created provisions for granting access to drugs prior to approval for patients who have exhausted all alternative treatment options and do not match clinical trial entry criteria. Often grouped under the labels of compassionate use, expanded access, or named patient supply, these programs are governed by rules which vary by country defining access criteria, data collection, promotion, and control of drug distribution.[40]
 Within the United States, pre-approval demand is generally met through treatment IND (investigational new drug) applications (INDs), or single-patient INDs. These mechanisms, which fall under the label of expanded access programs, provide access to drugs for groups of patients or individuals residing in the US. Outside the US, Named Patient Programs provide controlled, pre-approval access to drugs in response to requests by physicians on behalf of specific, or ""named"", patients before those medicines are licensed in the patient's home country.  Through these programs, patients are able to access drugs in late-stage clinical trials or approved in other countries for a genuine, unmet medical need, before those drugs have been licensed in the patient's home country.[citation needed]
 Patients who have not been able to get access to drugs in development have organized and advocated for greater access.  In the United States, ACT UP formed in the 1980s, and eventually formed its Treatment Action Group in part to pressure the US government to put more resources into discovering treatments for AIDS and then to speed release of drugs that were under development.[41]
 The Abigail Alliance was established in November 2001 by Frank Burroughs in memory of his daughter, Abigail.[42] The Alliance seeks broader availability of investigational drugs on behalf of terminally ill patients.
 In 2013, BioMarin Pharmaceutical was at the center of a high-profile debate regarding expanded access of cancer patients to experimental drugs.[43][44]
 Essential medicines, as defined by the World Health Organization (WHO), are ""those drugs that satisfy the health care needs of the majority of the population; they should therefore be available at all times in adequate amounts and in appropriate dosage forms, at a price the community can afford.""[45]  Recent studies have found that most of the medicines on the WHO essential medicines list, outside of the field of HIV drugs, are not patented in the developing world, and that lack of widespread access to these medicines arise from issues fundamental to economic development – lack of infrastructure and poverty.[46] Médecins Sans Frontières also runs a Campaign for Access to Essential Medicines campaign, which includes advocacy for greater resources to be devoted to currently untreatable diseases that primarily occur in the developing world. The Access to Medicine Index tracks how well pharmaceutical companies make their products available in the developing world.[citation needed]
 World Trade Organization negotiations in the 1990s, including the TRIPS Agreement and the Doha Declaration, have centered on issues at the intersection of international trade in pharmaceuticals and intellectual property rights, with developed world nations seeking strong intellectual property rights to protect investments made to develop new drugs, and developing world nations seeking to promote their generic pharmaceuticals industries and their ability to make medicine available to their people via compulsory licenses.
 Some have raised ethical objections specifically with respect to pharmaceutical patents and the high prices for drugs that they enable their proprietors to charge, which poor people around the world, cannot afford.[47][48] Critics also question the rationale that exclusive patent rights and the resulting high prices are required for pharmaceutical companies to recoup the large investments needed for research and development.[47] One study concluded that marketing expenditures for new drugs often doubled the amount that was allocated for research and development.[49] Other critics claim that patent settlements would be costly for consumers, the health care system, and state and federal governments because it would result in delaying access to lower cost generic medicines.[50]
 Novartis fought a protracted battle with the government of India over the patenting of its drug, Gleevec, in India, which ended up in a Supreme Court in a case known as Novartis v. Union of India & Others.  The Supreme Court ruled narrowly against Novartis, but opponents of patenting drugs claimed it as a major victory.[51]
 Pharmaceutical medications are commonly described as ""ubiquitous"" in nearly every type of environmental medium (i.e. lakes, rivers, streams, estuaries, seawater, and soil) worldwide.[52][53][54][55] Their chemical components are typically present at relatively low concentrations in the ng/L to μg/L ranges.[56][54] The primary avenue for medications reaching the environment are through the effluent of wastewater treatment plants, both from industrial plants during production, and from municipal plants after consumption.[57] Agricultural pollution is another significant source derived from the prevalence of antibiotic use in livestock.[56]
 Scientists generally divide environmental impacts of a chemical into three primary categories: persistence, bioaccumulation, and toxicity.[53] Since medications are inherently bio-active, most are naturally degradable in the environment, however they are classified as ""pseudopersistent"" because they are constantly being replenished from their sources.[52] These Environmentally Persistent Pharmaceutical Pollutants (EPPPs) rarely reach toxic concentrations in the environment, however they have been known to bioaccumulate in some species.[58] Their effects have been observed to compound gradually across food webs, rather than becoming acute, leading to their classification by the US Geological Survey as ""Ecological Disrupting Compounds.""[52]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['psychiatry', 'Oliver Wendell Holmes Sr.', 'drugs were discovered by identifying the active ingredient', 'persistence, bioaccumulation, and toxicity', 'every 8 hours versus 3 times a day'], 'answer_start': [], 'answer_end': []}"
"
 Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in a defined population.
 It is a cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.[1]
 Major areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, environmental epidemiology, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.
 Epidemiology, literally meaning ""the study of what is upon the people"", is derived from Greek  epi 'upon, among',  demos 'people, district', and  logos 'study, word, discourse', suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term ""epizoology"" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).[2]
 The distinction between ""epidemic"" and ""endemic"" was first drawn by Hippocrates,[3] to distinguish between diseases that are ""visited upon"" a population (epidemic) from those that ""reside within"" a population (endemic).[4] The term ""epidemiology"" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Joaquín de Villalba in Epidemiología Española.[4] Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic.
 The term epidemiology is now widely applied to cover the description and causation of not only epidemic, infectious disease, but of disease in general, including related conditions. Some examples of topics examined through epidemiology include as high blood pressure, mental illness and obesity. Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of human beings.
 The Greek physician Hippocrates, taught by Democritus, was known as the father of medicine,[5][6] sought a logic to sickness; he is the first person known to have examined the relationships between the occurrence of disease and environmental influences.[7] Hippocrates believed sickness of the human body to be caused by an imbalance of the four humors (black bile, yellow bile, blood, and phlegm). The cure to the sickness was to remove or add the humor in question to balance the body. This belief led to the application of bloodletting and dieting in medicine.[8] He coined the terms endemic (for diseases usually found in some places but not in others) and epidemic (for diseases that are seen at some times but not others).[9]
 In the middle of the 16th century, a doctor from Verona named Girolamo Fracastoro was the first to propose a theory that the very small, unseeable, particles that cause disease were alive. They were considered to be able to spread by air, multiply by themselves and to be destroyable by fire. In this way he refuted Galen's miasma theory (poison gas in sick people). In 1543 he wrote a book De contagione et contagiosis morbis, in which he was the first to promote personal and environmental hygiene to prevent disease. The development of a sufficiently powerful microscope by Antonie van Leeuwenhoek in 1675 provided visual evidence of living particles consistent with a germ theory of disease.[citation needed]
 During the Ming dynasty, Wu Youke (1582–1652) developed the idea that some diseases were caused by transmissible agents, which he called Li Qi (戾气 or pestilential factors) when he observed various epidemics rage around him between 1641 and 1644.[10] His book Wen Yi Lun (瘟疫论, Treatise on Pestilence/Treatise of Epidemic Diseases) can be regarded as the main etiological work that brought forward the concept.[11] His concepts were still being considered in analysing SARS outbreak by WHO in 2004 in the context of traditional Chinese medicine.[12]
 Another pioneer, Thomas Sydenham (1624–1689), was the first to distinguish the fevers of Londoners in the later 1600s. His theories on cures of fevers met with much resistance from traditional physicians at the time. He was not able to find the initial cause of the smallpox fever he researched and treated.[8]
 John Graunt, a haberdasher and amateur statistician, published Natural and Political Observations ... upon the Bills of Mortality in 1662. In it, he analysed the mortality rolls in London before the Great Plague, presented one of the first life tables, and reported time trends for many diseases, new and old. He provided statistical evidence for many theories on disease, and also refuted some widespread ideas on them.[citation needed]
 John Snow is famous for his investigations into the causes of the 19th-century cholera epidemics, and is also known as the father of (modern) Epidemiology.[13][14] He began with noticing the significantly higher death rates in two areas supplied by Southwark Company. His identification of the Broad Street pump as the cause of the Soho epidemic is considered the classic example of epidemiology. Snow used chlorine in an attempt to clean the water and removed the handle; this ended the outbreak. This has been perceived as a major event in the history of public health and regarded as the founding event of the science of epidemiology, having helped shape public health policies around the world.[15][16] However, Snow's research and preventive measures to avoid further outbreaks were not fully accepted or put into practice until after his death due to the prevailing Miasma Theory of the time, a model of disease in which poor air quality was blamed for illness. This was used to rationalize high rates of infection in impoverished areas instead of addressing the underlying issues of poor nutrition and sanitation, and was proven false by his work.[17]
 Other pioneers include Danish physician Peter Anton Schleisner, who in 1849 related his work on the prevention of the epidemic of neonatal tetanus on the Vestmanna Islands in Iceland.[18][19] Another important pioneer was Hungarian physician Ignaz Semmelweis, who in 1847 brought down infant mortality at a Vienna hospital by instituting a disinfection procedure. His findings were published in 1850, but his work was ill-received by his colleagues, who discontinued the procedure. Disinfection did not become widely practiced until British surgeon Joseph Lister 'discovered' antiseptics in 1865 in light of the work of Louis Pasteur.[citation needed]
 In the early 20th century, mathematical methods were introduced into epidemiology by Ronald Ross, Janet Lane-Claypon, Anderson Gray McKendrick, and others.[20][21][22][23] In a parallel development during the 1920s, German-Swiss pathologist Max Askanazy and others founded the International Society for Geographical Pathology to systematically investigate the geographical pathology of cancer and other non-infectious diseases across populations in different regions. After World War II, Richard Doll and other non-pathologists joined the field and advanced methods to study cancer, a disease with patterns and mode of occurrences that could not be suitably studied with the methods developed for epidemics of infectious diseases. Geography pathology eventually combined with infectious disease epidemiology to make the field that is epidemiology today.[24]
 Another breakthrough was the 1954 publication of the results of a British Doctors Study, led by Richard Doll and Austin Bradford Hill, which lent very strong statistical support to the link between tobacco smoking and lung cancer.[citation needed]
 In the late 20th century, with the advancement of biomedical sciences, a number of molecular markers in blood, other biospecimens and environment were identified as predictors of development or risk of a certain disease. Epidemiology research to examine the relationship between these biomarkers analyzed at the molecular level and disease was broadly named ""molecular epidemiology"". Specifically, ""genetic epidemiology"" has been used for epidemiology of germline genetic variation and disease. Genetic variation is typically determined using DNA from peripheral blood leukocytes.[citation needed]
 Since the 2000s, genome-wide association studies (GWAS) have been commonly performed to identify genetic risk factors for many diseases and health conditions.[citation needed]
 While most molecular epidemiology studies are still using conventional disease diagnosis and classification systems, it is increasingly recognized that disease progression represents inherently heterogeneous processes differing from person to person. Conceptually, each individual has a unique disease process different from any other individual (""the unique disease principle""),[25][26] considering uniqueness of the exposome (a totality of endogenous and exogenous / environmental exposures) and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly cancer) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges, including lack of research guidelines and standardized statistical methodologies, and paucity of interdisciplinary experts and training programs.[27] Furthermore, the concept of disease heterogeneity appears to conflict with the long-standing premise in epidemiology that individuals with the same disease name have similar etiologies and disease processes. To resolve these issues and advance population health science in the era of molecular precision medicine, ""molecular pathology"" and ""epidemiology"" was integrated to create a new interdisciplinary field of ""molecular pathological epidemiology"" (MPE),[28][29] defined as ""epidemiology of molecular pathology and heterogeneity of disease"". In MPE, investigators analyze the relationships between (A) environmental, dietary, lifestyle and genetic factors; (B) alterations in cellular or extracellular molecules; and (C) evolution and progression of disease. A better understanding of heterogeneity of disease pathogenesis will further contribute to elucidate etiologies of disease. The MPE approach can be applied to not only neoplastic diseases but also non-neoplastic diseases.[30] The concept and paradigm of MPE have become widespread in the 2010s.[31][32][33][34][35][36][37][excessive citations]
 By 2012, it was recognized that many pathogens' evolution is rapid enough to be highly relevant to epidemiology, and that therefore much could be gained from an interdisciplinary approach to infectious disease integrating epidemiology and molecular evolution to ""inform control strategies, or even patient treatment.""[38][39] Modern epidemiological studies can use advanced statistics and machine learning to create predictive models as well as to define treatment effects.[40][41] There is increasing recognition that a wide range of modern data sources, many not originating from healthcare or epidemiology, can be used for epidemiological study.[42] Such digital epidemiology can include data from internet searching, mobile phone records and retail sales of drugs.[citation needed]
 Epidemiologists employ a range of study designs from the observational to experimental and generally categorized as descriptive (involving the assessment of data covering time, place, and person), analytic (aiming to further examine known associations or hypothesized relationships), and experimental (a term often equated with clinical or community trials of treatments and other interventions). In observational studies, nature is allowed to ""take its course"", as epidemiologists observe from the sidelines. Conversely, in experimental studies, the epidemiologist is the one in control of all of the factors entering a certain case study.[43] Epidemiological studies are aimed, where possible, at revealing unbiased relationships between exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity. The identification of causal relationships between these exposures and outcomes is an important aspect of epidemiology. Modern epidemiologists use informatics and infodemiology[44][45] as a tools.[citation needed][46][47][48]
 Observational studies have two components, descriptive and analytical. Descriptive observations pertain to the ""who, what, where and when of health-related state occurrence"". However, analytical observations deal more with the 'how' of a health-related event.[43] Experimental epidemiology contains three case types: randomized controlled trials (often used for a new medicine or drug testing), field trials (conducted on those at a high risk of contracting a disease), and community trials (research on social originating diseases).[43]
 The term 'epidemiologic triad' is used to describe the intersection of Host, Agent, and Environment in analyzing an outbreak.[citation needed]
 Case-series may refer to the qualitative study of the experience of a single patient, or small group of patients with a similar diagnosis, or to a statistical factor with the potential to produce illness with periods when they are unexposed.[49]
 The former type of study is purely descriptive and cannot be used to make inferences about the general population of patients with that disease. These types of studies, in which an astute clinician identifies an unusual feature of a disease or a patient's history, may lead to a formulation of a new hypothesis. Using the data from the series, analytic studies could be done to investigate possible causal factors. These can include case-control studies or prospective studies. A case-control study would involve matching comparable controls without the disease to the cases in the series. A prospective study would involve following the case series over time to evaluate the disease's natural history.[50]
 The latter type, more formally described as self-controlled case-series studies, divide individual patient follow-up time into exposed and unexposed periods and use fixed-effects Poisson regression processes to compare the incidence rate of a given outcome between exposed and unexposed periods. This technique has been extensively used in the study of adverse reactions to vaccination and has been shown in some circumstances to provide statistical power comparable to that available in cohort studies.[citation needed]
 Case-control studies select subjects based on their disease status. It is a retrospective study. A group of individuals that are disease positive (the ""case"" group) is compared with a group of disease negative individuals (the ""control"" group). The control group should ideally come from the same population that gave rise to the cases. The case-control study looks back through time at potential exposures that both groups (cases and controls) may have encountered. A 2×2 table is constructed, displaying exposed cases (A), exposed controls (B), unexposed cases (C) and unexposed controls (D). The statistic generated to measure association is the odds ratio (OR), which is the ratio of the odds of exposure in the cases (A/C) to the odds of exposure in the controls (B/D), i.e. OR = (AD/BC).[citation needed]
 If the OR is significantly greater than 1, then the conclusion is ""those with the disease are more likely to have been exposed,"" whereas if it is close to 1 then the exposure and disease are not likely associated. If the OR is far less than one, then this suggests that the exposure is a protective factor in the causation of the disease.
Case-control studies are usually faster and more cost-effective than cohort studies but are sensitive to bias (such as recall bias and selection bias). The main challenge is to identify the appropriate control group; the distribution of exposure among the control group should be representative of the distribution in the population that gave rise to the cases. This can be achieved by drawing a random sample from the original population at risk. This has as a consequence that the control group can contain people with the disease under study when the disease has a high attack rate in a population.[citation needed]
 A major drawback for case control studies is that, in order to be considered to be statistically significant, the minimum number of cases required at the 95% confidence interval is related to the odds ratio by the equation:
 where N is the ratio of cases to controls.
As the odds ratio approaches 1, the number of cases required for statistical significance grows towards infinity; rendering case-control studies all but useless for low odds ratios. For instance, for an odds ratio of 1.5 and cases = controls, the table shown above would look like this:
 For an odds ratio of 1.1:
 Cohort studies select subjects based on their exposure status. The study subjects should be at risk of the outcome under investigation at the beginning of the cohort study; this usually means that they should be disease free when the cohort study starts. The cohort is followed through time to assess their later outcome status. An example of a cohort study would be the investigation of a cohort of smokers and non-smokers over time to estimate the incidence of lung cancer. The same 2×2 table is constructed as with the case control study. However, the point estimate generated is the relative risk (RR), which is the probability of disease for a person in the exposed group, Pe = A / (A + B) over the probability of disease for a person in the unexposed group, Pu = C / (C + D), i.e. RR = Pe / Pu.
 As with the OR, a RR greater than 1 shows association, where the conclusion can be read ""those with the exposure were more likely to develop the disease.""
 Prospective studies have many benefits over case control studies. The RR is a more powerful effect measure than the OR, as the OR is just an estimation of the RR, since true incidence cannot be calculated in a case control study where subjects are selected based on disease status. Temporality can be established in a prospective study, and confounders are more easily controlled for. However, they are more costly, and there is a greater chance of losing subjects to follow-up based on the long time period over which the cohort is followed.
 Cohort studies also are limited by the same equation for number of cases as for cohort studies, but, if the base incidence rate in the study population is very low, the number of cases required is reduced by 1⁄2.
 Although epidemiology is sometimes viewed as a collection of statistical tools used to elucidate the associations of exposures to health outcomes, a deeper understanding of this science is that of discovering causal relationships.
 ""Correlation does not imply causation"" is a common theme for much of the epidemiological literature. For epidemiologists, the key is in the term inference. Correlation, or at least association between two variables, is a necessary but not sufficient criterion for the inference that one variable causes the other. Epidemiologists use gathered data and a broad range of biomedical and psychosocial theories in an iterative way to generate or expand theory, to test hypotheses, and to make educated, informed assertions about which relationships are causal, and about exactly how they are causal.
 Epidemiologists emphasize that the ""one cause – one effect"" understanding is a simplistic mis-belief.[51] Most outcomes, whether disease or death, are caused by a chain or web consisting of many component causes.[52] Causes can be distinguished as necessary, sufficient or probabilistic conditions. If a necessary condition can be identified and controlled (e.g., antibodies to a disease agent, energy in an injury), the harmful outcome can be avoided (Robertson, 2015). One tool regularly used to conceptualize the multicausality associated with disease is the causal pie model.[53]
 In 1965, Austin Bradford Hill proposed a series of considerations to help assess evidence of causation,[54] which have come to be commonly known as the ""Bradford Hill criteria"". In contrast to the explicit intentions of their author, Hill's considerations are now sometimes taught as a checklist to be implemented for assessing causality.[55] Hill himself said ""None of my nine viewpoints can bring indisputable evidence for or against the cause-and-effect hypothesis and none can be required sine qua non.""[54]
 Epidemiological studies can only go to prove that an agent could have caused, but not that it did cause, an effect in any particular case:
 Epidemiology is concerned with the incidence of disease in populations and does not address the question of the cause of an individual's disease. This question, sometimes referred to as specific causation, is beyond the domain of the science of epidemiology. Epidemiology has its limits at the point where an inference is made that the relationship between an agent and a disease is causal (general causation) and where the magnitude of excess risk attributed to the agent has been determined; that is, epidemiology addresses whether an agent can cause disease, not whether an agent did cause a specific plaintiff's disease.[56] In United States law, epidemiology alone cannot prove that a causal association does not exist in general. Conversely, it can be (and is in some circumstances) taken by US courts, in an individual case, to justify an inference that a causal association does exist, based upon a balance of probability.
 The subdiscipline of forensic epidemiology is directed at the investigation of specific causation of disease or injury in individuals or groups of individuals in instances in which causation is disputed or is unclear, for presentation in legal settings.
 Epidemiological practice and the results of epidemiological analysis make a significant contribution to emerging population-based health management frameworks.
 Population-based health management encompasses the ability to:
 Modern population-based health management is complex, requiring a multiple set of skills (medical, political, technological, mathematical, etc.) of which epidemiological practice and analysis is a core component, that is unified with management science to provide efficient and effective health care and health guidance to a population. This task requires the forward-looking ability of modern risk management approaches that transform health risk factors, incidence, prevalence and mortality statistics (derived from epidemiological analysis) into management metrics that not only guide how a health system responds to current population health issues but also how a health system can be managed to better respond to future potential population health issues.[57]
 Examples of organizations that use population-based health management that leverage the work and results of epidemiological practice include Canadian Strategy for Cancer Control, Health Canada Tobacco Control Programs, Rick Hansen Foundation, Canadian Tobacco Control Research Initiative.[58][59][60]
 Each of these organizations uses a population-based health management framework called Life at Risk that combines epidemiological quantitative analysis with demographics, health agency operational research and economics to perform:
 Applied epidemiology is the practice of using epidemiological methods to protect or improve the health of a population. Applied field epidemiology can include investigating communicable and non-communicable disease outbreaks, mortality and morbidity rates, and nutritional status, among other indicators of health, with the purpose of communicating the results to those who can implement appropriate policies or disease control measures.
 As the surveillance and reporting of diseases and other health factors become increasingly difficult in humanitarian crisis situations, the methodologies used to report the data are compromised. One study found that less than half (42.4%) of nutrition surveys sampled from humanitarian contexts correctly calculated the prevalence of malnutrition and only one-third (35.3%) of the surveys met the criteria for quality. Among the mortality surveys, only 3.2% met the criteria for quality. As nutritional status and mortality rates help indicate the severity of a crisis, the tracking and reporting of these health factors is crucial.
 Vital registries are usually the most effective ways to collect data, but in humanitarian contexts these registries can be non-existent, unreliable, or inaccessible. As such, mortality is often inaccurately measured using either prospective demographic surveillance or retrospective mortality surveys. Prospective demographic surveillance requires much manpower and is difficult to implement in a spread-out population. Retrospective mortality surveys are prone to selection and reporting biases. Other methods are being developed, but are not common practice yet.[61][62][63][64]
 The concept of waves in epidemics has implications especially for communicable diseases. A working definition for the term ""epidemic wave"" is based on two key features: 1) it comprises periods of upward or downward trends, and 2) these increases or decreases must be substantial and sustained over a period of time, in order to distinguish them from minor fluctuations or reporting errors.[65] The use of a consistent scientific definition is to provide a consistent language that can be used to communicate about and understand the progression of the COVID-19 pandemic, which would aid healthcare organizations and policymakers in resource planning and allocation.
 Different fields in epidemiology have different levels of validity. One way to assess the validity of findings is the ratio of false-positives (claimed effects that are not correct) to false-negatives (studies which fail to support a true effect). In genetic epidemiology, candidate-gene studies may produce over 100 false-positive findings for each false-negative. By contrast genome-wide association appear close to the reverse, with only one false positive for every 100 or more false-negatives.[66] This ratio has improved over time in genetic epidemiology, as the field has adopted stringent criteria. By contrast, other epidemiological fields have not required such rigorous reporting and are much less reliable as a result.[66]
 Random error is the result of fluctuations around a true value because of sampling variability. Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random errors include poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error. There is a random error in all sampling procedures – sampling error.[citation needed]
 Precision in epidemiological variables is a measure of random error. Precision is also inversely related to random error, so that to reduce random error is to increase precision. Confidence intervals are computed to demonstrate the precision of relative risk estimates. The narrower the confidence interval, the more precise the relative risk estimate.
 There are two basic ways to reduce random error in an epidemiological study. The first is to increase the sample size of the study. In other words, add more subjects to your study. The second is to reduce the variability in measurement in the study. This might be accomplished by using a more precise measuring device or by increasing the number of measurements.
 Note, that if sample size or number of measurements are increased, or a more precise measuring tool is purchased, the costs of the study are usually increased. There is usually an uneasy balance between the need for adequate precision and the practical issue of study cost.
 A systematic error or bias occurs when there is a difference between the true value (in the population) and the observed value (in the study) from any cause other than sampling variability. An example of systematic error is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).
 A mistake in coding that affects all responses for that particular question is another example of a systematic error.
 The validity of a study is dependent on the degree of systematic error. Validity is usually separated into two components:
 Selection bias occurs when study subjects are selected or become part of the study as a result of a third, unmeasured variable which is associated with both the exposure and outcome of interest.[67] For instance, it has repeatedly been noted that cigarette smokers and non smokers tend to differ in their study participation rates. (Sackett D cites the example of Seltzer et al., in which 85% of non smokers and 67% of smokers returned mailed questionnaires.)[68] Such a difference in response will not lead to bias if it is not also associated with a systematic difference in outcome between the two response groups.
 Information bias is bias arising from systematic error in the assessment of a variable.[69] An example of this is recall bias. A typical example is again provided by Sackett in his discussion of a study examining the effect of specific exposures on fetal health: ""in questioning mothers whose recent pregnancies had ended in fetal death or malformation (cases) and a matched group of mothers whose pregnancies ended normally (controls) it was found that 28% of the former, but only 20% of the latter, reported exposure to drugs which could not be substantiated either in earlier prospective interviews or in other health records"".[68] In this example, recall bias probably occurred as a result of women who had had miscarriages having an apparent tendency to better recall and therefore report previous exposures.
 Confounding has traditionally been defined as bias arising from the co-occurrence or mixing of effects of extraneous factors, referred to as confounders, with the main effect(s) of interest.[69][70] A more recent definition of confounding invokes the notion of counterfactual effects.[70] According to this view, when one observes an outcome of interest, say Y=1 (as opposed to Y=0), in a given population A which is entirely exposed (i.e. exposure X = 1 for every unit of the population) the risk of this event will be RA1. The counterfactual or unobserved risk RA0 corresponds to the risk which would have been observed if these same individuals had been unexposed (i.e. X = 0 for every unit of the population). The true effect of exposure therefore is: RA1 − RA0 (if one is interested in risk differences) or RA1/RA0 (if one is interested in relative risk). Since the counterfactual risk RA0 is unobservable we approximate it using a second population B and we actually measure the following relations: RA1 − RB0 or RA1/RB0. In this situation, confounding occurs when RA0 ≠ RB0.[70] (NB: Example assumes binary outcome and exposure variables.)
 Some epidemiologists prefer to think of confounding separately from common categorizations of bias since, unlike selection and information bias, confounding stems from real causal effects.[67]
 Few universities have offered epidemiology as a course of study at the undergraduate level. One notable undergraduate program exists at Johns Hopkins University, where students who major in public health can take graduate-level courses, including epidemiology, during their senior year at the Bloomberg School of Public Health.[71]
 Although epidemiologic research is conducted by individuals from diverse disciplines, including clinically trained professionals such as physicians, formal training is available through Masters or Doctoral programs including Master of Public Health (MPH), Master of Science of Epidemiology (MSc.), Doctor of Public Health (DrPH), Doctor of Pharmacy (PharmD), Doctor of Philosophy (PhD), Doctor of Science (ScD).  Many other graduate programs, e.g., Doctor of Social Work (DSW), Doctor of Clinical Practice (DClinP), Doctor of Podiatric Medicine (DPM), Doctor of Veterinary Medicine (DVM), Doctor of Nursing Practice (DNP), Doctor of Physical Therapy (DPT), or for clinically trained physicians, Doctor of Medicine (MD) or Bachelor of Medicine and Surgery (MBBS or MBChB) and Doctor of Osteopathic Medicine (DO), include some training in epidemiologic research or related topics, but this training is generally substantially less than offered in training programs focused on epidemiology or public health. Reflecting the strong historical tie between epidemiology and medicine, formal training programs may be set in either schools of public health or medical schools.
 As public health/health protection practitioners, epidemiologists work in a number of different settings. Some epidemiologists work 'in the field'; i.e., in the community, commonly in a public health/health protection service, and are often at the forefront of investigating and combating disease outbreaks. Others work for non-profit organizations, universities, hospitals and larger government entities such as state and local health departments, various Ministries of Health, Doctors without Borders, the Centers for Disease Control and Prevention (CDC), the Health Protection Agency, the World Health Organization (WHO), or the Public Health Agency of Canada. Epidemiologists can also work in for-profit organizations such as pharmaceutical and medical device companies in groups such as market research or clinical development.
 An April 2020 University of Southern California article noted that ""The coronavirus epidemic... thrust epidemiology – the study of the incidence, distribution and control of disease in a population – to the forefront of scientific disciplines across the globe and even made temporary celebrities out of some of its practitioners.""[72]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['public health', 'Host, Agent, and Environment', 'strong historical tie between epidemiology and medicine', 'high blood pressure, mental illness and obesity', 'discovering causal relationships'], 'answer_start': [], 'answer_end': []}"
"
 Public health is ""the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals"".[1][2] Analyzing the determinants of health of a population and the threats it faces is the basis for public health.[3] The public can be as small as a handful of people or as large as a village or an entire city; in the case of a pandemic it may encompass several continents. The concept of health takes into account physical, psychological, and social well-being.[1][4]
 Public health is an interdisciplinary field. For example, epidemiology, biostatistics, social sciences and management of health services are all relevant. Other important sub-fields include environmental health, community health, behavioral health, health economics, public policy, mental health, health education, health politics, occupational safety, disability, oral health, gender issues in health, and sexual and reproductive health.[5] Public health, together with primary care, secondary care, and tertiary care, is part of a country's overall healthcare system. Public health is implemented through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promotion of hand-washing and breastfeeding, delivery of vaccinations, promoting ventilation and improved air quality both indoors and outdoors, suicide prevention, smoking cessation, obesity education, increasing healthcare accessibility and distribution of condoms to control the spread of sexually transmitted diseases.
 There is a significant disparity in access to health care and public health initiatives between developed countries and developing countries, as well as within developing countries. In developing countries, public health infrastructures are still forming. There may not be enough trained healthcare workers, monetary resources, or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention.[6][7] A major public health concern in developing countries is poor maternal and child health, exacerbated by malnutrition and poverty coupled with governments' reluctance in implementing public health policies.
 From the beginnings of human civilization, communities promoted health and fought disease at the population level.[8][9] In complex, pre-industrialized societies, interventions designed to reduce health risks could be the initiative of different stakeholders, such as army generals, the clergy or rulers. Great Britain became a leader in the development of public health initiatives, beginning in the 19th century, due to the fact that it was the first modern urban nation worldwide.[10] The public health initiatives that began to emerge initially focused on sanitation (for example, the Liverpool and London sewerage systems), control of infectious diseases (including vaccination and quarantine) and an evolving infrastructure of various sciences, e.g. statistics, microbiology, epidemiology, sciences of engineering.[10]
 Public health has been defined as ""the science and art of preventing disease"", prolonging life and improving quality of life through organized efforts and informed choices of society, organizations (public and private), communities and individuals.[2]  The public can be as small as a handful of people or as large as a village or an entire city. The concept of health takes into account physical, psychological, and social well-being. As such, according to the World Health Organization, ""health is a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity"".[4]
 Public health is related to global health which is the health of populations in the worldwide context.[11] It has been defined as ""the area of study, research and practice that places a priority on improving health and achieving equity in ""Health for all"" people worldwide"".[12] International health is a field of health care, usually with a public health emphasis, dealing with health across regional or national boundaries.[citation needed] Public health is not the same as public healthcare (publicly funded health care).
 The term preventive medicine is related to public health. The American Board of Preventive Medicine separates three categories of preventive medicine: aerospace health, occupational health, and public health and general preventative medicine. Jung, Boris and Lushniak argue that preventive medicine should be considered the medical specialty for public health but note that the American College of Preventive Medicine and American Board of Preventive Medicine do not prominently use the term ""public health"".[13]: 1  Preventive medicine specialists are trained as clinicians and address complex health needs of a population such as by assessing the need for disease prevention programs, using the best methods to implement them, and assessing their effectiveness.[13]: 1, 3 
 Since the 1990s many scholars in public health have been using the term population health.[14]: 3  There are no medical specialties directly related to population health.[13]: 4  Valles argues that consideration of health equity is a fundamental part of population health. Scholars such as Coggon and Pielke express concerns about bringing general issues of wealth distribution into population health. Pielke worries about ""stealth issue advocacy"" in population health.[14]: 163  Jung, Boris and Lushniak consider population health to be a concept that is the goal of an activity called public health practiced through the specialty preventive medicine.[13]: 4 
 Lifestyle medicine uses individual lifestyle modification to prevent or revert disease and can be considered a component of preventive medicine and public health. It is implemented as part of primary care rather than a specialty in its own right.[13]: 3  Valles argues that the term social medicine has a narrower and more biomedical focus than the term population health.[14]: 7 
 The purpose of a public health intervention is to prevent and mitigate diseases, injuries and other health conditions. The overall goal is to improve the health of populations and increase life expectancy.[citation needed]
 Public health is a complex term, composed of many elements and different practices. It is a multi-faceted, interdisciplinary field.[10] For example, epidemiology, biostatistics, social sciences and management of health services are all relevant. Other important sub-fields include environmental health, community health, behavioral health, health economics, public policy, mental health, health education, health politics, occupational safety, disability, gender issues in health, and sexual and reproductive health.[5]
 Modern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, physician assistants, public health nurses, midwives, medical microbiologists, pharmacists, economists, sociologists, geneticists, data managers, environmental health officers (public health inspectors), bioethicists, gender experts, sexual and reproductive health specialists, physicians, and veterinarians.[15]
 The elements and priorities of public health have evolved over time, and are continuing to evolve.[10] Different regions in the world can have different public health concerns at a given time.[citation needed]
 Common public health initiatives include promotion of hand-washing and breastfeeding, delivery of vaccinations, suicide prevention, smoking cessation, obesity education, increasing healthcare accessibility and distribution of condoms to control the spread of sexually transmitted diseases.[citation needed]
 Public health aims are achieved through surveillance of cases and the promotion of healthy behaviors, communities and environments. Analyzing the determinants of health of a population and the threats it faces is the basis for public health.[3]
 Many diseases are preventable through simple, nonmedical methods. For example, research has shown that the simple act of handwashing with soap can prevent the spread of many contagious diseases.[16] In other cases, treating a disease or controlling a pathogen can be vital to preventing its spread to others, either during an outbreak of infectious disease or through contamination of food or water supplies. Public health communications programs, vaccination programs and distribution of condoms are examples of common preventive public health measures.[citation needed]
 Public health, together with primary care, secondary care, and tertiary care, is part of a country's overall health care system. Many interventions of public health interest are delivered outside of health facilities, such as food safety surveillance, distribution of condoms and needle-exchange programs for the prevention of transmissible diseases.Public health plays an important role in disease prevention efforts in both the developing world and in developed countries through local health systems and non-governmental organizations.[citation needed]
 Public health requires Geographic Information Systems (GIS) because risk, vulnerability and exposure involve geographic aspects.[17]
 A dilemma in public health ethics is dealing with the conflict between individual rights and maximizing right to health.[18]: 28  Public health is justified by consequentialist utilitarian ideas,[18]: 153  but is constrained and critiqued by liberal,[18] deontological, principlist and libertarian philosophies[18]: 99, 95, 74, 123  Stephen Holland argues that it can be easy to find a particular framework to justify any viewpoint on public health issues, but that the correct approach is to find a framework that best describes a situation and see what it implies about public health policy.[18]: 154 
 The definition of health is vague and there are many conceptualizations. Public health practitioners definition of health can different markedly from members of the public or clinicians. This can mean that members of the public view the values behind public health interventions as alien which can cause resentment amongst the public towards certain interventions.[18]: 230  Such vagueness can be a problem for health promotion.[18]: 241  Critics have argued that public health tends to place more focus on individual factors associated with health at the expense of factors operating at the population level.[14]: 9 
 Historically, public health campaigns have been criticized as a form of ""healthism"",  as moralistic in nature rather than being focused on health. Medical doctors, Petr Shkrabanek and James McCormick wrote a series of publications on this topic in the late 1980s and early 1990s criticizing the UK's the Health of The Nation campaign. These publications exposed abuse of epidemiology and statistics by the public health movement to support lifestyle interventions and screening programs.[19]: 85 [20] A combination of inculcating a fear of ill-health and a strong notion of individual responsibility has been criticized as a form of ""health fascism"" by a number of scholars, objectifying the individual with no considerations of emotional or social factors.[21]: 8 [20]: 7 [22]: 81 
 When public health initiatives began to emerge in England in modern times (18th century onwards) there were three core strands of public health which were all related to statecraft: Supply of clean water and sanitation (for example London sewerage system); control of infectious diseases (including vaccination and quarantine); an evolving infrastructure of various sciences, e.g. statistics, microbiology, epidemiology, sciences of engineering.[10] Great Britain was a leader in the development of public health during that time period out of necessity: Great Britain was the first modern urban nation (by 1851 more than half of the population lived in settlements of more than 2000 people).[10] This led to a certain type of distress which then led to public health initiatives.[10] Later that particular concern faded away.
 With the onset of the epidemiological transition and as the prevalence of infectious diseases decreased through the 20th century, public health began to put more focus on chronic diseases such as cancer and heart disease. Previous efforts in many developed countries had already led to dramatic reductions in the infant mortality rate using preventive methods. In Britain, the infant mortality rate fell from over 15% in 1870 to 7% by 1930.[23]
 A major public health concern in developing countries is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year.[24]
 Public health surveillance has led to the identification and prioritization of many public health issues facing the world today, including HIV/AIDS, diabetes, waterborne diseases, zoonotic diseases, and antibiotic resistance leading to the reemergence of infectious diseases such as tuberculosis. Antibiotic resistance, also known as drug resistance, was the theme of World Health Day 2011.
 For example, the WHO reports that at least 220 million people worldwide have diabetes. Its incidence is increasing rapidly, and it is projected that the number of diabetes deaths will double by 2030.[25] In a June 2010 editorial in the medical journal The Lancet, the authors opined that ""The fact that type 2 diabetes, a largely preventable disorder, has reached epidemic proportion is a public health humiliation.""[26] The risk of type 2 diabetes is closely linked with the growing problem of obesity. The WHO's latest estimates as of June 2016[update] highlighted that globally approximately 1.9 billion adults were overweight in 2014, and 41 million children under the age of five were overweight in 2014.[27] Once considered a problem in high-income countries, it is now on the rise in low-income countries, especially in urban settings.[citation needed]
 Many public health programs are increasingly dedicating attention and resources to the issue of obesity, with objectives to address the underlying causes including healthy diet and physical exercise. The National Institute for Health and Care Research (NIHR) has published a review of research on what local authorities can do to tackle obesity.[28] The review covers interventions in the food environment (what people buy and eat), the built and natural environments, schools, and the community, as well as those focussing on active travel, leisure services and public sports, weight management programmes, and system-wide approaches.[citation needed]
 Health inequalities, driven by the social determinants of health, are also a growing area of concern in public health.  A central challenge to securing health equity is that the same social structures that contribute to health inequities also operate and are reproduced by public health organizations.[29] In other words, public health organizations have evolved to better meet the needs of some groups more than others. The result is often that those most in need of preventative interventions are least likely to receive them[30] and interventions can actually aggravate inequities[31] as they are often inadvertently tailored to the needs of the normative group.[32] Identifying bias within public health research and practice is essential to ensuring public health efforts mitigate and don't aggravate health inequities.
 The World Health Organization (WHO) is a specialized agency of the United Nations responsible for international public health.[33] The WHO Constitution, which establishes the agency's governing structure and principles, states its main objective as ""the attainment by all peoples of the highest possible level of health"".[34] The WHO's broad mandate includes advocating for universal healthcare, monitoring public health risks, coordinating responses to health emergencies, and promoting human health and well-being.[35] The WHO has played a leading role in several public health achievements, most notably the eradication of smallpox, the near-eradication of polio, and the development of an Ebola vaccine. Its current priorities include communicable diseases, particularly HIV/AIDS, Ebola, COVID-19, malaria and tuberculosis; non-communicable diseases such as heart disease and cancer; healthy diet, nutrition, and food security; occupational health; and substance abuse.[citation needed]
 Most countries have their own governmental public health agency, often called the ministry of health, with responsibility for domestic health issues.
 For example, in the United States, state and local health departments are on the front line of public health initiatives. In addition to their national duties, the United States Public Health Service (PHS), led by the Surgeon General of the United States Public Health Service, and the Centers for Disease Control and Prevention, headquartered in Atlanta, are also involved with international health activities.[36]
 Most governments recognize the importance of public health programs in reducing the incidence of disease, disability, and the effects of aging and other physical and mental health conditions. However, public health generally receives significantly less government funding compared with medicine.[37] Although the collaboration of local health and government agencies is considered best practice to improve public health, the pieces of evidence available to support this is limited.[38] Public health programs providing vaccinations have made major progress in promoting health, including substantially reducing the occurrence of cholera and polio and eradicating smallpox, diseases that have plagued humanity for thousands of years.[39]
 The World Health Organization (WHO) identifies core functions of public health programs including:[40]
 In particular, public health surveillance programs can:[41]
 Many health problems are due to maladaptive personal behaviors. From an evolutionary psychology perspective, over consumption of novel substances that are harmful is due to the activation of an evolved reward system for substances such as drugs, tobacco, alcohol, refined salt, fat, and carbohydrates. New technologies such as modern transportation also cause reduced physical activity. Research has found that behavior is more effectively changed by taking evolutionary motivations into consideration instead of only presenting information about health effects. The marketing industry has long known the importance of associating products with high status and attractiveness to others. Films are increasingly being recognized as a public health tool.[citation needed] In fact, film festivals and competitions have been established to specifically promote films about health.[42] Conversely, it has been argued that emphasizing the harmful and undesirable effects of tobacco smoking on other persons and imposing smoking bans in public places have been particularly effective in reducing tobacco smoking.[43]
 As well as seeking to improve population health through the implementation of specific population-level interventions, public health contributes to medical care by identifying and assessing population needs for health care services, including:[44][45][46][47]
 Some programs and policies associated with public health promotion and prevention can be controversial. One such example is programs focusing on the prevention of HIV transmission through safe sex campaigns and needle-exchange programs. Another is the control of tobacco smoking. Many nations have implemented major initiatives to cut smoking, such as increased taxation and bans on smoking in some or all public places. Supporters argue by presenting evidence that smoking is one of the major killers, and that therefore governments have a duty to reduce the death rate, both through limiting passive (second-hand) smoking and by providing fewer opportunities for people to smoke. Opponents say that this undermines individual freedom and personal responsibility, and worry that the state may be encouraged to remove more and more choice in the name of better population health overall.[citation needed]
 Psychological research confirms this tension between concerns about public health and concerns about personal liberty: (i) the best predictor of complying with public health recommendations such as hand-washing, mask-wearing, and staying at home (except for essential activity) during the COVID-19 pandemic was people's perceived duties to prevent harm but (ii) the best predictor of flouting such public health recommendations was valuing liberty more than equality.[48]
 Simultaneously, while communicable diseases have historically ranged uppermost as a global health priority, non-communicable diseases and the underlying behavior-related risk factors have been at the bottom. This is changing, however, as illustrated by the United Nations hosting its first General Assembly Special Summit on the issue of non-communicable diseases in September 2011.[49]
 There is a significant disparity in access to health care and public health initiatives between developed countries and developing countries, as well as within developing countries. In developing countries, public health infrastructures are still forming. There may not be enough trained health workers, monetary resources or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention.[6][7] As a result, a large majority of disease and mortality in developing countries results from and contributes to extreme poverty. For example, many African governments spend less than $100 USD per person per year on health care, while, in the United States, the federal government spent approximately $10,600 USD per capita in 2019.[50] However, expenditures on health care should not be confused with spending on public health. Public health measures may not generally be considered ""health care"" in the strictest sense. For example, mandating the use of seat belts in cars can save countless lives and contribute to the health of a population, but typically money spent enforcing this rule would not count as money spent on health care.
 Large parts of the world remained plagued by largely preventable or treatable infectious diseases. In addition to this however, many developing countries are also experiencing an epidemiological shift and polarization in which populations are now experiencing more of the effects of chronic diseases as life expectancy increases, the poorer communities being heavily affected by both chronic and infectious diseases.[7] Another major public health concern in the developing world is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year.[24] Intermittent preventive therapy aimed at treating and preventing malaria episodes among pregnant women and young children is one public health measure in endemic countries.
 Since the 1980s, the growing field of population health has broadened the focus of public health from individual behaviors and risk factors to population-level issues such as inequality, poverty, and education. Modern public health is often concerned with addressing determinants of health across a population. There is a recognition that health is affected by many factors including class, race, income, educational status, region of residence, and social relationships; these are known as ""social determinants of health"". The upstream drivers such as environment, education, employment, income, food security, housing, social inclusion and many others effect the distribution of health between and within populations and are often shaped by policy.[53] A social gradient in health runs through society. The poorest generally have the worst health, but even the middle classes will generally have worse health outcomes than those of a higher social level.[54] The new public health advocates for population-based policies that improve health in an equitable manner.
 The health sector is one of Europe's most labor-intensive industries. In late 2020, it accounted for more than 21 million employment in the European Union when combined with social work.[55] According to the WHO, several countries began the COVID-19 pandemic with insufficient health and care professionals, inappropriate skill mixtures, and unequal geographical distributions. These issues were worsened by the pandemic, reiterating the importance of public health.[56] In the United States, a history of underinvestment in public health undermined the public health workforce and support for population health, long before the pandemic added to stress, mental distress, job dissatisfaction, and accelerated departures among public health workers.[57]
 Health aid to developing countries is an important source of public health funding for many developing countries.[59] Health aid to developing countries has shown a significant increase after World War II as concerns over the spread of disease as a result of globalization increased and the HIV/AIDS epidemic in sub-Saharan Africa surfaced.[60][61] From 1990 to 2010, total health aid from developed countries increased from 5.5 billion to 26.87 billion with wealthy countries continuously donating billions of dollars every year with the goal of improving population health.[61] Some efforts, however, receive a significantly larger proportion of funds such as HIV which received an increase in funds of over $6 billion between 2000 and 2010 which was more than twice the increase seen in any other sector during those years.[59] Health aid has seen an expansion through multiple channels including private philanthropy, non-governmental organizations, private foundations such as the Rockefeller Foundation or the Bill & Melinda Gates Foundation, bilateral donors, and multilateral donors such as the World Bank or UNICEF.[61] The result has been a sharp rise in uncoordinated and fragmented funding of an ever-increasing number of initiatives and projects. To promote better strategic cooperation and coordination between partners, particularly among bilateral development agencies and funding organizations, the Swedish International Development Cooperation Agency (Sida) spearheaded the establishment of ESSENCE,[62] an initiative to facilitate dialogue between donors/funders, allowing them to identify synergies. ESSENCE brings together a wide range of funding agencies to coordinate funding efforts.
 In 2009 health aid from the OECD amounted to $12.47 billion which amounted to 11.4% of its total bilateral aid.[63] In 2009, Multilateral donors were found to spend 15.3% of their total aid on bettering public healthcare.[63]
 Debates exist questioning the efficacy of international health aid. Supporters of aid claim that health aid from wealthy countries is necessary in order for developing countries to escape the poverty trap. Opponents of health aid claim that international health aid actually disrupts developing countries' course of development, causes dependence on aid, and in many cases the aid fails to reach its recipients.[59] For example, recently, health aid was funneled towards initiatives such as financing new technologies like antiretroviral medication, insecticide-treated mosquito nets, and new vaccines. The positive impacts of these initiatives can be seen in the eradication of smallpox and polio; however, critics claim that misuse or misplacement of funds may cause many of these efforts to never come into achievement.[59]
 Economic modeling based on the Institute for Health Metrics and Evaluation and the World Health Organization has shown a link between international health aid in developing countries and a reduction in adult mortality rates.[61] However, a 2014–2016 study suggests that a potential confounding variable for this outcome is the possibility that aid was directed at countries once they were already on track for improvement.[59] That same study, however, also suggests that 1 billion dollars in health aid was associated with 364,000 fewer deaths occurring between ages 0 and 5 in 2011.[59]
 To address current and future challenges in addressing health issues in the world, the United Nations have developed the Sustainable Development Goals to be completed by 2030.[64] These goals in their entirety encompass the entire spectrum of development across nations, however Goals 1–6 directly address health disparities, primarily in developing countries.[65] These six goals address key issues in global public health, poverty, hunger and food security, health, education, gender equality and women's empowerment, and water and sanitation.[65] Public health officials can use these goals to set their own agenda and plan for smaller scale initiatives for their organizations. These goals are designed to lessen the burden of disease and inequality faced by developing countries and lead to a healthier future. The links between the various sustainable development goals and public health are numerous and well established.[66][67]
 From the beginnings of human civilization, communities promoted health and fought disease at the population level.[8][9] Definitions of health as well as methods to pursue it differed according to the medical, religious and natural-philosophical ideas groups held, the resources they had, and the changing circumstances in which they lived. Yet few early societies displayed the hygienic stagnation or even apathy often attributed to them.[68][69][70] The latter reputation is mainly based on the absence of present-day bioindicators, especially immunological and statistical tools developed in light of the germ theory of disease transmission.[citation needed]
 Public health was born neither in Europe nor as a response to the Industrial Revolution. Preventive health interventions are attested almost anywhere historical communities have left their mark. In Southeast Asia, for instance, Ayurvedic medicine and subsequently Buddhism fostered occupational, dietary and sexual regimens that promised balanced bodies, lives and communities, a notion strongly present in Traditional Chinese Medicine as well.[71][72] Among the Mayans, Aztecs and other early civilizations in the Americas, population centers pursued hygienic programs, including by holding medicinal herbal markets.[73] And among Aboriginal Australians, techniques for preserving and protecting water and food sources, micro-zoning to reduce pollution and fire risks, and screens to protect people against flies were common, even in temporary camps.[74][75]
 Western European, Byzantine and Islamicate civilizations, which generally adopted a Hippocratic, Galenic or humoral medical system, fostered preventive programs as well.[76][77][78][79] These were developed on the basis of evaluating the quality of local climates, including topography, wind conditions and exposure to the sun, and the properties and availability of water and food, for both humans and nonhuman animals. Diverse authors of medical, architectural, engineering and military manuals explained how to apply such theories to groups of different origins and under different circumstances.[80][81][82] This was crucial, since under Galenism bodily constitutions were thought to be heavily shaped by their material environments, so their balance required specific regimens as they traveled during different seasons and between climate zones.[83][84][85]
 In complex, pre-industrialized societies, interventions designed to reduce health risks could be the initiative of different stakeholders. For instance, in Greek and Roman antiquity, army generals learned to provide for soldiers' wellbeing, including off the battlefield, where most combatants died prior to the twentieth century.[86][87] In Christian monasteries across the Eastern Mediterranean and western Europe since at least the fifth century CE, monks and nuns pursued strict but balanced regimens, including nutritious diets, developed explicitly to extend their lives.[88] And royal, princely and papal courts, which were often mobile as well, likewise adapted their behavior to suit environmental conditions in the sites they occupied. They could also choose sites they considered salubrious for their members and sometimes had them modified.[89]
 In cities, residents and rulers developed measures to benefit the general population, which faced a broad array of recognized health risks. These provide some of the most sustained evidence for preventive measures in earlier civilizations. In numerous sites the upkeep of infrastructures, including roads, canals and marketplaces, as well as zoning policies, were introduced explicitly to preserve residents' health.[90] Officials such as the muhtasib in the Middle East and the Road master in Italy, fought the combined threats of pollution through sin, ocular intromission and miasma.[91][92][93][94] Craft guilds were important agents of waste disposal and promoted harm reduction through honesty and labor safety among their members. Medical practitioners, including public physicians,[95] collaborated with urban governments in predicting and preparing for calamities and identifying and isolating people perceived as lepers, a disease with strong moral connotations.[96][97] Neighborhoods were also active in safeguarding local people's health, by monitoring at-risk sites near them and taking appropriate social and legal action against artisanal polluters and neglectful owners of animals. Religious institutions, individuals and charitable organizations in both Islam and Christianity likewise promoted moral and physical wellbeing by endowing urban amenities such as wells, fountains, schools and bridges, also in the service of pilgrims.[98][99] In western Europe and Byzantium, religious processions commonly took place, which purported to act as both preventive and curative measures for the entire community.[100]
 Urban residents and other groups also developed preventive measures in response to calamities such as war, famine, floods and widespread disease.[101][102][103][104] During and after the Black Death (1346–53), for instance, inhabitants of the Eastern Mediterranean and Western Europe reacted to massive population decline in part on the basis of existing medical theories and protocols, for instance concerning meat consumption and burial, and in part by developing new ones.[105][106][107] The latter included the establishment of quarantine facilities and health boards, some of which eventually became regular urban (and later national) offices.[108][109] Subsequent measures for protecting cities and their regions included issuing health passports for travelers, deploying guards to create sanitary cordons for protecting local inhabitants, and gathering morbidity and mortality statistics.[110][111][112]  Such measures relied in turn on better transportation and communication networks, through which news on human and animal disease was efficiently spread.
 With the onset of the Industrial Revolution, living standards amongst the working population began to worsen, with cramped and unsanitary urban conditions. In the first four decades of the 19th century alone, London's population doubled and even greater growth rates were recorded in the new industrial towns, such as Leeds and Manchester. This rapid urbanization exacerbated the spread of disease in the large conurbations that built up around the workhouses and factories. These settlements were cramped and primitive with no organized sanitation. Disease was inevitable and its incubation in these areas was encouraged by the poor lifestyle of the inhabitants. Unavailable housing led to the rapid growth of slums and the per capita death rate began to rise alarmingly, almost doubling in Birmingham and Liverpool. Thomas Malthus warned of the dangers of overpopulation in 1798. His ideas, as well as those of Jeremy Bentham, became very influential in government circles in the early years of the 19th century.[113] The latter part of the century brought the establishment of the basic pattern of improvements in public health over the next two centuries: a social evil was identified, private philanthropists brought attention to it, and changing public opinion led to government action.[113] The 18th century saw rapid growth in voluntary hospitals in England.[114]
 The practice of vaccination began in the 1800s, following the pioneering work of Edward Jenner in treating smallpox. James Lind's discovery of the causes of scurvy amongst sailors and its mitigation via the introduction of fruit on lengthy voyages was published in 1754 and led to the adoption of this idea by the Royal Navy.[115] Efforts were also made to promulgate health matters to the broader public; in 1752 the British physician Sir John Pringle published Observations on the Diseases of the Army in Camp and Garrison, in which he advocated for the importance of adequate ventilation in the military barracks and the provision of latrines for the soldiers.[116]
 The first attempts at sanitary reform and the establishment of public health institutions were made in the 1840s. Thomas Southwood Smith, physician at the London Fever Hospital, began to write papers on the importance of public health, and was one of the first physicians brought in to give evidence before the Poor Law Commission in the 1830s, along with Neil Arnott and James Phillips Kay.[117] Smith advised the government on the importance of quarantine and sanitary improvement for limiting the spread of infectious diseases such as cholera and yellow fever.[118][119]
 The Poor Law Commission reported in 1838 that ""the expenditures necessary to the adoption and maintenance of measures of prevention would ultimately amount to less than the cost of the disease now constantly engendered"". It recommended the implementation of large scale government engineering projects to alleviate the conditions that allowed for the propagation of disease.[113] The Health of Towns Association was formed at Exeter Hall London on 11 December 1844, and vigorously campaigned for the development of public health in the United Kingdom.[120] Its formation followed the 1843 establishment of the Health of Towns Commission, chaired by Sir Edwin Chadwick, which produced a series of reports on poor and insanitary conditions in British cities.[120]
 These national and local movements led to the Public Health Act, finally passed in 1848. It aimed to improve the sanitary condition of towns and populous places in England and Wales by placing the supply of water, sewerage, drainage, cleansing and paving under a single local body with the General Board of Health as a central authority. The Act was passed by the Liberal government of Lord John Russell, in response to the urging of Edwin Chadwick. Chadwick's seminal report on The Sanitary Condition of the Labouring Population was published in 1842[121] and was followed up with a supplementary report a year later.[122] During this time, James Newlands (appointed following the passing of the 1846 Liverpool Sanatory Act championed by the Borough of Liverpool Health of Towns Committee) designed the world's first integrated sewerage system, in Liverpool (1848–1869), with Joseph Bazalgette later creating London's sewerage system (1858–1875).
 The Vaccination Act 1853 introduced compulsory smallpox vaccination in England and Wales.[123] By 1871 legislation required a comprehensive system of registration run by appointed vaccination officers.[124]
 Further interventions were made by a series of subsequent Public Health Acts, notably the 1875 Act. Reforms included the building of sewers, the regular collection of garbage followed by incineration or disposal in a landfill, the provision of clean water and the draining of standing water to prevent the breeding of mosquitoes.
 The Infectious Disease (Notification) Act 1889 (52 & 53 Vict. c. 72) mandated the reporting of infectious diseases to the local sanitary authority, which could then pursue measures such as the removal of the patient to hospital and the disinfection of homes and properties.[125]
 In the United States, the first public health organization based on a state health department and local boards of health was founded in New York City in 1866.[126]
 In Germany during The Weimar Republic the country faced many public health catastrophes.[examples  needed] The Nazi Party had a goal of modernizing health care with Volksgesundheit, German for people's public health; this modernization was based on the growing field of eugenics and measures prioritizing group health over any care for the health of individuals.  The end of World War 2 led to the Nuremberg Code, a set of research ethics concerning human experimentation.[127]
 The science of epidemiology was founded by John Snow's identification of a polluted public water well as the source of an 1854 cholera outbreak in London. Snow believed in the germ theory of disease as opposed to the prevailing miasma theory. By talking to local residents (with the help of Reverend Henry Whitehead), he identified the source of the outbreak as the public water pump on Broad Street (now Broadwick Street). Although Snow's chemical and microscope examination of a water sample from the Broad Street pump did not conclusively prove its danger, his studies of the pattern of the disease were convincing enough to persuade the local council to close the well pump by removing its handle.[128]
 Snow later used a dot map to illustrate the cluster of cholera cases around the pump. He also used statistics to illustrate the connection between the quality of the water source and cholera cases. He showed that the Southwark and Vauxhall Waterworks Company was taking water from sewage-polluted sections of the Thames and delivering the water to homes, leading to an increased incidence of cholera. Snow's study was a major event in the history of public health and geography. It is regarded as the founding event of the science of epidemiology.[129][130]
 With the pioneering work in bacteriology of French chemist Louis Pasteur and German scientist Robert Koch, methods for isolating the bacteria responsible for a given disease and vaccines for remedy were developed at the turn of the 20th century. British physician Ronald Ross identified the mosquito as the carrier of malaria and laid the foundations for combating the disease.[131] Joseph Lister revolutionized surgery by the introduction of antiseptic surgery to eliminate infection. French epidemiologist Paul-Louis Simond proved that plague was carried by fleas on the back of rats,[132] and Cuban scientist Carlos J. Finlay and U.S. Americans Walter Reed and James Carroll demonstrated that mosquitoes carry the virus responsible for yellow fever.[133][134] Brazilian scientist Carlos Chagas identified a tropical disease and its vector.[135]
 Education and training of public health professionals is available throughout the world in Schools of Public Health, Medical Schools, Veterinary Schools, Schools of Nursing, and Schools of Public Affairs. The training typically requires a university degree with a focus on core disciplines of biostatistics, epidemiology, health services administration, health policy, health education, behavioral science, gender issues, sexual and reproductive health, public health nutrition, and occupational and environmental health.[136][137]
 In the global context, the field of public health education has evolved enormously in recent decades, supported by institutions such as the World Health Organization and the World Bank, among others. Operational structures are formulated by strategic principles, with educational and career pathways guided by competency frameworks, all requiring modulation according to local, national and global realities. Moreover, integrating technology or digital platforms to connect to low health literacy LHL groups could be a way to increase health literacy. [138]It is critically important for the health of populations that nations assess their public health human resource needs and develop their ability to deliver this capacity, and not depend on other countries to supply it.[139]
 In the United States, the Welch-Rose Report of 1915[140] has been viewed as the basis for the critical movement in the history of the institutional schism between public health and medicine because it led to the establishment of schools of public health supported by the Rockefeller Foundation.[141] The report was authored by William Welch, founding dean of the Johns Hopkins Bloomberg School of Public Health, and Wickliffe Rose of the Rockefeller Foundation. The report focused more on research than practical education.[141][142] Some have blamed the Rockefeller Foundation's 1916 decision to support the establishment of schools of public health for creating the schism between public health and medicine and legitimizing the rift between medicine's laboratory investigation of the mechanisms of disease and public health's nonclinical concern with environmental and social influences on health and wellness.[141][143]
 Even though schools of public health had already been established in Canada, Europe and North Africa, the United States had still maintained the traditional system of housing faculties of public health within their medical institutions.  A $25,000 donation from businessman Samuel Zemurray instituted the School of Public Health and Tropical Medicine at Tulane University in 1912 conferring its first doctor of public health degree in 1914.[144][145]  The Yale School of Public Health was founded by Charles-Edward Amory Winslow in 1915.[146] The Johns Hopkins School of Hygiene and Public Health was founded in 1916 and became an independent, degree-granting institution for research and training in public health, and the largest public health training facility in the United States.[147][148][149] By 1922, schools of public health were established at Columbia and Harvard on the Hopkins model. By 1999 there were twenty nine schools of public health in the US, enrolling around fifteen thousand students.[136][141]
 Over the years, the types of students and training provided have also changed. In the beginning, students who enrolled in public health schools typically had already obtained a medical degree; public health school training was largely a second degree for medical professionals. However, in 1978, 69% of American students enrolled in public health schools had only a bachelor's degree.[136]
 Schools of public health offer a variety of degrees generally fall into two categories: professional or academic.[151] The two major postgraduate degrees are the Master of Public Health (MPH) or the Master of Science in Public Health (MSPH). Doctoral studies in this field include Doctor of Public Health (DrPH) and Doctor of Philosophy (PhD) in a subspecialty of greater Public Health disciplines. DrPH is regarded as a professional degree and PhD as more of an academic degree.
 Professional degrees are oriented towards practice in public health settings. The Master of Public Health, Doctor of Public Health, Doctor of Health Science (DHSc/DHS) and the Master of Health Care Administration are examples of degrees which are geared towards people who want careers as practitioners of public health in health departments, managed care and community-based organizations, hospitals and consulting firms, among others. Master of Public Health degrees broadly fall into two categories, those that put more emphasis on an understanding of epidemiology and statistics as the scientific basis of public health practice and those that include a more wide range of methodologies.  A Master of Science of Public Health is similar to an MPH but is considered an academic degree (as opposed to a professional degree) and places more emphasis on scientific methods and research.  The same distinction can be made between the DrPH and the DHSc. The DrPH is considered a professional degree and the DHSc is an academic degree.[citation needed]
 Academic degrees are more oriented towards those with interests in the scientific basis of public health and preventive medicine who wish to pursue careers in research, university teaching in graduate programs, policy analysis and development, and other high-level public health positions. Examples of academic degrees are the Master of Science, Doctor of Philosophy, Doctor of Science (ScD), and Doctor of Health Science (DHSc). The doctoral programs are distinct from the MPH and other professional programs by the addition of advanced coursework and the nature and scope of a dissertation research project.
 In Canada, the Public Health Agency of Canada is the national agency responsible for public health, emergency preparedness and response, and infectious and chronic disease control and prevention.[164]
 Since the 1959 Cuban Revolution the Cuban government has devoted extensive resources to the improvement of health conditions for its entire population via universal access to health care. Infant mortality has plummeted.[165] Cuban medical internationalism as a policy has seen the Cuban government sent doctors as a form of aid and export to countries in need in Latin America, especially Venezuela, as well as Oceania and Africa countries.
 Public health was important elsewhere in Latin America in consolidating state power and integrating marginalized populations into the nation-state.  In Colombia, public health was a means for creating and implementing ideas of citizenship.[166] In Bolivia, a similar push came after their 1952 revolution.[167]
 Though curable and preventive, malaria remains a major public health issue and is the third leading cause of death in Ghana.[168] In the absence of a vaccine, mosquito control, or access to anti-malaria medication, public health methods become the main strategy for reducing the prevalence and severity of malaria.[169] These methods include reducing breeding sites, screening doors and windows, insecticide sprays, prompt treatment following infection, and usage of insecticide treated mosquito nets.[169] Distribution and sale of insecticide-treated mosquito nets is a common, cost-effective anti-malaria public health intervention; however, barriers to use exist including cost, household and family organization, access to resources, and social and behavioral determinants which have not only been shown to affect malaria prevalence rates but also mosquito net use.[170][169]
 Public health issues were important for the Spanish Empire during the colonial era. Epidemic disease was the main factor in the decline of indigenous populations in the era immediately following the sixteenth-century conquest era and was a problem during the colonial era. The Spanish crown took steps in eighteenth-century Mexico to bring in regulations to make populations healthier.[175] In the late nineteenth century, Mexico was in the process of modernization, and public health issues were again tackled from a scientific point of view.[176][177][178] As in the U.S., food safety became a public health issue, particularly focusing on meat slaughterhouses and meatpacking.[179]
 The United States Public Health Service (USPHS or PHS) is a collection of agencies of the Department of Health and Human Services concerned with public health, containing nine out of the department's twelve operating divisions. The Assistant Secretary for Health oversees the PHS. The Public Health Service Commissioned Corps (PHSCC) is the federal uniformed service of the PHS, and is one of the eight uniformed services of the United States.
 The United States lacks a coherent system for the governmental funding of public health, relying on a variety of agencies and programs at the federal, state and local levels.[185]
Between 1960 and 2001, public health spending in the United States tended to grow,
based on increasing expenditures by state and local government, which made up 80–90% of
total public health spending.  Spending in support of public health in the United States peaked in 2002 and declined in the following decade.[186] State cuts to public health funding during the Great Recession of 2007–2008 were not restored in subsequent years.[187]
As of 2012, a panel for the  U.S. Institute of Medicine panel warned that the United States spends disproportionately far more on clinical care than it does on public health, neglecting ""population-based activities that offer efficient and effective approaches to improving the nation's health.""[188][186]  As of 2018[update], about 3% of government health spending was directed to public health and prevention.[39][189][190] This situation has been described as an ""uneven patchwork""[191] and ""chronic underfunding"".[192][193][194][195]
The COVID-19 pandemic has been seen as drawing attention to problems in the public health system in the United States and to a lack of understanding of public health and its important role as a common good.[39]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['research than practical education', 'army generals, the clergy or rulers', 'institutional schism between public health and medicine', 'problems', 'Further interventions'], 'answer_start': [], 'answer_end': []}"
"
 A health system, health care system or healthcare system is an organization of people, institutions, and resources that delivers health care services to meet the health needs of target populations.
 There is a wide variety of health systems around the world, with as many histories and organizational structures as there are nations. Implicitly, nations must design and develop health systems in accordance with their needs and resources, although common elements in virtually all health systems are primary healthcare and public health measures.[1]
 In certain nations, the orchestration of health system planning is decentralized, with various stakeholders in the market assuming responsibilities. In contrast, in other regions, a collaborative endeavor exists among governmental entities, labor unions, philanthropic organizations, religious institutions, or other organized bodies, aimed at the meticulous provision of healthcare services tailored to the specific needs of their respective populations. Nevertheless, it is noteworthy that the process of healthcare planning is frequently characterized as an evolutionary progression rather than a revolutionary transformation.[2][3]
 As with other social institutional structures, health systems are likely to reflect the history, culture and economics of the states in which they evolve. These peculiarities bedevil and complicate international comparisons and preclude any universal standard of performance.
 According to the World Health Organization (WHO), the directing and coordinating authority for health within the United Nations system, healthcare systems' goals are good health for the citizens, responsiveness to the expectations of the population, and fair means of funding operations. Progress towards them depends on how systems carry out four vital functions: provision of health care services, resource generation, financing, and stewardship.[4] Other dimensions for the evaluation of health systems include quality, efficiency, acceptability, and equity.[2] They have also been described in the United States as ""the five C's"": Cost, Coverage, Consistency, Complexity, and Chronic Illness.[5] Also, continuity of health care is a major goal.[6]
 Often health system has been defined with a reductionist perspective. Some authors[7] have developed arguments to expand the concept of health systems, indicating additional dimensions that should be considered:
 The World Health Organization defines health systems as follows:
 A health system consists of all organizations, people and actions whose primary intent is to promote, restore or maintain health. This includes efforts to influence determinants of health as well as more direct health-improving activities. A health system is, therefore, more than the pyramid of publicly owned facilities that deliver personal health services. It includes, for example, a mother caring for a sick child at home; private providers; behaviour change programmes; vector-control campaigns; health insurance organizations; occupational health and safety legislation. It includes inter-sectoral action by health staff, for example, encouraging the ministry of education to promote female education, a well-known determinant of better health.[8] There are generally five primary methods of funding health systems:[9]
 Most countries' systems feature a mix of all five models. One study[10] based on data from the OECD concluded that all types of health care finance ""are compatible with"" an efficient health system. The study also found no relationship between financing and cost control.[citation needed] Another study examining single payer and multi payer systems in OECD countries found that single payer systems have significantly less hospital beds per 100,000 people than in multi payer systems.[11]
 The term health insurance is generally used to describe a form of insurance that pays for medical expenses. It is sometimes used more broadly to include insurance covering disability or long-term nursing or custodial care needs. It may be provided through a social insurance program, or from private insurance companies. It may be obtained on a group basis (e.g., by a firm to cover its employees) or purchased by individual consumers. In each case premiums or taxes protect the insured from high or unexpected health care expenses.[citation needed]
 Through the calculation of the comprehensive cost of healthcare expenditures, it becomes feasible to construct a standard financial framework, which may involve mechanisms like monthly premiums or annual taxes. This ensures the availability of funds to cover the healthcare benefits delineated in the insurance agreement. Typically, the administration of these benefits is overseen by a government agency, a nonprofit health fund, or a commercial corporation.[12]
 Many commercial health insurers control their costs by restricting the benefits provided, by such means as deductibles, co-payments, coinsurance, policy exclusions, and total coverage limits. They will also severely restrict or refuse coverage of pre-existing conditions. Many government systems also have co-payment arrangements but express exclusions are rare or limited because of political pressure. The larger insurance systems may also negotiate fees with providers.[citation needed]
 Many forms of social insurance systems control their costs by using the bargaining power of the community they are intended to serve to control costs in the health care delivery system. They may attempt to do so by, for example, negotiating drug prices directly with pharmaceutical companies, negotiating standard fees with the medical profession, or reducing unnecessary health care costs. Social systems sometimes feature contributions related to earnings as part of a system to deliver universal health care, which may or may not also involve the use of commercial and non-commercial insurers. Essentially the wealthier users pay proportionately more into the system to cover the needs of the poorer users who therefore contribute proportionately less. There are usually caps on the contributions of the wealthy and minimum payments that must be made by the insured (often in the form of a minimum contribution, similar to a deductible in commercial insurance models).[citation needed]
 In addition to these traditional health care financing methods, some lower income countries and development partners are also implementing non-traditional or innovative financing mechanisms for scaling up delivery and sustainability of health care,[13] such as micro-contributions, public-private partnerships, and market-based financial transaction taxes. For example, as of June 2011, UNITAID had collected more than one billion dollars from 29 member countries, including several from Africa, through an air ticket solidarity levy to expand access to care and treatment for HIV/AIDS, tuberculosis and malaria in 94 countries.[14]
 In most countries, wage costs for healthcare practitioners are estimated to represent between 65% and 80% of renewable health system expenditures.[15][16] There are three ways to pay medical practitioners: fee for service, capitation, and salary. There has been growing interest in blending elements of these systems.[17]
 Fee-for-service arrangements pay general practitioners (GPs) based on the service.[17] They are even more widely used for specialists working in ambulatory care.[17]
 There are two ways to set fee levels:[17]
 In capitation payment systems, GPs are paid for each patient on their ""list"", usually with adjustments for factors such as age and gender.[17] According to OECD (Organization for Economic Co-operation and Development), ""these systems are used in Italy (with some fees), in all four countries of the United Kingdom (with some fees and allowances for specific services), Austria (with fees for specific services), Denmark (one third of income with remainder fee for service), Ireland (since 1989), the Netherlands (fee-for-service for privately insured patients and public employees) and Sweden (from 1994). Capitation payments have become more frequent in ""managed care"" environments in the United States.""[17]
 According to OECD, ""capitation systems allow funders to control the overall level of primary health expenditures, and the allocation of funding among GPs is determined by patient registrations"". However, under this approach, GPs may register too many patients and under-serve them, select the better risks and refer on patients who could have been treated by the GP directly. Freedom of consumer choice over doctors, coupled with the principle of ""money following the patient"" may moderate some of these risks. Aside from selection, these problems are likely to be less marked than under salary-type arrangements.'[citation needed]
 In several OECD countries, general practitioners (GPs) are employed on salaries for the government.[17] According to OECD, ""Salary arrangements allow funders to control primary care costs directly; however, they may lead to under-provision of services (to ease workloads), excessive referrals to secondary providers and lack of attention to the preferences of patients.""[17] There has been movement away from this system.[17]
 In recent years, providers have been switching from fee-for-service payment models to a value-based care payment system, where they are compensated for providing value to patients. In this system, providers are given incentives to close gaps in care and provide better quality care for patients.
[18]
 Expand the OECD charts below to see the breakdown:
 Sound information plays an increasingly critical role in the delivery of modern health care and efficiency of health systems. Health informatics – the intersection of information science, medicine and healthcare – deals with the resources, devices, and methods required to optimize the acquisition and use of information in health and biomedicine. Necessary tools for proper health information coding and management include clinical guidelines, formal medical terminologies, and computers and other information and communication technologies. The kinds of health data processed may include patients' medical records, hospital administration and clinical functions, and human resources information.[20]
 The use of health information lies at the root of evidence-based policy and evidence-based management in health care. Increasingly, information and communication technologies are being utilised to improve health systems in developing countries through: the standardisation of health information; computer-aided diagnosis and treatment monitoring; informing population groups on health and treatment.[21]
 The management of any health system is typically directed through a set of policies and plans adopted by government, private sector business and other groups in areas such as personal healthcare delivery and financing, pharmaceuticals, health human resources, and public health.[citation needed]
 Public health is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people, or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health is typically divided into epidemiology, biostatistics and health services. Environmental, social, behavioral, and occupational health are also important subfields.[citation needed]
 Today, most governments recognize the importance of public health programs in reducing the incidence of disease, disability, the effects of ageing and health inequities, although public health generally receives significantly less government funding compared with medicine. For example, most countries have a vaccination policy, supporting public health programs in providing vaccinations to promote health. Vaccinations are voluntary in some countries and mandatory in some countries. Some governments pay all or part of the costs for vaccines in a national vaccination schedule.[citation needed]
 The rapid emergence of many chronic diseases, which require costly long-term care and treatment, is making many health managers and policy makers re-examine their healthcare delivery practices. An important health issue facing the world currently is HIV/AIDS.[22] Another major public health concern is diabetes.[23] In 2006, according to the World Health Organization, at least 171 million people worldwide had diabetes. Its incidence is increasing rapidly, and it is estimated that by 2030, this number will double. A controversial aspect of public health is the control of tobacco smoking, linked to cancer and other chronic illnesses.[24]
 Antibiotic resistance is another major concern, leading to the reemergence of diseases such as tuberculosis. The World Health Organization, for its World Health Day 2011 campaign, called for intensified global commitment to safeguard antibiotics and other antimicrobial medicines for future generations.[citation needed]
 Since 2000, more and more initiatives have been taken at the international and national levels in order to strengthen national health systems as the core components of the global health system. Having this scope in mind, it is essential to have a clear, and unrestricted, vision of national health systems that might generate further progress in global health. The elaboration and the selection of performance indicators are indeed both highly dependent on the conceptual framework adopted for the evaluation of the health systems performance.[26] Like most social systems, health systems are complex adaptive systems where change does not necessarily follow rigid management models.[27] In complex systems path dependency, emergent properties and other non-linear patterns are seen,[28] which can lead to the development of inappropriate guidelines for developing responsive health systems.[29]
 Quality frameworks are essential tools for understanding and improving health systems. They help define, prioritize, and implement health system goals and functions. Among the key frameworks is the World Health Organization's building blocks model, which enhances health quality by focusing on elements like financing, workforce, information, medical products, governance, and service delivery. This model influences global health evaluation and contributes to indicator development and research.[30]
 The Lancet Global Health Commission's 2018 framework builds upon earlier models by emphasizing system foundations, processes, and outcomes, guided by principles of efficiency, resilience, equity, and people-centeredness. This comprehensive approach addresses challenges associated with chronic and complex conditions and is particularly influential in health services research in developing countries.[31] Importantly, recent developments also highlight the need to integrate environmental sustainability into these frameworks, suggesting its inclusion as a guiding principle to enhance the environmental responsiveness of health systems.[32]
 An increasing number of tools and guidelines are being published by international agencies and development partners to assist health system decision-makers to monitor and assess health systems strengthening[33] including human resources development[34] using standard definitions, indicators and measures. In response to a series of papers published in 2012 by members of the World Health Organization's Task Force on Developing Health Systems Guidance, researchers from the Future Health Systems consortium argue that there is insufficient focus on the 'policy implementation gap'. Recognizing the diversity of stakeholders and complexity of health systems is crucial to ensure that evidence-based guidelines are tested with requisite humility and without a rigid adherence to models dominated by a limited number of disciplines.[29][35] Healthcare services often implement Quality Improvement Initiatives to overcome this policy implementation gap. Although many of these initiatives deliver improved healthcare, a large proportion fail to be sustained. Numerous tools and frameworks have been created to respond to this challenge and increase improvement longevity. One tool highlighted the need for these tools to respond to user preferences and settings to optimize impact.[36]
 Health Policy and Systems Research (HPSR) is an emerging multidisciplinary field that challenges 'disciplinary capture' by dominant health research traditions, arguing that these traditions generate premature and inappropriately narrow definitions that impede rather than enhance health systems strengthening.[37] HPSR focuses on low- and middle-income countries and draws on the relativist social science paradigm which recognises that all phenomena are constructed through human behaviour and interpretation. In using this approach, HPSR offers insight into health systems by generating a complex understanding of context in order to enhance health policy learning.[38] HPSR calls for greater involvement of local actors, including policy makers, civil society and researchers, in decisions that are made around funding health policy research and health systems strengthening.[39]
 Health systems can vary substantially from country to country, and in the last few years, comparisons have been made on an international basis. The World Health Organization, in its World Health Report 2000, provided a ranking of health systems around the world according to criteria of the overall level and distribution of health in the populations, and the responsiveness and fair financing of health care services.[4] The goals for health systems, according to the WHO's World Health Report 2000 – Health systems: improving performance (WHO, 2000),[42] are good health, responsiveness to the expectations of the population, and fair financial contribution. There have been several debates around the results of this WHO exercise,[43] and especially based on the country ranking linked to it,[44] insofar as it appeared to depend mostly on the choice of the retained indicators.
 Direct comparisons of health statistics across nations are complex. The Commonwealth Fund, in its annual survey, ""Mirror, Mirror on the Wall"", compares the performance of the health systems in Australia, New Zealand, the United Kingdom, Germany, Canada and the United States. Its 2007 study found that, although the United States system is the most expensive, it consistently underperforms compared to the other countries.[45] A major difference between the United States and the other countries in the study is that the United States is the only country without universal health care. The OECD also collects comparative statistics, and has published brief country profiles.[46][47][48] Health Consumer Powerhouse makes comparisons between both national health care systems in the Euro health consumer index and specific areas of health care such as diabetes[49] or hepatitis.[50]
 Ipsos MORI produces an annual study of public perceptions of healthcare services across 30 countries.[51]
 Physicians and hospital beds per 1000 inhabitants vs Health Care Spending in 2008 for OECD Countries. The data source is OECD.org - OECD.[47][48]
 

","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the United States is the only country without universal health care', 'United Kingdom, Germany, Canada and the United States', 'the United States is the only country without universal health care', 'the United States is the only country without universal health care', 'Direct comparisons of health statistics across nations are complex'], 'answer_start': [], 'answer_end': []}"
"Medical ethics is an applied branch of ethics which analyzes the practice of clinical medicine and related scientific research.[1] Medical ethics is based on a set of values that professionals can refer to in the case of any confusion or conflict. These values include the respect for autonomy, non-maleficence, beneficence, and justice.[2] Such tenets may allow doctors, care providers, and families to create a treatment plan and work towards the same common goal.[3] These four values are not ranked in order of importance or relevance and they all encompass values pertaining to medical ethics.[4] However, a conflict may arise leading to the need for hierarchy in an ethical system, such that some moral elements overrule others with the purpose of applying the best moral judgement to a difficult medical situation.[5] Medical ethics is particularly relevant in decisions regarding involuntary treatment and involuntary commitment.
 There are several codes of conduct. The Hippocratic Oath discusses basic principles for medical professionals.[5] This document dates back to the fifth century BCE.[6] Both The Declaration of Helsinki (1964) and The Nuremberg Code (1947) are two well-known and well respected documents contributing to medical ethics. Other important markings in the history of medical ethics include Roe v. Wade[why?] in 1973 and the development of hemodialysis in the 1960s. With hemodialysis now available, but a limited number of dialysis machines to treat patients, an ethical question arose on which patients to treat and which ones not to treat, and which factors to use in making such a decision.[7] More recently, new techniques for gene editing aiming at treating, preventing and curing diseases utilizing gene editing, are raising important moral questions about their applications in medicine and treatments as well as societal impacts on future generations,[8][9] yet remain controversial due to their association with eugenics.[10]
 As this field continues to develop and change throughout history, the focus remains on fair, balanced, and moral thinking across all cultural and religious backgrounds around the world.[11][12] The field of medical ethics encompasses both practical application in clinical settings and scholarly work in philosophy, history, and sociology.
 Medical ethics encompasses beneficence, autonomy, and justice as they relate to conflicts such as euthanasia, patient confidentiality, informed consent, and conflicts of interest in healthcare.[13][14][15] In addition, medical ethics and culture are interconnected as different cultures implement ethical values differently, sometimes placing more emphasis on family values and downplaying the importance of autonomy. This leads to an increasing need for culturally sensitive physicians and ethical committees in hospitals and other healthcare settings.[11][12][16]
 The term medical ethics first dates back to 1803, when English author and physician Thomas Percival published a document describing the requirements and expectations of medical professionals within medical facilities. The Code of Ethics was then adapted in 1847, relying heavily on Percival's words.[17] Over the years in 1903, 1912, and 1947, revisions have been made to the original document.[17] The practice of medical ethics is widely accepted and practiced throughout the world.[4]
 Historically, Western medical ethics may be traced to guidelines on the duty of physicians in antiquity, such as the Hippocratic Oath, and early Christian teachings. The first code of medical ethics, Formula Comitis Archiatrorum, was published in the 5th century, during the reign of the Ostrogothic Christian king Theodoric the Great. In the medieval and early modern period, the field is indebted to Islamic scholarship such as Ishaq ibn Ali al-Ruhawi (who wrote the Conduct of a Physician, the first book dedicated to medical ethics), Avicenna's Canon of Medicine and Muhammad ibn Zakariya ar-Razi (known as Rhazes in the West), Jewish thinkers such as Maimonides, Roman Catholic scholastic thinkers such as Thomas Aquinas, and the case-oriented analysis (casuistry) of Catholic moral theology. These intellectual traditions continue in Catholic, Islamic and Jewish medical ethics.
 By the 18th and 19th centuries, medical ethics emerged as a more self-conscious discourse. In England, Thomas Percival, a physician and author, crafted the first modern code of medical ethics. He drew up a pamphlet with the code in 1794 and wrote an expanded version in 1803, in which he coined the expressions ""medical ethics"" and ""medical jurisprudence"".[18] However, there are some who see Percival's guidelines that relate to physician consultations as being excessively protective of the home physician's reputation. Jeffrey Berlant is one such critic who considers Percival's codes of physician consultations as being an early example of the anti-competitive, ""guild""-like nature of the physician community.[19][20] In addition, since the mid 19th century up to the 20th century, physician-patient relationships that once were more familiar became less prominent and less intimate, sometimes leading to malpractice, which resulted in less public trust and a shift in decision-making power from the paternalistic physician model to today's emphasis on patient autonomy and self-determination.[21]
 In 1815, the Apothecaries Act was passed by the Parliament of the United Kingdom. It introduced compulsory apprenticeship and formal qualifications for the apothecaries of the day under the license of the Society of Apothecaries. This was the beginning of regulation of the medical profession in the UK.
 In 1847, the American Medical Association adopted its first code of ethics, with this being based in large part upon Percival's work.[22] While the secularized field borrowed largely from Catholic medical ethics, in the 20th century a distinctively liberal Protestant approach was articulated by thinkers such as Joseph Fletcher. In the 1960s and 1970s, building upon liberal theory and procedural justice, much of the discourse of medical ethics went through a dramatic shift and largely reconfigured itself into bioethics.[23]
 Well-known medical ethics cases include:
 Since the 1970s, the growing influence of ethics in contemporary medicine can be seen in the increasing use of Institutional Review Boards to evaluate experiments on human subjects, the establishment of hospital ethics committees, the expansion of the role of clinician ethicists, and the integration of ethics into many medical school curricula.[24]
 In December 2019, the virus COVID-19 emerged as a threat to worldwide public health and, over the following years, ignited novel inquiry into modern-age medical ethics. For example, since the first discovery of COVID-19 in Wuhan, China[25] and subsequent global spread by mid-2020, calls for the adoption of open science principles dominated research communities.[26] Some academics believed that open science principles — like constant communication between research groups, rapid translation of study results into public policy, and transparency of scientific processes to the public — represented the only solutions to halt the impact of the virus. Others, however, cautioned that these interventions may lead to side-stepping safety in favor of speed, wasteful use of research capital, and creation of public confusion.[26] Drawbacks of these practices include resource-wasting and public confusion surrounding the use of hydroxychloroquine and azithromycin as treatment for COVID-19 — a combination which was later shown to have no impact on COVID-19 survivorship and carried notable cardiotoxic side-effects[27] — as well as a type of vaccine hesitancy specifically due to the speed at which COVID-19 vaccines were created and made publicly available.[28] However, open science also allowed for the rapid implementation of life-saving public interventions like wearing masks and social distancing, the rapid development of multiple vaccines and monoclonal antibodies that have significantly lowered transmission and death rates, and increased public awareness about the severity of the pandemic as well as explanation of daily protective actions against COVID-19 infection, like hand washing.[26]
 Other notable areas of medicine impacted by COVID-19 ethics include:
 The ethics of COVID-19 spans many more areas of medicine and society than represented in this paragraph — some of these principles will likely not be discovered until the end of the pandemic which, as of September 12, 2022, is still ongoing.
 A common framework used when analysing medical ethics is the ""four principles"" approach postulated by Tom Beauchamp and James Childress in their textbook Principles of Biomedical Ethics. It recognizes four basic moral principles, which are to be judged and weighed against each other, with attention given to the scope of their application. The four principles are:[38]
 The principle of autonomy, broken down into ""autos"" (self) and ""nomos (rule), views the rights of an individual to self-determination.[21] This is rooted in society's respect for individuals' ability to make informed decisions about personal matters with freedom. Autonomy has become more important as social values have shifted to define medical quality in terms of outcomes that are important to the patient and their family rather than medical professionals.[21] The increasing importance of autonomy can be seen as a social reaction against the ""paternalistic"" tradition within healthcare.[21][40] Some have questioned whether the backlash against historically excessive paternalism in favor of patient autonomy has inhibited the proper use of soft paternalism to the detriment of outcomes for some patients.[41]
 The definition of autonomy is the ability of an individual to make a rational, uninfluenced decision. Therefore, it can be said that autonomy is a general indicator of a healthy mind and body. The progression of many terminal diseases are characterized by loss of autonomy, in various manners and extents. For example, dementia, a chronic and progressive disease that attacks the brain can induce memory loss and cause a decrease in rational thinking, almost always results in the loss of autonomy.[42]
 Psychiatrists and clinical psychologists are often asked to evaluate a patient's capacity for making life-and-death decisions at the end of life. Persons with a psychiatric condition such as delirium or clinical depression may lack capacity to make end-of-life decisions. For these persons, a request to refuse treatment may be taken in the context of their condition. Unless there is a clear advance directive to the contrary, persons lacking mental capacity are treated according to their best interests. This will involve an assessment involving people who know the person best to what decisions the person would have made had they not lost capacity.[43] Persons with the mental capacity to make end-of-life decisions may refuse treatment with the understanding that it may shorten their life. Psychiatrists and psychologists may be involved to support decision making.[44]
 The term beneficence refers to actions that promote the well-being of others. In the medical context, this means taking actions that serve the best interests of patients and their families.[2] However, uncertainty surrounds the precise definition of which practices do in fact help patients.
 James Childress and Tom Beauchamp in Principles of Biomedical Ethics (1978) identify beneficence as one of the core values of healthcare ethics. Some scholars, such as Edmund Pellegrino, argue that beneficence is the only fundamental principle of medical ethics. They argue that healing should be the sole purpose of medicine, and that endeavors like cosmetic surgery and euthanasia are severely unethical and against the Hippocratic Oath.[citation needed]
 The concept of non-maleficence is embodied by the phrase, ""first, do no harm,"" or the Latin, primum non nocere. Many consider that should be the main or primary consideration (hence primum): that it is more important not to harm your patient, than to do them good, which is part of the Hippocratic oath that doctors take.[45] This is partly because enthusiastic practitioners are prone to using treatments that they believe will do good, without first having evaluated them adequately to ensure they do no harm to the patient. Much harm has been done to patients as a result, as in the saying, ""The treatment was a success, but the patient died."" It is not only more important to do no harm than to do good; it is also important to know how likely it is that your treatment will harm a patient. So a physician should go further than not prescribing medications they know to be harmful—he or she should not prescribe medications (or otherwise treat the patient) unless s/he knows that the treatment is unlikely to be harmful; or at the very least, that patient understands the risks and benefits, and that the likely benefits outweigh the likely risks.
 In practice, however, many treatments carry some risk of harm. In some circumstances, e.g. in desperate situations where the outcome without treatment will be grave, risky treatments that stand a high chance of harming the patient will be justified, as the risk of not treating is also very likely to do harm. So the principle of non-maleficence is not absolute, and balances against the principle of beneficence (doing good), as the effects of the two principles together often give rise to a double effect (further described in next section). Even basic actions like taking a blood sample or an injection of a drug cause harm to the patient's body. Euthanasia also goes against the principle of beneficence because the patient dies as a result of the medical treatment by the doctor.
 Double effect refers to two types of consequences that may be produced by a single action,[46] and in medical ethics it is usually regarded as the combined effect of beneficence and non-maleficence.[47]
 A commonly cited example of this phenomenon is the use of morphine or other analgesic in the dying patient. Such use of morphine can have the beneficial effect of easing the pain and suffering of the patient while simultaneously having the maleficent effect of shortening the life of the patient through the deactivation of the respiratory system.[48]
 The human rights era started with the formation of the United Nations in 1945, which was charged with the promotion of human rights. The Universal Declaration of Human Rights (1948) was the first major document to define human rights. Medical doctors have an ethical duty to protect the human rights and human dignity of the patient so the advent of a document that defines human rights has had its effect on medical ethics.[49] Most codes of medical ethics now require respect for the human rights of the patient.
 The Council of Europe promotes the rule of law and observance of human rights in Europe. The Council of Europe adopted the European Convention on Human Rights and Biomedicine (1997) to create a uniform code of medical ethics for its 47 member-states. The Convention applies international human rights law to medical ethics. It provides special protection of physical integrity for those who are unable to consent, which includes children. 
 No organ or tissue removal may be carried out on a person who does not have the capacity to consent under Article 5.[50]
 As of December 2013, the convention had been ratified or acceded to by twenty-nine member-states of the Council of Europe.[51]
 The United Nations Educational, Scientific and Cultural Organization (UNESCO) also promotes the protection of human rights and human dignity. According to UNESCO, ""Declarations are another means of defining norms, which are not subject to ratification. Like recommendations, they set forth universal principles to which the community of States wished to attribute the greatest possible authority and to afford the broadest possible support."" UNESCO adopted the Universal Declaration on Human Rights and Biomedicine (2005) to advance the application of international human rights law in medical ethics. The Declaration provides special protection of human rights for incompetent persons. 
 In applying and advancing scientific knowledge, medical practice and associated technologies, human vulnerability should be taken into account. Individuals and groups of special vulnerability should be protected and the personal integrity of such individuals respected.[52]
 Individualistic standards of autonomy and personal human rights as they relate to social justice seen in the Anglo-Saxon community, clash with and can also supplement the concept of solidarity, which stands closer to a European healthcare perspective focused on community, universal welfare, and the unselfish wish to provide healthcare equally for all.[53] In the United States individualistic and self-interested healthcare norms are upheld, whereas in other countries, including European countries, a sense of respect for the community and personal support is more greatly upheld in relation to free healthcare.[53]
 The concept of normality, that there is a human physiological standard contrasting with conditions of illness, abnormality and pain, leads to assumptions and bias that negatively affects health care practice.[54] It is important to realize that normality is ambiguous and that ambiguity in healthcare and the acceptance of such ambiguity is necessary in order to practice humbler medicine and understand complex, sometimes unusual usual medical cases.[54] Thus,  society's views on central concepts in philosophy and clinical beneficence must be questioned and revisited, adopting ambiguity as a central player in medical practice.[54]
 Beneficence can come into conflict with non-maleficence when healthcare professionals are deciding between a “first, do no harm” approach vs. a “first, do good” approach, such as when deciding whether or not to operate when the balance between the risk and benefit of the operation is not known and must be estimated. Healthcare professionals who place beneficence below other principles like non-maleficence may decide not to help a patient more than a limited amount if they feel they have met the standard of care and are not morally obligated to provide additional services.  Young and Wagner argued that, in general, beneficence takes priority over non-maleficence (“first, do good,” not “first, do no harm”), both historically and philosophically.[1]
 Autonomy can come into conflict with beneficence when patients disagree with recommendations that healthcare professionals believe are in the patient's best interest. When the patient's interests conflict with the patient's welfare, different societies settle the conflict in a wide range of manners. In general, Western medicine defers to the wishes of a mentally competent patient to make their own decisions, even in cases where the medical team believes that they are not acting in their own best interests. However, many other societies prioritize beneficence over autonomy. People deemed to not be mentally competent or having a mental disorder may be treated involuntarily.
 Examples include when a patient does not want treatment because of, for example, religious or cultural views. In the case of euthanasia, the patient, or relatives of a patient, may want to end the life of the patient. Also, the patient may want an unnecessary treatment, as can be the case in hypochondria or with cosmetic surgery; here, the practitioner may be required to balance the desires of the patient for medically unnecessary potential risks against the patient's informed autonomy in the issue. A doctor may want to prefer autonomy because refusal to respect the patient's self-determination would harm the doctor-patient relationship.
 Organ donations can sometimes pose interesting scenarios, in which a patient is classified as a non-heart beating donor (NHBD), where life support fails to restore the heartbeat and is now considered futile but brain death has not occurred. Classifying a patient as a NHBD can qualify someone to be subject to non-therapeutic intensive care, in which treatment is only given to preserve the organs that will be donated and not to preserve the life of the donor. This can bring up ethical issues as some may see respect for the donors wishes to donate their healthy organs as respect for autonomy, while others may view the sustaining of futile treatment during vegetative state maleficence for the patient and the patient's family. Some are worried making this process a worldwide customary measure may dehumanize and take away from the natural process of dying and what it brings along with it.
 Individuals' capacity for informed decision-making may come into question during resolution of conflicts between autonomy and beneficence. The role of surrogate medical decision-makers is an extension of the principle of autonomy.
 On the other hand, autonomy and beneficence/non-maleficence may also overlap. For example, a breach of patients' autonomy may cause decreased confidence for medical services in the population and subsequently less willingness to seek help, which in turn may cause inability to perform beneficence.
 The principles of autonomy and beneficence/non-maleficence may also be expanded to include effects on the relatives of patients or even the medical practitioners, the overall population and economic issues when making medical decisions.
 There is disagreement among American physicians as to whether the non-maleficence principle excludes the practice of euthanasia. Euthanasia is currently legal in the states of Washington, DC, California, Colorado, Oregon, Vermont, and Washington.[55] Around the world, there are different organizations that campaign to change legislation about the issue of physician-assisted death, or PAD. Examples of such organizations are the Hemlock Society of the United States and the Dignity in Dying campaign in the United Kingdom.[56] These groups believe that doctors should be given the right to end a patient's life only if the patient is conscious enough to decide for themselves, is knowledgeable about the possibility of alternative care, and has willingly asked to end their life or requested access to the means to do so.
 This argument is disputed in other parts of the world. For example, in the state of Louisiana, giving advice or supplying the means to end a person's life is considered a criminal act and can be charged as a felony.[57] In state courts, this crime is comparable to manslaughter.[58] The same laws apply in the states of Mississippi and Nebraska.[59]
 Informed consent refers to a patient's right to receive information relevant to a recommended treatment, in order to be able to make a well-considered, voluntary decision about their care.[60] To give informed consent, a patient must be competent to make a decision regarding their treatment and be presented with relevant information regarding a treatment recommendation, including its nature and purpose, and the burdens, risks and potential benefits of all options and alternatives.[61] After receiving and understanding this information, the patient can then make a fully informed decision to either consent or refuse treatment.[62] In certain circumstances, there can be an exception to the need for informed consent, including, but not limited to, in cases of a medical emergency or patient incompetency.[63] The ethical concept of informed consent also applies in a clinical research setting; all human participants in research must voluntarily decide to participate in the study after being fully informed of all relevant aspects of the research trial necessary to decide whether to participate or not.[64] Informed consent is both an ethical and legal duty; if proper consent is not received prior to a procedure, treatment, or participation in research, providers can be held liable for battery and/or other torts.[65] In the United States, informed consent is governed by both federal and state law, and the specific requirements for obtaining informed consent vary state to state.[66]
 Confidentiality is commonly applied to conversations between doctors and patients.[67] This concept is commonly known as patient-physician privilege. Legal protections prevent physicians from revealing their discussions with patients, even under oath in court.
 Confidentiality is mandated in the United States by the Health Insurance Portability and Accountability Act of 1996 known as HIPAA,[68] specifically the Privacy Rule, and various state laws, some more rigorous than HIPAA. However, numerous exceptions to the rules have been carved out over the years. For example, many states require physicians to report gunshot wounds to the police and impaired drivers to the Department of Motor Vehicles. Confidentiality is also challenged in cases involving the diagnosis of a sexually transmitted disease in a patient who refuses to reveal the diagnosis to a spouse, and in the termination of a pregnancy in an underage patient, without the knowledge of the patient's parents. Many states in the U.S. have laws governing parental notification in underage abortion.[69][70] Those working in mental health have a duty to warn those who they deem to be at risk from their patients in some countries.[71]
 Traditionally, medical ethics has viewed the duty of confidentiality as a relatively non-negotiable tenet of medical practice. More recently, critics like Jacob Appel have argued for a more nuanced approach to the duty that acknowledges the need for flexibility in many cases.[13]
 Confidentiality is an important issue in primary care ethics, where physicians care for many patients from the same family and community, and where third parties often request information from the considerable medical database typically gathered in primary health care.
 In increasing frequency, medical researchers are researching activities in online environments such as discussion boards and bulletin boards, and there is concern that the requirements of informed consent and privacy are not applied, although some guidelines do exist.[72]
 One issue that has arisen, however, is the disclosure of information. While researchers wish to quote from the original source in order to argue a point, this can have repercussions when the identity of the patient is not kept confidential. The quotations and other information about the site can be used to identify the patient, and researchers have reported cases where members of the site, bloggers and others have used this information as 'clues' in a game in an attempt to identify the site.[73] Some researchers have employed various methods of ""heavy disguise.""[73] including discussing a different condition from that under study.[74][75]
 Healthcare institutions' websites have the responsibility to ensure that the private medical records of their online visitors are secure from being marketed and monetized into the hands of drug companies, occupation records, and insurance companies. The delivery of diagnosis online leads patients to believe that doctors in some parts of the country are at the direct service of drug companies, finding diagnosis as convenient as what drug still has patent rights on it.[76] Physicians and drug companies are found to be competing for top ten search engine ranks to lower costs of selling these drugs with little to no patient involvement.[77]
 With the expansion of internet healthcare platforms, online practitioner legitimacy and privacy accountability face unique challenges such as e-paparazzi, online information brokers, industrial spies, unlicensed information providers that work outside of traditional medical codes for profit. The American Medical Association (AMA) states that medical websites have the responsibility to ensure the health care privacy of online visitors and protect patient records from being marketed and monetized into the hands of insurance companies, employers, and marketers. [40] With the rapid unification of healthcare, business practices, computer science and e-commerce to create these online diagnostic websites, efforts to maintain health care system's ethical confidentiality standard need to keep up as well. Over the next few years, the Department of Health and Human Services has stated that they will be working towards lawfully protecting the online privacy and digital transfers of patient Electronic Medical Records (EMR) under The Health Insurance Portability and Accountability Act (HIPAA). [41]. Looking forward, strong governance and accountability mechanisms will need to be considered with respect to digital health ecosystems, including potential metaverse healthcare platforms, to ensure the highest ethical standards are upheld relating to medical confidentiality and patient data.[78]
 In the UK, medical ethics forms part of the training of physicians and surgeons[79] and disregard for ethical principles can result in doctors barred from medical practice after a decision by the Medical Practitioners Tribunal Service.[80]: 32 
 To ensure that appropriate ethical values are being applied within hospitals, effective hospital accreditation requires that ethical considerations are taken into account, for example with respect to physician integrity, conflict of interest, research ethics and organ transplantation ethics.
 There is much documentation of the history and necessity of the Declaration of Helsinki. The first code of conduct for research including medical ethics was the Nuremberg Code. This document had large ties to Nazi war crimes, as it was introduced in 1997, so it didn't make much of a difference in terms of regulating practice. This issue called for the creation of the Declaration. There are some stark differences between the Nuremberg Code and the Declaration of Helsinki, including the way it is written. Nuremberg was written in a very concise manner, with a simple explanation. The Declaration of Helsinki is written with a thorough explanation in mind and including many specific commentaries.[81]
 In the United Kingdom, General Medical Council provides clear overall modern guidance in the form of its 'Good Medical Practice' statement.[82] Other organizations, such as the Medical Protection Society and a number of university departments, are often consulted by British doctors regarding issues relating to ethics.
 Often, simple communication is not enough to resolve a conflict, and a hospital ethics committee must convene to decide a complex matter.
 These bodies are composed primarily of healthcare professionals, but may also include philosophers, lay people, and clergy – indeed, in many parts of the world their presence is considered mandatory in order to provide balance.
 With respect to the expected composition of such bodies in the US, Europe and Australia, the following applies.[83]
 U.S. recommendations suggest that Research and Ethical Boards (REBs) should have five or more members, including at least one scientist, one non-scientist, and one person not affiliated with the institution.[84] The REB should include people knowledgeable in the law and standards of practice and professional conduct.[84] Special memberships are advocated for handicapped or disabled concerns, if required by the protocol under review.
 The European Forum for Good Clinical Practice (EFGCP) suggests that REBs include two practicing physicians who share experience in biomedical research and are independent from the institution where the research is conducted; one lay person; one lawyer; and one paramedical professional, e.g. nurse or pharmacist. They recommend that a quorum include both sexes from a wide age range and reflect the cultural make-up of the local community.
 The 1996 Australian Health Ethics Committee recommendations were entitled, ""Membership Generally of Institutional Ethics Committees"". They suggest a chairperson be preferably someone not employed or otherwise connected with the institution. Members should include a person with knowledge and experience in professional care, counseling or treatment of humans; a minister of religion or equivalent, e.g. Aboriginal elder; a layman; a laywoman; a lawyer and, in the case of a hospital-based ethics committee, a nurse.
 The assignment of philosophers or religious clerics will reflect the importance attached by the society to the basic values involved. An example from Sweden with Torbjörn Tännsjö on a couple of such committees indicates secular trends gaining influence.
 Cultural differences can create difficult medical ethics problems. Some cultures have spiritual or magical theories about the origins and cause of disease, for example, and reconciling these beliefs with the tenets of Western medicine can be very difficult. As different cultures continue to intermingle and more cultures live alongside each other, the healthcare system, which tends to deal with important life events such as birth, death and suffering, increasingly experiences difficult dilemmas that can sometimes lead to cultural clashes and conflict. Efforts to respond in a culturally sensitive manner go hand in hand with a need to distinguish limits to cultural tolerance.[11]
 As more people from different cultural and religious backgrounds move to other countries, among these, the United States, it is becoming increasingly important to be culturally sensitive to all communities in order to provide the best health care for all people.[12] Lack of cultural knowledge can lead to misunderstandings and even inadequate care, which can lead to ethical problems. A common complaint patients have is feeling like they are not being heard, or perhaps, understood.[12] Preventing escalating conflict can be accomplished by seeking interpreters, noticing body language and tone of both yourself and the patient as well as attempting to understand the patient's perspective in order to reach an acceptable option.[12]
 Some believe most medical practitioners in the future will have to be or greatly benefit from being bilingual. In addition to knowing the language, truly understanding culture is best for optimal care.[85] Recently, a practice called 'narrative medicine' has gained some interest as it has a potential for improving patient-physician communication and understanding of patient's perspective. Interpreting a patient's stories or day-to-day activities as opposed to standardizing and collecting patient data may help in acquiring a better sense of what each patient needs, individually, with respect to their illness. Without this background information, many physicians are unable to properly understand the cultural differences that may set two different patients apart, and thus, may diagnose or recommend treatments that are culturally insensitive or inappropriate. In short, patient narrative has the potential for uncovering patient information and preferences that may otherwise be overlooked.
 In order to address the underserved, uneducated communities in need of nutrition, housing, and healthcare disparities seen in much of the world today, some argue that we must fall back on ethical values in order to create a foundation to move towards a reasonable understanding, which encourages commitment and motivation to improve factors causing premature death as a goal in a global community.[14] Such factors – such as poverty, environment and education – are said to be out of national or individual control and so this commitment is by default a social and communal responsibility placed on global communities that are able to aid others in need.[14] This is based on the framework of 'provincial globalism,' which seeks a world in which all people have the capability to be healthy.[14]
 One concern regarding the intersection of medical ethics and humanitarian medical aid is how medical assistance can be as harmful as it is helpful to the community being served. One such example being how political forces may control how foreign humanitarian aid can be utilized in the region it is meant to be provided in. This would be congruous in situations where political strife could lead such aid being used in favor of one group over another. Another example of how foreign humanitarian aid can be misused in its intended community includes the possibility of dissonance forming between a foreign humanitarian aid group and the community being served.[86] Examples of this could include the relationships being viewed between aid workers, style of dress, or the lack of education regarding local culture and customs.[87]
 Humanitarian practices in areas lacking optimum care can also pause other interesting and difficult ethical dilemmas in terms of beneficence and non-maleficence. Humanitarian practices are based upon providing better medical equipment and care for communities whose country does not provide adequate healthcare.[88] The issues with providing healthcare to communities in need may sometimes be religious or cultural backgrounds keeping people from performing certain procedures or taking certain drugs. On the other hand, wanting certain procedures done in a specific manner due to religious or cultural belief systems may also occur. The ethical dilemma stems from differences in culture between communities helping those with medical disparities and the societies receiving aid. Women's rights, informed consent and education about health become controversial, as some treatments needed are against societal law, while some cultural traditions involve procedures against humanitarian efforts.[88] Examples of this are female genital mutilation (FGM), aiding in reinfibulation, providing sterile equipment in order to perform procedures such as FGM, as well as informing patients of their HIV positive testing. The latter is controversial because certain communities have in the past outcast or killed HIV positive individuals.[88]
 Leading causes of death in the United States and around the world are highly related to behavioral consequences over genetic or environmental factors.[89] This leads some to believe true healthcare reform begins with cultural reform, habit and overall lifestyle.[89] Lifestyle, then, becomes the cause of many illnesses and the illnesses themselves are the result or side-effect of a larger problem.[89] Some people believe this to be true and think that cultural change is needed in order for developing societies to cope and dodge the negative effects of drugs, food and conventional modes of transportation available to them.[89] In 1990, tobacco use, diet, and exercise alone accounted for close to 80 percent of all premature deaths and continue to lead in this way through the 21st century.[89] Heart disease, stroke, dementia, and diabetes are some of the diseases that may be affected by habit-forming patterns throughout our life.[89] Some believe that medical lifestyle counseling and building healthy habits around our daily lives is one way to tackle health care reform.[89]
 Buddhist ethics and medicine are based on religious teachings of compassion and understanding[90] of suffering and cause and effect and the idea that there is no beginning or end to life, but that instead there are only rebirths in an endless cycle.[11] In this way, death is merely a phase in an indefinitely lengthy process of life, not an end. However, Buddhist teachings support living one's life to the fullest so that through all the suffering which encompasses a large part of what is life, there are no regrets. Buddhism accepts suffering as an inescapable experience, but values happiness and thus values life.[11] Because of this, suicide and euthanasia, are prohibited. However, attempts to rid oneself of any physical or mental pain and suffering are seen as good acts. On the other hand, sedatives and drugs are thought to impair consciousness and awareness in the dying process, which is believed to be of great importance, as it is thought that one's dying consciousness remains and affects new life. Because of this, analgesics must not be part of the dying process, in order for the dying person to be present entirely and pass on their consciousness wholesomely. This can pose significant conflicts during end of life care in Western medical practice.[11]
 In traditional Chinese philosophy,  human life is believed to be connected to nature, which is thought of as the foundation and encompassing force sustaining all of life's phases.[11] Passing and coming of the seasons, life, birth and death are perceived as a cyclic and perpetual occurrences that are believed to be regulated by the principles of yin and yang.[11] When one dies, the life-giving material force referred to as ch'i, encompassing both body and spirit, rejoins the material force of the universe and cycles on with respect to the rhythms set forth by yin and yang.[11]
 Because many Chinese people believe that circulation of both physical and 'psychic energy' is important to stay healthy, procedures which require surgery, as well as donations and transplantations of organs, are seen as a loss of ch'i, resulting in the loss of someone's vital energy supporting their consciousness and purpose in their lives. Furthermore, a person is never seen as a single unit but rather as a source of relationship, interconnected in a social web.[11] Thus, it is believed that what makes a human one of us is relatedness and communication and family is seen as the basic unit of a community.[11][16] This can greatly affect the way medical decisions are made among family members, as diagnoses are not always expected to be announced to the dying or sick, the elderly are expected to be cared for and represented by their children and physicians are expected to act in a paternalistic way.[11][16] In short, informed consent as well as patient privacy can be difficult to enforce when dealing with Confucian families.[11]
 Furthermore, some Chinese people may be inclined to continue futile treatment in order to extend life and allow for fulfillment of the practice of benevolence and humanity.[11] In contrast, patients with strong Daoist beliefs may see death as an obstacle and dying as a reunion with nature that should be accepted, and are therefore less likely to ask for treatment of an irreversible condition.[11]
 Some believe Islamic medical ethics and framework remain poorly understood by many working in healthcare. It is important to recognize that for people of Islamic faith, Islam envelops and affects all aspects of life, not just medicine.[91] Because many believe it is faith and a supreme deity that hold the cure to illness, it is common that the physician is viewed merely as help or intermediary player during the process of healing or medical care.[91]
 In addition to Chinese culture's emphasis on family as the basic unit of a community intertwined and forming a greater social construct, Islamic traditional medicine also places importance on the values of family and the well-being of a community.[16][91] Many Islamic communities uphold paternalism as an acceptable part of medical care.[91] However, autonomy and self-rule is also valued and protected and, in Islamic medicine, it is particularly upheld in terms of providing and expecting privacy in the healthcare setting. An example of this is requesting same gender providers in order to retain modesty.[91] Overall, Beauchamp's principles of beneficence, non-maleficence and justice[2] are promoted and upheld in the medical sphere with as much importance as in Western culture.[91] In contrast, autonomy is important but more nuanced. Furthermore, Islam also brings forth the principles of jurisprudence, Islamic law and legal maxims, which also allow for Islam to adapt to an ever-changing medical ethics framework.[91]
 Physicians should not allow a conflict of interest to influence medical judgment. In some cases, conflicts are hard to avoid, and doctors have a responsibility to avoid entering such situations. Research has shown that conflicts of interests are very common among both academic physicians[92] and physicians in practice.[93][94]
 Doctors who receive income from referring patients for medical tests have been shown to refer more patients for medical tests.[95] This practice is proscribed by the American College of Physicians Ethics Manual.[96] Fee splitting and the payments of commissions to attract referrals of patients is considered unethical and unacceptable in most parts of the world.[citation needed]
 Studies show that doctors can be influenced by drug company inducements, including gifts and food.[15] Industry-sponsored Continuing Medical Education (CME) programs influence prescribing patterns.[97] Many patients surveyed in one study agreed that physician gifts from drug companies influence prescribing practices.[98] A growing movement among physicians is attempting to diminish the influence of pharmaceutical industry marketing upon medical practice, as evidenced by Stanford University's ban on drug company-sponsored lunches and gifts. Other academic institutions that have banned pharmaceutical industry-sponsored gifts and food include the Johns Hopkins Medical Institutions, University of Michigan, University of Pennsylvania, and Yale University.[99][100]
 The American Medical Association (AMA) states that ""Physicians generally should not treat themselves or members of their immediate family"".[101] This code seeks to protect patients and physicians because professional objectivity can be compromised when the physician is treating a loved one. Studies from multiple health organizations have illustrated that physician-family member relationships may cause an increase in diagnostic testing and costs.[102] Many doctors still treat their family members. Doctors who do so must be vigilant not to create conflicts of interest or treat inappropriately.[103][104] Physicians that treat family members need to be conscious of conflicting expectations and dilemmas when treating relatives, as established medical ethical principles may not be morally imperative when family members are confronted with serious illness.[102][105]
 Sexual relationships between doctors and patients can create ethical conflicts, since sexual consent may conflict with the fiduciary responsibility of the physician.[106] Out of the many disciplines in current medicine, there are studies that have been conducted in order to ascertain the occurrence of Doctor-Patient sexual misconduct. Results from those studies appear to indicate that certain disciplines are more likely to be offenders than others. Psychiatrists and obstetrician-gynecologists, for example, are two disciplines noted for having a higher rate of sexual misconduct.[107] The violation of ethical conduct between doctors and patients also has an association with the age and sex of doctor and patient. Male physicians aged 40–59 years have been found to be more likely to have been reported for sexual misconduct; women aged 20–39 have been found to make up a significant portion of reported victims of sexual misconduct.[108] Doctors who enter into sexual relationships with patients face the threats of losing their medical license and prosecution. In the early 1990s, it was estimated that 2–9% of doctors had violated this rule.[109] Sexual relationships between physicians and patients' relatives may also be prohibited in some jurisdictions, although this prohibition is highly controversial.[110]
 In some hospitals, medical futility is referred to as treatment that is unable to benefit the patient.[111] An important part of practicing good medical ethics is by attempting to avoid futility by practicing non-maleficence.[111] What should be done if there is no chance that a patient will survive or benefit from a potential treatment but the family members insist on advanced care?[111] Previously, some articles defined futility as the patient having less than a one percent chance of surviving. Some of these cases are examined in court.
 Advance directives include living wills and durable powers of attorney for health care. (See also Do Not Resuscitate and cardiopulmonary resuscitation) In many cases, the ""expressed wishes"" of the patient are documented in these directives, and this provides a framework to guide family members and health care professionals in the decision-making process when the patient is incapacitated. Undocumented expressed wishes can also help guide decisions in the absence of advance directives, as in the Quinlan case in Missouri.
 ""Substituted judgment"" is the concept that a family member can give consent for treatment if the patient is unable (or unwilling) to give consent themselves. The key question for the decision-making surrogate is not, ""What would you like to do?"", but instead, ""What do you think the patient would want in this situation?"".
 Courts have supported family's arbitrary definitions of futility to include simple biological survival, as in the Baby K case (in which the courts ordered a child born with only a brain stem instead of a complete brain to be kept on a ventilator based on the religious belief that all life must be preserved).
 Baby Doe Law establishes state protection for a disabled child's right to life, ensuring that this right is protected even over the wishes of parents or guardians in cases where they want to withhold treatment.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['a different condition from that under study', 'Jacob Appel', 'non-maleficence', 'birth, death and suffering', '""What do you think the patient would want in this situation?""'], 'answer_start': [], 'answer_end': []}"
"
 Forensic science, also known as criminalistics,[1] is the application of science principles and methods to support legal decision-making in matters of criminal and civil law.
 During criminal investigation in particular, it is governed by the legal standards of admissible evidence and criminal procedure. It is a broad field utilizing numerous practices such as the analysis of DNA, fingerprints, bloodstain patterns, firearms, ballistics, toxicology, microscopy and fire debris analysis.
 Forensic scientists collect, preserve, and analyze evidence during the course of an investigation. While some forensic scientists travel to the scene of the crime to collect the evidence themselves, others occupy a laboratory role, performing analysis on objects brought to them by other individuals.[2] Others are involved in analysis of financial, banking, or other numerical data for use in financial crime investigation, and can be employed as consultants from private firms, academia, or as government employees.[3]
 In addition to their laboratory role, forensic scientists testify as expert witnesses in both criminal and civil cases and can work for either the prosecution or the defense. While any field could technically be forensic, certain sections have developed over time to encompass the majority of forensically related cases.[4]
 The term forensic stems from the Latin word, forēnsis (3rd declension, adjective), meaning ""of a forum, place of assembly"".[5] The history of the term originates in Roman times, when a criminal charge meant presenting the case before a group of public individuals in the forum. Both the person accused of the crime and the accuser would give speeches based on their sides of the story. The case would be decided in favor of the individual with the best argument and delivery. This origin is the source of the two modern usages of the word forensic—as a form of legal evidence; and as a category of public presentation.[6]
 In modern use, the term forensics is often used in place of ""forensic science.""
 The word ""science"", is derived from the Latin word for 'knowledge' and is today closely tied to the scientific method, a systematic way of acquiring knowledge. Taken together, forensic science means the use of scientific methods and processes for crime solving.
 The ancient world lacked standardized forensic practices, which enabled criminals to escape punishment. Criminal investigations and trials relied heavily on forced confessions and witness testimony. However, ancient sources do contain several accounts of techniques that foreshadow concepts in forensic science developed centuries later.[7]
 The first written account of using medicine and entomology to solve criminal cases is attributed to the book of Xi Yuan Lu (translated as Washing Away of Wrongs[8][9]), written in China in 1248 by Song Ci (宋慈, 1186–1249), a director of justice, jail and supervision,[10] during the Song dynasty.
 Song Ci introduced regulations concerning autopsy reports to court,[11] how to protect the evidence in the examining process, and explained why forensic workers must demonstrate impartiality to the public.[12] He devised methods for making antiseptic and for promoting the reappearance of hidden injuries to dead bodies and bones (using sunlight and vinegar under a red-oil umbrella);[13] for calculating the time of death (allowing for weather and insect activity);[14] described how to wash and examine the dead body to ascertain the reason for death.[15] At that time the book had described methods for distinguishing between suicide and faked suicide.[16] He wrote the book on forensics stating that all wounds or dead bodies should be examined, not avoided. The book became the first form of literature to help determine the cause of death.[17]
 In one of Song Ci's accounts (Washing Away of Wrongs), the case of a person murdered with a sickle was solved by an investigator who instructed each suspect to bring his sickle to one location. (He realized it was a sickle by testing various blades on an animal carcass and comparing the wounds.) Flies, attracted by the smell of blood, eventually gathered on a single sickle. In light of this, the owner of that sickle confessed to the murder. The book also described how to distinguish between a drowning (water in the lungs) and strangulation (broken neck cartilage), and described evidence from examining corpses to determine if a death was caused by murder, suicide or accident.[18]
 Methods from around the world involved saliva and examination of the mouth and tongue to determine innocence or guilt, as a precursor to the Polygraph test. In ancient India,[19] some suspects were made to fill their mouths with dried rice and spit it back out. Similarly, in ancient China, those accused of a crime would have rice powder placed in their mouths.[20] In ancient middle-eastern cultures, the accused were made to lick hot metal rods briefly. It is thought that these tests had some validity[21] since a guilty person would produce less saliva and thus have a drier mouth;[22] the accused would be considered guilty if rice was sticking to their mouths in abundance or if their tongues were severely burned due to lack of shielding from saliva.[23]
 Initial glance, forensic intelligence may appear as a nascent facet of forensic science facilitated by advancements in information technologies such as computers, databases, and data-flow management software. However, a more profound examination reveals that forensic intelligence represents a genuine and emerging inclination among forensic practitioners to actively participate in investigative and policing strategies. In doing so, it elucidates existing practices within scientific literature, advocating for a paradigm shift from the prevailing conception of forensic science as a conglomerate of disciplines merely aiding the criminal justice system. Instead, it urges a perspective that views forensic science as a discipline studying the informative potential of traces—remnants of criminal activity. Embracing this transformative shift poses a significant challenge for education, necessitating a shift in learners' mindset to accept concepts and methodologies in forensic intelligence.[24]
 Recent calls advocating for the integration of forensic scientists into the criminal justice system, as well as policing and intelligence missions, underscore the necessity for the establishment of educational and training initiatives in the field of forensic intelligence. This article contends that a discernible gap exists between the perceived and actual comprehension of forensic intelligence among law enforcement and forensic science managers, positing that this asymmetry can be rectified only through educational interventions [25]
 The primary challenge in forensic intelligence education and training is identified as the formulation of programs aimed at heightening awareness, particularly among managers, to mitigate the risk of making suboptimal decisions in information processing. The paper highlights two recent European courses as exemplars of educational endeavors, elucidating lessons learned and proposing future directions at an initial glance, forensic intelligence may appear as a nascent facet of forensic science facilitated by advancements in information technologies such as computers, databases, and data-flow management software. 
 However, a more profound examination reveals that forensic intelligence represents a genuine and emerging inclination among forensic practitioners to actively participate in investigative and policing strategies. In doing so, it elucidates existing practices within scientific literature, advocating for a paradigm shift from the prevailing conception of forensic science as a conglomerate of disciplines merely aiding the criminal justice system. Instead, it urges a perspective that views forensic science as a discipline studying the informative potential of traces—remnants of criminal activity. Embracing this transformative shift poses a significant challenge for education, necessitating a shift in learners' mindset to accept concepts and methodologies in forensic intelligence. 
 The overarching conclusion is that the heightened focus on forensic intelligence has the potential to rejuvenate a proactive approach to forensic science, enhance quantifiable efficiency, and foster greater involvement in investigative and managerial decision-making. A novel educational challenge is articulated for forensic science university programs worldwide: a shift in emphasis from a fragmented criminal trace analysis to a more comprehensive security problem-solving approach.
 In 16th-century Europe, medical practitioners in army and university settings began to gather information on the cause and manner of death. Ambroise Paré, a French army surgeon, systematically studied the effects of violent death on internal organs.[26][27] Two Italian surgeons, Fortunato Fidelis and Paolo Zacchia, laid the foundation of modern pathology by studying changes that occurred in the structure of the body as the result of disease.[28]  In the late 18th century, writings on these topics began to appear. These included A Treatise on Forensic Medicine and Public Health by the French physician Francois Immanuele Fodéré[29] and The Complete System of Police Medicine by the German medical expert Johann Peter Frank.[30]
 As the rational values of the Enlightenment era increasingly permeated society in the 18th century, criminal investigation became a more evidence-based, rational procedure − the use of torture to force confessions was curtailed, and belief in witchcraft and other powers of the occult largely ceased to influence the court's decisions. Two examples of English forensic science in individual legal proceedings demonstrate the increasing use of logic and procedure in criminal investigations at the time. In 1784, in Lancaster, John Toms was tried and convicted for murdering Edward Culshaw with a pistol. When the dead body of Culshaw was examined, a pistol wad (crushed paper used to secure powder and balls in the muzzle) found in his head wound matched perfectly with a torn newspaper found in Toms's pocket, leading to the conviction.[31]
 In Warwick 1816, a farm laborer was tried and convicted of the murder of a young maidservant. She had been drowned in a shallow pool and bore the marks of violent assault. The police found footprints and an impression from corduroy cloth with a sewn patch in the damp earth near the pool. There were also scattered grains of wheat and chaff. The breeches of a farm labourer who had been threshing wheat nearby were examined and corresponded exactly to the impression in the earth near the pool.[32]
 An article appearing in Scientific American in 1885 describes the use of microscopy to distinguish between the blood of two persons in a criminal case in Chicago.[33]
 Chromatography is a common technique used in the field of Forensic Science. Chromatography is a method of separating the components of a mixture from a mobile phase.[34] Chromatography is an essential tool used in forensic science, helping analysts identify and compare trace amounts of samples including ignitable liquids, drugs, and biological samples. Many laboratories utilize gas chromatography/mass spectrometry (GC/MS) to examine these kinds of samples; this analysis provides rapid and reliant data to identify samples in question.[35]
 A method for detecting arsenious oxide, simple arsenic, in corpses was devised in 1773 by the Swedish chemist, Carl Wilhelm Scheele.[36] His work was expanded upon, in 1806, by German chemist Valentin Ross, who learned to detect the poison in the walls of a victim's stomach.[37]
 James Marsh was the first to apply this new science to the art of forensics. He was called by the prosecution in a murder trial to give evidence as a chemist in 1832. The defendant, John Bodle, was accused of poisoning his grandfather with arsenic-laced coffee. Marsh performed the standard test by mixing a suspected sample with hydrogen sulfide and hydrochloric acid. While he was able to detect arsenic as yellow arsenic trisulfide, when it was shown to the jury it had deteriorated, allowing the suspect to be acquitted due to reasonable doubt.[38]
 Annoyed by that, Marsh developed a much better test. He combined a sample containing arsenic with sulfuric acid and arsenic-free zinc, resulting in arsine gas. The gas was ignited, and it decomposed to pure metallic arsenic, which, when passed to a cold surface, would appear as a silvery-black deposit.[39] So sensitive was the test, known formally as the Marsh test, that it could detect as little as one-fiftieth of a milligram of arsenic. He first described this test in The Edinburgh Philosophical Journal in 1836.[40]
 Ballistics is ""the science of the motion of projectiles in flight"".[41] In forensic science, analysts examine the patterns left on bullets and cartridge casings after being ejected from a weapon. When fired, a bullet is left with indentations and markings that are unique to the barrel and firing pin of the firearm that ejected the bullet. This examination can help scientists identify possible makes and models of weapons connected to a crime.
 Henry Goddard at Scotland Yard pioneered the use of bullet comparison in 1835. He noticed a flaw in the bullet that killed the victim and was able to trace this back to the mold that was used in the manufacturing process.[42]
 The French police officer Alphonse Bertillon was the first to apply the anthropological technique of anthropometry to law enforcement, thereby creating an identification system based on physical measurements. Before that time, criminals could be identified only by name or photograph.[43][44] Dissatisfied with the ad hoc methods used to identify captured criminals in France in the 1870s, he began his work on developing a reliable system of anthropometrics for human classification.[45]
 Bertillon created many other forensics techniques, including forensic document examination, the use of galvanoplastic compounds to preserve footprints, ballistics, and the dynamometer, used to determine the degree of force used in breaking and entering. Although his central methods were soon to be supplanted by fingerprinting, ""his other contributions like the mug shot and the systematization of crime-scene photography remain in place to this day.""[44]
 Sir William Herschel was one of the first to advocate the use of fingerprinting in the identification of criminal suspects. While working for the Indian Civil Service, he began to use thumbprints on documents as a security measure to prevent the then-rampant repudiation of signatures in 1858.[46]
 In 1877 at Hooghly (near Kolkata), Herschel instituted the use of fingerprints on contracts and deeds, and he registered government pensioners' fingerprints to prevent the collection of money by relatives after a pensioner's death.[47]
 In 1880, Henry Faulds, a Scottish surgeon in a Tokyo hospital, published his first paper on the subject in the scientific journal Nature, discussing the usefulness of fingerprints for identification and proposing a method to record them with printing ink. He established their first classification and was also the first to identify fingerprints left on a vial.[48] Returning to the UK in 1886, he offered the concept to the Metropolitan Police in London, but it was dismissed at that time.[49]
 Faulds wrote to Charles Darwin with a description of his method, but, too old and ill to work on it, Darwin gave the information to his cousin, Francis Galton, who was interested in anthropology. Having been thus inspired to study fingerprints for ten years, Galton published a detailed statistical model of fingerprint analysis and identification and encouraged its use in forensic science in his book Finger Prints. He had calculated that the chance of a ""false positive"" (two different individuals having the same fingerprints) was about 1 in 64 billion.[50]
 Juan Vucetich, an Argentine chief police officer, created the first method of recording the fingerprints of individuals on file. In 1892, after studying Galton's pattern types, Vucetich set up the world's first fingerprint bureau. In that same year, Francisca Rojas of Necochea was found in a house with neck injuries whilst her two sons were found dead with their throats cut. Rojas accused a neighbour, but despite brutal interrogation, this neighbour would not confess to the crimes. Inspector Alvarez, a colleague of Vucetich, went to the scene and found a bloody thumb mark on a door. When it was compared with Rojas' prints, it was found to be identical with her right thumb. She then confessed to the murder of her sons.
 A Fingerprint Bureau was established in Calcutta (Kolkata), India, in 1897, after the Council of the Governor General approved a committee report that fingerprints should be used for the classification of criminal records. Working in the Calcutta Anthropometric Bureau, before it became the Fingerprint Bureau, were Azizul Haque and Hem Chandra Bose. Haque and Bose were Indian fingerprint experts who have been credited with the primary development of a fingerprint classification system eventually named after their supervisor, Sir Edward Richard Henry.[51][52] The Henry Classification System, co-devised by Haque and Bose, was accepted in England and Wales when the first United Kingdom Fingerprint Bureau was founded in Scotland Yard, the Metropolitan Police headquarters, London, in 1901. Sir Edward Richard Henry subsequently achieved improvements in dactyloscopy.[53]
 In the United States, Henry P. DeForrest used fingerprinting in the New York Civil Service in 1902, and by December 1905, New York City Police Department Deputy Commissioner Joseph A. Faurot, an expert in the Bertillon system and a fingerprint advocate at Police Headquarters, introduced the fingerprinting of criminals to the United States.[54]
 The Uhlenhuth test, or the antigen–antibody precipitin test for species, was invented by Paul Uhlenhuth in 1901 and could distinguish human blood from animal blood, based on the discovery that the blood of different species had one or more characteristic proteins. The test represented a major breakthrough and came to have tremendous importance in forensic science.[55] The test was further refined for forensic use by the Swiss chemist Maurice Müller in the  year 1960s.[56]
 Forensic DNA analysis was first used in 1984. It was developed by Sir Alec Jeffreys, who realized that variation in the genetic sequence could be used to identify individuals and to tell individuals apart from one another. The first application of DNA profiles was used by Jeffreys in a double murder mystery in the small English town of Narborough, Leicestershire, in 1985. A 15-year-old school girl by the name of Lynda Mann was raped and murdered in Carlton Hayes psychiatric hospital. The police did not find a suspect but were able to obtain a semen sample.
 In 1986, Dawn Ashworth, 15 years old, was also raped and strangled in the nearby village of Enderby. Forensic evidence showed that both killers had the same blood type. Richard Buckland became the suspect because he worked at Carlton Hayes psychiatric hospital, had been spotted near Dawn Ashworth's murder scene and knew unreleased details about the body. He later confessed to Dawn's murder but not Lynda's.  Jefferys was brought into the case to analyze the semen samples. He concluded that there was no match between the samples and Buckland, who became the first person to be exonerated using DNA. Jefferys confirmed that the DNA profiles were identical for the two murder semen samples. To find the perpetrator, DNA samples from the entire male population, more than 4,000 aged from 17 to 34, of the town were collected. They all were compared to semen samples from the crime.  A friend of Colin Pitchfork was heard saying that he had given his sample to the police claiming to be Colin. Colin Pitchfork was arrested in 1987 and it was found that his DNA profile matched the semen samples from the murder.
 Because of this case, DNA databases were developed. There is the national (FBI) and international databases as well as the European countries (ENFSI: European Network of Forensic Science Institutes). These searchable databases are used to match crime scene DNA profiles to those already in a database.[57]
 By the turn of the 20th century, the science of forensics had become largely established in the sphere of criminal investigation. Scientific and surgical investigation was widely employed by the Metropolitan Police during their pursuit of the mysterious Jack the Ripper, who had killed a number of women in the 1880s. This case is a watershed in the application of forensic science. Large teams of policemen conducted house-to-house inquiries throughout Whitechapel. Forensic material was collected and examined. Suspects were identified, traced and either examined more closely or eliminated from the inquiry. Police work follows the same pattern today.[58] Over 2000 people were interviewed, ""upwards of 300"" people were investigated, and 80 people were detained.[59]
 The investigation was initially conducted by the Criminal Investigation Department (CID), headed by Detective Inspector Edmund Reid. Later, Detective Inspectors Frederick Abberline, Henry Moore, and Walter Andrews were sent from Central Office at Scotland Yard to assist. Initially, butchers, surgeons and physicians were suspected because of the manner of the mutilations. The alibis of local butchers and slaughterers were investigated, with the result that they were eliminated from the inquiry.[60] Some contemporary figures thought the pattern of the murders indicated that the culprit was a butcher or cattle drover on one of the cattle boats that plied between London and mainland Europe. Whitechapel was close to the London Docks,[61] and usually such boats docked on Thursday or Friday and departed on Saturday or Sunday.[62] The cattle boats were examined, but the dates of the murders did not coincide with a single boat's movements, and the transfer of a crewman between boats was also ruled out.[63]
 At the end of October, Robert Anderson asked police surgeon Thomas Bond to give his opinion on the extent of the murderer's surgical skill and knowledge.[64] The opinion offered by Bond on the character of the ""Whitechapel murderer"" is the earliest surviving offender profile.[65] Bond's assessment was based on his own examination of the most extensively mutilated victim and the post mortem notes from the four previous canonical murders.[66] In his opinion the killer must have been a man of solitary habits, subject to ""periodical attacks of homicidal and erotic mania"", with the character of the mutilations possibly indicating ""satyriasis"".[66] Bond also stated that ""the homicidal impulse may have developed from a revengeful or brooding condition of the mind, or that religious mania may have been the original disease but I do not think either hypothesis is likely"".[66]
 Handbook for Coroners, police officials, military policemen was written by the Austrian criminal jurist Hans Gross in 1893, and is generally acknowledged as the birth of the field of criminalistics. The work combined in one system fields of knowledge that had not been previously integrated, such as psychology and physical science, and which could be successfully used against crime. Gross adapted some fields to the needs of criminal investigation, such as crime scene photography. He went on to found the Institute of Criminalistics in 1912, as part of the University of Graz' Law School. This Institute was followed by many similar institutes all over the world.[67]
 In 1909, Archibald Reiss founded the Institut de police scientifique of the University of Lausanne (UNIL), the first school of forensic science in the world. Dr. Edmond Locard, became known as the ""Sherlock Holmes of France"". He formulated the basic principle of forensic science: ""Every contact leaves a trace"", which became known as Locard's exchange principle. In 1910, he founded what may have been the first criminal laboratory in the world, after persuading the Police Department of Lyon (France) to give him two attic rooms and two assistants.[68]
 Symbolic of the newfound prestige of forensics and the use of reasoning in detective work was the popularity of the fictional character Sherlock Holmes, written by Arthur Conan Doyle in the late 19th century. He remains a great inspiration for forensic science, especially for the way his acute study of a crime scene yielded small clues as to the precise sequence of events. He made great use of trace evidence such as shoe and tire impressions, as well as fingerprints, ballistics and handwriting analysis, now known as questioned document examination.[69] Such evidence is used to test theories conceived by the police, for example, or by the investigator himself.[70] All of the techniques advocated by Holmes later became reality, but were generally in their infancy at the time Conan Doyle was writing. In many of his reported cases, Holmes frequently complains of the way the crime scene has been contaminated by others, especially by the police, emphasising the critical importance of maintaining its integrity, a now well-known feature of crime scene examination. He used analytical chemistry for blood residue analysis as well as toxicology examination and determination for poisons. He used ballistics by measuring bullet calibres and matching them with a suspected murder weapon.[71]
 Hans Gross applied scientific methods to crime scenes and was responsible for the birth of criminalistics.
 Edmond Locard expanded on Gross' work with Locard's Exchange Principle which stated ""whenever two objects come into contact with one another, materials are exchanged between them"". This means that every contact by a criminal leaves a trace.
 Alexander Lacassagne, who taught Locard, produced autopsy standards on actual forensic cases.
 Alphonse Bertillon was a French criminologist and founder of Anthropometry (scientific study of measurements and proportions of the human body). He used anthropometry for identification, stating that, since each individual is unique, by measuring aspects of physical difference there could be a personal identification system. He created the Bertillon System around 1879, a way of identifying criminals and citizens by measuring 20 parts of the body. In 1884, over 240 repeat offenders were caught using the Bertillon system, but the system was largely superseded by fingerprinting.
 Frances Glessner Lee, known as ""the mother of forensic science"",[72] was instrumental in the development of forensic science in the US. She lobbied to have coroners replaced by medical professionals, endowed the Harvard Associates in Police Science, and conducted many seminars to educate homicide investigators. She also created the Nutshell Studies of Unexplained Death, intricate crime scene dioramas used to train investigators, which are still in use today.
 Later in the 20th century several British pathologists, Mikey Rochman, Francis Camps, Sydney Smith and Keith Simpson pioneered new forensic science methods. Alec Jeffreys pioneered the use of DNA profiling in forensic science in 1984. He realized the scope of DNA fingerprinting, which uses variations in the genetic code to identify individuals. The method has since become important in forensic science to assist police detective work, and it has also proved useful in resolving paternity and immigration disputes.[73] DNA fingerprinting was first used as a police forensic test to identify the rapist and killer of two teenagers, Lynda Mann and Dawn Ashworth, who were both murdered in Narborough, Leicestershire, in 1983 and 1986 respectively. Colin Pitchfork was identified and convicted of murder after samples taken from him matched semen samples taken from the two dead girls.
 Forensic science has been fostered by a number of national and international forensic science learned bodies including the American Academy of Forensic Sciences (founded 1948), publishers of the Journal of Forensic Sciences;[74] the Canadian Society of Forensic Science (founded 1953), publishers of the Journal of the Canadian Society of Forensic Science; the Chartered Society of Forensic Sciences,[75] (founded 1959), then known as the Forensic Science Society, publisher of Science & Justice;[76] the British Academy of Forensic Sciences[77] (founded 1960), publishers of Medicine, Science and the Law;[78] the Australian Academy of Forensic Sciences (founded 1967), publishers of the Australian Journal of Forensic Sciences; and the European Network of Forensic Science Institutes (founded 1995).
 In the past decade, documenting forensics scenes has become more efficient. Forensic scientists have started using laser scanners, drones and photogrammetry to obtain 3D point clouds of accidents or crime scenes. Reconstruction of an accident scene on a highway using drones involves data acquisition time of only 10–20 minutes and can be performed without shutting down traffic. The results are not just accurate, in centimeters, for measurement to be presented in court but also easy to digitally preserve in the long term.[79]
Now, in the 21st century, much of forensic science's future is up for discussion. The National Institute of Standards and Technology (NIST) has several forensic science-related programs: CSAFE, a NIST Center of Excellence in Forensic Science, the National Commission on Forensic Science (now concluded), and administration of the Organization of Scientific Area Committees for Forensic Science (OSAC).[80] One of the more recent additions by NIST is a document called NISTIR-7941, titled ""Forensic Science Laboratories: Handbook for Facility Planning, Design, Construction, and Relocation"". The handbook provides a clear blueprint for approaching forensic science. The details even include what type of staff should be hired for certain positions.[81]
 Some forensic techniques, believed to be scientifically sound at the time they were used, have turned out later to have much less scientific merit or none.[89] Some such techniques include:
 ""Litigation science"" describes analysis or data developed or produced expressly for use in a trial versus those produced in the course of independent research. This distinction was made by the U.S. 9th Circuit Court of Appeals when evaluating the admissibility of experts.[98]
 This uses demonstrative evidence, which is evidence created in preparation of trial by attorneys or paralegals.
 In the United States there are over 17,200 forensic science technicians as of 2019.[99]
 Real-life crime scene investigators and forensic scientists warn that popular television shows do not give a realistic picture of the work, often wildly distorting its nature, and exaggerating the ease, speed, effectiveness, drama, glamour, influence and comfort level of their jobs—which they describe as far more mundane, tedious and boring.[100][101]
 Some claim these modern TV shows have changed individuals' expectations of forensic science, sometimes unrealistically—an influence termed the ""CSI effect"".[102][103]
 Further, research has suggested that public misperceptions about criminal forensics can create, in the mind of a juror, unrealistic expectations of forensic evidence—which they expect to see before convicting—implicitly biasing the juror towards the defendant. Citing the ""CSI effect,"" at least one researcher has suggested screening jurors for their level of influence from such TV programs.[103]
 Questions about certain areas of forensic science, such as fingerprint evidence and the assumptions behind these disciplines have been brought to light in some publications[104][105] including the New York Post.[106] The article stated that ""No one has proved even the basic assumption: That everyone's fingerprint is unique.""[106] The article also stated that ""Now such assumptions are being questioned—and with it may come a radical change in how forensic science is used by police departments and prosecutors.""[106] Law professor Jessica Gabel said on NOVA that forensic science ""lacks the rigors, the standards, the quality controls and procedures that we find, usually, in science"".[107]
 The National Institute of Standards and Technology has reviewed the scientific foundations of bite-mark analysis used in forensic science. Bite mark analysis is a forensic science technique that analyzes the marks on the victim's skin compared to the suspects teeth. [108]  NIST reviewed the findings of the National Academies of Sciences, Engineering, and Medicine 2009 study. The National Academics of Sciences, Engineering, and Medicine conducted research to address the issues of reliability, accuracy, and reliability of bitemark analysis, where they concluded that there is a lack of sufficient scientific foundation to support the data.[109] Yet the technique is still legal to use in court as evidence. NIST funded a 2019 meeting that consisted of dentists, lawyers, researchers and others to address the gaps in this field. [110]
 In the US, on 25 June 2009, the Supreme Court issued a 5-to-4 decision in Melendez-Diaz v. Massachusetts stating that crime laboratory reports may not be used against criminal defendants at trial unless the analysts responsible for creating them give testimony and subject themselves to cross-examination.[111]  The Supreme Court cited the National Academies of Sciences report Strengthening Forensic Science in the United States[112] in their decision. Writing for the majority, Justice Antonin Scalia referred to the National Research Council report in his assertion that ""Forensic evidence is not uniquely immune from the risk of manipulation.""
 In the US, another area of forensic science that has come under question in recent years is the lack of laws requiring the accreditation of forensic labs.  Some states require accreditation, but some states do not.  Because of this,[113][114] many labs have been caught performing very poor work resulting in false convictions or acquittals.  For example, it was discovered after an audit of the Houston Police Department in 2002 that the lab had fabricated evidence which led George Rodriguez being convicted of raping a fourteen-year-old girl.[115]  The former director of the lab, when asked, said that the total number of cases that could have been contaminated by improper work could be in the range of 5,000 to 10,000.[115]
 The Innocence Project[116] database of DNA exonerations shows that many wrongful convictions contained forensic science errors. According to the Innocence project and the US Department of Justice, forensic science has contributed to about 39 percent to 46 percent of wrongful convictions. [117] As indicated by the National Academy of Sciences report Strengthening Forensic Sciences in the United States,[112] part of the problem is that many traditional forensic sciences have never been empirically validated; and part of the problem is that all examiners are subject to forensic confirmation biases and should be shielded from contextual information not relevant to the judgment they make.
 Many studies have discovered a difference in rape-related injuries reporting based on race, with white victims reporting a higher frequency of injuries than black victims.[118] However, since current forensic examination techniques may not be sensitive to all injuries across a range of skin colors, more research needs to be conducted to understand if this trend is due to skin confounding healthcare providers when examining injuries or if darker skin extends a protective element.[118] In clinical practice, for patients with darker skin, one study recommends that attention must be paid to the thighs, labia majora, posterior fourchette and fossa navicularis, so that no rape-related injuries are missed upon close examination.[118]
 The International Committee of the Red Cross (ICRC) uses forensic science for humanitarian purposes to clarify the fate of missing persons after armed conflict, disasters or migration,[119] and is one of the services related to Restoring Family Links and Missing Persons. Knowing what has happened to a missing relative can often make it easier to proceed with the grieving process and move on with life for families of missing persons.
 Forensic science is used by various other organizations to clarify the fate and whereabouts of persons who have gone missing. Examples include the NGO Argentine Forensic Anthropology Team, working to clarify the fate of people who disappeared during the period of the 1976–1983 military dictatorship. The International Commission on Missing Persons (ICMP) used forensic science to find missing persons,[120] for example after the conflicts in the Balkans.[121]
 Recognising the role of forensic science for humanitarian purposes, as well as the importance of forensic investigations in fulfilling the state's responsibilities to investigate human rights violations, a group of experts in the late-1980s devised a UN Manual on the Prevention and Investigation of Extra-Legal, Arbitrary and Summary Executions, which became known as the Minnesota Protocol. This document was revised and re-published by the Office of the High Commissioner for Human Rights in 2016.[122]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['criminal and civil law', 'Lynda Mann and Dawn Ashworth', 'Roman times', 'paternity and immigration disputes', 'adapted some fields to the needs of criminal investigation'], 'answer_start': [], 'answer_end': []}"
"
 Criminology (from Latin crimen, ""accusation"", and Ancient Greek -λογία, -logia, from λόγος logos meaning: ""word, reason"") is the interdisciplinary study of crime and deviant behaviour.[1] Criminology is a multidisciplinary field in both the behavioural and social sciences, which draws primarily upon the research of sociologists, political scientists, economists, legal sociologists, psychologists, philosophers, psychiatrists, social workers, biologists, social anthropologists, scholars of law and jurisprudence, as well as the processes that define administration of justice and the criminal justice system.
 Criminologists are individuals who engage in the exploration and investigation of the intersection between crime and society's reactions to it. Certain criminologists delve into the behavioral trends of potential offenders. In a broader sense, these professionals undertake research and inquiries, formulating hypotheses, and scrutinizing observable trends in a systematic manner.
 The interests of criminologists include the study of nature of crime and criminals, origins of criminal law, etiology of crime, social reaction to crime, and the functioning of law enforcement agencies and the penal institutions. It can be broadly said that criminology directs its inquiries along three lines: first, it investigates the nature of criminal law and its administration and conditions under which it develops; second, it analyzes the causation of crime and the personality of criminals; and third, it studies the control of crime and the rehabilitation of offenders. Thus, criminology includes within its scope the activities of legislative bodies, law-enforcement agencies, judicial institutions, correctional institutions and educational, private and public social agencies.
 Modern academic criminology has direct roots in the 19th-century Italian School of ""criminal anthropology"", which according to the historian Mary Gibson ""caused a radical refocusing of criminological discussion throughout Europe and the United States from law to the criminal. While this 'Italian School' was in turn attacked and partially supplanted in countries such as France by 'sociological' theories of delinquency, they retained the new focus on the criminal.""[2] According to Gibson, the term criminology was most likely coined in 1885 by Italian law professor Raffaele Garofalo as Criminologia [it].[3] In the late 19th century, French anthropologist Paul Topinard used the analogous French term Criminologie [fr].[4]
 Criminology grew substantially as a discipline in the first quarter of the twentieth century. From 1900 through to 2000 this field of research underwent three significant phases in the United States: (1) Golden Age of Research (1900–1930) which has been described as a multiple-factor approach, (2) Golden Age of Theory (1930–1960) which endeavored to show the limits of systematically connecting criminological research to theory, and (3) a 1960–2000 period, which was seen as a significant turning point for criminology.[5]
 There were three main schools of thought in early criminological theory, spanning the period from the mid-18th century to the mid-twentieth century: Classical, Positivist, and Chicago. These schools of thought were superseded by several contemporary paradigms of criminology, such as the sub-culture, control, strain, labelling, critical criminology, cultural criminology, postmodern criminology, feminist criminology, Queer criminology, and others discussed below.
 The Classical school arose in the mid-18th century and reflects ideas from utilitarian philosophy.[6] Cesare Beccaria,[7] author of On Crimes and Punishments (1763–64), Jeremy Bentham (inventor of the panopticon), and other early criminological philosophers proposed ideas including:[8]
 This school developed during a major reform in penology when society began designing prisons for the sake of extreme punishment. This period also saw many legal reforms, the French Revolution, and the development of the legal system in the United States.[10]
 The Positivist school argues criminal behaviour comes from internal and external factors out of the individual's control. Its key method of thought is that criminals are born as criminals and not made into them;[11] this school of thought also supports theory of nature in the debate between nature versus nurture. They also argue that criminal behavior is innate and within a person. Philosophers within this school applied the scientific method to study human behavior. Positivism comprises three segments: biological, psychological and social positivism.[12]
 Psychological Positivism is the concept that criminal acts or the people doing said crimes do them because of internal factors driving them.
 Social Positivism, which is often referred to as Sociological Positivism, discusses the thought process that criminals are produced by society. This school claims that low income levels, high poverty/unemployment rates, and poor educational systems create and motivate criminals.[13]
 The notion of having a criminal personality is achieved from the school of thought of psychological positivism. It essentially means that parts of an individual's personality have traits that align with many of those possessed by criminals, such as neuroticism, anti-social tendencies, aggressive behaviors, and other factors. There is evidence of correlation, but not causation, between these personality traits and criminal actions.[14][15][16][17][18][19][20][21]
 Cesare Lombroso (1835–1909), an Italian sociologist working in the late 19th century, is often called ""the father of criminology"".[22] He was one of the key contributors to biological positivism and founded the Italian school of criminology.[23] Lombroso took a scientific approach, insisting on empirical evidence for studying crime.[24]  He suggested physiological traits such as the measurements of cheekbones or hairline, or a cleft palate could indicate ""atavistic"" criminal tendencies. This approach, whose influence came via the theory of phrenology and by Charles Darwin's theory of evolution, has been superseded. Enrico Ferri, a student of Lombroso, believed social as well as biological factors played a role, and believed criminals should not be held responsible when factors causing their criminality were beyond their control. Criminologists have since rejected Lombroso's biological theories since control groups were not used in his studies.[25][26]
 Sociological positivism suggests societal factors such as poverty, membership of subcultures, or low levels of education can predispose people to crime. Adolphe Quetelet used data and statistical analysis to study the relationship between crime and sociological factors. He found age, gender, poverty, education, and alcohol consumption were important factors to crime.[27] Lance Lochner performed three different research experiments, each one proving education reduces crime.[28] Rawson W. Rawson used crime statistics to suggest a link between population density and crime rates, with crowded cities producing more crime.[29] Joseph Fletcher and John Glyde read papers to the Statistical Society of London on their studies of crime and its distribution.[30] Henry Mayhew used empirical methods and an ethnographic approach to address social questions and poverty, and gave his studies in London Labour and the London Poor.[31] Émile Durkheim viewed crime as an inevitable aspect of a society with uneven distribution of wealth and other differences among people.
 Differential association (sub-cultural) posits that people learn crime through association. This theory was advocated by Edwin Sutherland, who focused on how ""a person becomes delinquent because of an excess of definitions favorable to violation of law over definitions unfavorable to violation of law.""[32] Associating with people who may condone criminal conduct, or justify crime under specific circumstances makes one more likely to take that view, under his theory. Interacting with this type of ""antisocial"" peer is a major cause of delinquency. Reinforcing criminal behavior makes it chronic. Where there are criminal subcultures, many individuals learn crime, and crime rates swell in those areas.[33]
 The Chicago school arose in the early twentieth century, through the work of Robert E. Park, Ernest Burgess, and other urban sociologists at the University of Chicago.  In the 1920s, Park and Burgess identified five concentric zones that often exist as cities grow, including the ""zone of transition"", which was identified as the most volatile and subject to disorder.  In the 1940s, Henry McKay and Clifford R. Shaw focused on juvenile delinquents, finding that they were concentrated in the zone of transition. The Chicago School was a school of thought developed that blames social structures for human behaviors. This thought can be associated or used within criminology, because it essentially takes the stance of defending criminals and criminal behaviors. The defense and argument lies in the thoughts that these people and their acts are not their faults but they are actually the result of society (i.e. unemployment, poverty, etc.), and these people are actually, in fact, behaving properly.[34]
 Chicago school sociologists adopted a social ecology approach to studying cities and postulated that urban neighborhoods with high levels of poverty often experience a breakdown in the social structure and institutions, such as family and schools.  This results in social disorganization, which reduces the ability of these institutions to control behavior and creates an environment ripe for deviant behavior.
 Other researchers suggested an added social-psychological link.  Edwin Sutherland suggested that people learn criminal behavior from older, more experienced criminals with whom they may associate.
 Theoretical perspectives used in criminology include psychoanalysis, functionalism, interactionism, Marxism, econometrics, systems theory, postmodernism, behavioural genetics, personality psychology, evolutionary psychology, etc.
 This theory is applied to a variety of approaches within the bases of criminology in particular and in sociology more generally as a conflict theory or structural conflict perspective in sociology and sociology of crime. As this perspective is itself broad enough, embracing as it does a diversity of positions.[35]
 Social disorganization theory is based on the work of Henry McKay and Clifford R. Shaw of the Chicago School.[36]  Social disorganization theory postulates that neighborhoods plagued with poverty and economic deprivation tend to experience high rates of population turnover.[37] This theory suggests that crime and deviance is valued within groups in society, 'subcultures' or 'gangs'. These groups have different values to the social norm. These neighborhoods also tend to have high population heterogeneity.[37] With high turnover, informal social structure often fails to develop, which in turn makes it difficult to maintain social order in a community.
 Since the 1950s, social ecology studies have built on the social disorganization theories.  Many studies have found that crime rates are associated with poverty, disorder, high numbers of abandoned buildings, and other signs of community deterioration.[37][38] As working and middle-class people leave deteriorating neighborhoods, the most disadvantaged portions of the population may remain. William Julius Wilson suggested a poverty ""concentration effect"", which may cause neighborhoods to be isolated from the mainstream of society and become prone to violence.[39]
 Strain theory, also known as Mertonian Anomie, advanced by American sociologist Robert Merton, suggests that mainstream culture, especially in the United States, is saturated with dreams of opportunity, freedom, and prosperity—as Merton put it, the American Dream. Most people buy into this dream, and it becomes a powerful cultural and psychological motivator. Merton also used the term anomie, but it meant something slightly different for him than it did for Durkheim.  Merton saw the term as meaning a dichotomy between what society expected of its citizens and what those citizens could actually achieve.  Therefore, if the social structure of opportunities is unequal and prevents the majority from realizing the dream, some of those dejected will turn to illegitimate means (crime) in order to realize it. Others will retreat or drop out into deviant subcultures (such as gang members, or what he calls ""hobos""). Robert Agnew developed this theory further to include types of strain which were not derived from financial constraints. This is known as general strain theory.[40]
 Following the Chicago school and strain theory, and also drawing on Edwin Sutherland's idea of differential association, sub-cultural theorists focused on small cultural groups fragmenting away from the mainstream to form their own values and meanings about life.
 Albert K. Cohen tied anomie theory with Sigmund Freud's reaction formation idea, suggesting that delinquency among lower-class youths is a reaction against the social norms of the middle class.[41]  Some youth, especially from poorer areas where opportunities are scarce, might adopt social norms specific to those places that may include ""toughness"" and disrespect for authority.  Criminal acts may result when youths conform to norms of the deviant subculture.[42]
 Richard Cloward and Lloyd Ohlin suggested that delinquency can result from a differential opportunity for lower class youth.[43]  Such youths may be tempted to take up criminal activities, choosing an illegitimate path that provides them more lucrative economic benefits than conventional, over legal options such as minimum wage-paying jobs available to them.[43]
 Delinquency tends to occur among the lower-working-class males who have a lack of resources available to them and live in impoverished areas, as mentioned extensively by Albert Cohen (Cohen, 1965). Bias has been known to occur among law enforcement agencies, where officers tend to place a bias on minority groups, without knowing for sure if they had committed a crime or not.
 British sub-cultural theorists focused more heavily on the issue of class, where some criminal activities were seen as ""imaginary solutions"" to the problem of belonging to a subordinate class. A further study by the Chicago school looked at gangs and the influence of the interaction of gang leaders under the observation of adults.
 Sociologists such as Raymond D. Gastil have explored the impact of a Southern culture of honor on violent crime rates.[44]
 Another approach is made by the social bond or social control theory. Instead of looking for factors that make people become criminal, these theories try to explain why people do not become criminal. Travis Hirschi identified four main characteristics: ""attachment to others"", ""belief in moral validity of rules"", ""commitment to achievement"", and ""involvement in conventional activities"".[45]  The more a person features those characteristics, the less likely he or she is to become deviant (or criminal).  On the other hand, if these factors are not present, a person is more likely to become a criminal. Hirschi expanded on this theory with the idea that a person with low self-control is more likely to become criminal. As opposed to most criminology theories, these do not look at why people commit crime but rather why they do not commit crime.[46]
 A simple example: Someone wants a big yacht but does not have the means to buy one.  If the person cannot exert self-control, he or she might try to get the yacht (or the means for it) in an illegal way, whereas someone with high self-control will (more likely) either wait, deny themselves of what want or seek an intelligent intermediate solution, such as joining a yacht club to use a yacht by group consolidation of resources without violating social norms.
 Social bonds, through peers, parents, and others can have a countering effect on one's low self-control.  For families of low socio-economic status, a factor that distinguishes families with delinquent children, from those who are not delinquent, is the control exerted by parents or chaperonage.[47] In addition, theorists such as David Matza and Gresham Sykes argued that criminals are able to temporarily neutralize internal moral and social-behavioral constraints through techniques of neutralization.
 Psychoanalysis is a psychological theory (and therapy) which regards the unconscious mind, repressed memories and trauma, as the key drivers of behavior, especially deviant behavior.[48] Sigmund Freud talks about how the unconscious desire for pain relates to psychoanalysis in his essay, Beyond the Pleasure Principle,.[48] Freud suggested that unconscious impulses such as 'repetition compulsion' and a 'death drive' can dominate a person's creativity, leading to self-destructive behavior. Phillida Rosnick, in the article Mental Pain and Social Trauma, posits a difference in the thoughts of individuals suffering traumatic unconscious pain which corresponds to them having thoughts and feelings which are not reflections of their true selves. There is enough correlation between this altered state of mind and criminality to suggest causation.[49] Sander Gilman, in the article Freud and the Making of Psychoanalysis, looks for evidence in the physical mechanisms of the human brain and the nervous system and suggests there is a direct link between an unconscious desire for pain or punishment and the impulse to commit crime or deviant acts.[50]
 Symbolic interactionism draws on the phenomenology of Edmund Husserl and George Herbert Mead, as well as subcultural theory and conflict theory.[51] This school of thought focused on the relationship between state, media, and conservative-ruling elite and other less powerful groups. The powerful groups had the ability to become the ""significant other"" in the less powerful groups' processes of generating meaning. The former could to some extent impose their meanings on the latter; therefore they were able to ""label"" minor delinquent youngsters as criminal. These youngsters would often take the label on board, indulge in crime more readily, and become actors in the ""self-fulfilling prophecy"" of the powerful groups. Later developments in this set of theories were by Howard Becker and Edwin Lemert, in the mid-20th century.[52] Stanley Cohen developed the concept of ""moral panic"" describing the societal reaction to spectacular, alarming social phenomena (e.g. post-World War 2 youth cultures like the Mods and Rockers in the UK in 1964, AIDS epidemic and football hooliganism).
 Labeling theory refers to an individual who is labeled by others in a particular way. The theory was studied in great detail by Becker.[53] It was originally derived from sociology, but is regularly used in criminological studies. When someone is given the label of a criminal they may reject or accept it and continue to commit crime. Even those who initially reject the label can eventually accept it as the label becomes more well known, particularly among their peers. This stigma can become even more profound when the labels are about deviancy, and it is thought that this stigmatization can lead to deviancy amplification. Malcolm Klein conducted a test which showed that labeling theory affected some youth offenders but not others.[54]
 At the other side of the spectrum, criminologist Lonnie Athens developed a theory about how a process of brutalization by parents or peers that usually occurs in childhood results in violent crimes in adulthood. Richard Rhodes' Why They Kill describes Athens' observations about domestic and societal violence in the criminals' backgrounds. Both Athens and Rhodes reject the genetic inheritance theories.[55]
 Rational choice theory is based on the utilitarian, classical school philosophies of Cesare Beccaria, which were popularized by Jeremy Bentham.  They argued that punishment, if certain, swift, and proportionate to the crime, was a deterrent for crime, with risks outweighing possible benefits to the offender. In Dei delitti e delle pene (On Crimes and Punishments, 1763–1764), Beccaria advocated a rational penology. Beccaria conceived of punishment as the necessary application of the law for a crime; thus, the judge was simply to confirm his or her sentence to the law. Beccaria also distinguished between crime and sin, and advocated against the death penalty, as well as torture and inhumane treatments, as he did not consider them as rational deterrents.
 This philosophy was replaced by the positivist and Chicago schools and was not revived until the 1970s with the writings of James Q. Wilson, Gary Becker's 1965 article Crime and Punishment[56] and George Stigler's 1970 article The Optimum Enforcement of Laws.[57] Rational choice theory argues that criminals, like other people, weigh costs or risks and benefits when deciding whether to commit crime and think in economic terms.[58] They will also try to minimize risks of crime by considering the time, place, and other situational factors.[58]
 Becker, for example, acknowledged that many people operate under a high moral and ethical constraint but considered that criminals rationally see that the benefits of their crime outweigh the cost, such as the probability of apprehension and conviction, severity of punishment, as well as their current set of opportunities. From the public policy perspective, since the cost of increasing the fine is marginal to that of the cost of increasing surveillance, one can conclude that the best policy is to maximize the fine and minimize surveillance.
 With this perspective, crime prevention or reduction measures can be devised to increase the effort required to commit the crime, such as target hardening.[59] Rational choice theories also suggest that increasing risk and likelihood of being caught, through added surveillance, law enforcement presence, added street lighting, and other measures, are effective in reducing crime.[59]
 One of the main differences between this theory and Bentham's rational choice theory, which had been abandoned in criminology, is that if Bentham considered it possible to completely annihilate crime (through the panopticon), Becker's theory acknowledged that a society could not eradicate crime beneath a certain level. For example, if 25% of a supermarket's products were stolen, it would be very easy to reduce this rate to 15%, quite easy to reduce it until 5%, difficult to reduce it under 3% and nearly impossible to reduce it to zero (a feat which the measures required would cost the supermarket so much that it would outweigh the benefits). This reveals that the goals of utilitarianism and classical liberalism have to be tempered and reduced to more modest proposals to be practically applicable.
 Such rational choice theories, linked to neoliberalism, have been at the basics of crime prevention through environmental design and underpin the Market Reduction Approach to theft [60] by Mike Sutton, which is a systematic toolkit for those seeking to focus attention on ""crime facilitators"" by tackling the markets for stolen goods[61] that provide motivation for thieves to supply them by theft.[62]
 Routine activity theory, developed by Marcus Felson and Lawrence Cohen, draws upon control theories and explains crime in terms of crime opportunities that occur in everyday life.[63] A crime opportunity requires that elements converge in time and place including a motivated offender, suitable target or victim, and lack of a capable guardian.[64] A guardian at a place, such as a street, could include security guards or even ordinary pedestrians who would witness the criminal act and possibly intervene or report it to law enforcement.[64] Routine activity theory was expanded by John Eck, who added a fourth element of ""place manager"" such as rental property managers who can take nuisance abatement measures.[65]
 Biosocial criminology is an interdisciplinary field that aims to explain crime and antisocial behavior by exploring both biological factors and environmental factors. While contemporary criminology has been dominated by sociological theories, biosocial criminology also recognizes the potential contributions of fields such as behavioral genetics, personality psychology, and evolutionary psychology.[66] Various theoretical frameworks such as evolutionary neuroandrogenic theory have sought to explain trends in criminality through the lens of evolutionary biology. Specifically, they seek to explain why criminality is so much higher in men than in women and why young men are most likely to exhibit criminal behavior.[67] See also: genetics of aggression.
 Aggressive behavior has been associated with abnormalities in three principal regulatory systems in the body: serotonin systems, catecholamine systems, and the hypothalamic-pituitary-adrenocortical axis. Abnormalities in these systems also are known to be induced by stress, either severe, acute stress or chronic low-grade stress.[68]
 Biosocial approaches remain very controversial within the scientific field.[69][70]
 In 1968, young British sociologists formed the National Deviance Conference (NDC) group. The group was restricted to academics and consisted of 300 members. Ian Taylor, Paul Walton and Jock Young – members of the NDC – rejected previous explanations of crime and deviance. Thus, they decided to pursue a new Marxist criminological approach.[71] In The New Criminology, they argued against the biological ""positivism"" perspective represented by Lombroso, Hans Eysenck and Gordon Trasler.[72]
 According to the Marxist perspective on crime, ""defiance is normal – the sense that men are now consciously involved ... in assuring their human diversity."" Thus Marxists criminologists argued in support of society in which the facts of human diversity, be it social or personal, would not be criminalized.[73] They further attributed the processes of crime creation not to genetic or psychological facts, but rather to the material basis of a given society.[74]
 State crime is a distinct field of crimes that is studied by Marxist criminology, which considers these crimes to be some of the most costly to society in terms of overall harm/injury. In a Marxist framework, genocides, environmental degradation, and war are not crimes that occur out of contempt for one's fellow man, but are crimes of power. They continue systems of control and hegemony which allow state crime and state-corporate crime, along with state-corporate non-profit criminals, to continue governing people.[75]
 Convict criminology is a school of thought in the realm of criminology. Convict criminologists have been directly affected by the criminal justice system, oftentimes having spent years inside the prison system. Researchers in the field of convict criminology such as John Irwin and Stephan Richards argue that traditional criminology can better be understood by those who lived in the walls of a prison.[76] Martin Leyva argues that ""prisonization"" oftentimes begins before prison, in the home, community, and schools.[77]
 According to Rod Earle, Convict Criminology started in the United States after the major expansion of prisons in the 1970s, and the U.S. still remains the main focus for those who study convict criminology.[78]
 Queer criminology is a field of study that focuses on LGBT individuals and their interactions with the criminal justice system. The goals of this field of study are as follows:
 Legitimacy of Queer criminology:
 The value of pursuing criminology from a queer theorist perspective is contested; some believe that it is not worth researching and not relevant to the field as a whole, and as a result is a subject that lacks a wide berth of research available. On the other hand, it could be argued that this subject is highly valuable in highlighting how LGBT individuals are affected by the criminal justice system. This research also has the opportunity to ""queer"" the curriculum of criminology in educational institutions by shifting the focus from controlling and monitoring LGBT communities to liberating and protecting them.[79]
 As more and more people identify as something other than heterosexual, queer criminology continues to grow in relevance. At the same time, in jurisdictions such as Russia, Uganda, and Ghana, governments have become even more punitive through laws that expand the criminalisation of LGBTQ+ conduct, relationships, and organising.[80] 'Digiqueer criminology' has emerged as a sub discipline of queer criminology and aims to deepen understanding of the relationship between digital technology, LGBTQ+ identity, and justice.[81][82]
 Cultural criminology views crime and its control within the context of culture.[83][84] Ferrell believes criminologists can examine the actions of criminals, control agents, media producers, and others to construct the meaning of crime.[84] He discusses these actions as a means to show the dominant role of culture.[84] Kane adds that cultural criminology has three tropes; village, city street, and media, in which males can be geographically influenced by society's views on what is broadcast and accepted as right or wrong.[85] The village is where one engages in available social activities. Linking the history of an individual to a location can help determine social dynamics.[85] The city street involves positioning oneself in the cultural area. This is full of those affected by poverty, poor health and crime, and large buildings that impact the city but not neighborhoods.[85] Mass media gives an all-around account of the environment and the possible other subcultures that could exist beyond a specific geographical area.[85]
 It was later that Naegler and Salman introduced feminist theory to cultural criminology and discussed masculinity and femininity, sexual attraction and sexuality, and intersectional themes.[86] Naegler and Salman believed that Ferrell's mold was limited and that they could add to the understanding of cultural criminology by studying women and those who do not fit Ferrell's mold.[86] Hayward would later add that not only feminist theory, but green theory as well, played a role in the cultural criminology theory through the lens of adrenaline, the soft city, the transgressive subject, and the attentive gaze.[83] The adrenaline lens deals with rational choice and what causes a person to have their own terms of availability, opportunity, and low levels of social control.[83] The soft city lens deals with reality outside of the city and the imaginary sense of reality: the world where transgression occurs, where rigidity is slanted, and where rules are bent.[83] The transgressive subject refers to a person who is attracted to rule-breaking and is attempting to be themselves in a world where everyone is against them.[83] The attentive gaze is when someone, mainly an ethnographer, is immersed into the culture and interested in lifestyle(s) and the symbolic, aesthetic, and visual aspects. When examined, they are left with the knowledge that they are not all the same, but come to a settlement of living together in the same space.[83] Through it all, sociological perspective on cultural criminology theory attempts to understand how the environment an individual is in determines their criminal behavior.[84]
 Relative deprivation involves the process where an individual measures his or her own well-being and materialistic worth against that of other people and perceive that they are worse off in comparison.[87] When humans fail to obtain what they believe they are owed, they can experience anger or jealousy over the notion that they have been wrongly disadvantaged.
 Relative deprivation was originally utilized in the field of sociology by Samuel A. Stouffer, who was a pioneer of this theory. Stouffer revealed that soldiers fighting in World War II measured their personal success by the experience in their units rather than by the standards set by the military.[88] Relative deprivation can be made up of societal, political, economic, or personal factors which create a sense of injustice. It is not based on absolute poverty, a condition where one cannot meet a necessary level to maintain basic living standards. Rather, relative deprivation enforces the idea that even if a person is financially stable, he or she can still feel relatively deprived. The perception of being relatively deprived can result in criminal behavior and/or morally problematic decisions.[89] Relative deprivation theory has increasingly been used to partially explain crime as rising living standards can result in rising crime levels. In criminology, the theory of relative deprivation explains that people who feel jealous and discontent of others might turn to crime to acquire the things that they can not afford.
 Rural criminology is the study of crime trends outside of metropolitan and suburban areas. Rural criminologists have used social disorganization and routine activity theories. The FBI Uniform Crime Report shows that rural communities have significantly different crime trends as opposed to metropolitan and suburban areas. The crime in rural communities consists predominantly of narcotic related crimes such as the production, use, and trafficking of narcotics. Social disorganization theory is used to examine the trends involving narcotics.[90] Social disorganization leads to narcotic use in rural areas because of low educational opportunities and high unemployment rates. Routine activity theory is used to examine all low-level street crimes such as theft.[91] Much of the crime in rural areas is explained through routine activity theory because there is often a lack of capable guardians in rural areas.[citation needed]
 Public criminology is a strand within criminology closely tied to ideas associated with ""public sociology"", focused on disseminating criminological insights to a broader audience than academia. Advocates of public criminology argue that criminologists should be ""conducting and disseminating research on crime, law, and deviance in dialogue with affected communities.""[92] Its goal is for academics and researchers in criminology to provide their research to the public in order to inform public decisions and policymaking. This allows criminologists to avoid the constraints of traditional criminological research.[93] In doing so, public criminology takes on many forms, including media and policy advising as well as activism, civic-oriented education, community outreach, expert testimony, and knowledge co-production.[94]
 Both the positivist and classical schools take a consensus view of crime: that a crime is an act that violates the basic values and beliefs of society.  Those values and beliefs are manifested as laws that society agrees upon. However, there are two types of laws:
 There have been moves in contemporary criminological theory to move away from liberal pluralism, culturalism, and postmodernism by introducing the universal term ""harm"" into the criminological debate as a replacement for the legal term ""crime"".[95][96]
 Areas of study in criminology include:
 
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['crime and deviant behaviour', 'McKay and Clifford R. Shaw', 'highlighting how LGBT individuals are affected by the criminal justice system', 'age, gender, poverty, education, and alcohol consumption', 'highlighting how LGBT individuals are affected by the criminal justice system'], 'answer_start': [], 'answer_end': []}"
"Law enforcement is the activity of some members of government who act in an organized manner to enforce the law by discovering, investigating, deterring, rehabilitating, or punishing people who violate the rules and norms governing that society.[1] The term encompasses police, courts, and corrections. These three components of the criminal Justice system may operate independently of each other or collectively through the use of record sharing and cooperation. Throughout the world, law enforcement are also associated with protecting the public, life, property, and keeping the peace in society.[2]
 The concept of law enforcement dates back to ancient times, and forms of law enforcement and police have existed in various forms across many human societies. Modern state legal codes use the term law enforcement officer or peace officer to include every person vested by the legislating state with police power or authority; traditionally, anyone sworn or badged who can arrest any person for a violation of criminal law is included under the umbrella term of law enforcement.
 Although law enforcement may be most concerned with the prevention and punishment of crimes, organizations exist to discourage a wide variety of non-criminal violations of rules and norms, effected through the imposition of less severe consequences such as probation.
 Law enforcement organizations existed in ancient times, such as prefects in ancient China, paqūdus in Babylonia, curaca in the Inca Empire, vigiles in the Roman Empire, and Medjay in ancient Egypt. Who law enforcers were and reported to depended on the civilization and often changed over time, but they were typically  enslaved people, soldiers, officers of a judge, or hired by settlements and households. Aside from their duties to enforce laws, many ancient law enforcers also served as slave catchers, firefighters, watchmen, city guards, and bodyguards.
 By the post-classical period and the Middle Ages, forces such as the Santa Hermandades, the shurta, and the Maréchaussée provided services ranging from law enforcement and personal protection to customs enforcement and waste collection. In England, a complex law enforcement system emerged, where tithings, groups of ten families, were responsible for ensuring good behavior and apprehending criminals; groups of ten tithings (""hundreds"") were overseen by a reeve; hundreds were governed by administrative divisions known as shires; and shires were overseen by shire-reeves. In feudal Japan, samurai were responsible for enforcing laws.
 The concept of police as the primary law enforcement organization originated in Europe in the early modern period; the first statutory police force was the High Constables of Edinburgh in 1611, while the first organized police force was the Paris lieutenant général de police in 1667. Until the 18th century, law enforcement in England was mostly the responsibility of private citizens and thief-takers, albeit also including constables and watchmen. This system gradually shifted to government control following the 1749 establishment of the London Bow Street Runners, the first formal police force in Britain. In 1800, Napoleon reorganized French law enforcement to form the Paris Police Prefecture; the British government passed the Glasgow Police Act, establishing the City of Glasgow Police; and the Thames River Police was formed in England to combat theft on the River Thames. In September 1829, Robert Peel merged the Bow Street Runners and the Thames River Police to form the Metropolitan Police. The title of the ""first modern police force"" has still been claimed by the modern successors to these organizations.[3][4]
 
Following European colonization of the Americas, the first law enforcement agencies in the Thirteen Colonies were the New York Sheriff's Office and the Albany County Sheriff's Department, both formed in the 1660s in the Province of New York. The Province of Carolina established slave-catcher patrols in the 1700s, and by 1785, the Charleston Guard and Watch was reported to have the duties and organization of a modern police force. The first municipal police department in the United States was the Philadelphia Police Department, while the first American state police, federal law enforcement agency was the United States Marshals Service, both formed in 1789. In the American frontier, law enforcement was the responsibility of county sheriffs, rangers, constables, and marshals. The first law enforcement agency in Canada was the Royal Newfoundland Constabulary, established in 1729, while the first Canadian national law enforcement agency was the Dominion Police, established in 1868. By the late modern period, improvements in technology, greater global connections, and changes in the sociopolitical order led to the establishment of police forces worldwide. National, regional, and municipal civilian law enforcement agencies exist in practically all countries; to promote their international cooperation, the International Criminal Police Organization, also known as Interpol, was formed in September 1923. Technology has made an immense impact on law enforcement, leading to the development and regular use of police cars, police radio systems, police aviation, police tactical units, and police body cameras.
 Most law enforcement is conducted by some law enforcement agency, typically a police force. Civilians generally staff police agencies, which are typically not a military branch. However, some militaries do have branches that enforce laws among the civilian populace, often called gendarmerie, security forces, or internal troops. Social investment in enforcement through such organizations can be massive in terms of the resources invested in the activity and the number of people professionally engaged to perform those functions.[5]
 Law enforcement agencies are limited to operating within a specified jurisdiction. These are typically organized into three basic levels: national, regional, and municipal. However, depending on certain factors, there may be more or less levels, or they may be merged: in the United States, there are federal, state, and local police and sheriff agencies; in Canada, some territories may only have national-level law enforcement, while some provinces have national, provincial, and municipal; in Japan, there is a national police agency, which supervises the police agencies for each individual prefecture; and in Niger, there is a national police for urban areas and a gendarmerie for rural areas, both technically national-level. In some cases, there may be multiple agencies at the same level but with different focuses: for example, in the United States, the Drug Enforcement Administration and the Bureau of Alcohol, Tobacco, Firearms and Explosives are both national-level federal law enforcement agencies, but the DEA focuses on narcotics crimes, while the ATF focuses on weapon regulation violations.
 Various segments of society may have their own specialist law enforcement agency, such as the military having military police, schools having school police or campus police, or airports having airport police. Private police may exist in some jurisdictions, often to provide dedicated law enforcement for privately-owned property or infrastructure, such as railroad police for private railways or hospital police for privately-owned hospital campuses.
 Depending on various factors, such as whether an agency is autonomous or dependent on other organizations for its operations, the governing body that funds and oversees the agency may decide to dissolve or consolidate its operations. Dissolution of an agency may occur when the governing body or the agent itself decides to end operations. This can occur due to multiple reasons, including criminal justice reform,[6] a lack of population in the jurisdiction, mass resignations,[7] efforts to deter corruption, or the governing body contracting with a different agency that renders the original agency redundant or obsolete. According to the International Association of Chiefs of Police, agency consolidation can occur to improve efficiency, consolidate resources, or when forming a new type of government.[8]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Law enforcement', 'Charleston Guard and Watch', 'ancient times', 'probation', 'probation'], 'answer_start': [], 'answer_end': []}"
"Other reasons this message may be displayed:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['this message may be displayed:', 'this message may be displayed:', 'this message may be displayed', 'this message may be displayed:', 'reasons'], 'answer_start': [], 'answer_end': []}"
"Constitutional law is a body of law which defines the role, powers, and structure of different entities within a state, namely, the executive, the parliament or legislature, and the judiciary; as well as the basic rights of citizens and, in federal countries such as the United States and Canada, the relationship between the central government and state, provincial, or territorial governments.
 Not all nation states have codified constitutions, though all such states have a jus commune, or law of the land, that may consist of a variety of imperative and consensual rules. These may include customary law, conventions, statutory law, judge-made law, or international rules and norms. Constitutional law deals with the fundamental principles by which the government exercises its authority. In some instances, these principles grant specific powers to the government, such as the power to tax and spend for the welfare of the population. Other times, constitutional principles act to place limits on what the government can do, such as prohibiting the arrest of an individual without sufficient cause.
 In most nations, such as the United States, India, and Singapore, constitutional law is based on the text of a document ratified at the time the nation came into being. Other constitutions, notably that of the United Kingdom,[1][2] rely heavily on uncodified rules, as several legislative statutes and constitutional conventions, their status within constitutional law varies, and the terms of conventions are in some cases strongly contested.[3]
 Constitutional laws can be considered second order rule making or rules about making rules to exercise power. It governs the relationships between the judiciary, the legislature and the executive with the bodies under its authority. One of the key tasks of constitutions within this context is to indicate hierarchies and relationships of power. For example, in a unitary state, the constitution will vest ultimate authority in one central administration and legislature, and judiciary, though there is often a delegation of power or authority to local or municipal authorities. When a constitution establishes a federal state, it will identify multiple levels of government coexisting with exclusive or shared areas of jurisdiction over lawmaking, application and enforcement. Some federal states, most notably the United States, have separate and parallel federal and state judiciaries, with each having its own hierarchy of courts with a supreme court for each state. India, on the other hand, has one judiciary divided into district courts, high courts, and the Supreme Court of India.
 Human rights or civil liberties form a crucial part of a country's constitution and uphold the rights of the individual against the state. Most jurisdictions, like the United States and France, have a codified constitution, with a bill of rights. A recent example is the Charter of Fundamental Rights of the European Union which was intended to be included in the Treaty establishing a Constitution for Europe, that failed to be ratified. Perhaps the most important example is the Universal Declaration of Human Rights under the UN Charter. These are intended to ensure basic political, social and economic standards that a nation state, or intergovernmental body is obliged to provide to its citizens but many do include its governments. Canada is another instance where a codified constitution. with the Canadian Charter of Rights and Freedoms, protects human rights for people under the nation's jurisdiction.[4]
 Some countries like the United Kingdom have no entrenched document setting out fundamental rights; in those jurisdictions the constitution is composed of statute, case law and convention. A case named Entick v. Carrington[5] is a constitutional principle deriving from the common law. John Entick's house was searched and ransacked by Sherriff Carrington. Carrington argued that a warrant from a Government minister, the Earl of Halifax was valid authority, even though there was no statutory provision or court order for it. The court, led by Lord Camden stated that,
 ""The great end, for which men entered into society, was to secure their property. That right is preserved sacred and incommunicable in all instances, where it has not been taken away or abridged by some public law for the good of the whole. By the laws of England, every invasion of private property, be it ever so minute, is a trespass... If no excuse can be found or produced, the silence of the books is an authority against the defendant, and the plaintiff must have judgment.""[6] The common law and the civil law jurisdictions do not share the same constitutional law underpinnings. Common law nations, such as those in the Commonwealth as well as the United States, derive their legal systems from that of the United Kingdom, and as such place emphasis on judicial precedent,[7][8][9] whereby consequential court rulings (especially those by higher courts) are a source of law. Civil law jurisdictions, on the other hand, place less emphasis on judicial review and only the parliament or legislature has the power to effect law. As a result, the structure of the judiciary differs significantly between the two, with common law judiciaries being adversarial and civil law judiciaries being inquisitorial. Common law judicatures consequently separate the judiciary from the prosecution,[10][11][12] thereby establishing the courts as completely independent from both the legislature and law enforcement. Human rights law in these countries is as a result, largely built on legal precedent in the courts' interpretation of constitutional law, whereas that of civil law countries is almost exclusively composed of codified law, constitutional or otherwise.
 Another main function of constitutions may be to describe the procedure by which parliaments may legislate. For instance, special majorities may be required to alter the constitution. In bicameral legislatures, there may be a process laid out for second or third readings of bills before a new law can enter into force. Alternatively, there may further be requirements for maximum terms that a government can keep power before holding an election.
 Constitutional law is a major focus of legal studies and research. For example, most law students in the United States are required to take a class in Constitutional Law during their first year, and several law journals are devoted to the discussion of constitutional issues.
 The doctrine of the rule of law dictates that government must be conducted according to law. This was first established by British legal theorist A. V. Dicey.
 Dicey identified three essential elements of the British Constitution which were indicative of the rule of law:
 Dicey's rule of law formula consists of three classic tenets. The first is that the regular law is supreme over arbitrary and discretionary powers. ""[N]o man is punishable ... except for a distinct breach of the law established in the ordinary legal manner before the ordinary courts of the land.""[14]
 The second is that all men are to stand equal in the eyes of the law. ""...no man is above the law...every man, whatever be his rank or condition, is subject to the ordinary law of the realm and amenable to the jurisdiction of the ordinary tribunals"" [15]
 The third is that the general ideas and principles that the constitution supports arise directly from the judgements and precedents issued by the judiciary. ""We may say that the constitution is pervaded by the rule of law on the ground that the general principles of the constitution... are with us the result of judicial decisions determining the rights of private persons in particular cases brought before the courts"" [16]
 Separation of powers is often regarded as a second limb functioning alongside the rule of law to curb the powers of the government. In many modern nation states, power is divided and vested into three branches of government: The legislature, the executive, and the judiciary are known as the horizontal separation of powers. The first and the second are harmonised in traditional Westminster system.[17] Vertical separation of powers is decentralisation.
 Election law is a subfield of constitutional law. It includes the rules governing the process of elections. These rules enable the translation of the will of the people into functioning democracies. Election law addresses issues who is entitled to vote, voter registration, ballot access, campaign finance and party funding, redistricting, apportionment, electronic voting and voting machines, accessibility of elections, election systems and formulas, vote counting, election disputes, referendums, and issues such as electoral fraud and electoral silence.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['constitutional issues', 'United States and Canada', 'indicate hierarchies and relationships of power', 'electoral fraud and electoral silence', 'indicate hierarchies and relationships of power'], 'answer_start': [], 'answer_end': []}"
"International law (also known as public international law and the law of nations) is the set of rules, norms, and standards generally recognized as binding between states. It establishes norms for states across a broad range of domains, including war and diplomacy, economic relations, and human rights. International law differs from state-based domestic legal systems in that it is primarily, though not exclusively, applicable to states, rather than to individuals, and operates largely through consent, since there is no universally accepted authority to enforce it upon sovereign states. States may choose to not abide by international law, and even to breach a treaty but such violations, particularly of peremptory norms, can be met with disapproval by others and in some cases coercive action ranging from diplomatic and economic sanctions to war.
 With origins tracing back to antiquity, states have a long history of negotiating interstate agreements. An initial framework was conceptualised by the Ancient Romans and this idea of ius gentium has been used by various academics to establish the modern concept of international law. The sources of international law include international custom (general state practice accepted as law), treaties, and general principles of law recognised by most national legal systems. Although international law may also be reflected in international comity—the practices adopted by states to maintain good relations and mutual recognition—such traditions are not legally binding. The relationship and interaction between a national legal system and international law is complex and variable. National law may become international law when treaties permit national jurisdiction to supranational tribunals such as the European Court of Human Rights or the International Criminal Court. Treaties such as the Geneva Conventions require national law to conform to treaty provisions. National laws or constitutions may also provide for the implementation or integration of international legal obligations into domestic law.
 The modern term ""international law"" was originally coined by Jeremy Bentham in his 1789 book Introduction to the Principles of Morals and Legislation to replace the older law of nations, a direct translation of the late medieval concepts of ius gentium, used by Hugo Grotius, and droits des gens, used by Emer de Vattel.[1][2] The definition of international law has been debated; Bentham referred specifically to relationships between states which has been criticised for its narrow scope.[3] Lassa Oppenheim defined it in his treatise as ""a law between sovereign and equal states based on the common consent of these states"" and this definition has been largely adopted by international legal scholars.[4]
 There is a distinction between public and private international law; the latter is concerned with whether national courts can claim jurisdiction over cases with a foreign element and the application of foreign judgments in domestic law, whereas public international law covers rules with an international origin.[5] The difference between the two areas of law has been debated as scholars disagree about the nature of their relationship. Joseph Story, who originated the term ""private international law"", emphasised that it must be governed by the principles of public international law but other academics view them as separate bodies of law.[6][7] Another term, transnational law, is sometimes used to refer to a body of both national and international rules that transcend the nation state, although some academics emphasise that it is distinct from either type of law. It was defined by Philip Jessup as ""all law which regulates actions or events that transcend national frontiers"".[8]
 A more recent concept is supranational law, which was described in a 1969 paper as ""[a] relatively new word in the vocabulary of politics"".[9] Systems of supranational law arise when nations explicitly cede their right to make decisions to this system's judiciary and legislature, which then have the right to make laws that are directly effective in each member state.[9][10] This has been described as ""a level of international integration beyond mere intergovernmentalism yet still short of a federal system"".[9] The most common example of a supranational system is the European Union.[10]
 The origins of international law can be traced back to antiquity.[12] Among the earliest recorded examples are peace treaties between the Mesopotamian city-states of Lagash and Umma (approximately 3100 BCE), and an agreement between the Egyptian pharaoh, Ramesses II, and the Hittite king, Ḫattušili III, concluded in 1279 BCE.[11] Interstate pacts and agreements were negotiated and agreed upon by polities across the world, from the eastern Mediterranean to East Asia.[13] In Ancient Greece, many early peace treaties were negotiated between its city-states and, occasionally, with neighbouring states.[14] The Roman Empire established an early conceptual framework for international law, jus gentium, which governed the status of foreigners living in Rome and relations between foreigners and Roman citizens.[15][16] Adopting the Greek concept of natural law, the Romans conceived of jus gentium as being universal.[17] However, in contrast to modern international law, the Roman law of nations applied to relations with and between foreign individuals rather than among political units such as states.[18]
 Beginning with the Spring and Autumn period of the eighth century BCE, China was divided into numerous states that were often at war with each other. Rules for diplomacy and treaty-making emerged, including notions regarding just grounds for war, the rights of neutral parties, and the consolidation and partition of states; these concepts were sometimes applied to relations with barbarians along China's western periphery beyond the Central Plains.[19][20] The subsequent Warring States period saw the development of two major schools of thought, Confucianism and Legalism, both of which held that the domestic and international legal spheres were closely interlinked, and sought to establish competing normative principles to guide foreign relations.[20][21] Similarly, the Indian subcontinent was divided into various states, which over time developed rules of neutrality, treaty law, and international conduct, and established both temporary and permanent embassies.[22][23]
 Following the collapse of the western Roman Empire in the fifth century CE, Europe fragmented into numerous often-warring states for much of the next five centuries. Political power was dispersed across a range of entities, including the Church, mercantile city-states, and kingdoms, most of which had overlapping and ever-changing jurisdictions. As in China and India, these divisions prompted the development of rules aimed at providing stable and predictable relations. Early examples include canon law, which governed ecclesiastical institutions and clergy throughout Europe; the lex mercatoria (""merchant law""), which concerned trade and commerce; and various codes of maritime law, such as the Rolls of Oléron— aimed at regulating shipping in North-western Europe — and the later Laws of Wisby, enacted among the commercial Hanseatic League of northern Europe and the Baltic region.[24]
 In the Islamic world, Muhammad al-Shaybani published Al-Siyar Al-Kabīr in the eighth century, which served as a fundamental reference work for siyar, a subset of Sharia law, which governed foreign relations.[25][26] This was based on the division of the world into three categories: the dar al-Islam, where Islamic law prevailed; the dar al-sulh, non-Islamic realms that concluded an armistice with a Muslim government; and the dar al-harb, non-Islamic lands which were contested through jihad.[27][28] Islamic legal principles concerning military conduct served as precursors to modern international humanitarian law and institutionalised limitations on military conduct, including guidelines for commencing war, distinguishing between civilians and combatants and caring for the sick and wounded.[29][30]
 During the European Middle Ages, international law was concerned primarily with the purpose and legitimacy of war, seeking to determine what constituted ""just war"".[31] The Greco-Roman concept of natural law was combined with religious principles by Jewish philosopher Maimonides (1135–1204) and Christian theologian Thomas Aquinas (1225–1274) to create the new discipline of the ""law of nations"", which unlike its eponymous Roman predecessor, applied natural law to relations between states.[32][33] In Islam, a similar framework was developed wherein the law of nations was derived, in part, from the principles and rules set forth in treaties with non-Muslims.[34]
 
The 15th century witnessed a confluence of factors that contributed to an accelerated development of international law. Italian jurist Bartolus de Saxoferrato (1313–1357) was considered the founder of private international law. Another Italian jurist, Baldus de Ubaldis (1327–1400), provided commentaries and compilations of Roman, ecclesiastical, and feudal law, creating an organised source of law that could be referenced by different nations.[citation needed] Alberico Gentili (1552–1608) took a secular view to international law, authoring various books on issues in international law, notably Law of War, which provided comprehensive commentary on the laws of war and treaties.[35] Francisco de Vitoria (1486–1546), who was concerned with the treatment of indigenous peoples by Spain, invoked the law of nations as a basis for their innate dignity and rights, articulating an early version of sovereign equality between peoples.[36] Francisco Suárez (1548–1617) emphasised that international law was founded upon natural law and human positive law.[37][38] Dutch jurist Hugo Grotius (1583–1645) is widely regarded as the father of international law,[39] being one of the first scholars to articulate an international order that consists of a ""society of states"" governed not by force or warfare but by actual laws, mutual agreements, and customs.[40] Grotius secularised international law;[41] his 1625 work, De Jure Belli ac Pacis, laid down a system of principles of natural law that bind all nations regardless of local custom or law.[39] He inspired two nascent schools of international law, the naturalists and the positivists.[42] In the former camp was German jurist Samuel von Pufendorf (1632–1694), who stressed the supremacy of the law of nature over states.[43][44] His 1672 work, Of the Law of Nature and Nations, expanded on the theories of Grotius and grounded natural law to reason and the secular world, asserting that it regulated only external acts of states.[43] Pufendorf challenged the Hobbesian notion that the state of nature was one of war and conflict, arguing that the natural state of the world is actually peaceful but weak and uncertain without adherence to the law of nations.[45] The actions of a state consist of nothing more than the sum of the individuals within that state, thereby requiring the state to apply a fundamental law of reason, which is the basis of natural law. He was among the earliest scholars to expand international law beyond European Christian nations, advocating for its application and recognition among all peoples on the basis of shared humanity.[46]
 In contrast, positivist writers, such as Richard Zouche (1590–1661) in England and Cornelis van Bynkershoek (1673–1743) in the Netherlands, argued that international law should derive from the actual practice of states rather than Christian or Greco-Roman sources. The study of international law shifted away from its core concern on the law of war and towards the domains such as the law of the sea and commercial treaties.[47] The positivist school grew more popular as it reflected accepted views of state sovereignty and was consistent with the empiricist approach to philosophy that was then gaining acceptance in Europe.[48]
 The developments of the 17th century culminated at the conclusion of the Peace of Westphalia in 1648, which is considered the seminal event in international law.[49] The resulting Westphalian sovereignty is said to have established the current international legal order characterised by independent nation states, which have equal sovereignty regardless of their size and power, defined primarily by non-interference in the domestic affairs of sovereign states, although historians have challenged this narrative.[50] The idea of nationalism further solidified the concept and formation of nation-states.[51] Elements of the naturalist and positivist schools were synthesised, notably by German philosopher Christian Wolff (1679–1754) and Swiss jurist Emer de Vattel (1714–1767), both of whom sought a middle-ground approach.[52][53] During the 18th century, the positivist tradition gained broader acceptance, although the concept of natural rights remained influential in international politics, particularly through the republican revolutions of the United States and France.[citation needed]
 Until the mid-19th century, relations between states were dictated mostly by treaties, agreements between states to behave in a certain way, unenforceable except by force, and nonbinding except as matters of honour and faithfulness.[citation needed] One of the first instruments of modern armed conflict law was the Lieber Code of 1863, which governed the conduct of warfare during the American Civil War, and is noted for codifying rules and articles of war adhered to by nations across the world, including the United Kingdom, Prussia, Serbia and Argentina.[54] In the years that followed, numerous other treaties and bodies were created to regulate the conduct of states towards one another, including the Permanent Court of Arbitration in 1899, and the Hague and Geneva Conventions, the first of which was passed in 1864.[55][56]
 Colonial expansion by European powers reached its peak in the late 19th century and its influence began to wane following the unprecedented bloodshed of World War I, which spurred the creation of international organisations. Right of conquest was generally recognized as international law before World War II.[57] The League of Nations was founded to safeguard peace and security.[58][59] International law began to incorporate notions such as self-determination and human rights.[60] The United Nations (UN) was established in 1945 to replace the League, with an aim of maintaining collective security.[61] A more robust international legal order followed, buttressed by institutions such as the International Court of Justice (ICJ) and the UN Security Council (UNSC).[62] The International Law Commission (ILC) was established in 1947 to develop and codify international law.[61]
 In the 1940s through the 1970s, the dissolution of the Soviet bloc and decolonisation across the world resulted in the establishment of scores of newly independent states.[63] As these former colonies became their own states, they adopted European views of international law.[64] A flurry of institutions, ranging from the International Monetary Fund (IMF) and the International Bank for Reconstruction and Development (World Bank) to the World Health Organization furthered the development of a multilateralist approach as states chose to compromise on sovereignty to benefit from international cooperation.[65] Since the 1980s, there has been an increasing focus on the phenomenon of globalisation and on protecting human rights on the global scale, particularly when minorities or indigenous communities are involved, as concerns are raised that globalisation may be increasing inequality in the international legal system.[66]
 The sources of international law applied by the community of nations are listed in Article 38(1) of the Statute of the International Court of Justice, which is considered authoritative in this regard. These categories are, in order, international treaties, customary international law, general legal principles and judicial decisions and the teachings of prominent legal scholars as ""a subsidiary means for the determination of rules of law"".[67] It was originally considered that the arrangement of the sources sequentially would suggest an implicit hierarchy of sources; however, the statute does not provide for a hierarchy and other academics have argued that therefore the sources must be equivalent.[68][69]
 General principles of law have been defined in the Statute as ""general principles of law recognized by civilized nations"" but there is no academic consensus about what is included within this scope.[70][71] They are considered to be derived from both national and international legal systems, although including the latter category has led to debate about potential cross-over with international customary law.[72][73] The relationship of general principles to treaties or custom has generally been considered to be ""fill[ing] the gaps"" although there is still no conclusion about their exact relationship in the absence of a hierarchy.[74]
 A treaty is defined in Article 2 of the Vienna Convention on the Law of Treaties (VCLT) as ""an international agreement concluded between States in written form and governed by international law, whether embodied in a single instrument or in two or more related instruments and whatever its particular designation"".[75] The definition specifies that the parties must be states, however international organisations are also considered to have the capacity to enter treaties.[75][76] Treaties are binding through the principle of pacta sunt servanda, which allows states to create legal obligations on themselves through consent.[77][78] The treaty must be governed by international law; however it will likely be interpreted by national courts.[79] The VCLT, which codifies several bedrock principles of treaty interpretation, holds that a treaty ""shall be interpreted in good faith in accordance with the ordinary meaning to be given to the terms of the treaty in their context and in the light of its object and purpose"".[80] This represents a compromise between three theories of interpretation: the textual approach which looks to the ordinary meaning of the text, the subjective approach which considers factors such as the drafters' intention, and the teleological approach which interprets a treaty according to its objective and purpose.[80][81]
 A state must express its consent to be bound by a treaty through signature, exchange of instruments, ratification, acceptance, approval or accession. Accession refers to a state choosing to become party to a treaty that it is unable to sign, such as when establishing a regional body. Where a treaty states that it will be enacted through ratification, acceptance or approval, the parties must sign to indicate acceptance of the wording but there is no requirement on a state to later ratify the treaty, although they may still be subject to certain obligations.[82] When signing or ratifying a treaty, a state can make a unilateral statement to negate or amend certain legal provisions which can have one of three effects: the reserving state is bound by the treaty but the effects of the relevant provisions are precluded or changes, the reserving state is bound by the treaty but not the relevant provisions, or the reserving state is not bound by the treaty.[83][84] An interpretive declaration is a separate process, where a state issues a unilateral statement to specify or clarify a treaty provision. This can affect the interpretation of the treaty but it is generally not legally binding.[85][86] A state is also able to issue a conditional declaration stating that it will consent to a given treaty only on the condition of a particular provision or interpretation.[87]
 Article 54 of the VCLT provides that either party may terminate or withdraw from a treaty in accordance with its terms or at any time with the consent of the other party, with 'termination' applying to a bilateral treaty and 'withdrawal' applying to a multilateral treaty.[88] Where a treaty does not have provisions allowing for termination or withdrawal, such as the Genocide Convention, it is prohibited unless the right was implied into the treaty or the parties had intended to allow for it.[89] A treaty can also be held invalid, including where parties act ultra vires or negligently, where execution has been obtained through fraudulent, corrupt or forceful means, or where the treaty contradicts peremptory norms.[90]
 Customary international law requires two elements: a consistent practice of states and the conviction of those states that the consistent practice is required by a legal obligation, referred to as opinio juris.[91][92] Custom distinguishes itself from treaty law as it is binding on all states, regardless of whether they have participated in the practice, with the exception of states who have been persistent objectors during the process of the custom being formed and special or local forms of customary law.[93] The requirement for state practice relates to the practice, either through action or failure to act, of states in relation to other states or international organisations.[94] There is no legal requirement for state practice to be uniform or for the practice to be long-running, although the ICJ has set a high bar for enforcement in the cases of Anglo-Norwegian Fisheries and North Sea Continental Shelf.[95] There has been legal debate on this topic with the only prominent view on the length of time necessary to establish custom explained by Humphrey Waldock as varying ""according to the nature of the case"".[96] The practice is not required to be followed universally by states, but there must be a ""general recognition"" by states ""whose interests are specially affected"".[97]
 The second element of the test, opinio juris, the belief of a party that a particular action is required by the law is referred to as the subjective element.[98] The ICJ has stated in dictum in North Sea Continental Shelf that, ""Not only must the acts concerned amount to a settled practice, but they must also be such, or be carried out in such a way, as to be evidence of a belief that this practice is rendered obligatory by the existence of a rule of law requiring it"".[99] A committee of the International Law Association has argued that there is a general presumption of an opinio juris where state practice is proven but it may be necessary if the practice suggests that the states did not believe it was creating a precedent.[99] The test in these circumstances is whether opinio juris can be proven by the states' failure to protest.[100] Other academics believe that intention to create customary law can be shown by states including the principle in multiple bilateral and multilateral treaties, so that treaty law is necessary to form customs.[101]
 The adoption of the VCLT in 1969 established the concept of jus cogens, or peremptory norms, which are ""a norm accepted and recognized by the international community of States as a whole as a norm from which no derogation is permitted and which can be modified only by a subsequent norm of general international law having the same character"".[102] Where customary or treaty law conflicts with a peremptory norm, it will be considered invalid, but there is no agreed definition of jus cogens.[103] Academics have debated what principles are considered peremptory norms but the mostly widely agreed is the principle of non-use of force.[104] The next year, the ICJ defined erga omnes obligations as those owed to ""the international community as a whole"", which included the illegality of genocide and human rights.[102]
 There are generally two approaches to the relationship between international and national law, namely monism and dualism.[105] Monism assumes that international and national law are part of the same legal order.[106] Therefore, a treaty can directly become part of national law without the need for enacting legislation, although they will generally need to be approved by the legislature. Once approved, the content of the treaty is considered as a law that has a higher status than national laws. Examples of countries with a monism approach are France and the Netherlands.[107] The dualism approach considers that national and international law are two separate legal orders, so treaties are not granted a special status.[105][108] The rules in a treaty can only be considered national law if the contents of the treaty have been enacted first.[108] An example is the United Kingdom; after the country ratified the European Convention on Human Rights, the convention was only considered to have the force of law in national law after Parliament passed the Human Rights Act 1998.[109]
 In practice, the division of countries between monism and dualism is often more complicated; countries following both approaches may accept peremptory norms as being automatically binding and they may approach treaties, particularly later amendments or clarifications, differently than they would approach customary law.[110] Many countries with older or unwritten constitutions do not have explicit provision for international law in their domestic system and there has been an upswing in support for monism principles in relation to human rights and humanitarian law, as most principles governing these concepts can be found in international law.[111]
 A state is defined under Article 1 of the Montevideo Convention on the Rights and Duties of States as a legal person with a permanent population, a defined territory, government and capacity to enter relations with other states. There is no requirement on population size, allowing micro-states such as San Marino and Monaco to be admitted to the UN, and no requirement of fully defined boundaries, allowing Israel to be admitted despite border disputes. There was originally an intention that a state must have self-determination, but now the requirement is for a stable political environment. The final requirement of being able to enter relations is commonly evidenced by independence and sovereignty.[112]
 Under the principle of par in parem non habet imperium, all states are sovereign and equal,[113] but state recognition often plays a significant role in political conceptions. A country may recognise another nation as a state and, separately, it may recognise that nation's government as being legitimate and capable of representing the state on the international stage.[114][115] There are two theories on recognition; the declaratory theory sees recognition as commenting on a current state of law which has been separately satisfied whereas the constitutive theory states that recognition by other states determines whether a state can be considered to have legal personality.[116] States can be recognised explicitly through a released statement or tacitly through conducting official relations, although some countries have formally interacted without conferring recognition.[117]
 Throughout the 19th century and the majority of the 20th century, states were protected by absolute immunity, so they could not face criminal prosecution for any actions. However a number of countries began to distinguish between acta jure gestionis, commercial actions, and acta jure imperii, government actions; the restrictive theory of immunity said states were immune where they were acting in a governmental capacity but not a commercial one. The European Convention on State Immunity in 1972 and the UN Convention on Jurisdictional Immunities of States and their Property attempt to restrict immunity in accordance with customary law.[118]
 Historically individuals have not been seen as entities in international law, as the focus was on the relationship between states.[119][120] As human rights have become more important on the global stage, being codified by the UN General Assembly (UNGA) in the Universal Declaration of Human Rights in 1948, individuals have been given the power to defend their rights to judicial bodies.[121] International law is largely silent on the issue of nationality law with the exception of cases of dual nationality or where someone is claiming rights under refugee law but as, argued by the political theorist Hannah Arendt, human rights are often tied to someone's nationality.[122] The European Court of Human Rights allows individuals to petition the court where their rights have been violated and national courts have not intervened and the Inter-American Court of Human Rights and the African Court on Human and Peoples' Rights have similar powers.[121]
 Traditionally, sovereign states and the Holy See were the sole subjects of international law. With the proliferation of international organisations over the last century, they have also been recognised as relevant parties.[123] One definition of international organisations comes from the ILC's 2011 Draft Articles on the Responsibility of International Organizations which in Article 2(a) states that it is ""an organization established by treaty or other instrument governed by international law and possessing its own international legal personality"".[124] This definition functions as a starting point but does not recognise that organisations can have no separate personality but nevertheless function as an international organisation.[124] The UN Economic and Social Council has emphasised a split between inter-government organisations (IGOs), which are created by inter-governmental agreements, and international non-governmental organisations (INGOs).[125] All international organisations have members; generally this is restricted to states, although it can include other international organisations.[126] Sometimes non-members will be allowed to participate in meetings as observers.[127]
 The Yearbook of International Organizations sets out a list of international organisations, which include the UN, the WTO, the World Bank and the IMF.[128][129] Generally organisations consist of a plenary organ, where member states can be represented and heard; an executive organ, to decide matters within the competence of the organisation; and an administrative organ, to execute the decisions of the other organs and handle secretarial duties.[130] International organisations will typically provide for their privileges and immunity in relation to its member states in their constitutional documents or in multilateral agreements, such as the Convention on the Privileges and Immunities of the United Nations.[131] These organisations also have the power to enter treaties, using the Vienna Convention on the Law of Treaties between States and International Organizations or between International Organizations as a basis although it is not yet in force.[76] They may also have the right to bring legal claims against states depending, as set out in Reparation for Injuries, where they have legal personality and the right to do so in their constitution.[132]
 The UNSC has the power under Chapter VII of the UN Charter to take decisive and binding actions against states committing ""a threat to the peace, breach of the peace, or an act of aggression"" for collective security although prior to 1990, it has only intervened once, in the case of Korea in 1950.[133][134] This power can only be exercised, however, where a majority of member states vote for it, as well as receiving the support of the permanent five members of the UNSC.[135] This can be followed up with economic sanctions, military action, and similar uses of force.[136] The UNSC also has a wide discretion under Article 24, which grants ""primary responsibility"" for issues of international peace and security.[133] The UNGA, concerned during the Cold War with the requirement that the USSR would have to authorise any UNSC action, adopted the ""Uniting for Peace"" resolution of 3 November 1950, which allowed the organ to pass recommendations to authorize the use of force. This resolution also led to the practice of UN peacekeeping, which has been notably been used in East Timor and Kosovo.[137]
 There are more than one hundred international courts in the global community, although states have generally been reluctant to allow their sovereignty to be limited in this way.[138] The first known international court was the Central American Court of Justice, prior to World War I, when the Permanent Court of International Justice (PCIJ) was established. The PCIJ was replaced by the ICJ, which is the best known international court due to its universal scope in relation to geographical jurisdiction and subject matter.[139] There are additionally a number of regional courts, including the Court of Justice of the European Union, the EFTA Court and the Court of Justice of the Andean Community.[140] Interstate arbitration can also be used to resolve disputes between states, leading in 1899 to the creation of the Permanent Court of Arbitration which facilitates the process by maintaining a list of arbitrators. This process was used in the Island of Palmas case and to resolve disputes during the Eritrean-Ethiopian war.[141]
 The ICJ operates as one of the six organs of the UN, based out of the Hague with a panel of fifteen permanent judges.[142] It has jurisdiction to hear cases involving states but cannot get involved in disputes involving individuals or international organizations. The states that can bring cases must be party to the Statute of the ICJ, although in practice most states are UN members and would therefore be eligible. The court has jurisdiction over all cases that are referred to it and all matters specifically referred to in the UN Charter or international treaties, although in practice there are no relevant matters in the UN Charter.[143] The ICJ may also be asked by an international organisation to provide an advisory opinion on a legal question, which are generally considered non-binding but authoritative.[144]
 Conflict of laws, also known as private international law, was originally concerned with choice of law, determining which nation's laws should govern a particular legal circumstance.[145][146] Historically the comity theory has been used although the definition is unclear, sometimes referring to reciprocity and sometimes being used as a synonym for private international law.[147][148] Story distinguished it from ""any absolute paramount obligation, superseding all discretion on the subject"".[148] There are three aspects to conflict of laws – determining which domestic court has jurisdiction over a dispute, determining if a domestic court has jurisdiction and determining whether foreign judgments can be enforced. The first question relates to whether the domestic court or a foreign court is best placed to decide the case.[149] When determining the national law that should apply, the lex causae is the law that has been chosen to govern the case, which is generally foreign, and the lexi fori is the national law of the court making the determination. Some examples are lex domicilii, the law of the domicile, and les patriae, the law of the nationality.[150]
 The rules which are applied to conflict of laws will vary depending on the national system determining the question. There have been attempts to codify an international standard to unify the rules so differences in national law cannot lead to inconsistencies, such as through the Hague Convention on the Recognition and Enforcement of Foreign Judgments in Civil and Commercial Matters and the Brussels Regulations.[151][152][153] These treaties codified practice on the enforcement of international judgments, stating that a foreign judgment would be automatically recognised and enforceable where required in the jurisdiction where the party resides, unless the judgement was contrary to public order or conflicted with a local judgment between the same parties. On a global level, the New York Convention on the Recognition and Enforcement of Foreign Arbitral Awards was introduced in 1958 to internationalise the enforcement of arbitral awards, although it does not have jurisdiction over court judgments.[154]
 A state must prove that it has jurisdiction before it can exercise its legal authority.[155] This concept can be divided between prescriptive jurisdiction, which is the authority of a legislature to enact legislation on a particular issue, and adjudicative jurisdiction, which is the authority of a court to hear a particular case.[156] This aspect of private international law should first be resolved by reference to domestic law, which may incorporate international treaties or other supranational legal concepts, although there are consistent international norms.[157] There are five forms of jurisdiction which are consistently recognised in international law; an individual or act can be subject to multiple forms of jurisdiction.[158][159] The first is the territorial principle, which states that a nation has jurisdiction over actions which occur within its territorial boundaries.[160] The second is the nationality principle, also known as the active personality principle, whereby a nation has jurisdiction over actions committed by its nationals regardless of where they occur. The third is the passive personality principle, which gives a country jurisdiction over any actions which harm its nationals.[161] The fourth is the protective principle, where a nation has jurisdiction in relation to threats to its ""fundamental national interests"". The final form is universal jurisdiction, where a country has jurisdiction over certain acts based on the nature of the crime itself.[161][162]
 Following World War II, the modern system for international human rights was developed to make states responsible for their human rights violations.[163] The UN Economic and Security Council established the UN Commission on Human Rights in 1946, which developed the Universal Declaration of Human Rights (UDHR), which established non-binding international human rights standards, for work, standards of living, housing and education, non-discrimination, a fair trial and prohibition of torture. Two further human rights treaties were adopted by the UN in 1966, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR). These two documents along with the UDHR are considered the International Bill of Human Rights.[164]
 Non-domestic human rights enforcement operates at both the international and regional levels. Established in 1993, the Office of the UN High Commissioner for Human Rights supervises Charter-based and treaty-based procedures.[164] The former are based on the UN Charter and operate under the UN Human Rights Council, where each global region is represented by elected member states. The Council is responsible for Universal Periodic Review, which requires each UN member state to review its human rights compliance every four years, and for special procedures, including the appointment of special rapporteurs, independent experts and working groups.[165] The treaty-based procedure allows individuals to rely on the nine primary human rights treaties:
 The regional human rights enforcement systems operate in Europe, Africa and the Americas through the European Court of Human Rights, the Inter-American Court of Human Rights and the African Court on Human and Peoples' Rights.[167] International human rights has faced criticism for its Western focus, as many countries were subject to colonial rule at the time that the UDHR was drafted, although many countries in the Global South have led the development of human rights on the global stage in the intervening decades.[168]
 International labour law is generally defined as ""the substantive rules of law established at the international level and the procedural rules relating to their adoption and implementation"". It operates primarily through the International Labor Organization (ILO), a UN agency with the mission of protecting employment rights which was established in 1919.[169][170] The ILO has a constitution setting out a number of aims, including regulating work hours and labour supply, protecting workers and children and recognising equal pay and the right to free association, as well as the Declaration of Philadelphia of 1944, which re-defined the purpose of the ILO.[170][171] The 1998 Declaration on Fundamental Principles and Rights at Work further binds ILO member states to recognise fundamental labour rights including free association, collective bargaining and eliminating forced labour, child labour and employment discrimination.[171]
 The ILO have also created labour standards which are set out in their conventions and recommendations. Member states then have the choice as to whether or not to ratify and implement these standards.[171] The secretariat of the ILO is the International Labour Office, which can be consulted by states to determine the meaning of a convention, which forms the ILO's case law. Although the Right to Organise Convention does not provide an explicit right to strike, this has been interpreted into the treaty through case law.[172][173] The UN does not specifically focus on international labour law, although some of its treaties cover the same topics. Many of the primary human rights conventions also form part of international labour law, providing protection in employment and against discrimination on the grounds of gender and race.[174]
 It has been claimed that there is no concept of discrete international environmental law, with the general principles of international law instead being applied to these issues.[175] Since the 1960s, a number of treaties focused on environmental protection were ratified, including the Declaration of the United Nations Conference on the Human Environment of 1972, the World Charter for Nature of 1982, and the Vienna Convention for the Protection of the Ozone Layer of 1985. States generally agreed to co-operate with each other in relation to environmental law, as codified by principle 24 of the Rio Declaration of 1972.[176] Despite these, and other, multilateral environmental agreements covering specific issues, there is no overarching policy on international environmental protection or one specific international organisation, with the exception of the UN Environmental Programme. Instead, a general treaty setting out the framework for tackling an issue has then been supplemented by more specific protocols.[177]
 Climate change has been one of the most important and heavily debated topics in recent environmental law. The United Nations Framework Convention on Climate Change, intended to set out a framework for the mitigation of greenhouse gases and responses to resulting environmental changes, was introduced in 1992 and came into force two years later. As of 2023, 198 states were a party.[178][179] Separate protocols have been introduced through conferences of the parties, including the Kyoto Protocol which was introduced in 1997 to set specific targets for greenhouse gas reduction and the 2015 Paris Agreement which set the goal of keeping global warming at least below 2 °C (3.6 °F) above pre-industrial levels.[180]
 Individuals and organisations have some rights under international environmental law as the Aarhus Convention in 1998 set obligations on states to provide information and allow public input on these issues.[181] However few disputes under the regimes set out in environmental agreements are referred to the ICJ, as the agreements tend to specify their compliance procedures. These procedures generally focus on encouraging the state to once again become compliant through recommendations but there is still uncertainty on how these procedures should operate and efforts have been made to regulate these processes although some worry that this will undercut the efficiency of the procedures themselves.[182]
 Legal territory can be divided into four categories. There is territorial sovereignty which covers land and territorial sea, including the airspace above it and the subsoil below it, territory outside the sovereignty of any state, res nullius which is not yet within territorial sovereignty but is territory that is legally capable of being acquired by a state and res communis which is territory that cannot be acquired by a state.[183] There have historically been five methods of acquiring territorial sovereignty, reflecting Roman property law: occupation, accretion, cession, conquest and prescription.[184]
 The law of the sea is the area of international law concerning the principles and rules by which states and other entities interact in maritime matters. It encompasses areas and issues such as navigational rights, sea mineral rights, and coastal waters jurisdiction.[185] The law of the sea was primarily composed of customary law until the 20th century, beginning with the League of Nations Codification Conference in 1930, the UN Conference on the Law of the Sea and the adoption of the UNCLOS in 1982.[186] The UNCLOS was particularly notable for making international courts and tribunals responsible for the law of the sea.[187]
 The boundaries of a nation's territorial sea were initially proposed to be three miles in the late 18th century.[188] The UNCLOS instead defined it as being at most 12 nautical miles from the baseline (usually the coastal low-water mark) of a state; both military and civilian foreign ships are allowed innocent passage through these waters despite the sea being within the state's sovereignty.[189][190] A state can have jurisdiction beyond its territorial waters where it claims a contiguous zone of up to 24 nautical miles from its baseline for the purpose of preventing the infringement of its ""customs, fiscal, immigration and sanitary regulations"".[191] States are also able to claim an exclusive economic zone (EEZ) following passage of the UNCLOS, which can stretch up to 200 nautical miles from the baseline and gives the sovereign state rights over natural resources. Some states have instead chosen to retain their exclusive fishery zones, which cover the same territory.[192] There are specific rules in relation to the continental shelf, as this can extend further than 200 nautical miles. The International Tribunal for the Law of the Sea has specified that a state has sovereign rights over the resources of the entire continental shelf, regardless of its distance from the baseline, but different rights apply to the continental shelf and the water column above it where it is further than 200 nautical miles from the coast.[193]
 The UNCLOS defines the high seas as all parts of the sea that are not within a state's EEZ, territorial sea or internal waters.[194] There are six freedoms of the high seas—navigation, overflight, laying submarine cables and pipelines, constructing artificial islands, fishing and scientific research—some of which are subject to legal restrictions.[195] Ships in the high seas are deemed to have the nationality of the flag that they have the right to fly and no other state can exercise jurisdiction over them; the exception is ships used for piracy, which are subject to universal jurisdiction.[196]
 In 1944, the Bretton Woods Conference established the International Bank for Reconstruction and Development (later the World Bank) and the IMF. At the conference, the International Trade Organization was proposed but failed to be instituted due to the refusal of the United States to ratify its charter. Three years later, Part IV of the statute was adopted to create the General Agreement on Tariffs and Trade, which operated between 1948 and 1994, when the WTO was established. The OPEC, which banded together to control global oil supply and prices, caused the previous reliance on fixed currency exchange rates to be dropped in favour of floating exchange rates in 1971. During this recession, British Prime Minister Margaret Thatcher and US President Ronald Reagan pushed for free trade and deregulation under a neo-liberal agenda known as the Washington Consensus.[197]
 The law relating to the initiation of armed conflict is jus ad bellum.[198] This was codified in 1928 in the Kellogg–Briand Pact, which stated that conflicts should be settled through peaceful negotiations with the exception, through reservations drafted by some state parties, of self-defence.[199] These fundamental principles were re-affirmed in the UN Charter, which provided for ""an almost absolute prohibition on the use of force"", with the only three exceptions.[200][201] The first involves force authorised by the UNSC, as the entity is responsible in the first instance for responding to breaches or threats to the peace and acts of aggression, including the use of force or peacekeeping missions.[202] The second exception is where a state is acting in individual or collective self-defence. A state is allowed to act in self-defence in the case of an ""armed attack"" but the intention behind this exception has been challenged, particularly as nuclear weapons have become more common, with many states relying instead on the customary right of self-defence as set out in the Caroline test.[203][204] The ICJ considered collective self-defence in Nicaragua v. United States, where the U.S. unsuccessfully argued that it had mined harbours in Nicaragua in pre-emption of an attack by the Sandinista government against another member of the Organization of American States.[205] The final exception is where the UNSC delegates its responsibility for collective security to a regional organisation, such as NATO.[206]
 International humanitarian law (IHL) is an effort to ""mitigate the human suffering caused by war"" and it is often complementary to the law of armed conflict and international human rights law.[207] The concept of jus in bello (law in war) covers IHL, which is distinct from jus ad bellum.[198] Its scope lasts from the initiation of conflict until a peaceful settlement is reached.[208] There are two main principles in IHL; the principle of distinction dictates that combatants and non-combatants must be treated differently and the principle of not causing disproportionate suffering to combatants. In Legality of the Threat or Use of Nuclear Weapons, the ICJ described these concepts as ""intransgressible principles of international customary law"".[209]
 The two Hague Conventions of 1899 and 1907 considered restrictions on the conduct of war and the Geneva Conventions of 1949, which were organised by the International Committee of the Red Cross, considered the protection of innocent parties in conflict zones.[210] The First Geneva Convention covers wounded and ill combatants, the Second Geneva Convention covers combatants at sea who are wounded, ill or shipwrecked, the Third Geneva Convention covers prisoners of war and the Fourth Geneva Convention covers civilians.[209] These conventions were supplemented the additional Protocol I and Protocol II, which were codified in 1977.[210] Initially IHL conventions were only considered to apply to a conflict if all parties had ratified the relevant convention under the si omnes clause, but this posed concerns and the Martens clause began to be implemented, providing that the law would generally be deemed to apply.[211]
 There have been various agreements to outlaw particular types of weapons, such as the Chemical Weapons Convention and the Biological Weapons Convention. The use of nuclear weapons was determined to be in conflict with principles of IHL by the ICJ in 1995, although the court also held that it ""cannot conclude definitively whether the threat or use of nuclear weapons would be lawful or unlawful in an extreme circumstance of self-defence.""[212] Multiple treaties have attempted to regulate the use of these weapons, including the Non-Proliferation Treaty and the Joint Comprehensive Plan of Action, but key states have failed to sign or have withdrawn. There have been similar debates on the use of drones and cyberwarefare on the international stage.[213]
 International criminal law sets out the definition of international crimes and compels states to prosecute these crimes.[214] While war crimes were prosecuted throughout history, this has historically been done by national courts.[215] The International Military Tribunal in Nuremberg and the International Military Tribunal for the Far East in Tokyo were established at the end of World War II to prosecute key actors in Germany and Japan.[216] The jurisdiction of the tribunals was limited to crimes against peace (based on the Kellogg–Briand Pact), war crimes (based on the Hague Conventions) and crimes against humanity, establishing new categories of international crime.[217][218] Throughout the twentieth century, the separate crimes of genocide, torture and terrorism were also recognised.[218]
 Initially these crimes were intended to be prosecuted by national courts and subject to their domestic procedures.[219] The Geneva Conventions of 1949, the Additional Protocols of 1977 and the 1984 UN Convention against Torture mandated that the national courts of the contracting countries must prosecute these offenses where the perpetrator is on their territory or extradite them to any other interested state.[220] It was in the 1990s that two ad hoc tribunals, the International Criminal Tribunal for the Former Yugoslavia (ICTY) and the International Criminal Tribunal for Rwanda (ICTR), were established by the UNSC to address specific atrocities.[221][222] The ICTY had authority to prosecute war crimes, crimes against humanity and genocide occurring in Yugoslavia after 1991 and the ICTR had authority to prosecute genocide, crimes against humanity and grave breaches of the 1949 Geneva Conventions during the 1994 Rwandan genocide.[223][224]
 The International Criminal Court (ICC), established by the 1998 Rome Statute, is the first and only permanent international court to prosecute genocide, war crimes, crimes against humanity, and the crime of aggression.[225] There are 123 state parties to the ICC although a number of states have declared their opposition to the court; it has been criticised by African countries including The Gambia and Kenya for ""imperialist"" prosecutions.[226][227] One particular aspect of the court that has received scrutiny is the principle of complementarity, whereby the ICC only has jurisdiction if the national courts of a state with jurisdiction are ""unwilling or unable to prosecute"" or where a state has investigated but chosen not to prosecute a case.[228][229] The United States has a particularly complicated relationship with the ICC; originally signing the treaty in 2000, the US stated in 2002 that it did not intend to become a party as it believed the ICC threatened its national sovereignty and the country does not recognise the court's jurisdiction.[230][231]
 Hybrid courts are the most recent type of international criminal court; they aim to combine both national and international components, operating in the jurisdiction where the crimes in question occurred.[232][233] International courts have been criticised for a lack of legitimacy, as they can seem disconnected from the crimes that have occurred, but the hybrid courts are able to provide the resources that may be lacking in countries facing the aftermath of serious conflict.[232] There has been debate about what courts can be included within this definition, but generally the Special Panels for Serious Crimes in East Timor, the Kosovo Specialist Chambers, the Special Court for Sierra Leone, the Special Tribunal for Lebanon and the Extraordinary Chambers in the Courts of Cambodia have been listed.[234][225][233]
 International legal theory comprises a variety of theoretical and methodological approaches used to explain and analyse the content, formation and effectiveness of international law and institutions and to suggest improvements. Some approaches center on the question of compliance: why states follow international norms in the absence of a coercive power that ensures compliance. Other approaches focus on the problem of the formation of international rules: why states voluntarily adopt international law norms, that limit their freedom of action, in the absence of a world legislature; while other perspectives are policy oriented: they elaborate theoretical frameworks and instruments to criticize the existing norms and to make suggestions on how to improve them. Some of these approaches are based on domestic legal theory, some are interdisciplinary, and others have been developed expressly to analyse international law. Classical approaches to International legal theory are the natural law, the Eclectic and the legal positivism schools of thought.[235][page needed]
 The natural law approach argues that international norms should be based on axiomatic truths. The 16th-century natural law writer de Vitoria examined the questions of the just war, the Spanish authority in the Americas, and the rights of the Native American peoples. In 1625, Grotius argued that nations as well as persons ought to be governed by universal principle based on morality and divine justice while the relations among polities ought to be governed by the law of peoples, the jus gentium, established by the consent of the community of nations on the basis of the principle of pacta sunt servanda, that is, on the basis of the observance of commitments. On his part, de Vattel argued instead for the equality of states as articulated by 18th-century natural law and suggested that the law of nations was composed of custom and law on the one hand, and natural law on the other. During the 17th century, the basic tenets of the Grotian or eclectic school, especially the doctrines of legal equality, territorial sovereignty, and independence of states, became the fundamental principles of the European political and legal system and were enshrined in the 1648 Peace of Westphalia.[citation needed]
 The early positivist school emphasized the importance of custom and treaties as sources of international law. In the 16th-century, Gentili used historical examples to posit that positive law (jus voluntarium) was determined by general consent. van Bynkershoek asserted that the bases of international law were customs and treaties commonly consented to by various states, while John Jacob Moser emphasized the importance of state practice in international law. The positivism school narrowed the range of international practice that might qualify as law, favouring rationality over morality and ethics. The 1815 Congress of Vienna marked the formal recognition of the political and international legal system based on the conditions of Europe.[citation needed] Modern legal positivists consider international law as a unified system of rules that emanates from the states' will. International law, as it is, is an ""objective"" reality that needs to be distinguished from law ""as it should be"". Classic positivism demands rigorous tests for legal validity and it deems irrelevant all extralegal arguments.[236]
 John Austin asserted that due to the principle of par in parem non habet imperium, ""so-called"" international law, lacking a sovereign power and so unenforceable, was not really law at all, but ""positive morality"", consisting of ""opinions and sentiments...more ethical than legal in nature.""[237] Since states are few in number, diverse and atypical in character, unindictable, lack a centralised sovereign power, and their agreements unpoliced and decentralised, Martin Wight argued that international society is better described as anarchy.[238]
 Hans Morgenthau believed international law to be the weakest and most primitive system of law enforcement; he likened its decentralised nature to the law that prevails in preliterate tribal societies. Monopoly on violence is what makes domestic law enforceable; but between nations, there are multiple competing sources of force. The confusion created by treaty laws, which resemble private contracts between persons, is mitigated only by the relatively small number of states.[239] He asserted that no state may be compelled to submit a dispute to an international tribunal, making laws unenforceable and voluntary. International law is also unpoliced, lacking agencies for enforcement. He cites a 1947 US opinion poll in which 75% of respondents wanted ""an international police to maintain world peace"", but only 13% wanted that force to exceed the US armed forces. Later surveys have produced similar contradictory results.[240]
 International law is currently navigating a complex array of challenges and controversies that have underscored the dynamic nature of international relations in the 21st century. Some of these challenges include enforcement difficulties, the impact of technological advancements, climate change, and worldwide pandemics.[241] The possible re-emergence of right of conquest as international law is contentious.[242]
 Among the most pressing issues are enforcement difficulties, where the lack of a centralized global authority often leads to non-compliance with international norms, particularly evident in violations of International Humanitarian Law (IHL). Sovereignty disputes further complicate the international legal landscape, as conflicts over territorial claims and jurisdictional boundaries arise, challenging the principles of non-interference and peaceful resolution. Furthermore, the emergence of new global powers introduces additional layers of complexity, as these nations assert their interests and challenge established norms, necessitating a reevaluation of the global legal order to accommodate shifting power dynamics.[243]
 Cybersecurity has also emerged as a critical concern, with international law striving to address the threats posed by cyber-attacks to national security, infrastructure, and individual privacy. Climate change demands unprecedented international cooperation, as evidenced by agreements like the Paris Agreement, though disparities in responsibilities among nations pose significant challenges to collective action.[244]
 The COVID-19 pandemic has further highlighted the interconnectedness of the global community, emphasizing the need for coordinated efforts to manage health crises, vaccine distribution, and economic recovery.[245]
 These contemporary issues underscore the need for ongoing adaptation and cooperation within the framework of international law to address the multifaceted challenges of the modern world, ensuring a just, peaceful, and sustainable global order.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Climate change', 'prominent legal scholars', 'individuals have not been seen as entities in international law', 'war and diplomacy, economic relations, and human rights', 'economic sanctions, military action, and similar uses of force'], 'answer_start': [], 'answer_end': []}"
"
 International humanitarian law (IHL), also referred to as the laws of armed conflict, is the law that regulates the conduct of war (jus in bello).[1][2] It is a branch of international law that seeks to limit the effects of armed conflict by protecting persons who are not participating in hostilities and by restricting and regulating the means and methods of warfare available to combatants.
 International humanitarian law is inspired by considerations of humanity and the mitigation of human suffering. It comprises a set of rules, which is established by treaty or custom and that seeks to protect persons and property/objects that are or may be affected by armed conflict, and it limits the rights of parties to a conflict to use methods and means of warfare of their choice.[3] Sources of international law include international agreements (the Geneva Conventions), customary international law, general principles of nations, and case law.[2][4] It defines the conduct and responsibilities of belligerent nations, neutral nations, and individuals engaged in warfare, in relation to each other and to protected persons, usually meaning non-combatants. It is designed to balance humanitarian concerns and military necessity, and subjects warfare to the rule of law by limiting its destructive effect and alleviating human suffering.[3] Serious violations of international humanitarian law are called war crimes. 
 While IHL (jus in bello) concerns the rules and principles governing the conduct of warfare once armed conflict has begun, jus ad bellum pertains to the justification for resorting to war and includes the crime of aggression. Together the jus in bello and jus ad bellum comprise the two strands of the laws of war governing all aspects of international armed conflicts. The law is mandatory for nations bound by the appropriate treaties. There are also other customary unwritten rules of war, many of which were explored at the Nuremberg trials. IHL operates on a strict division between rules applicable in international armed conflict and internal armed conflict.[5]
 International humanitarian law is traditionally seen as distinct from international human rights law (which governs the conduct of a state towards its people), although the two branches of law are complementary and in some ways overlap.[6][7][8]
 Modern international humanitarian law is made up of two historical streams:
 The two streams take their names from a number of international conferences which drew up treaties relating to war and conflict, in particular the Hague Conventions of 1899 and 1907, and the Geneva Conventions, the first of which was drawn up in 1863.  Both deal with jus in bello, which deals with the question of whether certain practices are acceptable during armed conflict.[10]
 The Law of The Hague, or the laws of war proper, ""determines the rights and duties of belligerents in the conduct of operations and limits the choice of means in doing harm"".[11] In particular, it concerns itself with
 Systematic attempts to limit the savagery of warfare only began to develop in the 19th century.  Such concerns were able to build on the changing view of warfare by states influenced by the Age of Enlightenment. The purpose of warfare was to overcome the enemy state, which could be done by disabling the enemy combatants. Thus, ""the distinction between combatants and civilians, the requirement that wounded and captured enemy combatants must be treated humanely, and that quarter must be given, some of the pillars of modern humanitarian law, all follow from this principle"".[13]
 Fritz Munch sums up historical military practice before 1800: ""The essential points seem to be these: In battle and in towns taken by force, combatants and non-combatants were killed and property was destroyed or looted.""[14]  In the 17th century, the Dutch jurist Hugo Grotius, widely regarded as the founder or father of public international law, wrote that ""wars, for the attainment of their objects, it cannot be denied, must employ force and terror as their most proper agents"".[15]
 Even in the midst of the carnage of history, however, there have been frequent expressions and invocation of humanitarian norms for the protection of the victims of armed conflicts: the wounded, the sick and the shipwrecked. These date back to ancient times.[16]
 In the Old Testament, the King of Israel prevents the slaying of the captured, following the prophet Elisha's admonition to spare enemy prisoners. In answer to a question from the King, Elisha said, ""You shall not slay them. Would you slay those whom you have taken captive with your sword and with your bow? Set bread and water before them, that they may eat and drink and go to their master.""[17]
 In ancient India there are records (the Laws of Manu, for example) describing the types of weapons that should not be used: ""When he fights with his foes in battle, let him not strike with weapons concealed (in wood), nor with (such as are) barbed, poisoned, or the points of which are blazing with fire.""[18] There is also the command not to strike a eunuch nor the enemy ""who folds his hands in supplication ... Nor one who sleeps, nor one who has lost his coat of mail, nor one who is naked, nor one who is disarmed, nor one who looks on without taking part in the fight.""[19]
 Islamic law states that ""non-combatants who did not take part in fighting such as women, children, monks and hermits, the aged, blind, and insane"" were not to be molested.[20] The first Caliph, Abu Bakr, proclaimed, ""Do not mutilate. Do not kill little children or old men or women. Do not cut off the heads of palm trees or burn them. Do not cut down fruit trees. Do not slaughter livestock except for food.""[21] Islamic jurists have held that a prisoner should not be killed, as he ""cannot be held responsible for mere acts of belligerency"".[22] 
However, the prohibition against killing non-combatants is not necessarily absolute in Islamic Law. For example, in situations where an ""enemy retreats inside fortifications and one-to-one combat is not an option"", Islamic jurists have been unanimous as to the permissibility on the use of less discriminating weapons such as mangonels (a weapon for catapulting large stones) if required by military necessity but have differed with respect to the use of fire in such cases. [23]
 The most important antecedent of IHL is the current Armistice Agreement and Regularization of War, signed and ratified in 1820 between the authorities of the then Government of Great Colombia and the Chief of the Expeditionary Forces of the Spanish Crown, in the Venezuelan city of Santa Ana de Trujillo. This treaty was signed under the conflict of Independence, being the first of its kind in the West.
 It was not until the second half of the 19th century, however, that a more systematic approach was initiated. In the United States, a German immigrant, Francis Lieber, drew up a code of conduct in 1863, which came to be known as the Lieber Code, for the Union Army during the American Civil War. The Lieber Code included the humane treatment of civilian populations in areas of conflict, and also forbade the execution of POWs.
 At the same time, the involvement during the Crimean War of a number of such individuals as Florence Nightingale and Henry Dunant, a Genevese businessman who had worked with wounded soldiers at the Battle of Solferino, led to more systematic efforts to prevent the suffering of war victims. Dunant wrote a book, which he titled A Memory of Solferino, in which he described the horrors he had witnessed. His reports were so shocking that they led to the founding of the International Committee of the Red Cross (ICRC) in 1863, and the convening of a conference in Geneva in 1864, which drew up the Geneva Convention for the Amelioration of the Condition of the Wounded in Armies in the Field.[24]
 The Law of Geneva is directly inspired by the principle of humanity. It relates to those who are not participating in the conflict, as well as to military personnel hors de combat. It provides the legal basis for protection and humanitarian assistance carried out by impartial humanitarian organizations such as the ICRC.[25]  This focus can be found in the Geneva Conventions.
 The Geneva Conventions are the result of a process that developed in a number of stages between 1864 and 1949. It focused on the protection of civilians and those who can no longer fight in an armed conflict. As a result of World War II, all four conventions were revised, based on previous revisions and on some of the 1907 Hague Conventions, and readopted by the international community in 1949. Later conferences have added provisions prohibiting certain methods of warfare and addressing issues of civil wars.[26]
 The first three Geneva Conventions were revised, expanded, and replaced, and the fourth one was added, in 1949.
 There are three additional amendment protocols to the Geneva Convention:
 The Geneva Conventions of 1949 may be seen, therefore, as the result of a process which began in 1864. Today they have ""achieved universal participation with 194 parties"". This means that they apply to almost any international armed conflict.[30] The Additional Protocols, however, have yet to achieve near-universal acceptance, since the United States and several other significant military powers (like Iran, Israel, India and Pakistan) are currently not parties to them.[31]
 With the adoption of the 1977 Additional Protocols to the Geneva Conventions, the two strains of law began to converge, although provisions focusing on humanity could already be found in the Hague law (i.e. the protection of certain prisoners of war and civilians in occupied territories). The 1977 Additional Protocols, relating to the protection of victims in both international and internal conflict, not only incorporated aspects of both the Law of The Hague and the Law of Geneva, but also important human rights provisions.[32]
 Well-known examples of such rules include the prohibition on attacking doctors or ambulances displaying a red cross. It is also prohibited to fire at a person or vehicle bearing a white flag, since that, being considered the flag of truce, indicates an intent to surrender or a desire to communicate. In either case, the persons protected by the Red Cross or the white flag are expected to maintain neutrality, and may not engage in warlike acts themselves; engaging in war activities under a white flag or a red cross is itself a violation of the laws of war.
 These examples of the laws of war address:
 It is a violation of the laws of war to engage in combat without meeting certain requirements, among them the wearing of a distinctive uniform or other easily identifiable badge, and the carrying of weapons openly. Impersonating soldiers of the other side by wearing the enemy's uniform is allowed, though fighting in that uniform is unlawful perfidy, as is the taking of hostages.
 International humanitarian law now includes several treaties that outlaw specific weapons. These conventions were created largely because these weapons cause deaths and injuries long after conflicts have ended. Unexploded land mines have caused up to 7,000 deaths a year; unexploded bombs, particularly from cluster bombs that scatter many small ""bomblets"", have also killed many. An estimated 98% of the victims are civilian; farmers tilling their fields and children who find these explosives have been common victims. For these reasons, the following conventions have been adopted:
 The ICRC is the only institution explicitly named under international humanitarian law as a controlling authority. The legal mandate of the ICRC stems from the four Geneva Conventions of 1949, as well as from its own Statutes.
 The International Committee of the Red Cross (ICRC) is an impartial, neutral, and independent organization whose exclusively humanitarian mission is to protect the lives and dignity of victims of war and internal violence and to provide them with assistance. During conflict, punishment for violating the laws of war may consist of a specific, deliberate and limited violation of the laws of war in reprisal.
 Combatants who break specific provisions of the laws of war lose the protections and status afforded to them as prisoners of war, but only after facing a ""competent tribunal"".[35] At that point, they become unlawful combatants, but must still be ""treated with humanity and, in case of trial, shall not be deprived of the rights of fair and regular trial"", because they are still covered by GC IV, Article 5.
 Spies and terrorists are only protected by the laws of war if the ""power"" which holds them is in a state of armed conflict or war, and until they are found to be an ""unlawful combatant"". Depending on the circumstances, they may be subject to civilian law or a military tribunal for their acts. In practice, they have often have been subjected to torture and execution. The laws of war neither approve nor condemn such acts, which fall outside their scope.[citation needed] Spies may only be punished following a trial; if captured after rejoining their own army, they must be treated as prisoners of war.[36] Suspected terrorists who are captured during an armed conflict, without having participated in the hostilities, may be detained only in accordance with the GC IV, and are entitled to a regular trial.[37] Countries that have signed the UN Convention Against Torture have committed themselves not to use torture on anyone for any reason.
 After a conflict has ended, persons who have committed any breach of the laws of war, and especially atrocities, may be held individually accountable for war crimes through process of law.
 Reparation for victims of serious violations of International Humanitarian Law acknowledges the suffering endured by individuals and communities and seeks to provide a form of redress for the harms inflicted upon them. The evolving legal landscape, notably through the mechanisms of international courts like the ICC, has reinforced the notion that victims of war crimes and other serious breaches of International Humanitarian Law have a recognized right to seek reparations. These reparations can take various forms, including restitution, compensation, rehabilitation, satisfaction, and guarantees of non-repetition, aimed at addressing the physical, psychological, and material damage suffered by victims.[38]
 The Fourth Geneva Convention focuses on the civilian population. The two additional protocols adopted in 1977 extend and strengthen civilian protection in international (AP I) and non-international (AP II) armed conflict: for example, by introducing the prohibition of direct attacks against civilians. A ""civilian"" is defined as ""any person not belonging to the armed forces"", including non-nationals and refugees.[39] However, it is accepted that operations may cause civilian casualties. Luis Moreno Ocampo, chief prosecutor of the international criminal court, wrote in 2006: ""International humanitarian law and the Rome statute permit belligerents to carry out proportionate attacks against military objectives, even when it is known that some civilian deaths or injuries will occur. A crime occurs if there is an intentional attack directed against civilians (principle of distinction) ... or an attack is launched on a military objective in the knowledge that the incidental civilian injuries would be clearly excessive in relation to the anticipated military advantage (principle of proportionality).""[40]
 The provisions and principles of IHL which seek to protect civilians are:[41]
 The principle of distinction protects civilian population and civilian objects from the effects of military operations. It requires parties to an armed conflict to distinguish at all times, and under all circumstances, between combatants and military objectives on the one hand, and civilians and civilian objects on the other; and only to target the former. It also provides that civilians lose such protection should they take a direct part in hostilities.[42] The principle of distinction has also been found by the ICRC to be reflected in state practice; it is therefore an established norm of customary international law in both international and non-international armed conflicts.[43]
 Necessity and proportionality are established principles in humanitarian law. Under IHL, a belligerent may apply only the amount and kind of force necessary to defeat the enemy. Further, attacks on military objects must not cause loss of civilian life considered excessive in relation to the direct military advantage anticipated.[44] Every feasible precaution must be taken by commanders to avoid civilian casualties.[45] The principle of proportionality has also been found by the ICRC to form part of customary international law in international and non-international armed conflicts.[46]
 The principle of humane treatment requires that civilians be treated humanely at all times.[47] Common Article 3 of the GCs prohibits violence to life and person (including cruel treatment and torture), the taking of hostages, humiliating and degrading treatment, and execution without regular trial against non-combatants, including persons hors de combat (wounded, sick and shipwrecked). Civilians are entitled to respect for their physical and mental integrity, their honour, family rights, religious convictions and practices, and their manners and customs.[48] This principle of humane treatment has been affirmed by the ICRC as a norm of customary international law, applicable in both international and non-international armed conflicts.[46]
 The principle of non-discrimination is a core principle of IHL. Adverse distinction based on race, sex, nationality, religious belief or political opinion is prohibited in the treatment of prisoners of war,[49] civilians,[50] and persons hors de combat.[51] All protected persons shall be treated with the same consideration by parties to the conflict, without distinction based on race, religion, sex or political opinion.[52] Each and every person affected by armed conflict is entitled to his fundamental rights and guarantees, without discrimination.[48] The prohibition against adverse distinction is also considered by the ICRC to form part of customary international law in international and non-international armed conflict.[46]
 Women must be protected from rape, forced prostitution and from any form of indecent assault. Children under the age of eighteen must not be permitted to take part in hostilities, cannot be evacuated to a foreign country by a country other than theirs, except temporarily due to a compelling threat to their health and safety, and if orphaned or separated from their families, must be maintained and receive an education.[53]
 The European Union has made significant changes to its sanctions policy to better safeguard humanitarian efforts, in response to UN Security Council Resolution 2664. This includes incorporating humanitarian exemptions into EU sanctions regimes, ensuring that aid can reach those in need without legal barriers. This shift has led to the inclusion of comprehensive humanitarian exemptions in new sanctions frameworks for Niger and Sudan, and the amendment of existing regimes to incorporate similar exemptions, thereby covering key humanitarian contexts in countries like Lebanon, Myanmar, and Venezuela.[54]
 IHL emphasises, in various provisions in the GCs and APs, the concept of formal equality and non-discrimination. Protections should be provided ""without any adverse distinction founded on sex"". For example, with regard to female prisoners of war, women are required to receive treatment ""as favourable as that granted to men"".[55] In addition to claims of formal equality, IHL mandates special protections to women, providing female prisoners of war with separate dormitories from men, for example,[56] and prohibiting sexual violence against women.[57]
 The reality of women's and men's lived experiences of conflict has highlighted some of the gender limitations of IHL. Feminist critics have challenged IHL's focus on male combatants and its relegation of women to the status of victims, and its granting them legitimacy almost exclusively as child-rearers. A study of the 42 provisions relating to women within the Geneva Conventions and the Additional Protocols found that almost half address women who are expectant or nursing mothers.[58] Others have argued that the issue of sexual violence against men in conflict has not yet received the attention it deserves.[59]
 Soft-law instruments have been relied on to supplement the protection of women in armed conflict:
 Read together with other legal mechanisms, in particular the UN Convention for the Elimination of All Forms of Discrimination Against Women (CEDAW), these can enhance interpretation and implementation of IHL.
 In addition, international criminal tribunals (like the International Criminal Tribunals for the former Yugoslavia and Rwanda) and mixed tribunals (like the Special Court for Sierra Leone) have contributed to expanding the scope of definitions of sexual violence and rape in conflict. They have effectively prosecuted sexual and gender-based crimes committed during armed conflict. There is now well-established jurisprudence on gender-based crimes. Nonetheless, there remains an urgent need to further develop constructions of gender within international humanitarian law.[60]
 IHL has generally not been subject to the same debates and criticisms of ""cultural relativism"" as have international human rights. Although the modern codification of IHL in the Geneva Conventions and the Additional Protocols is relatively new, and European in name, the core concepts are not new, and laws relating to warfare can be found in all cultures. Indeed, non-Western participants played important roles in the development of this area of law at the global level as early as the 1907 Second Hague Conference, and have continued to do so since.[61]
 ICRC studies on the Middle East, Somalia, Latin America, and the Pacific, for example have found that there are traditional and long-standing practices in various cultures that preceded, but are generally consistent with, modern IHL. It is important to respect local and cultural practices that are in line with IHL. Relying on these links and on local practices can help to promote awareness of and adherence to IHL principles among local groups and communities.[citation needed]
 Durham cautions that, although traditional practices and IHL legal norms are largely compatible, it is important not to assume perfect alignment. There are areas in which legal norms and cultural practices clash. Violence against women, for example, is frequently legitimized by arguments from culture, and yet is prohibited in IHL and other international law. In such cases, it is important to ensure that IHL is not negatively affected.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Middle East, Somalia, Latin America, and the Pacific', 'Florence Nightingale and Henry Dunant', 'those who are not participating in the conflict', 'those who are not participating in the conflict', 'addressing the physical, psychological, and material damage suffered by victims'], 'answer_start': [], 'answer_end': []}"
"A trade agreement (also known as trade pact) is a wide-ranging taxes, tariff and trade treaty that often includes investment guarantees. It exists when two or more countries agree on terms that help them trade with each other. The most common trade agreements are of the preferential and free trade types, which are concluded in order to reduce (or eliminate) tariffs, quotas and other trade restrictions on items traded between the signatories.
 The logic of formal trade agreements is that they outline what is agreed upon and specify the punishments for deviation from the rules set in the agreement.[1] Trade agreements therefore make misunderstandings less likely, and create confidence on both sides that cheating will be punished; this increases the likelihood of long-term cooperation.[1] An international organization, such as the IMF, can further incentivize cooperation by monitoring compliance with agreements and reporting third countries of the violations.[1] Monitoring by international agencies may be needed to detect non-tariff barriers, which are disguised attempts at creating trade barriers.[1]
 Trade pacts are frequently politically contentious, as they might pit the winners and losers of an agreement against each other. Aside from their provisions on reducing tariffs, contentious issues in modern free trade agreements may revolve around regulatory harmonization on issues such as intellectual property regulations, labour rights,[2] and environmental and safety regulations.[3] Increasing efficiency and economic gains through free trade is a common goal. 
 The anti-globalization movement opposes trade agreements almost by definition, although some groups normally allied within that movement, such as leftist parties, might support fair trade or safe trade provisions that moderate real and perceived ill effects of globalization. In response to criticism, free trade agreements have increasingly over time come with measures that seek to reduce the negative externalities of trade liberalization.[4]
 There are three different types of trade agreements. The first is unilateral trade agreement,[5] this is what happens when a country wants certain restrictions to be enforced but no other countries want them to be imposed. This also allows countries to decrease the amount of trade restrictions. That is also something that does not happen often and could impair a country.
 The second type is a bilateral trade agreement,  when signed by two parties, where each party may be a country (or other customs territory), a trade bloc or an informal group of countries (or other customs territories). Both countries loosen their trade restrictions to help businesses, so that they can prosper better between the different countries. This definitely helps lower taxes and it helps them converse about their trade status. Usually, this revolves around subsided domestic industries. Mainly the industries fall under automotive, oil, or food industries.[6]
 A trade agreement signed between more than two sides (typically neighboring or in the same region) is classified as multilateral. These face the most obstacles- when negotiating substance, and for implementation. The more countries that are involved, the harder it is to reach mutual satisfaction. Once this type of trade agreement is settled on, it becomes a very powerful agreement. The larger the GDP of the signatories, the greater the impact on other global trade relationships. The largest multilateral trade agreement is the North American Free Trade Agreement,[7] involving the United States, Canada, and Mexico.[8]
 These are between countries in a certain area. The most powerful ones include a few countries that are near each other in a geographical area.[9] These countries often have similar histories, demographics and economic goals.
 The North American Free Trade Agreement (NAFTA) was established on January 1, 1989, between the United States, Canada, and Mexico. This agreement was designed to reduce tariff barriers in North America.
 The Eurasian Economic Union (EAEU) was established in 2015 and currently consists of five member states: Armenia, Belarus, Kazakhstan, Kyrgyzstan, and Russia. It is designed to foster economic integration among its member states and promote economic growth in the region. [10]
 The Association of Southeast Asian Nations (ASEAN) was formed in 1967 between the countries of Indonesia, Malaysia, the Philippines, Singapore, and Thailand. It was established to promote political partnership and maintain economic stability throughout the region.[9]
 There are a variety of trade agreements; with some being quite complex (European Union), while others are less intensive (North American Free Trade Agreement).[11] The resulting level of economic integration depends on the specific type of trade pacts and policies adopted by the trade bloc:
 Typically the benefits and obligations of the trade agreements apply only to their signatories.
 In the framework of the World Trade Organization, different agreement types are concluded (mostly during new member accessions), whose terms apply to all WTO members on the so-called most-favored basis (MFN), which means that beneficial terms agreed bilaterally with one trading partner will apply also to the rest of the WTO members.
 All agreements concluded outside of the WTO framework (and granting additional benefits beyond the WTO MFN level, but applicable only between the signatories and not to the rest of the WTO members) are called preferential by the WTO. According to WTO rules, these agreements are subject to certain requirements such as notification to the WTO and general reciprocity (the preferences should apply equally to each of the signatories of the agreement) where unilateral preferences (some of the signatories gain preferential access to the market of the other signatories, without lowering their own tariffs) are allowed only under exceptional circumstances and as temporary measure.[12]
 The trade agreements called preferential by the WTO are also known as regional (RTA), despite not necessarily concluded by countries within a certain region. There are currently 205 agreements in force as of July 2007. Over 300 have been reported to the WTO.[13] The number of FTA has increased significantly over the last decade. Between 1948 and 1994, the General Agreement on Tariffs and Trade (GATT), the predecessor to the WTO, received 124 notifications. Since 1995 over 300 trade agreements have been enacted.[14]
 The WTO is further classifying these agreements in the following types:
 Lists:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['taxes', 'Armenia, Belarus, Kazakhstan, Kyrgyzstan, and Russia', 'when two or more countries agree on terms that help them trade with each other', 'subsided domestic industries', 'applicable only between the signatories'], 'answer_start': [], 'answer_end': []}"
"
 Globalization, or globalisation (Commonwealth English; see spelling differences), is the process of interaction and integration among people, companies, and governments worldwide. The term globalization first appeared in the early 20th century (supplanting an earlier French term mondialisation), developed its current meaning sometime in the second half of the 20th century, and came into popular use in the 1990s to describe the unprecedented international connectivity of the post-Cold War world.[1] Its origins can be traced back to 18th and 19th centuries due to advances in transportation and communications technology. This increase in global interactions has caused a growth in international trade and the exchange of ideas, beliefs, and culture. Globalization is primarily an economic process of interaction and integration that is associated with social and cultural aspects. However, disputes and international diplomacy are also large parts of the history of globalization, and of modern globalization.
 Economically, globalization involves goods, services, data, technology, and the economic resources of capital.[2] The expansion of global markets liberalizes the economic activities of the exchange of goods and funds. Removal of cross-border trade barriers has made the formation of global markets more feasible.[3] Advances in transportation, like the steam locomotive, steamship, jet engine, and container ships, and developments in telecommunication infrastructure such as the telegraph, the Internet, mobile phones, and smartphones, have been major factors in globalization and have generated further interdependence of economic and cultural activities around the globe.[4][5][6]
 Though many scholars place the origins of globalization in modern times, others trace its history to long before the European Age of Discovery and voyages to the New World, and some even to the third millennium BCE.[7] Large-scale globalization began in the 1820s, and in the late 19th century and early 20th century drove a rapid expansion in the connectivity of the world's economies and cultures.[8] The term global city was subsequently popularized by sociologist Saskia Sassen in her work The Global City: New York, London, Tokyo (1991).[9]
 In 2000, the International Monetary Fund (IMF) identified four basic aspects of globalization: trade and transactions, capital and investment movements, migration and movement of people, and the dissemination of knowledge.[10] Globalizing processes affect and are affected by business and work organization, economics, sociocultural resources, and the natural environment. Academic literature commonly divides globalization into three major areas: economic globalization, cultural globalization, and political globalization.[11]
 The word globalization was used in the English language as early as the 1930s, but only in the context of education, and the term failed to gain traction. Over the next few decades, the term was occasionally used by other scholars and media, but it was not clearly defined.[1] One of the first usages of the term in the meaning resembling the later, common usage was by French economist François Perroux in his essays from the early 1960s (in his French works he used the term ""mondialisation"" (literarly worldization in French), also translated as mundialization).[1] Theodore Levitt is often credited with popularizing the term and bringing it into the mainstream business audience in the later in the middle of 1980s.[1]
 Though often treated as synonyms, in French, globalization is seen as a stage following mondialisation, a stage that implies the dissolution of national identities and the abolishment of borders inside the world network of economic exchanges.[12]
 Since its inception, the concept of globalization has inspired competing definitions and interpretations. Its antecedents date back to the great movements of trade and empire across Asia and the Indian Ocean from the 15th century onward.[13][14]
 In 1848, Karl Marx noticed the increasing level of national inter-dependence brought on by capitalism, and predicted the universal character of the modern world society. He states:
 The bourgeoisie has through its exploitation of the world market given a cosmopolitan character to production and consumption in every country. To the great chagrin of Reactionists, it has drawn from under the feet of industry the national ground on which it stood. All old-established national industries have been destroyed or are daily being destroyed. . . . In place of the old local and national seclusion and self-sufficiency, we have intercourse in every direction, universal inter-dependence of nations.[15] Sociologists Martin Albrow and Elizabeth King define globalization as ""all those processes by which the people of the world are incorporated into a single world society.""[2] In The Consequences of Modernity, Anthony Giddens writes: ""Globalization can thus be defined as the intensification of worldwide social relations which link distant localities in such a way that local happenings are shaped by events occurring many miles away and vice versa.""[16] In 1992, Roland Robertson, professor of sociology at the University of Aberdeen and an early writer in the field, described globalization as ""the compression of the world and the intensification of the consciousness of the world as a whole.""[17]
 In Global Transformations, David Held and his co-writers state:
 Although in its simplistic sense globalization refers to the widening, deepening and speeding up of global interconnection, such a definition begs further elaboration. ... Globalization can be on a continuum with the local, national and regional. At one end of the continuum lie social and economic relations and networks which are organized on a local and/or national basis; at the other end lie social and economic relations and networks which crystallize on the wider scale of regional and global interactions. Globalization can refer to those spatial-temporal processes of change which underpin a transformation in the organization of human affairs by linking together and expanding human activity across regions and continents. Without reference to such expansive spatial connections, there can be no clear or coherent formulation of this term. ... A satisfactory definition of globalization must capture each of these elements: extensity (stretching), intensity, velocity and impact.[18] Held and his co-writers' definition of globalization in that same book as ""transformation in the spatial organization of social relations and transactions—assessed in terms of their extensity, intensity, velocity and impact—generating transcontinental or inter-regional flows"" was called ""probably the most widely-cited definition"" in the 2014 DHL Global Connectiveness Index.[19]
 Swedish journalist Thomas Larsson, in his book The Race to the Top: The Real Story of Globalization, states that globalization:
 ...is the process of world shrinkage, of distances getting shorter, things moving closer. It pertains to the increasing ease with which somebody on one side of the world can interact, to mutual benefit, with somebody on the other side of the world.[20] Paul James defines globalization with a more direct and historically contextualized emphasis:
 Globalization is the extension of social relations across world-space, defining that world-space in terms of the historically variable ways that it has been practiced and socially understood through changing world-time.[21] Manfred Steger, professor of global studies and research leader in the Global Cities Institute at RMIT University, identifies four main empirical dimensions of globalization: economic, political, cultural, and ecological. A fifth dimension—the ideological—cutting across the other four. The ideological dimension, according to Steger, is filled with a range of norms, claims, beliefs, and narratives about the phenomenon itself.[22]
 James and Steger stated that the concept of globalization ""emerged from the intersection of four interrelated sets of 'communities of practice' (Wenger, 1998): academics, journalists, publishers/editors, and librarians.""[1]: 424  They note the term was used ""in education to describe the global life of the mind""; in international relations to describe the extension of the European Common Market, and in journalism to describe how the ""American Negro and his problem are taking on a global significance"".[1] They have also argued that four forms of globalization can be distinguished that complement and cut across the solely empirical dimensions.[21][23] According to James, the oldest dominant form of globalization is embodied globalization, the movement of people. A second form is agency-extended globalization, the circulation of agents of different institutions, organizations, and polities, including imperial agents. Object-extended globalization, a third form, is the movement of commodities and other objects of exchange. He calls the transmission of ideas, images, knowledge, and information across world-space disembodied globalization, maintaining that it is currently the dominant form of globalization. James holds that this series of distinctions allows for an understanding of how, today, the most embodied forms of globalization such as the movement of refugees and migrants are increasingly restricted, while the most disembodied forms such as the circulation of financial instruments and codes are the most deregulated.[24]
 The journalist Thomas L. Friedman popularized the term ""flat world"", arguing that globalized trade, outsourcing, supply-chaining, and political forces had permanently changed the world, for better and worse. He asserted that the pace of globalization was quickening and that its impact on business organization and practice would continue to grow.[25]
 Economist Takis Fotopoulos defined ""economic globalization"" as the opening and deregulation of commodity, capital, and labor markets that led toward present neoliberal globalization. He used ""political globalization"" to refer to the emergence of a transnational élite and a phasing out of the nation-state. Meanwhile, he used ""cultural globalization"" to reference the worldwide homogenization of culture. Other of his usages included ""ideological globalization"", ""technological globalization"", and ""social globalization"".[26]
 Lechner and Boli (2012) define globalization as more people across large distances becoming connected in more and different ways.[27]
 ""Globophobia"" is used to refer to the fear of globalization, though it can also mean the fear of balloons.[28][29][30]
 There are both distal and proximate causes which can be traced in the historical factors affecting globalization. Large-scale globalization began in the 19th century.[31]
 Archaic globalization conventionally refers to a phase in the history of globalization including globalizing events and developments from the time of the earliest civilizations until roughly the 1600s. This term is used to describe the relationships between communities and states and how they were created by the geographical spread of ideas and social norms at both local and regional levels.[32]
 In this schema, three main prerequisites are posited for globalization to occur. The first is the idea of Eastern Origins, which shows how Western states have adapted and implemented learned principles from the East.[32] Without the spread of traditional ideas from the East, Western globalization would not have emerged the way it did. The interactions of states were not on a global scale and most often were confined to Asia, North Africa, the Middle East, and certain parts of Europe.[32] With early globalization, it was difficult for states to interact with others that were not close. Eventually, technological advances allowed states to learn of others' existence and thus another phase of globalization can occur. The third has to do with inter-dependency, stability, and regularity. If a state is not dependent on another, then there is no way for either state to be mutually affected by the other. This is one of the driving forces behind global connections and trade; without either, globalization would not have emerged the way it did and states would still be dependent on their own production and resources to work. This is one of the arguments surrounding the idea of early globalization. It is argued that archaic globalization did not function in a similar manner to modern globalization because states were not as interdependent on others as they are today.[32]
 Also posited is a ""multi-polar"" nature to archaic globalization, which involved the active participation of non-Europeans. Because it predated the Great Divergence in the nineteenth century, where Western Europe pulled ahead of the rest of the world in terms of industrial production and economic output, archaic globalization was a phenomenon that was driven not only by Europe but also by other economically developed Old World centers such as Gujarat, Bengal, coastal China, and Japan.[33]
 The German historical economist and sociologist Andre Gunder Frank argues that a form of globalization began with the rise of trade links between Sumer and the Indus Valley civilization in the third millennium BCE. This archaic globalization existed during the Hellenistic Age, when commercialized urban centers enveloped the axis of Greek culture that reached from India to Spain, including Alexandria and the other Alexandrine cities. Early on, the geographic position of Greece and the necessity of importing wheat forced the Greeks to engage in maritime trade. Trade in ancient Greece was largely unrestricted: the state controlled only the supply of grain.[7]
 Trade on the Silk Road was a significant factor in the development of civilizations from China, the Indian subcontinent, Persia, Europe, and Arabia, opening long-distance political and economic interactions between them.[34] Though silk was certainly the major trade item from China, common goods such as salt and sugar were traded as well; and religions, syncretic philosophies, and various technologies, as well as diseases, also traveled along the Silk Routes. In addition to economic trade, the Silk Road served as a means of carrying out cultural trade among the civilisations along its network.[35] The movement of people, such as refugees, artists, craftsmen, missionaries, robbers, and envoys, resulted in the exchange of religions, art, languages, and new technologies.[36]
 ""Early modern"" or ""proto-globalization"" covers a period of the history of globalization roughly spanning the years between 1600 and 1800. The concept of ""proto-globalization"" was first introduced by historians A. G. Hopkins and Christopher Bayly. The term describes the phase of increasing trade links and cultural exchange that characterized the period immediately preceding the advent of high ""modern globalization"" in the late 19th century.[37] This phase of globalization was characterized by the rise of maritime European empires, in the 15th and 17th centuries, first the Portuguese Empire (1415) followed by the Spanish Empire (1492), and later the Dutch and British Empires. In the 17th century, world trade developed further when chartered companies like the British East India Company (founded in 1600) and the Dutch East India Company (founded in 1602, often described as the first multinational corporation in which stock was offered) were established.[38]
 An alternative view from historians Dennis Flynn and Arturo Giraldez, postulated that: globalization began with the first circumnavigation of the globe under the Magellan-Elcano expedition which preluded the rise of global silver trade.[39][40]
 Early modern globalization is distinguished from modern globalization on the basis of expansionism, the method of managing global trade, and the level of information exchange. The period is marked by the shift of hegemony to Western Europe, the rise of larger-scale conflicts between powerful nations such as the Thirty Years' War, and demand for commodities, most particularly slaves. The triangular trade made it possible for Europe to take advantage of resources within the Western Hemisphere. The transfer of animal stocks, plant crops, and epidemic diseases associated with Alfred W. Crosby's concept of the Columbian exchange also played a central role in this process. European, Middle Eastern, Indian, Southeast Asian, and Chinese merchants were all involved in early modern trade and communications, particularly in the Indian Ocean region.
 According to economic historians Kevin H. O'Rourke, Leandro Prados de la Escosura, and Guillaume Daudin, several factors promoted globalization in the period 1815–1870:[41]
 During the 19th century, globalization approached its form as a direct result of the Industrial Revolution. Industrialization allowed standardized production of household items using economies of scale while rapid population growth created sustained demand for commodities. In the 19th century, steamships reduced the cost of international transportation significantly and railroads made inland transportation cheaper. The transportation revolution occurred some time between 1820 and 1850.[31] More nations embraced international trade.[31] Globalization in this period was decisively shaped by nineteenth-century imperialism such as in Africa and Asia. The invention of shipping containers in 1956 helped advance the globalization of commerce.[42][43]
 After World War II, work by politicians led to the agreements of the Bretton Woods Conference, in which major governments laid down the framework for international monetary policy, commerce, and finance, and the founding of several international institutions intended to facilitate economic growth by lowering trade barriers. Initially, the General Agreement on Tariffs and Trade (GATT) led to a series of agreements to remove trade restrictions. GATT's successor was the World Trade Organization (WTO), which provided a framework for negotiating and formalizing trade agreements and a dispute resolution process. Exports nearly doubled from 8.5% of total gross world product in 1970 to 16.2% in 2001.[44] The approach of using global agreements to advance trade stumbled with the failure of the Doha Development Round of trade negotiation. Many countries then shifted to bilateral or smaller multilateral agreements, such as the 2011 United States–Korea Free Trade Agreement.
 Since the 1970s, aviation has become increasingly affordable to middle classes in developed countries. Open skies policies and low-cost carriers have helped to bring competition to the market. In the 1990s, the growth of low-cost communication networks cut the cost of communicating between countries. More work can be performed using a computer without regard to location. This included accounting, software development, and engineering design.
 Student exchange programs became popular after World War II, and are intended to increase the participants' understanding and tolerance of other cultures, as well as improving their language skills and broadening their social horizons. Between 1963 and 2006 the number of students studying in a foreign country increased 9 times.[45]
 Since the 1980s, modern globalization has spread rapidly through the expansion of capitalism and neoliberal ideologies.[46] The implementation of neoliberal policies has allowed for the privatization of public industry, deregulation of laws or policies that interfered with the free flow of the market, as well as cut-backs to governmental social services.[47] These neoliberal policies were introduced to many developing countries in the form of structural adjustment programs (SAPs) that were implemented by the World Bank and the International Monetary Fund (IMF).[46] These programs required that the country receiving monetary aid would open its markets to capitalism, privatize public industry, allow free trade, cut social services like healthcare and education and allow the free movement of giant multinational corporations.[48] These programs allowed the World Bank and the IMF to become global financial market regulators that would promote neoliberalism and the creation of free markets for multinational corporations on a global scale.[49]
 In the late 19th and early 20th century, the connectedness of the world's economies and cultures grew very quickly. This slowed down from the 1910s onward due to the World Wars and the Cold War,[50] but picked up again in the 1980s and 1990s.[51] The revolutions of 1989 and subsequent liberalization in many parts of the world resulted in a significant expansion of global interconnectedness. The migration and movement of people can also be highlighted as a prominent feature of the globalization process. In the period between 1965 and 1990, the proportion of the labor force migrating approximately doubled. Most migration occurred between the developing countries and least developed countries (LDCs).[52] As economic integration intensified workers moved to areas with higher wages and most of the developing world oriented toward the international market economy. The collapse of the Soviet Union not only ended the Cold War's division of the world – it also left the United States its sole policeman and an unfettered advocate of free market.[according to whom?] It also resulted in the growing prominence of attention focused on the movement of diseases, the proliferation of popular culture and consumer values, the growing prominence of international institutions like the UN, and concerted international action on such issues as the environment and human rights.[53] Other developments as dramatic were the Internet's becoming influential in connecting people across the world; As of June 2012[update], more than 2.4 billion people—over a third of the world's human population—have used the services of the Internet.[54][55] Growth of globalization has never been smooth. One influential event was the late 2000s recession, which was associated with lower growth (in areas such as cross-border phone calls and Skype usage) or even temporarily negative growth (in areas such as trade) of global interconnectedness.[56][57]
 The China–United States trade war, starting in 2018, negatively affected trade between the two largest national economies. The economic impact of the COVID-19 pandemic included a massive decline in tourism and international business travel as many countries temporarily closed borders. The 2021–2022 global supply chain crisis resulted from temporary shutdowns of manufacturing and transportation facilities, and labor shortages. Supply problems incentivized some switches to domestic production.[58] The economic impact of the 2022 Russian invasion of Ukraine included a blockade of Ukrainian ports and international sanctions on Russia, resulting in some de-coupling of the Russian economy with global trade, especially with the European Union and other Western countries.
 Modern consensus for the last 15 years regards globalization as having run its course and gone into decline.[59] A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the global financial crisis. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum.[60][61]
 Economic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital.[63] Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market.[64] Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes[65] competition, technology, and corporations and industries.[63]
 Current globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration.[66]
 International standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization.[42] International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.
 A multinational corporation, or worldwide enterprise,[67] is an organization that owns or controls the production of goods or services in one or more countries other than their home country.[68] It can also be referred to as an international corporation, a transnational corporation, or a stateless corporation.[69]
 A free-trade area is the region encompassing a trade bloc whose member countries have signed a free-trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers –  import quotas and tariffs –  and to increase trade of goods and services with each other.[70]
 If people are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border. Arguably, the most significant free-trade area in the world is the European Union, a politico-economic union of 27 member states that are primarily located in Europe. The EU has developed European Single Market through a standardized system of laws that apply in all member states. EU policies aim to ensure the free movement of people, goods, services, and capital within the internal market,[71]
 Trade facilitation looks at how procedures and controls governing the movement of goods across national borders can be improved to reduce associated cost burdens and maximize efficiency while safeguarding legitimate regulatory objectives.
 Global trade in services is also significant. For example, in India, business process outsourcing has been described as the ""primary engine of the country's development over the next few decades, contributing broadly to GDP growth, employment growth, and poverty alleviation"".[72][73]
 William I. Robinson's theoretical approach to globalization is a critique of Wallerstein's World Systems Theory. He believes that the global capital experienced today is due to a new and distinct form of globalization which began in the 1980s. Robinson argues not only are economic activities expanded across national boundaries but also there is a transnational fragmentation of these activities.[74] One important aspect of Robinson's globalization theory is that production of goods are increasingly global. This means that one pair of shoes can be produced by six countries, each contributing to a part of the production process.
 Cultural globalization refers to the transmission of ideas, meanings, and values around the world in such a way as to extend and intensify social relations.[75] This process is marked by the common consumption of cultures that have been diffused by the Internet, popular culture media, and international travel. This has added to processes of commodity exchange and colonization which have a longer history of carrying cultural meaning around the globe. The circulation of cultures enables individuals to partake in extended social relations that cross national and regional borders. The creation and expansion of such social relations is not merely observed on a material level. Cultural globalization involves the formation of shared norms and knowledge with which people associate their individual and collective cultural identities. It brings increasing interconnectedness among different populations and cultures.[76]
 Cross-cultural communication is a field of study that looks at how people from differing cultural backgrounds communicate, in similar and different ways among themselves, and how they endeavour to communicate across cultures. Intercultural communication is a related field of study.
 Cultural diffusion is the spread of cultural items—such as ideas, styles, religions, technologies, languages etc.
Cultural globalization has increased cross-cultural contacts, but may be accompanied by a decrease in the uniqueness of once-isolated communities. For example, sushi is available in Germany as well as Japan, but Euro-Disney outdraws the city of Paris, potentially reducing demand for ""authentic"" French pastry.[77][78][79] Globalization's contribution to the alienation of individuals from their traditions may be modest compared to the impact of modernity itself, as alleged by existentialists such as Jean-Paul Sartre and Albert Camus. Globalization has expanded recreational opportunities by spreading pop culture, particularly via the Internet and satellite television. The cultural diffusion can create a homogenizing force, where globalisation is seen as synonymous with homogenizing force via connectedness of markets, cultures, politics and the desire for modernizations through imperial countries sphere of influence.[80]
 Religions were among the earliest cultural elements to globalize, being spread by force, migration, evangelists, imperialists, and traders. Christianity, Islam, Buddhism, and more recently sects such as Mormonism are among those religions which have taken root and influenced endemic cultures in places far from their origins.[81]
 Globalization has strongly influenced sports.[82] For example, the modern Olympic Games has athletes from more than 200 nations participating in a variety of competitions.[83] The FIFA World Cup is the most widely viewed and followed sporting event in the world, exceeding even the Olympic Games; a ninth of the entire population of the planet watched the 2006 FIFA World Cup Final.[84][85][86]
 The term globalization implies transformation. Cultural practices including traditional music can be lost or turned into a fusion of traditions. Globalization can trigger a state of emergency for the preservation of musical heritage. Archivists may attempt to collect, record, or transcribe repertoires before melodies are assimilated or modified, while local musicians may struggle for authenticity and to preserve local musical traditions. Globalization can lead performers to discard traditional instruments. Fusion genres can become interesting fields of analysis.[87]
 Music has an important role in economic and cultural development during globalization. Music genres such as jazz and reggae began locally and later became international phenomena. Globalization gave support to the world music phenomenon by allowing music from developing countries to reach broader audiences.[88] Though the term ""World Music"" was originally intended for ethnic-specific music, globalization is now expanding its scope such that the term often includes hybrid subgenres such as ""world fusion"", ""global fusion"", ""ethnic fusion"",[89] and worldbeat.[90][91]
 Bourdieu claimed that the perception of consumption can be seen as self-identification and the formation of identity. Musically, this translates into each individual having their own musical identity based on likes and tastes. These likes and tastes are greatly influenced by culture, as this is the most basic cause for a person's wants and behavior. The concept of one's own culture is now in a period of change due to globalization. Also, globalization has increased the interdependency of political, personal, cultural, and economic factors.[93]
 A 2005 UNESCO report[94] showed that cultural exchange is becoming more frequent from Eastern Asia, but that Western countries are still the main exporters of cultural goods. In 2002, China was the third largest exporter of cultural goods, after the UK and US. Between 1994 and 2002, both North America's and the European Union's shares of cultural exports declined while Asia's cultural exports grew to surpass North America. Related factors are the fact that Asia's population and area are several times that of North America. Americanization is related to a period of high political American clout and of significant growth of America's shops, markets and objects being brought into other countries.
 Some critics of globalization argue that it harms the diversity of cultures. As a dominating country's culture is introduced into a receiving country through globalization, it can become a threat to the diversity of local culture. Some argue that globalization may ultimately lead to Westernization or Americanization of culture, where the dominating cultural concepts of economically and politically powerful Western countries spread and cause harm to local cultures.[95]
 Globalization is a diverse phenomenon that relates to a multilateral political world and to the increase of cultural objects and markets between countries. The Indian experience particularly reveals the plurality of the impact of cultural globalization.[96]
 Transculturalism is defined as ""seeing oneself in the other"".[97] Transcultural[98] is in turn described as ""extending through all human cultures""[98] or ""involving, encompassing, or combining elements of more than one culture"".[99] Children brought up in transcultural backgrounds are sometimes called third-culture kids.
 Political globalization refers to the growth of the worldwide political system, both in size and complexity. That system includes national governments, their governmental and intergovernmental organizations as well as government-independent elements of global civil society such as international non-governmental organizations and social movement organizations. One of the key aspects of the political globalization is the declining importance of the nation-state and the rise of other actors on the political scene.
William R. Thompson has defined it as ""the expansion of a global political system, and its institutions, in which inter-regional transactions (including, but certainly not limited to trade) are managed"".[100] 
Political globalization is one of the three main dimensions of globalization commonly found in academic literature, with the two other being economic globalization and cultural globalization.[11]
 Intergovernmentalism is a term in political science with two meanings. The first refers to a theory of regional integration originally proposed by Stanley Hoffmann; the second treats states and the national government as the primary factors for integration. Multi-level governance is an approach in political science and public administration theory that originated from studies on European integration. Multi-level governance gives expression to the idea that there are many interacting authority structures at work in the emergent global political economy. It illuminates the intimate entanglement between the domestic and international levels of authority.
 Some people are citizens of multiple nation-states. Multiple citizenship, also called dual citizenship or multiple nationality or dual nationality, is a person's citizenship status, in which a person is concurrently regarded as a citizen of more than one state under the laws of those states.
 Increasingly, non-governmental organizations influence public policy across national boundaries, including humanitarian aid and developmental efforts.[102] Philanthropic organizations with global missions are also coming to the forefront of humanitarian efforts; charities such as the Bill and Melinda Gates Foundation, Accion International, the Acumen Fund (now Acumen) and the Echoing Green have combined the business model with philanthropy, giving rise to business organizations such as the Global Philanthropy Group and new associations of philanthropists such as the Global Philanthropy Forum. The Bill and Melinda Gates Foundation projects include a current multibillion-dollar commitment to funding immunizations in some of the world's more impoverished but rapidly growing countries.[103] The Hudson Institute estimates total private philanthropic flows to developing countries at US$59 billion in 2010.[104]
 As a response to globalization, some countries have embraced isolationist policies. For example, the North Korean government makes it very difficult for foreigners to enter the country and strictly monitors their activities when they do. Aid workers are subject to considerable scrutiny and excluded from places and regions the government does not wish them to enter. Citizens cannot freely leave the country.[105][106]
 Globalization has been a gendered process where giant multinational corporations have outsourced jobs to low-wage, low skilled, quota free economies like the ready made garment industry in Bangladesh where poor women make up the majority of labor force. Despite a large proportion of women workers in the garment industry, women are still heavily underemployed compared to men. Most women that are employed in the garment industry come from the countryside of Bangladesh triggering migration of women in search of garment work. It is still unclear as to whether or not access to paid work for women where it did not exist before has empowered them. The answers varied depending on whether it is the employers perspective or the workers and how they view their choices. Women workers did not see the garment industry as economically sustainable for them in the long run due to long hours standing and poor working conditions. Although women workers did show significant autonomy over their personal lives including their ability to negotiate with family, more choice in marriage, and being valued as a wage earner in the family. This did not translate into workers being able to collectively organize themselves in order to negotiate a better deal for themselves at work.[107]
 Another example of outsourcing in manufacturing includes the maquiladora industry in Ciudad Juarez, Mexico where poor women make up the majority of the labor force. Women in the maquiladora industry have produced high levels of turnover not staying long enough to be trained compared to men. A gendered two tiered system within the maquiladora industry has been created that focuses on training and worker loyalty. Women are seen as being untrainable, placed in un-skilled, low wage jobs, while men are seen as more trainable with less turnover rates, and placed in more high skilled technical jobs. The idea of training has become a tool used against women to blame them for their high turnover rates which also benefit the industry keeping women as temporary workers.[108]
 Scholars also occasionally discuss other, less common dimensions of globalization, such as environmental globalization (the internationally coordinated practices and regulations, often in the form of international treaties, regarding environmental protection)[109] or military globalization (growth in global extent and scope of security relationships).[110] Those dimensions, however, receive much less attention the three described above, as academic literature commonly subdivides globalization into three major areas: economic globalization, cultural globalization and political globalization.[11]
 An essential aspect of globalization is movement of people, and state-boundary limits on that movement have changed across history.[111] The movement of tourists and business people opened up over the last century. As transportation technology improved, travel time and costs decreased dramatically between the 18th and early 20th century. For example, travel across the Atlantic Ocean used to take up to 5 weeks in the 18th century, but around the time of the 20th century it took a mere 8 days.[112] Today, modern aviation has made long-distance transportation quick and affordable.
 Tourism is travel for pleasure. The developments in technology and transportation infrastructure, such as jumbo jets, low-cost airlines, and more accessible airports have made many types of tourism more affordable. At any given moment half a million people are in the air.[113] International tourist arrivals surpassed the milestone of 1 billion tourists globally for the first time in 2012.[114]
A visa is a conditional authorization granted by a country to a foreigner, allowing them to enter and temporarily remain within, or to leave that country. Some countries – such as those in the Schengen Area – have agreements with other countries allowing each other's citizens to travel between them without visas (for example, Switzerland is part of a Schengen Agreement allowing easy travel for people from countries within the European Union). The World Tourism Organization announced that the number of tourists who require a visa before traveling was at its lowest level ever in 2015.[115]
 Immigration is the international movement of people into a destination country of which they are not natives or where they do not possess citizenship in order to settle or reside there, especially as permanent residents or naturalized citizens, or to take-up employment as a migrant worker or temporarily as a foreign worker.[116][117][118] According to the International Labour Organization, as of 2014[update] there were an estimated 232 million international migrants in the world (defined as persons outside their country of origin for 12 months or more) and approximately half of them were estimated to be economically active (i.e. being employed or seeking employment).[119] International movement of labor is often seen as important to economic development. For example, freedom of movement for workers in the European Union means that people can move freely between member states to live, work, study or retire in another country.
 Globalization is associated with a dramatic rise in international education. The development of global cross-cultural competence in the workforce through ad-hoc training has deserved increasing attention in recent times.[121][122] More and more students are seeking higher education in foreign countries and many international students now consider overseas study a stepping-stone to permanent residency within a country.[123] The contributions that foreign students make to host nation economies, both culturally and financially has encouraged major players to implement further initiatives to facilitate the arrival and integration of overseas students, including substantial amendments to immigration and visa policies and procedures.[45]
 A transnational marriage is a marriage between two people from different countries. A variety of special issues arise in marriages between people from different countries, including those related to citizenship and culture, which add complexity and challenges to these kinds of relationships. In an age of increasing globalization, where a growing number of people have ties to networks of people and places across the globe, rather than to a current geographic location, people are increasingly marrying across national boundaries. Transnational marriage is a by-product of the movement and migration of people.
 Before electronic communications, long-distance communications relied on mail. Speed of global communications was limited by the maximum speed of courier services (especially horses and ships) until the mid-19th century. The electric telegraph was the first method of instant long-distance communication. For example, before the first transatlantic cable, communications between Europe and the Americas took weeks because ships had to carry mail across the ocean. The first transatlantic cable reduced communication time considerably, allowing a message and a response in the same day. Lasting transatlantic telegraph connections were achieved in the 1865–1866. The first wireless telegraphy transmitters were developed in 1895.
 The Internet has been instrumental in connecting people across geographical boundaries. For example, Facebook is a social networking service which has more than 1.65 billion monthly active users as of 31 March 2016[update].[125]
 Globalization can be spread by Global journalism which provides massive information and relies on the internet to interact, ""makes it into an everyday routine to investigate how people and their actions, practices, problems, life conditions, etc. in different parts of the world are interrelated. possible to assume that global threats such as climate change precipitate the further establishment of global journalism.""[126]
 In the current era of globalization, the world is more interdependent than at any other time. Efficient and inexpensive transportation has left few places inaccessible, and increased global trade has brought more and more people into contact with animal diseases that have subsequently jumped species barriers (see zoonosis).[127]
 Coronavirus disease 2019, abbreviated COVID-19, first appeared in Wuhan, China in November 2019. More than 180 countries have reported cases since then.[128] As of April 6, 2020[update], the U.S. has the most confirmed active cases in the world.[129] More than 3.4 million people from the worst-affected countries entered the U.S. in the first three months since the inception of the COVID-19 pandemic.[130] This has caused a detrimental impact on the global economy, particularly for SME's and Microbusinesses with unlimited liability/self-employed, leaving them vulnerable to financial difficulties, increasing the market share for oligopolistic markets as well as increasing the barriers of entry.
 One index of globalization is the KOF Index of Globalization, which measures three important dimensions of globalization: economic, social, and political.[131] Another is the A.T. Kearney / Foreign Policy Magazine Globalization Index.[132]
 Measurements of economic globalization typically focus on variables such as trade, Foreign Direct Investment (FDI), Gross Domestic Product (GDP), portfolio investment, and income. However, newer indices attempt to measure globalization in more general terms, including variables related to political, social, cultural, and even environmental aspects of globalization.[133][134]
 The DHL Global Connectedness Index studies four main types of cross-border flow: trade (in both goods and services), information, people (including tourists, students, and migrants), and capital. It shows that the depth of global integration fell by about one-tenth after 2008, but by 2013 had recovered well above its pre-crash peak.[19][56] The report also found a shift of economic activity to emerging economies.[19]
 Reactions to processes contributing to globalization have varied widely with a history as long as extraterritorial contact and trade. Philosophical differences regarding the costs and benefits of such processes give rise to a broad-range of ideologies and social movements. Proponents of economic growth, expansion, and development, in general, view globalizing processes as desirable or necessary to the well-being of human society.[135]
 Antagonists view one or more globalizing processes as detrimental to social well-being on a global or local scale;[135] this includes those who focus on social or natural sustainability of long-term and continuous economic expansion, the social structural inequality caused by these processes, and the colonial, imperialistic, or hegemonic ethnocentrism, cultural assimilation and cultural appropriation that underlie such processes.
 Globalization tends to bring people into contact with foreign people and cultures. Xenophobia is the fear of that which is perceived to be foreign or strange.[136][137] Xenophobia can manifest itself in many ways involving the relations and perceptions of an ingroup towards an outgroup, including a fear of losing identity, suspicion of its activities, aggression, and desire to eliminate its presence to secure a presumed purity.[138]
 Critiques of globalization generally stem from discussions surrounding the impact of such processes on the planet as well as the human costs. They challenge directly traditional metrics, such as GDP, and look to other measures, such as the Gini coefficient[139] or the Happy Planet Index,[140] and point to a ""multitude of interconnected fatal consequences–social disintegration, a breakdown of democracy, more rapid and extensive deterioration of the environment, the spread of new diseases, increasing poverty and alienation""[141] which they claim are the unintended consequences of globalization. Others point out that, while the forces of globalization have led to the spread of western-style democracy, this has been accompanied by an increase in inter-ethnic tension and violence as free market economic policies combine with democratic processes of universal suffrage as well as an escalation in militarization to impose democratic principles and as a means to conflict resolution.[142]
 On 9 August 2019, Pope Francis denounced isolationism and hinted that the Catholic Church will embrace globalization at the October 2019 Amazonia Synod, stating ""the whole is greater than the parts. Globalization and unity should not be conceived as a sphere, but as a polyhedron: each people retains its identity in unity with others""[143]
 As a complex and multifaceted phenomenon, globalization is considered by some as a form of capitalist expansion which entails the integration of local and national economies into a global, unregulated market economy.[144] A 2005 study by Peer Fis and Paul Hirsch found a large increase in articles negative towards globalization in the years prior. In 1998, negative articles outpaced positive articles by two to one.[145] The number of newspaper articles showing negative framing rose from about 10% of the total in 1991 to 55% of the total in 1999. This increase occurred during a period when the total number of articles concerning globalization nearly doubled.[145]
 A number of international polls have shown that residents of Africa and Asia tend to view globalization more favorably than residents of Europe or North America. In Africa, a Gallup poll found that 70% of the population views globalization favorably.[146] The BBC found that 50% of people believed that economic globalization was proceeding too rapidly, while 35% believed it was proceeding too slowly.[147]
 In 2004, Philip Gordon stated that ""a clear majority of Europeans believe that globalization can enrich their lives, while believing the European Union can help them take advantage of globalization's benefits while shielding them from its negative effects."" The main opposition consisted of socialists, environmental groups, and nationalists. Residents of the EU did not appear to feel threatened by globalization in 2004. The EU job market was more stable and workers were less likely to accept wage/benefit cuts. Social spending was much higher than in the US.[148] In a Danish poll in 2007, 76% responded that globalization is a good thing.[149]
 Fiss, et al., surveyed US opinion in 1993. Their survey showed that, in 1993, more than 40% of respondents were unfamiliar with the concept of globalization. When the survey was repeated in 1998, 89% of the respondents had a polarized view of globalization as being either good or bad. At the same time, discourse on globalization, which began in the financial community before shifting to a heated debate between proponents and disenchanted students and workers. Polarization increased dramatically after the establishment of the WTO in 1995; this event and subsequent protests led to a large-scale anti-globalization movement.[145]
Initially, college educated workers were likely to support globalization. Less educated workers, who were more likely to compete with immigrants and workers in developing countries, tended to be opponents. The situation changed after the financial crisis of 2007. According to a 1997 poll 58% of college graduates said globalization had been good for the US. By 2008 only 33% thought it was good. Respondents with high school education also became more opposed.[150]
 According to Takenaka Heizo and Chida Ryokichi, as of 1998[update] there was a perception in Japan that the economy was ""Small and Frail"". However, Japan was resource-poor and used exports to pay for its raw materials. Anxiety over their position caused terms such as internationalization and globalization to enter everyday language. However, Japanese tradition was to be as self-sufficient as possible, particularly in agriculture.[151]
 Many in developing countries see globalization as a positive force that lifts them out of poverty.[152] Those opposing globalization typically combine environmental concerns with nationalism. Opponents consider governments as agents of neo-colonialism that are subservient to multinational corporations.[153] Much of this criticism comes from the middle class; the Brookings Institution suggested this was because the middle class perceived upwardly mobile low-income groups as threatening to their economic security.[154]
 The literature analyzing the economics of free trade is extremely rich with extensive work having been done on the theoretical and empirical effects. Though it creates winners and losers, the broad consensus among economists is that free trade is a large and unambiguous net gain for society.[155][156] In a 2006 survey of 83 American economists, ""87.5% agree that the U.S. should eliminate remaining tariffs and other barriers to trade"" and ""90.1% disagree with the suggestion that the U.S. should restrict employers from outsourcing work to foreign countries.""[157]
 Quoting Harvard economics professor N. Gregory Mankiw, ""Few propositions command as much consensus among professional economists as that open world trade increases economic growth and raises living standards.""[158] In a survey of leading economists, none disagreed with the notion that ""freer trade improves productive efficiency and offers consumers better choices, and in the long run these gains are much larger than any effects on employment.""[159] Most economists would agree that although increasing returns to scale might mean that certain industry could settle in a geographical area without any strong economic reason derived from comparative advantage, this is not a reason to argue against free trade because the absolute level of output enjoyed by both ""winner"" and ""loser"" will increase with the ""winner"" gaining more than the ""loser"" but both gaining more than before in an absolute level.
 In the book The End of Poverty, Jeffrey Sachs discusses how many factors can affect a country's ability to enter the world market, including government corruption; legal and social disparities based on gender, ethnicity, or caste; diseases such as AIDS and malaria; lack of infrastructure (including transportation, communications, health, and trade); unstable political landscapes; protectionism; and geographic barriers.[160] Jagdish Bhagwati, a former adviser to the U.N. on globalization, holds that, although there are obvious problems with overly rapid development, globalization is a very positive force that lifts countries out of poverty by causing a virtuous economic cycle associated with faster economic growth.[152] However, economic growth does not necessarily mean a reduction in poverty; in fact, the two can coexist. Economic growth is conventionally measured using indicators such as GDP and GNI that do not accurately reflect the growing disparities in wealth.[161] Additionally, Oxfam International argues that poor people are often excluded from globalization-induced opportunities ""by a lack of productive assets, weak infrastructure, poor education and ill-health;""[162] effectively leaving these marginalized groups in a poverty trap. Economist Paul Krugman is another staunch supporter of globalization and free trade with a record of disagreeing with many critics of globalization. He argues that many of them lack a basic understanding of comparative advantage and its importance in today's world.[163]
 The flow of migrants to advanced economies has been claimed to provide a means through which global wages converge. An IMF study noted a potential for skills to be transferred back to developing countries as wages in those a countries rise.[10] Lastly, the dissemination of knowledge has been an integral aspect of globalization. Technological innovations (or technological transfer) are conjectured to benefit most developing and least developing countries (LDCs), as for example in the adoption of mobile phones.[52]
 There has been a rapid economic growth in Asia after embracing market orientation-based economic policies that encourage private property rights, free enterprise and competition. In particular, in East Asian developing countries, GDP per head rose at 5.9% a year from 1975 to 2001 (according to 2003 Human Development Report[164] of UNDP). Like this, the British economic journalist Martin Wolf says that incomes of poor developing countries, with more than half the world's population, grew substantially faster than those of the world's richest countries that remained relatively stable in its growth, leading to reduced international inequality and the incidence of poverty.
 Certain demographic changes in the developing world after active economic liberalization and international integration resulted in rising general welfare and, hence, reduced inequality. According to Wolf, in the developing world as a whole, life expectancy rose by four months each year after 1970 and infant mortality rate declined from 107 per thousand in 1970 to 58 in 2000 due to improvements in standards of living and health conditions. Also, adult literacy in developing countries rose from 53% in 1970 to 74% in 1998 and much lower illiteracy rate among the young guarantees that rates will continue to fall as time passes. Furthermore, the reduction in fertility rate in the developing world as a whole from 4.1 births per woman in 1980 to 2.8 in 2000 indicates improved education level of women on fertility, and control of fewer children with more parental attention and investment.[166] Consequently, more prosperous and educated parents with fewer children have chosen to withdraw their children from the labor force to give them opportunities to be educated at school improving the issue of child labor. Thus, despite seemingly unequal distribution of income within these developing countries, their economic growth and development have brought about improved standards of living and welfare for the population as a whole.
 Per capita gross domestic product (GDP) growth among post-1980 globalizing countries accelerated from 1.4 percent a year in the 1960s and 2.9 percent a year in the 1970s to 3.5 percent in the 1980s and 5.0 percent in the 1990s. This acceleration in growth seems even more remarkable given that the rich countries saw steady declines in growth from a high of 4.7 percent in the 1960s to 2.2 percent in the 1990s. Also, the non-globalizing developing countries seem to fare worse than the globalizers, with the former's annual growth rates falling from highs of 3.3 percent during the 1970s to only 1.4 percent during the 1990s. This rapid growth among the globalizers is not simply due to the strong performances of China and India in the 1980s and 1990s—18 out of the 24 globalizers experienced increases in growth, many of them quite substantial.[167]
 The globalization of the late 20th and early 21st centuries has led to the resurfacing of the idea that the growth of economic interdependence promotes peace.[168] This idea had been very powerful during the globalization of the late 19th and early 20th centuries, and was a central doctrine of classical liberals of that era, such as the young John Maynard Keynes (1883–1946).[169]
 Some opponents of globalization see the phenomenon as a promotion of corporate interests.[170] They also claim that the increasing autonomy and strength of corporate entities shapes the political policy of countries.[171][172] They advocate global institutions and policies that they believe better address the moral claims of poor and working classes as well as environmental concerns.[173] Economic arguments by fair trade theorists claim that unrestricted free trade benefits those with more financial leverage (i.e. the rich) at the expense of the poor.[174]
 Globalization allows corporations to outsource manufacturing and service jobs from high-cost locations, creating economic opportunities with the most competitive wages and worker benefits.[72] Critics of globalization say that it disadvantages poorer countries. While it is true that free trade encourages globalization among countries, some countries try to protect their domestic suppliers. The main export of poorer countries is usually agricultural productions. Larger countries often subsidize their farmers (e.g., the EU's Common Agricultural Policy), which lowers the market price for foreign crops.[175]
 Democratic globalization is a movement towards an institutional system of global democracy that would give world citizens a say in political organizations. This would, in their view, bypass nation-states, corporate oligopolies, ideological non-governmental organizations (NGO), political cults and mafias. One of its most prolific proponents is the British political thinker David Held. Advocates of democratic globalization argue that economic expansion and development should be the first phase of democratic globalization, which is to be followed by a phase of building global political institutions. Francesco Stipo, Director of the United States Association of the Club of Rome, advocates unifying nations under a world government, suggesting that it ""should reflect the political and economic balances of world nations. A world confederation would not supersede the authority of the State governments but rather complement it, as both the States and the world authority would have power within their sphere of competence"".[176] Former Canadian Senator Douglas Roche, O.C., viewed globalization as inevitable and advocated creating institutions such as a directly elected United Nations Parliamentary Assembly to exercise oversight over unelected international bodies.[177]
 Global civics suggests that civics can be understood, in a global sense, as a social contract between global citizens in the age of interdependence and interaction. The disseminators of the concept define it as the notion that we have certain rights and responsibilities towards each other by the mere fact of being human on Earth.[178] World citizen has a variety of similar meanings, often referring to a person who disapproves of traditional geopolitical divisions derived from national citizenship. An early incarnation of this sentiment can be found in Socrates, whom Plutarch quoted as saying: ""I am not an Athenian, or a Greek, but a citizen of the world.""[179] In an increasingly interdependent world, world citizens need a compass to frame their mindsets and create a shared consciousness and sense of global responsibility in world issues such as environmental problems and nuclear proliferation.[180]
 Baha'i-inspired author Meyjes, while favoring the single world community and emergent global consciousness, warns of globalization[181] as a cloak for an expeditious economic, social, and cultural Anglo-dominance that is insufficiently inclusive to inform the emergence of an optimal world civilization. He proposes a process of ""universalization"" as an alternative.
 Cosmopolitanism is the proposal that all human ethnic groups belong to a single community based on a shared morality. A person who adheres to the idea of cosmopolitanism in any of its forms is called a cosmopolitan or cosmopolite.[182] A cosmopolitan community might be based on an inclusive morality, a shared economic relationship, or a political structure that encompasses different nations. The cosmopolitan community is one in which individuals from different places (e.g. nation-states) form relationships based on mutual respect. For instance, Kwame Anthony Appiah suggests the possibility of a cosmopolitan community in which individuals from varying locations (physical, economic, etc.) enter relationships of mutual respect despite their differing beliefs (religious, political, etc.).[183]
 Canadian philosopher Marshall McLuhan popularized the term Global Village beginning in 1962.[184] His view suggested that globalization would lead to a world where people from all countries will become more integrated and aware of common interests and shared humanity.[185]
 Military cooperation – Past examples of international cooperation exist. One example is the security cooperation between the United States and the former Soviet Union after the end of the Cold War, which astonished international society. Arms control and disarmament agreements, including the Strategic Arms Reduction Treaty (see START I, START II, START III, and New START) and the establishment of NATO's Partnership for Peace, the Russia NATO Council, and the G8 Global Partnership against the Spread of Weapons and Materials of Mass Destruction, constitute concrete initiatives of arms control and de-nuclearization. The US–Russian cooperation was further strengthened by anti-terrorism agreements enacted in the wake of 9/11.[186]
 Environmental cooperation – One of the biggest successes of environmental cooperation has been the agreement to reduce chlorofluorocarbon (CFC) emissions, as specified in the Montreal Protocol, in order to stop ozone depletion. The most recent debate around nuclear energy and the non-alternative coal-burning power plants constitutes one more consensus on what not to do. Thirdly, significant achievements in IC can be observed through development studies.[186]
 Economic cooperation – One of the biggest challenges in 2019 with globalization is that many believe the progress made in the past decades are now back tracking. The back tracking of globalization has coined the term ""Slobalization."" Slobalization is a new, slower pattern of globalization.[187]
 Anti-globalization, or counter-globalization,[188] consists of a number of criticisms of globalization but, in general, is critical of the globalization of corporate capitalism.[189] The movement is also commonly referred to as the alter-globalization movement, anti-globalist movement, anti-corporate globalization movement,[190] or movement against neoliberal globalization. Opponents of globalization argue that power and respect in terms of international trade between the developed and underdeveloped countries of the world are unequally distributed.[191] The diverse subgroups that make up this movement include some of the following: trade unionists, environmentalists, anarchists, land rights and indigenous rights activists, organizations promoting human rights and sustainable development, opponents of privatization, and anti-sweatshop campaigners.[192]
 In The Revolt of the Elites and the Betrayal of Democracy, Christopher Lasch analyzes[193] the widening gap between the top and bottom of the social composition in the United States. For him, our epoch is determined by a social phenomenon: the revolt of the elites, in reference to The Revolt of the Masses (1929) by the Spanish philosopher José Ortega y Gasset. According to Lasch, the new elites, i.e. those who are in the top 20% in terms of income, through globalization which allows total mobility of capital, no longer live in the same world as their fellow-citizens. In this, they oppose the old bourgeoisie of the nineteenth and twentieth centuries, which was constrained by its spatial stability to a minimum of rooting and civic obligations. Globalization, according to the sociologist, has turned elites into tourists in their own countries. The denationalization of business enterprise tends to produce a class who see themselves as ""world citizens, but without accepting ... any of the obligations that citizenship in a polity normally implies"". Their ties to an international culture of work, leisure, information – make many of them deeply indifferent to the prospect of national decline. Instead of financing public services and the public treasury, new elites are investing their money in improving their voluntary ghettos: private schools in their residential neighborhoods, private police, garbage collection systems. They have ""withdrawn from common life"". Composed of those who control the international flows of capital and information, who preside over philanthropic foundations and institutions of higher education, manage the instruments of cultural production and thus fix the terms of public debate. So, the political debate is limited mainly to the dominant classes and political ideologies lose all contact with the concerns of the ordinary citizen. The result of this is that no one has a likely solution to these problems and that there are furious ideological battles on related issues. However, they remain protected from the problems affecting the working classes: the decline of industrial activity, the resulting loss of employment, the decline of the middle class, increasing the number of the poor, the rising crime rate, growing drug trafficking, the urban crisis.
 D.A. Snow et al. contend that the anti-globalization movement is an example of a new social movement, which uses tactics that are unique and use different resources than previously used before in other social movements.[194]
 One of the most infamous tactics of the movement is the Battle of Seattle in 1999, where there were protests against the World Trade Organization's Third Ministerial Meeting. All over the world, the movement has held protests outside meetings of institutions such as the WTO, the International Monetary Fund (IMF), the World Bank, the World Economic Forum, and the Group of Eight (G8).[192] Within the Seattle demonstrations the protesters that participated used both creative and violent tactics to gain the attention towards the issue of globalization.
 Capital markets have to do with raising and investing money in various human enterprises. Increasing integration of these financial markets between countries leads to the emergence of a global capital marketplace or a single world market. In the long run, increased movement of capital between countries tends to favor owners of capital more than any other group; in the short run, owners and workers in specific sectors in capital-exporting countries bear much of the burden of adjusting to increased movement of capital.[195]
 Those opposed to capital market integration on the basis of human rights issues are especially disturbed[according to whom?] by the various abuses which they think are perpetuated by global and international institutions that, they say, promote neoliberalism without regard to ethical standards. Common targets include the World Bank (WB), International Monetary Fund (IMF), the Organisation for Economic Co-operation and Development (OECD) and the World Trade Organization (WTO) and free trade treaties like the North American Free Trade Agreement (NAFTA), Free Trade Area of the Americas (FTAA), the Multilateral Agreement on Investment (MAI) and the General Agreement on Trade in Services (GATS). In light of the economic gap between rich and poor countries, movement adherents claim free trade without measures in place to protect the under-capitalized will contribute only to the strengthening the power of industrialized nations (often termed the ""North"" in opposition to the developing world's ""South"").[196][better source needed]
 Corporatist ideology, which privileges the rights of corporations (artificial or juridical persons) over those of natural persons, is an underlying factor in the recent rapid expansion of global commerce.[197] In recent years, there have been an increasing number of books (Naomi Klein's 2000 No Logo, for example) and films (e.g. The Corporation & Surplus) popularizing an anti-corporate ideology to the public.
 A related contemporary ideology, consumerism, which encourages the personal acquisition of goods and services, also drives globalization.[198] Anti-consumerism is a social movement against equating personal happiness with consumption and the purchase of material possessions. Concern over the treatment of consumers by large corporations has spawned substantial activism, and the incorporation of consumer education into school curricula. Social activists hold materialism is connected to global retail merchandizing and supplier convergence, war, greed, anomie, crime, environmental degradation, and general social malaise and discontent. One variation on this topic is activism by postconsumers, with the strategic emphasis on moving beyond addictive consumerism.[199]
 The global justice movement is the loose collection of individuals and groups—often referred to as a ""movement of movements""—who advocate fair trade rules and perceive current institutions of global economic integration as problems.[201] The movement is often labeled an anti-globalization movement by the mainstream media. Those involved, however, frequently deny that they are anti-globalization, insisting that they support the globalization of communication and people and oppose only the global expansion of corporate power.[202] The movement is based in the idea of social justice, desiring the creation of a society or institution based on the principles of equality and solidarity, the values of human rights, and the dignity of every human being.[203][204][205] Social inequality within and between nations, including a growing global digital divide, is a focal point of the movement. Many nongovernmental organizations have now arisen to fight these inequalities that many in Latin America, Africa and Asia face. A few very popular and well known non-governmental organizations (NGOs) include: War Child, Red Cross, Free The Children and CARE International. They often create partnerships where they work towards improving the lives of those who live in developing countries by building schools, fixing infrastructure, cleaning water supplies, purchasing equipment and supplies for hospitals, and other aid efforts.
 The economies of the world have developed unevenly, historically, such that entire geographical regions were left mired in poverty and disease while others began to reduce poverty and disease on a wholesale basis. From around 1980 through at least 2011, the GDP gap, while still wide, appeared to be closing and, in some more rapidly developing countries, life expectancies began to rise.[206] If we look at the Gini coefficient for world income, since the late 1980s, the gap between some regions has markedly narrowed—between Asia and the advanced economies of the West, for example—but huge gaps remain globally. Overall equality across humanity, considered as individuals, has improved very little. Within the decade between 2003 and 2013, income inequality grew even in traditionally egalitarian countries like Germany, Sweden and Denmark. With a few exceptions—France, Japan, Spain—the top 10 percent of earners in most advanced economies raced ahead, while the bottom 10 percent fell further behind.[207] By 2013, 85 multibillionaires had amassed wealth equivalent to all the wealth owned by the poorest half (3.5 billion) of the world's total population of 7 billion.[208]
 Critics of globalization argue that globalization results in weak labor unions: the surplus in cheap labor coupled with an ever-growing number of companies in transition weakened labor unions in high-cost areas. Unions become less effective and workers their enthusiasm for unions when membership begins to decline.[175] They also cite an increase in the exploitation of child labor: countries with weak protections for children are vulnerable to infestation by rogue companies and criminal gangs who exploit them. Examples include quarrying, salvage, and farm work as well as trafficking, bondage, forced labor, prostitution and pornography.[209]
 Women often participate in the workforce in precarious work, including export-oriented employment. Evidence suggests that while globalization has expanded women's access to employment, the long-term goal[whose?] of transforming gender inequalities remains unmet and appears unattainable without regulation of capital and a reorientation and expansion of the state's role in funding public goods and providing a social safety net.[210] Furthermore, the intersectionality of gender, race, class, and more remain overlooked[by whom?] when assessing the impact of globalization.[211]
 In 2016, a study published by the IMF posited that neoliberalism, the ideological backbone of contemporary globalized capitalism, has been ""oversold"", with the benefits of neoliberal policies being ""fairly difficult to establish when looking at a broad group of countries"" and the costs, most significantly higher income inequality within nations, ""hurt the level and sustainability of growth.""[212]
 Beginning in the 1930s, opposition arose to the idea of a world government, as advocated by organizations such as the World Federalist Movement (WFM). Those who oppose global governance typically do so on objections that the idea is unfeasible, inevitably oppressive, or simply unnecessary.[213] In general, these opponents are wary of the concentration of power or wealth that such governance might represent. Such reasoning dates back to the founding of the League of Nations and, later, the United Nations.
 Environmentalism is a broad philosophy, ideology[214][215][216] and social movement regarding concerns for environmental conservation and improvement of the health of the environment. Environmentalist concerns with globalization include issues such as global warming, global water supply and water crises, inequity in energy consumption and energy conservation, transnational air pollution and pollution of the world ocean, overpopulation, world habitat sustainability, deforestation, biodiversity loss and species extinction.
 One critique of globalization is that natural resources of the poor have been systematically taken over by the rich and the pollution promulgated by the rich is systematically dumped on the poor.[217] Some argue that Northern corporations are increasingly exploiting resources of less wealthy countries for their global activities while it is the South that is disproportionately bearing the environmental burden of the globalized economy. Globalization is thus leading to a type of"" environmental apartheid"".[218]
 Helena Norberg-Hodge, the director and founder of Local Futures/International Society for Ecology and Culture, criticizes globalization in many ways. In her book Ancient Futures, Norberg-Hodge claims that ""centuries of ecological balance and social harmony are under threat from the pressures of development and globalization."" She also criticizes the standardization and rationalization of globalization, as it does not always yield the expected growth outcomes. Although globalization takes similar steps in most countries, scholars such as Hodge claim that it might not be effective to certain countries and that globalization has actually moved some countries backward instead of developing them.[219]
 A related area of concern is the pollution haven hypothesis, which posits that, when large industrialized nations seek to set up factories or offices abroad, they will often look for the cheapest option in terms of resources and labor that offers the land and material access they require (see Race to the bottom).[220] This often comes at the cost of environmentally sound practices. Developing countries with cheap resources and labor tend to have less stringent environmental regulations, and conversely, nations with stricter environmental regulations become more expensive for companies as a result of the costs associated with meeting these standards. Thus, companies that choose to physically invest in foreign countries tend to (re)locate to the countries with the lowest environmental standards or weakest enforcement.
 The European Union–Mercosur Free Trade Agreement, which would form one of the world's largest free trade areas,[221] has been denounced by environmental activists and indigenous rights campaigners.[222] The fear is that the deal could lead to more deforestation of the Amazon rainforest as it expands market access to Brazilian beef.[223]
 Globalization is associated with a more efficient system of food production. This is because crops are grown in countries with optimum growing conditions. This improvement causes an increase in the world's food supply which encourages improved food security.[224] The political movement 'BREXIT' was considered a step back in globalisation; it has greatly disrupted food chains within the UK, as they import 26% of food produce from the EU.
 Norway's limited crop range advocates globalization of food production and availability. The northernmost country in Europe requires trade with other countries to ensure population food demands are met. The degree of self-sufficiency in food production was around 50% in Norway in 2007.[225]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Social inequality', 'Takenaka Heizo and Chida Ryokichi', 'Social inequality', 'economic globalization, cultural globalization and political globalization', 'competing definitions and interpretations'], 'answer_start': [], 'answer_end': []}"
"
 Cultural heritage is the heritage of tangible and intangible heritage assets of a group or society that is inherited from past generations. Not all heritages of past generations are ""heritage""; rather, heritage is a product of selection by society.[1]
 Cultural heritage includes tangible culture (such as buildings, monuments, landscapes, archive materials, books, works of art, and artifacts), intangible culture (such as folklore, traditions, language, and knowledge), and natural heritage (including culturally significant landscapes, and biodiversity).[2] The term is often used in connection with issues relating to the protection of indigenous intellectual property.[3]
 The deliberate action of keeping cultural heritage from the present for the future is known as preservation (American English) or conservation (British English), which cultural and historical ethnic museums and cultural centers promote, though these terms may have more specific or technical meanings in the same contexts in the other dialect. Preserved heritage has become an anchor of the global tourism industry, a major contributor of economic value to local communities.[1]
 Legal protection of cultural property comprises a number of international agreements and national laws.
United Nations, UNESCO and Blue Shield International deal with the protection of cultural heritage. This also applies to the integration of United Nations peacekeeping.[4][5][6][7][8][9]
 Cultural property includes the physical, or ""tangible"" cultural heritage, such as artworks. These are generally split into two groups of movable and immovable heritage. Immovable heritage includes buildings (which themselves may include installed art such as organs, stained glass windows, and frescos), large industrial installations, residential projects or other historic places and monuments. Moveable heritage includes books, documents, moveable artworks, machines, clothing, and other artifacts, that are considered worthy of preservation for the future. These include objects significant to the archaeology, architecture, science or technology of a specified culture.[2]
 Aspects and disciplines of the preservation and conservation of tangible culture include:
 ""Intangible cultural heritage"" consists of non-physical aspects of a particular culture, more often maintained by social customs during a specific period in history. The concept includes the ways and means of behavior in a society, and the often formal rules for operating in a particular cultural climate. These include social values and traditions, customs and practices, aesthetic and spiritual beliefs, artistic expression, language and other aspects of human activity. The significance of physical artifacts can be interpreted as an act against the backdrop of socioeconomic, political, ethnic, religious and philosophical values of a particular group of people. Naturally, intangible cultural heritage is more difficult to preserve than physical objects.[citation needed]
 Aspects of the preservation and conservation of cultural intangibles include:
 ""Natural heritage"" is also an important part of a society's heritage, encompassing the countryside and natural environment, including flora and fauna, scientifically known as biodiversity, as well as geological elements (including mineralogical, geomorphological, paleontological, etc.), scientifically known as geodiversity. These kind of heritage sites often serve as an important component in a country's tourist industry, attracting many visitors from abroad as well as locally. Heritage can also include cultural landscapes (natural features that may have cultural attributes).
 Aspects of the preservation and conservation of natural heritage include:
 Digital heritage is made up of computer-based materials such as texts, databases, images, sounds and software being retained for future generations.[10] Digital heritage includes physical objects such as documents which have been digitized for retention and artifacts which are ""born digital"", i.e. originally created digitally and having no physical form.
 There have been examples of respect for the cultural assets of enemies since ancient times. The roots of today's legal situation for the precise protection of cultural heritage also lie in some of the regulations of Austria's ruler Maria Theresa (1717 - 1780) and the demands of the Congress of Vienna (1814/15) not to remove works of art from their place of origin in the war.[11] The 1863 Lieber code, a military legal code governing the wartime conduct of the Union Army also set rules for the protection of cultural heritage.[12] The process continued at the end of the 19th century when, in 1874 (in Brussels), at least a draft international agreement on the laws and customs of war was agreed. 25 years later, in 1899, an international peace conference was held in the Netherlands on the initiative of Tsar Nicholas II of Russia, with the aim of revising the declaration (which was never ratified) and adopting a convention. The Hague Conventions of 1899 and 1907 also significantly advanced international law and laid down the principle of the immunity of cultural property. Three decades later, in 1935, the preamble to the Treaty on the Protection of Artistic and Scientific Institutions (Roerich Pact) was formulated. On the initiative of UNESCO, the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict was signed in 1954.[13]
 Protection of cultural heritage or protection of cultural goods refers to all measures aimed to protect cultural property against damage, destruction, theft, embezzlement or other loss. The term ""monument protection"" is also used for immovable cultural property. Protection of cultural heritage relates in particular to the prevention of robbery digs at archaeological sites, the looting or destruction of cultural sites and the theft of works of art from churches and museums all over the world and basically measures regarding the conservation and general access to our common cultural heritage. Legal protection of cultural heritage comprises a number of international agreements and national laws.[14][15][16][17][18]
 There is a close partnership between the UN, United Nations peacekeeping, UNESCO, the International Committee of the Red Cross and Blue Shield International.[9][19]
 The protection of cultural heritage should also preserve the particularly sensitive cultural memory, the growing cultural diversity and the economic basis of a state, a municipality or a region. Whereby there is also a connection between cultural user disruption or cultural heritage and the cause of flight. But only through the fundamental cooperation, including the military units and the planning staff, with the locals can the protection of world heritage sites, archaeological finds, exhibits and archaeological sites from destruction, looting and robbery be implemented sustainably. The founding president of Blue Shield International Karl von Habsburg summed it up with the words: ""Without the local community and without the local participants, that would be completely impossible"".[9][20][21][22]
 Objects are a part of the study of human history because they provide a concrete basis for ideas, and can validate them. Their preservation demonstrates a recognition of the necessity of the past and of the things that tell its story.[23] In The Past is a Foreign Country, David Lowenthal observes that preserved objects also validate memories. While digital acquisition techniques can provide a technological solution that is able to acquire the shape and the appearance of artifacts with an unprecedented precision[24] in human history, the actuality of the object, as opposed to a reproduction, draws people in and gives them a literal way of touching the past. This poses a danger as places and things are damaged by the hands of tourists, the light required to display them, and other risks of making an object known and available. The reality of this risk reinforces the fact that all artifacts are in a constant state of chemical transformation, so that what is considered to be preserved is actually changing – it is never as it once was.[25] Similarly changing is the value each generation may place on the past and on the artifacts that link it to the past.
 
The equality or inseparability of cultural preservation and the protection of human life has been argued by several agencies and writers,[26] for example former French president François Hollande stated in 2016 Our responsibility is to save lives and also to save the stones -- there is no choice to be made, because today both are destroyed.[27] Classical civilizations, especially Indian, have attributed supreme importance to the preservation of tradition. Its central idea was that social institutions, scientific knowledge and technological applications need to use a ""heritage"" as a ""resource"".[28] Using contemporary language, we could say that ancient Indians considered, as social resources, both economic assets (like natural resources and their exploitation structure) and factors promoting social integration (like institutions for the preservation of knowledge and for the maintenance of civil order).[29] Ethics considered that what had been inherited should not be consumed, but should be handed over, possibly enriched, to successive generations. This was a moral imperative for all, except in the final life stage of sannyasa.
 What one generation considers ""cultural heritage"" may be rejected by the next generation, only to be revived by a subsequent generation.[according to whom?]
 Significant was the Convention Concerning the Protection of World Cultural and Natural Heritage that was adopted by the General Conference of UNESCO in 1972. As of 2011, there are 936 World Heritage Sites: 725 cultural, 183 natural, and 28 mixed properties, in 153 countries. Each of these sites is considered important to the international community.
 The underwater cultural heritage is protected by the UNESCO Convention on the Protection of the Underwater Cultural Heritage. This convention is a legal instrument helping states parties to improve the protection of their underwater cultural heritage.[30][31]
 In addition, UNESCO has begun designating masterpieces of the Oral and Intangible Heritage of Humanity. The Committee on Economic, Social and Cultural Rights sitting as part of the United Nations Economic and Social Council with article 15 of its Covenant had sought to instill the principles under which cultural heritage is protected as part of a basic human right.
 Key international documents and bodies include:
 The U.S. Government Accountability Office issued a report describing some of the United States' cultural property protection efforts.[32]
 Much of heritage preservation work is done at the national, regional, or local levels of society. Various national and regional regimes include:
 National Heritage Conservation Commission
 National Museums Board
 Broad philosophical, technical, and political issues and dimensions of cultural heritage include:
 Issues in cultural heritage management include:
 Ancient archaeological artefacts and archaeological sites are naturally prone to damage due to their age and environmental conditions. Also, there have been tragic occurrences of unexpected human-made disasters, such as in the cases of a fire that took place in the 200 years old National Museum of Brazil and the UNESCO World Heritage Site of the Notre Dame Cathedral in Paris.
 Therefore, there is a growing need to digitize cultural heritage in order to preserve them in the face of potential calamities such as climate change, natural disaster, poor policy or inadequate infrastructure. For example, the Library of Congress has started to digitize its collections in a special program called the National Digital Library Program.[35] The Smithsonian has also been actively digitizing its collection with the release of the ""Smithsonian X 3D Explorer,"" allowing anyone to engage with the digitized versions of the museum's millions of artifacts, of which only two percent are on display.[36][37]
 3D scanning devices have become a practical reality in the field of heritage preservation. 3D scanners can produce a high-precision digital reference model that not only digitizes condition but also provides a 3D virtual model for replication. The high cost and relative complexity of 3D scanning technologies have made it quite impractical for many heritage institutions in the past, but this is changing, as technology advances and its relative costs are decreasing to reach a level where even mobile based scanning applications can be used to create a virtual museum.
 There is still a low level of digital archiving of archaeological data obtained via excavation,[38] even in the UK where the lead digital archive for archaeology, the Archaeology Data Service, was established in the 1990s. Across the globe, countries are at different stages of dealing with digital archaeological archives,[39] all dealing with differences in statutory requirements, legal ownership of archives and infrastructure.[40]
[41]
 
 
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Heritage of Humanity', 'National Heritage Conservation Commission\n National Museums Board', 'Cultural heritage is the heritage of tangible and intangible heritage assets', 'issues relating to the protection of indigenous intellectual property', 'socioeconomic, political, ethnic, religious and philosophical values'], 'answer_start': [], 'answer_end': []}"
"World literature is used to refer to the total of the world's national literature and the circulation of works into the wider world beyond their country of origin. In the past, it primarily referred to the masterpieces of Western European literature; however, world literature today is increasingly seen in an international context. Now, readers have access to a wide range of global works in various translations.
 Many scholars assert that what makes a work considered world literature is its circulation beyond its country of origin. For example, David Damrosch states, ""A work enters into world literature by a double process: first, by being read as literature; second, by circulating out into a broader world beyond its linguistic and cultural point of origin"".[1] Likewise, the world literature scholar Venkat Mani believes that the ""worlding"" of literature is brought about by ""information transfer"" largely generated by developments in print culture. Because of the advent of the library, ""Publishers and booksellers who print and sell affordable books, literate citizens who acquire these books, and public libraries that make these books available to those who cannot afford to buy them collectively play a very important role in the ""making"" of world literature"".[2]
 Johann Wolfgang Goethe used the concept of world literature in several of his essays in the early decades of the nineteenth century to describe the international circulation and reception of literary works in Europe, including works of non-Western origin. The concept achieved wide currency after his disciple Johann Peter Eckermann published a collection of conversations with Goethe in 1835.[3] Goethe spoke with Eckermann about the excitement of reading Chinese novels and Persian and Serbian poetry as well as of his fascination with seeing how his own works were translated and discussed abroad, especially in France. He made a famous statement in January 1827, predicting that world literature would replace the national literature as the major mode of literary creativity in the future:
 I am more and more convinced that poetry is the universal possession of mankind, revealing itself everywhere and at all times in hundreds and hundreds of men. ... I therefore like to look about me in foreign nations, and advise everyone to do the same.  National literature is now a rather unmeaning term; the epoch of world literature is at hand, and everyone must strive to hasten its approach.[4] Reflecting a fundamentally economic understanding of world literature as a process of trade and exchange, Karl Marx and Friedrich Engels used the term in their Communist Manifesto (1848) to describe the ""cosmopolitan character"" of bourgeois literary production, asserting that:
 In place of the old wants, satisfied by the productions of the country, we find new wants, requiring for their satisfaction the products of distant lands and climates. ... And as in material, so also in intellectual production. The intellectual creations of individual nations become common property. National one-sidedness and narrow-mindedness become more and more impossible, and from the numerous national and local literatures, there arises a world literature. Martin Puchner has stated that Goethe had a keen sense of world literature as driven by a new world market in literature.[5] This market-based approach was sought by Marx and Engels in 1848 through their Manifesto document, which was published in four languages and distributed among several European countries, and has since become one of the most influential texts of the twentieth century.[3] While Marx and Engels followed Goethe in viewing world literature as a modern or future phenomenon, in 1886 the Irish scholar H. M. Posnett argued that world literature first arose in ancient empires, such as the Roman Empire, long before the rise of the modern national literature. Today, world literature is understood to encompass classical works from all periods, including contemporary literature that is written for a global audience.
 In the postwar era, the study of comparative and world literature was revived in the United States. Comparative literature was seen at the graduate level while world literature was taught as a first-year general education class. The focus remained largely on the Greek and Roman classics and the literature of major, modern Western-European powers, but a combination of factors in the late 1980s and early 1990s led to greater access to the world. The end of the Cold War, the growing globalization of the world economy, and new waves of immigration led to several efforts to expand the study of world literature. This change is illustrated by the expansion of The Norton Anthology of World Masterpieces, whose first edition in 1956 featured only Western-European and North American works, to a new ""expanded edition"" in 1995 with non-Western selections.[6] Major survey anthologies today, including those published by Longman, Bedford and Norton, showcase several hundred authors from dozens of countries.
 The explosive growth in the range of cultures studied under the rubric of world literature has inspired a variety of theoretical attempts to define the field and to propose effective modes of research and teaching. In his 2003 book What Is World Literature? David Damrosch understands world literature to be less of a vast collection of works and more a matter of circulation and reception. He proposed that works that thrive as world literature are ones that work well and even gain meaning through translation. Whereas Damrosch's approach remains tied to the close reading of individual works, a different view was taken by the Stanford critic Franco Moretti in a pair of articles offering ""Conjectures on World Literature"".[7] Moretti believes that the scale of world literature exceeds what can be grasped by traditional methods of close reading, and advocates instead a mode of ""distant reading"" that would look at large-scale patterns as discerned from publication records and national literary histories.
 Moretti's approach combined elements of evolutionary theory with the world-systems analysis pioneered by Immanuel Wallerstein, an approach further discussed since then by Emily Apter in her influential book The Translation Zone.[8] Related to their world-systems approach is the work of French critic Pascale Casanova, La République mondiale des lettres (1999).[9] Drawing on the theories of cultural production developed by the sociologist Pierre Bourdieu, Casanova explores the ways in which the works of peripheral writers must circulate into metropolitan centers in order to achieve recognition as being world literature.
 The field of world literature continues to generate debate, with critics such as Gayatri Chakravorty Spivak arguing that too often the study of world literature in translation smooths out both the linguistic richness of the original and the political force a work can have in its original context. Other scholars, on the contrary, emphasize that world literature can and should be studied with close attention to original languages and contexts, even as works take on new dimensions and new meanings abroad.
 World literature series are now being published in China and in Estonia, and a new Institute for World Literature, offering month-long summer sessions on theory and pedagogy, had its inaugural session at Peking University in 2011, with its next sessions at Istanbul Bilgi University in 2012 and at Harvard University in 2013. Since the middle of the first decade of the new century, a steady stream of works has provided materials for the study of the history of world literature and the current debates. Valuable collections of essays include:
 Individual studies include:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Western European literature', 'Karl Marx and Friedrich Engels', 'masterpieces of Western European literature', 'Greek and Roman classics and the literature of major, modern Western-European powers', 'work well and even gain meaning through translation'], 'answer_start': [], 'answer_end': []}"
"Folklore is the body of expressive culture shared by a particular group of people, culture or subculture.[1] This includes oral traditions such as tales, myths, legends,[a] proverbs, poems, jokes, and other oral traditions.[3][4] This also includes material culture, such as traditional building styles common to the group. Folklore also encompasses customary lore, taking actions for folk beliefs, and the forms and rituals of celebrations such as Christmas, weddings, folk dances, and initiation rites.[3]
 Each one of these, either singly or in combination, is considered a folklore artifact or traditional cultural expression. Just as essential as the form, folklore also encompasses the transmission of these artifacts from one region to another or from one generation to the next. Folklore is not something one can typically gain from a formal school curriculum or study in the fine arts. Instead, these traditions are passed along informally from one individual to another, either through verbal instruction or demonstration.[5]
 The academic study of folklore is called folklore studies or folkloristics, and it can be explored at the undergraduate, graduate, and Ph.D. levels.[6]
 The word folklore, a compound of folk and lore, was coined in 1846 by the Englishman William Thoms,[7] who contrived the term as a replacement for the contemporary terminology of ""popular antiquities"" or ""popular literature"". The second half of the word, lore, comes from Old English lār 'instruction'. It is the knowledge and traditions of a particular group, frequently passed along by word of mouth.[8][9]
 The concept of folk has varied over time. When Thoms first created this term, folk applied only to rural, frequently poor and illiterate peasants. A more modern definition of folk is a social group that includes two or more people with common traits who express their shared identity through distinctive traditions. ""Folk is a flexible concept which can refer to a nation as in American folklore or to a single family.""[10] This expanded social definition of folk supports a broader view of the material, i.e., the lore, considered to be folklore artifacts. These now include all ""things people make with words (verbal lore), things they make with their hands (material lore), and things they make with their actions (customary lore)"".[11] Folklore is no longer considered to be limited to that which is old or obsolete. These folk artifacts continue to be passed along informally, as a rule anonymously, and always in multiple variants. The folk group is not individualistic; it is community-based and nurtures its lore in community. ""As new groups emerge, new folklore is created… surfers, motorcyclists, computer programmers"".[12] In direct contrast to high culture, where any single work of a named artist is protected by copyright law, folklore is a function of shared identity within a common social group.[13]
 Having identified folk artifacts, the professional folklorist strives to understand the significance of these beliefs, customs, and objects for the group, since these cultural units[14] would not be passed along unless they had some continued relevance within the group. That meaning can, however, shift and morph; for example, the Halloween celebration of the 21st century is not the All Hallows' Eve of the Middle Ages and even gives rise to its own set of urban legends independent of the historical celebration; the cleansing rituals of Orthodox Judaism were originally good public health in a land with little water, but now these customs signify for some people identification as an Orthodox Jew. By comparison, a common action such as tooth brushing, which is also transmitted within a group, remains a practical hygiene and health issue and does not rise to the level of a group-defining tradition.[15] Tradition is initially remembered behavior; once it loses its practical purpose, there is no reason for further transmission unless it has been imbued with meaning beyond the initial practicality of the action. This meaning is at the core of folkloristics, the study of folklore.[16]
 With the increasing theoretical sophistication of the social sciences, it has become evident that folklore is a naturally occurring and necessary component of any social group; it is indeed all around us.[17] Folklore does not have to be old or antiquated; it continues to be created and transmitted, and in any group, it is used to differentiate between ""us"" and ""them"".
 Folklore began to distinguish itself as an autonomous discipline during the period of romantic nationalism, in Europe. A particular figure in this development was Johann Gottfried von Herder, whose writings in the 1770s presented oral traditions as organic processes grounded in locale. After the German states were invaded by Napoleonic France, Herder's approach was adopted by many of his fellow Germans, who systematized the recorded folk traditions, and used them in their process of nation building. This process was enthusiastically embraced by smaller nations, like Finland, Estonia, and Hungary, which were seeking political independence from their dominant neighbors.[18]
 Folklore, as a field of study, further developed among 19th century European scholars, who were contrasting tradition with the newly developing modernity. Its focus was the oral folklore of the rural peasant populations, which were considered as residue and survivals of the past that continued to exist within the lower strata of society.[19] The ""Kinder- und Hausmärchen"" of the Brothers Grimm (first published 1812) is the best known but by no means only collection of verbal folklore of the European peasantry of that time. This interest in stories, sayings and songs continued throughout the 19th century and aligned the fledgling discipline of folkloristics with literature and mythology. By the turn into the 20th century the number and sophistication of folklore studies and folklorists had grown both in Europe and North America. Whereas European folklorists remained focused on the oral folklore of the homogenous peasant populations in their regions, the American folklorists, led by Franz Boas and Ruth Benedict, chose to consider Native American cultures in their research, and included the totality of their customs and beliefs as folklore. This distinction aligned American folkloristics with cultural anthropology and ethnology, using the same techniques of data collection in their field research. This divided alliance of folkloristics between the humanities in Europe and the social sciences in America offers a wealth of theoretical vantage points and research tools to the field of folkloristics as a whole, even as it continues to be a point of discussion within the field itself.[20]
 The term folkloristics, along with the alternative name folklore studies,[b] became widely used in the 1950s to distinguish the academic study of traditional culture from the folklore artifacts themselves. When the American Folklife Preservation Act (Public Law 94-201) was passed by the U.S. Congress in January 1976,[21] to coincide with the Bicentennial Celebration, folkloristics in the United States came of age.
 ""…[Folklife] means the traditional expressive culture shared within the various groups in the United States: familial, ethnic, occupational, religious, regional; expressive culture includes a wide range of creative and symbolic forms such as custom, belief, technical skill, language, literature, art, architecture, music, play, dance, drama, ritual, pageantry, handicraft; these expressions are mainly learned orally, by imitation, or in performance, and are generally maintained without benefit of formal instruction or institutional direction.""  Added to the extensive array of other legislation designed to protect the natural and cultural heritage of the United States, this law also marks a shift in national awareness. It gives voice to a growing understanding that cultural diversity is a national strength and a resource worthy of protection. Paradoxically, it is a unifying feature, not something that separates the citizens of a country. ""We no longer view cultural difference as a problem to be solved, but as a tremendous opportunity. In the diversity of American folklife we find a marketplace teeming with the exchange of traditional forms and cultural ideas, a rich resource for Americans"".[22] This diversity is celebrated annually at the Smithsonian Folklife Festival and many other folklife fests around the country.
 There are numerous other definitions. According to William Bascom major article on the topic there are ""four functions to folklore"":[23]
 The folk of the 19th century, the social group identified in the original term ""folklore"", was characterized by being rural, illiterate and poor. They were the peasants living in the countryside, in contrast to the urban populace of the cities. Only toward the end of the century did the urban proletariat (on the coattails of Marxist theory) become included with the rural poor as folk. The common feature in this expanded definition of folk was their identification as the underclass of society.[24]
 Moving forward into the 20th century, in tandem with new thinking in the social sciences, folklorists also revised and expanded their concept of the folk group. By the 1960s it was understood that social groups, i.e. folk groups, were all around us; each individual is enmeshed in a multitude of differing identities and their concomitant social groups. The first group that each of us is born into is the family, and each family has its own unique family folklore. As a child grows into an individual, its identities also increase to include age, language, ethnicity, occupation, etc. Each of these cohorts has its own folklore, and as one folklorist points out, this is ""not idle speculation… Decades of fieldwork have demonstrated conclusively that these groups do have their own folklore.""[12] In this modern understanding, folklore is a function of shared identity within any social group.[13]
 This folklore can include jokes, sayings and expected behavior in multiple variants, always transmitted in an informal manner. For the most part it will be learned by observation, imitation, repetition or correction by other group members. This informal knowledge is used to confirm and re-inforce the identity of the group. It can be used both internally within the group to express their common identity, for example in an initiation ceremony for new members. Or it can be used externally to differentiate the group from outsiders, like a folkdance demonstration at a community festival. Significant to folklorists here is that there are two opposing but equally valid ways to use this in the study of a group: you can start with an identified group in order to explore its folklore, or you can identify folklore items and use them to identify the social group.[25]
 Beginning in the 1960s, a further expansion of the concept of folk began to unfold through the study of folklore. Individual researchers identified folk groups that had previously been overlooked and ignored. One notable example of this is found in an issue of the Journal of American Folklore, published in 1975, which is dedicated exclusively to articles on women's folklore, with approaches that had not come from a man's perspective.[c] Other groups that were highlighted as part of this broadened understanding of the folk group were non-traditional families, occupational groups, and families that pursued the production of folk items over multiple generations.
 Folklorist Richard Dorson explained in 1976 that the study of folklore is ""concerned with the study of traditional culture, or the unofficial culture"" that is the folk culture, ""as opposed to the elite culture, not for the sake of proving a thesis but to learn about the mass of [humanity] overlooked by the conventional disciplines"".[26]
 Individual folklore artifacts are commonly classified as one of three types: material, verbal or customary lore. For the most part self-explanatory, these categories include physical objects (material folklore), common sayings, expressions, stories and songs (verbal folklore), and beliefs and ways of doing things (customary folklore). There is also a fourth major subgenre defined for children's folklore and games (childlore), as the collection and interpretation of this fertile topic is particular to school yards and neighborhood streets.[27] Each of these genres and their subtypes is intended to organize and categorize the folklore artifacts; they provide common vocabulary and consistent labeling for folklorists to communicate with each other.
 That said, each artifact is unique; in fact one of the characteristics of all folklore artifacts is their variation within genres and types.[28] This is in direct contrast to manufactured goods, where the goal in production is to create identical products and any variations are considered mistakes. It is however just this required variation that makes identification and classification of the defining features a challenge. And while this classification is essential for the subject area of folkloristics, it remains just labeling, and adds little to an understanding of the traditional development and meaning of the artifacts themselves.[29]
 Necessary as they are, genre classifications are misleading in their oversimplification of the subject area. Folklore artifacts are never self-contained, they do not stand in isolation but are particulars in the self-representation of a community. Different genres are frequently combined with each other to mark an event.[30] So a birthday celebration might include a song or formulaic way of greeting the birthday child (verbal), presentation of a cake and wrapped presents (material), as well as customs to honor the individual, such as sitting at the head of the table, and blowing out the candles with a wish. There might also be special games played at birthday parties which are not generally played at other times. Adding to the complexity of the interpretation, the birthday party for a seven-year-old will not be identical to the birthday party for that same child as a six-year-old, even though they follow the same model. For each artifact embodies a single variant of a performance in a given time and space. The task of the folklorist becomes to identify within this surfeit of variables the constants and the expressed meaning that shimmer through all variations: honoring of the individual within the circle of family and friends, gifting to express their value and worth to the group, and of course, the festival food and drink as signifiers of the event.
 The formal definition of verbal lore is words, both written and oral, that are ""spoken, sung, voiced forms of traditional utterance that show repetitive patterns.""[31] Crucial here are the repetitive patterns. Verbal lore is not just any conversation, but words and phrases conforming to a traditional configuration recognized by both the speaker and the audience. For narrative types by definition have consistent structure, and follow an existing model in their narrative form.[d] As just one simple example, in English the phrase ""An elephant walks into a bar…"" instantaneously flags the following text as a joke. It might be one you have already heard, but it might be one that the speaker has just thought up within the current context. Another example is the child's song Old MacDonald Had a Farm, where each performance is distinctive in the animals named, their order and their sounds. Songs such as this are used to express cultural values (farms are important, farmers are old and weather-beaten) and teach children about different domesticated animals.[32]
 Verbal folklore was the original folklore, the artifacts defined by William Thoms as older, oral cultural traditions of the rural populace. In his 1846 published call for help in documenting antiquities, Thoms was echoing scholars from across the European continent to collect artifacts of verbal lore. By the beginning of the 20th century these collections had grown to include artifacts from around the world and across several centuries. A system to organize and categorize them became necessary.[33] Antti Aarne published a first classification system for folktales in 1910. This was later expanded into the Aarne–Thompson classification system by Stith Thompson and remains the standard classification system for European folktales and other types of oral literature. As the number of classified oral artifacts grew, similarities were noted in items that had been collected from very different geographic regions, ethnic groups and epochs, giving rise to the Historic–Geographic Method, a methodology that dominated folkloristics in the first half of the 20th century.
 When William Thoms first published his appeal to document the verbal lore of the rural populations, it was believed these folk artifacts would die out as the population became literate. Over the past two centuries this belief has proven to be wrong; folklorists continue to collect verbal lore in both written and spoken form from all social groups. Some variants might have been captured in published collections, but much of it is still transmitted orally and indeed continues to be generated in new forms and variants at an alarming rate.
 Below is listed a small sampling of types and examples of verbal lore.
 The genre of material culture includes all artifacts that can be touched, held, lived in, or eaten. They are tangible objects with a physical or mental presence, either intended for permanent use or to be used at the next meal. Most of these folklore artifacts are single objects that have been created by hand for a specific purpose; however, folk artifacts can also be mass-produced, such as dreidels or Christmas decorations. These items continue to be considered folklore because of their long (pre-industrial) history and their customary use. All of these material objects ""existed prior to and continue alongside mechanized industry. … [They are] transmitted across the generations and subject to the same forces of conservative tradition and individual variation""[31] that are found in all folk artifacts. Folklorists are interested in the physical form, the method of manufacture or construction, the pattern of use, as well as the procurement of the raw materials.[34] The meaning to those who both make and use these objects is important. Of primary significance in these studies is the complex balance of continuity over change in both their design and their decoration.
 In Europe, prior to the Industrial Revolution, everything was made by hand. While some folklorists of the 19th century wanted to secure the oral traditions of the rural folk before the populace became literate, other folklorists sought to identify hand-crafted objects before their production processes were lost to industrial manufacturing. Just as verbal lore continues to be actively created and transmitted in today's culture, so these handicrafts can still be found all around us, with possibly a shift in purpose and meaning. There are many reasons for continuing to handmake objects for use, for example these skills may be needed to repair manufactured items, or a unique design might be required which is not (or cannot be) found in the stores. Many crafts are considered as simple home maintenance, such as cooking, sewing and carpentry. For many people, handicrafts have also become an enjoyable and satisfying hobby. Handmade objects are often regarded as prestigious, where extra time and thought is spent in their creation and their uniqueness is valued.[35] For the folklorist, these hand-crafted objects embody multifaceted relationships in the lives of the craftspeople and the users, a concept that has been lost with mass-produced items that have no connection to an individual craftsperson.[36]
 Many traditional crafts, such as ironworking and glass-making, have been elevated to the fine or applied arts and taught in art schools;[37] or they have been repurposed as folk art, characterized as objects whose decorative form supersedes their utilitarian needs. Folk art is found in hex signs on Pennsylvania Dutch barns, tin man sculptures made by metalworkers, front yard Christmas displays, decorated school lockers, carved gun stocks, and tattoos. ""Words such as naive, self-taught, and individualistic are used to describe these objects, and the exceptional rather than the representative creation is featured.""[38] This is in contrast to the understanding of folklore artifacts that are nurtured and passed along within a community.[e]
 Many objects of material folklore are challenging to classify, difficult to archive, and unwieldy to store. The assigned task of museums is to preserve and make use of these bulky artifacts of material culture. To this end, the concept of the living museum has developed, beginning in Scandinavia at the end of the 19th century. These open-air museums not only display the artifacts, but also teach visitors how the items were used, with actors reenacting the everyday lives of people from all segments of society, relying heavily on the material artifacts of a pre-industrial society. Many locations even duplicate the processing of the objects, thus creating new objects of an earlier historic time period. Living museums are now found throughout the world as part of a thriving heritage industry.
 This list represents just a small sampling of objects and skills that are included in studies of material culture.
 Customary culture is remembered enactment, i.e. re-enactment. It is the patterns of expected behavior within a group, the ""traditional and expected way of doing things""[39][40] A custom can be a single gesture, such as thumbs down or a handshake. It can also be a complex interaction of multiple folk customs and artifacts as seen in a child's birthday party, including verbal lore (Happy Birthday song), material lore (presents and a birthday cake), special games (Musical chairs) and individual customs (making a wish as you blow out the candles). Each of these is a folklore artifact in its own right, potentially worthy of investigation and cultural analysis. Together they combine to build the custom of a birthday party celebration, a scripted combination of multiple artifacts which have meaning within their social group.
 Folklorists divide customs into several different categories.[39] A custom can be a seasonal celebration, such as Thanksgiving or New Year's. It can be a life cycle celebration for an individual, such as baptism, birthday or wedding. A custom can also mark a community festival or event; examples of this are Carnival in Cologne or Mardi Gras in New Orleans. This category also includes the Smithsonian Folklife Festival celebrated each summer on the Mall in Washington, DC. A fourth category includes customs related to folk beliefs. Walking under a ladder is just one of many symbols considered unlucky. Occupational groups tend to have a rich history of customs related to their life and work, so the traditions of sailors or lumberjacks.[f] The area of ecclesiastical folklore, which includes modes of worship not sanctioned by the established church[41] tends to be so large and complex that it is usually treated as a specialized area of folk customs; it requires considerable expertise in standard church ritual in order to adequately interpret folk customs and beliefs that originated in official church practice.
 Customary folklore is always a performance, be it a single gesture or a complex of scripted customs, and participating in the custom, either as performer or audience, signifies acknowledgment of that social group. Some customary behavior is intended to be performed and understood only within the group itself, so the handkerchief code sometimes used in the gay community or the initiation rituals of the Freemasons. Other customs are designed specifically to represent a social group to outsiders, those who do not belong to this group. The St. Patrick's Day Parade in New York and in other communities across the continent is a single example of an ethnic group parading their separateness (differential behavior[42]), and encouraging Americans of all stripes to show alliance to this colorful ethnic group.
 These festivals and parades, with a target audience of people who do not belong to the social group, intersect with the interests and mission of public folklorists, who are engaged in the documentation, preservation, and presentation of traditional forms of folklife. With a swell in popular interest in folk traditions, these community celebrations are becoming more numerous throughout the western world. While ostensibly parading the diversity of their community, economic groups have discovered that these folk parades and festivals are good for business. All shades of people are out on the streets, eating, drinking and spending. This attracts support not only from the business community, but also from federal and state organizations for these local street parties.[43] Paradoxically, in parading diversity within the community, these events have come to authenticate true community, where business interests ally with the varied (folk) social groups to promote the interests of the community as a whole.
 This is just a small sampling of types and examples of customary lore.
 Childlore is a distinct branch of folklore that deals with activities passed on by children to other children, away from the influence or supervision of an adult.[44] Children's folklore contains artifacts from all the standard folklore genres of verbal, material, and customary lore; it is however the child-to-child conduit that distinguishes these artifacts. For childhood is a social group where children teach, learn and share their own traditions, flourishing in a street culture outside the purview of adults. This is also ideal where it needs to be collected; as Iona and Peter Opie demonstrated in their pioneering book Children's Games in Street and Playground.[27] Here the social group of children is studied on its own terms, not as a derivative of adult social groups. It is shown that the culture of children is quite distinctive; it is generally unnoticed by the sophisticated world of adults, and quite as little affected by it.[45]
 Of particular interest to folklorists here is the mode of transmission of these artifacts; this lore circulates exclusively within an informal pre-literate children's network or folk group. It does not include artifacts taught to children by adults. However children can take the taught and teach it further to other children, turning it into childlore. Or they can take the artifacts and turn them into something else; so Old McDonald's farm is transformed from animal noises to the scatological version of animal poop. This childlore is characterized by ""its lack of dependence on literary and fixed form. Children…operate among themselves in a world of informal and oral communication, unimpeded by the necessity of maintaining and transmitting information by written means.[46] This is as close as folklorists can come to observing the transmission and social function of this folk knowledge before the spread of literacy during the 19th century.
 As we have seen with the other genres, the original collections of children's lore and games in the 19th century was driven by a fear that the culture of childhood would die out.[47] Early folklorists, among them Alice Gomme in Britain and William Wells Newell in the United States, felt a need to capture the unstructured and unsupervised street life and activities of children before it was lost. This fear proved to be unfounded. In a comparison of any modern school playground during recess and the painting of ""Children's Games"" by Pieter Breugel the Elder we can see that the activity level is similar, and many of the games from the 1560 painting are recognizable and comparable to modern variations still played today.
 These same artifacts of childlore, in innumerable variations, also continue to serve the same function of learning and practicing skills needed for growth. So bouncing and swinging rhythms and rhymes encourage development of balance and coordination in infants and children. Verbal rhymes like Peter Piper picked... serve to increase both the oral and aural acuity of children. Songs and chants, accessing a different part of the brain, are used to memorize series (Alphabet song). They also provide the necessary beat to complex physical rhythms and movements, be it hand-clapping, jump roping, or ball bouncing. Furthermore, many physical games are used to develop strength, coordination and endurance of the players. For some team games, negotiations about the rules can run on longer than the game itself as social skills are rehearsed.[48] Even as we are just now uncovering the neuroscience that undergirds the developmental function of this childlore, the artifacts themselves have been in play for centuries.
 Below is listed just a small sampling of types and examples of childlore and games.
 A case has been made for considering folk history as a distinct sub-category of folklore, an idea that has received attention from such folklorists as Richard Dorson. This field of study is represented in The Folklore Historian, an annual journal sponsored by the History and Folklore Section of the American Folklore Society and concerned with the connections of folklore with history, as well as the history of folklore studies.[49]
 Lacking context, folklore artifacts would be uninspiring objects without any life of their own. It is only through performance that the artifacts come alive as an active and meaningful component of a social group; the intergroup communication arises in the performance and this is where transmission of these cultural elements takes place. American folklorist Roger D. Abrahams has described it thus: ""Folklore is folklore only when performed. As organized entities of performance, items of folklore have a sense of control inherent in them, a power that can be capitalized upon and enhanced through effective performance.""[50] Without transmission, these items are not folklore, they are just individual quirky tales and objects.
 This understanding in folkloristics only occurred in the second half of the 20th century, when the two terms ""folklore performance"" and ""text and context"" dominated discussions among folklorists. These terms are not contradictory or even mutually exclusive. As borrowings from other fields of study, one or the other linguistic formulation is more appropriate to any given discussion. Performance is frequently tied to verbal and customary lore, whereas context is used in discussions of material lore. Both formulations offer different perspectives on the same folkloric understanding, specifically that folklore artifacts need to remain embedded in their cultural environment if we are to gain insight into their meaning for the community.
 The concept of cultural (folklore) performance is shared with ethnography and anthropology among other social sciences. The cultural anthropologist Victor Turner identified four universal characteristics of cultural performance: playfulness, framing, the use of symbolic language, and employing the subjunctive mood.[51] In viewing the performance, the audience leaves the daily reality to move into a mode of make-believe, or ""what if?"" It is self-evident that this fits well with all types of verbal lore, where reality has no place among the symbols, fantasies, and nonsense of traditional tales, proverbs, and jokes. Customs and the lore of children and games also fit easily into the language of a folklore performance.
 Material culture requires some moulding to turn it into a performance. Should we consider the performance of the creation of the artifact, as in a quilting party, or the performance of the recipients who use the quilt to cover their marriage bed? Here the language of context works better to describe the quilting of patterns copied from the grandmother, quilting as a social event during the winter months, or the gifting of a quilt to signify the importance of the event. Each of these—the traditional pattern chosen, the social event, and the gifting—occur within the broader context of the community. Even so, when considering context, the structure and characteristics of performance can be recognized, including an audience, a framing event, and the use of decorative figures and symbols, all of which go beyond the utility of the object.
 Before the Second World War, folk artifacts had been understood and collected as cultural shards of an earlier time. They were considered individual vestigial artifacts, with little or no function in the contemporary culture. Given this understanding, the goal of the folklorist was to capture and document them before they disappeared. They were collected with no supporting data, bound in books, archived and classified more or less successfully. The Historic–Geographic Method worked to isolate and track these collected artifacts, mostly verbal lore, across space and time.
 Following the Second World War, folklorists began to articulate a more holistic approach toward their subject matter. In tandem with the growing sophistication in the social sciences, attention was no longer limited to the isolated artifact, but extended to include the artifact embedded in an active cultural environment. One early proponent was Alan Dundes with his essay ""Texture, Text and Context"", first published 1964.[52] A public presentation in 1967 by Dan Ben-Amos at the American Folklore Society brought the behavioral approach into open debate among folklorists. In 1972 Richard Dorson called out the ""young Turks"" for their movement toward a behavioral approach to folklore. This approach ""shifted the conceptualization of folklore as an extractable item or 'text' to an emphasis on folklore as a kind of human behavior and communication. Conceptualizing folklore as behavior redefined the job of folklorists...""[53][g]
 Folklore became a verb, an action, something that people do, not just something that they have.[54] It is in the performance and the active context that folklore artifacts get transmitted in informal, direct communication, either verbally or in demonstration. Performance includes all the different modes and manners in which this transmission occurs.
 Transmission is a communicative process requiring a binary: one individual or group who actively transmits information in some form to another individual or group. Each of these is a defined role in the folklore process. The tradition-bearer[55] is the individual who actively passes along the knowledge of an artifact; this can be either a mother singing a lullaby to her baby, or an Irish dance troupe performing at a local festival. They are named individuals, usually well known in the community as knowledgeable in their traditional lore. They are not the anonymous ""folk"", the nameless mass without of history or individuality.
 The audience of this performance is the other half in the transmission process; they listen, watch, and remember. Few of them will become active tradition-bearers; many more will be passive tradition-bearers who maintain a memory of this specific traditional artifact, in both its presentation and its content.
 There is active communication between the audience and the performer. The performer is presenting to the audience; the audience in turn, through its actions and reactions, is actively communicating with the performer.[56] The purpose of this performance is not to create something new but to re-create something that already exists; the performance is words and actions which are known, recognized and valued by both the performer and the audience. For folklore is first and foremost remembered behavior. As members of the same cultural reference group, they identify and value this performance as a piece of shared cultural knowledge.
 To initiate the performance, there must be a frame of some sort to indicate that what is to follow is indeed performance. The frame brackets it as outside of normal discourse. In customary lore such as life cycle celebrations (ex. birthday) or dance performances, the framing occurs as part of the event, frequently marked by location. The audience goes to the event location to participate. Games are defined primarily by rules,[57] it is with the initiation of the rules that the game is framed. The folklorist Barre Toelken describes an evening spent in a Navaho family playing string figure games, with each of the members shifting from performer to audience as they create and display different figures to each other.[58]
 In verbal lore, the performer will start and end with recognized linguistic formulas. An easy example is seen in the common introduction to a joke: ""Have you heard the one..."", ""Joke of the day..."", or ""An elephant walks into a bar"". Each of these signals to the listeners that the following is a joke, not to be taken literally. The joke is completed with the punch line of the joke. Another traditional narrative marker in English is the framing of a fairy tale between the phrases ""Once upon a time"" and ""They all lived happily ever after."" Many languages have similar phrases which are used to frame a traditional tale. Each of these linguistic formulas removes the bracketed text from ordinary discourse, and marks it as a recognized form of stylized, formulaic communication for both the performer and the audience.
 Framing as a narrative device serves to signal to both the story teller and the audience that the narrative which follows is indeed a fiction (verbal lore), and not to be understood as historical fact or reality. It moves the framed narration into the subjunctive mood, and marks a space in which ""fiction, history, story, tradition, art, teaching, all exist within the narrated or performed expressive 'event' outside the normal realms and constraints of reality or time.""[59] This shift from the realis to the irrealis mood is understood by all participants within the reference group. It enables these fictional events to contain meaning for the group, and can lead to very real consequences.[60][clarification needed]
 The theory of self-correction in folklore transmission was first articulated by the folklorist Walter Anderson in the 1920s; this posits a feedback mechanism which would keep folklore variants closer to the original form.[61][i] This theory addresses the question about how, with multiple performers and multiple audiences, the artifact maintains its identity across time and geography. Anderson credited the audience with censoring narrators who deviated too far from the known (traditional) text.[62]
 Any performance is a two-way communication process. The performer addresses the audience with words and actions; the audience in turn actively responds to the performer. If this performance deviates too far from audience expectations of the familiar folk artifact, they will respond with negative feedback. Wanting to avoid more negative reaction, the performer will adjust his performance to conform to audience expectations. ""Social reward by an audience [is] a major factor in motivating narrators...""[63] It is this dynamic feedback loop between performer and audience which gives stability to the text of the performance.[56]
 In reality, this model is not so simplistic; there are multiple redundancies in the active folklore process. The performer has heard the tale multiple times, he has heard it from different story tellers in multiple versions. In turn, he tells the tale multiple times to the same or a different audience, and they expect to hear the version they know. This expanded model of redundancy in a non-linear narrative process makes it difficult to innovate during any single performance; corrective feedback from the audience will be immediate.[64] ""At the heart of both autopoetic self-maintenance and the 'virality' of meme transmission... it is enough to assume that some sort of recursive action maintains a degree of integrity [of the artifact] in certain features ... sufficient to allow us to recognize it as an instance of its type.""[65]
 For material folk artifacts, it becomes more fruitful to return to the terminology of Alan Dundes: text and context. Here the text designates the physical artifact itself, the single item made by an individual for a specific purpose. The context is then unmasked by observation and questions concerning both its production and its usage. Why was it made, how was it made, who will use it, how will they use it, where did the raw materials come from, who designed it, etc. These questions are limited only by the skill of the interviewer.
 In his study of southeastern Kentucky chair makers, Michael Owen Jones describes production of a chair within the context of the life of the craftsman.[66] For Henry Glassie in his study of Folk Housing in Middle Virginia, the investigation concerns the historical pattern he finds repeated in the dwellings of this region: the house is planted in the landscape just as the landscape completes itself with the house.[67] The artisan in his roadside stand or shop in the nearby town wants to make and display products which appeal to customers. There is ""a craftsperson's eagerness to produce 'satisfactory items' due to a close personal contact with the customer and expectations to serve the customer again."" Here the role of consumer ""... is the basic force responsible for the continuity and discontinuity of behavior.""[63]
 In material culture the context becomes the cultural environment in which the object is made (chair), used (house), and sold (wares). None of these artisans is ""anonymous"" folk; they are individuals making a living with the tools and skills learned within and valued in the context of their community.
 No two performances are identical. The performer attempts to keep the performance within expectations, but this happens despite a multitude of changing variables. He has given this performance one time more or less, the audience is different, the social and political environment has changed. In the context of material culture, no two hand-crafted items are identical. Sometimes these deviations in the performance and the production are unintentional, just part of the process. But sometimes these deviations are intentional; the performer or artisan want to play with the boundaries of expectation and add their own creative touch. They perform within the tension of conserving the recognized form and adding innovation.
 The folklorist Barre Toelken identifies this tension as ""a combination of both changing ('dynamic') and static ('conservative') elements that evolve and change through sharing, communication and performance.""[68] Over time, the cultural context shifts and morphs: new leaders, new technologies, new values, new awareness. As the context changes, so must the artifact, for without modifications to map existing artifacts into the evolving cultural landscape, they lose their meaning. Joking as an active form of verbal lore makes this tension visible as joke cycles come and go to reflect new issues of concern. Once an artifact is no longer applicable to the context, transmission becomes a nonstarter; it loses relevancy for a contemporary audience. If it is not transmitted, then it is no longer folklore and becomes instead an historic relic.[63]
 Folklorists have begun to identify how the advent of electronic communications will modify and change the performance and transmission of folklore artifacts. It is clear that the internet is modifying folkloric process, not killing it, as despite the historic association between folklore and anti-modernity, people continue to use traditional expressive forms in new media, including the internet.[69] Jokes and joking are as plentiful as ever both in traditional face-to-face interactions and through electronic transmission. New communication modes are also transforming traditional stories into many different configurations.[70] The fairy tale Snow White is now offered in multiple media forms for both children and adults, including a television show and video game.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['folklorist', 'Franz Boas and Ruth Benedict', 'the complex balance of continuity over change in both their design and their decoration', 'festival food and drink', 'different perspectives on the same folk'], 'answer_start': [], 'answer_end': []}"
"
 A legendary creature (also called a mythical or mythological creature) is a type of fantasy entity, typically a hybrid, that has not been proven and that is described in folklore (including myths and legends), but may be featured in historical accounts before modernity.
 In the classical era, monstrous creatures such as the Cyclops and the Minotaur appear in heroic tales for the protagonist to destroy. Other creatures, such as the unicorn, were claimed in accounts of natural history by various scholars of antiquity.[1][2][3] Some legendary creatures have their origin in traditional mythology and were believed to be real creatures, for example dragons, griffins, and unicorns. Others were based on real encounters, originating in garbled accounts of travellers' tales, such as the Vegetable Lamb of Tartary, which supposedly grew tethered to the earth.[4]
 A variety of mythical animals appear in the art and stories of the classical era. For example, in the Odyssey, monstrous creatures include the Cyclops, Scylla and Charybdis for the hero Odysseus to confront. Other tales include Medusa to be defeated by Perseus, the (human/bull) Minotaur to be destroyed by Theseus, and the Hydra to be killed by Heracles, while Aeneas battles with the harpies. These monsters thus have the basic function of emphasizing the greatness of the heroes involved.[5][6][7]
 Some classical era creatures, such as the (horse/human) centaur, chimaera, Triton and the flying horse Pegasus, are found also in Indian art. Similarly, sphinxes appear as winged lions in Indian art and the Piasa Bird of North America.[8][9]
 In medieval art, animals, both real and mythical, played important roles. These included decorative forms as in medieval jewellery, sometimes with their limbs intricately interlaced. Animal forms were used to add humor or majesty to objects. In Christian art, animals carried symbolic meanings, where for example the lamb symbolized Christ, a dove indicated the Holy Spirit, and the classical griffin represented a guardian of the dead. Medieval bestiaries included animals regardless of biological reality; the basilisk represented the devil, while the manticore symbolised temptation.[10]
 One function of mythical animals in the Middle Ages was allegory. Unicorns, for example, were described as extraordinarily swift and uncatchable by traditional methods.[11]: 127  It was believed that the only way for one to catch this beast was to lead a virgin to its dwelling. Then, the unicorn was supposed to leap into her lap and go to sleep, at which point a hunter could finally capture it.[11]: 127  In terms of symbolism, the unicorn was a metaphor for Christ. Unicorns represented the idea of innocence and purity. In the King James Bible, Psalm 92:10 states, ""My horn shalt thou exalt like the horn of a unicorn."" This is because the translators of the King James erroneously translated the Hebrew word re'em as unicorn.[11]: 128  Later versions translate this as wild ox.[12] The unicorn's small size signifies the humility of Christ.[11]: 128 
 Another common legendary creature that served allegorical functions within the Middle Ages was the dragon. Dragons were identified with serpents, though their attributes were greatly intensified. The dragon was supposed to have been larger than all other animals.[11]: 126  It was believed that the dragon had no harmful poison but was able to slay anything it embraced without any need for venom. Biblical scriptures speak of the dragon in reference to the devil, and they were used to denote sin in general during the Middle Ages.[11]: 126  Dragons were said to have dwelled in places like Ethiopia and India, based on the idea that there was always heat present in these locations.[11]: 126 
 Physical detail was not the central focus of the artists depicting such animals, and medieval bestiaries were not conceived as biological categorizations. Creatures like the unicorn and griffin were not categorized in a separate ""mythological"" section in medieval bestiaries,[13]: 124  as the symbolic implications were of primary importance. Animals we know to have existed were still presented with a fantastical approach. It seems the religious and moral implications of animals were far more significant than matching a physical likeness in these renderings. Nona C. Flores explains, ""By the tenth century, artists were increasingly bound by allegorical interpretation, and abandoned naturalistic depictions.""[13]: 15 
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['folklore', 'the Cyclops and the Minotaur', 'may be featured in historical accounts before modernity', 'Biblical scriptures speak of the dragon in reference to the devil', 'to denote sin in general'], 'answer_start': [], 'answer_end': []}"
"
 Classical music generally refers to the art music of the Western world, considered to be distinct from Western folk music or popular music traditions. It is sometimes distinguished as Western classical music, as the term ""classical music"" can also be applied to non-Western art musics. Classical music is often characterized by formality and complexity in its musical form and harmonic organization,[1] particularly with the use of polyphony.[2] Since at least the ninth century it has been primarily a written tradition,[2] spawning a sophisticated notational system, as well as accompanying literature in analytical, critical, historiographical, musicological and philosophical practices. A foundational component of Western culture, classical music is frequently seen from the perspective of individual or groups of composers, whose compositions, personalities and beliefs have fundamentally shaped its history.
 Rooted in the patronage of churches and royal courts in Western Europe,[1] surviving early medieval music is chiefly religious, monophonic and vocal, with the music of ancient Greece and Rome influencing its thought and theory. The earliest extant music manuscripts date from the Carolingian Empire (800–888),[3] around the time which Western plainchant gradually unified into what is termed Gregorian chant.[4] Musical centers existed at the Abbey of Saint Gall, the Abbey of Saint Martial and Saint Emmeram's Abbey, while the 11th century saw the development of staff notation and increasing output from medieval music theorists. By the mid-12th century France became the major European musical center:[3] The religious Notre-Dame school first fully explored organized rhythms and polyphony, while secular music flourished with the troubadour and trouvère traditions led by poet-musician nobles.[5] This culminated in the court sponsored French ars nova and Italian Trecento, which evolved into ars subtilior, a stylistic movement of extreme rhythmic diversity.[5] Beginning in the early 15th century, Renaissance composers of the influential Franco-Flemish School built off the harmonic principles in the English contenance angloise, bringing choral music to new standards, particularly the mass and motet.[6] Northern Italy soon emerged as the central musical region, where the Roman School engaged in highly sophisticated methods of polyphony in genres such as the madrigal,[6] which inspired the brief English Madrigal School.
 The Baroque period (1580–1750) saw the relative standardization of common-practice tonality,[7] as well as the increasing importance of musical instruments, which grew into ensembles of considerable size. Italy remained dominant, being the birthplace of opera, the soloist centered concerto genre, the organized sonata form as well as the large scale vocal-centered genres of oratorio and cantata. The fugue technique championed by Johann Sebastian Bach exemplified the Baroque tendency for complexity, and as a reaction the simpler and song-like galant music and empfindsamkeit styles were developed. In the shorter but pivotal Classical period (1730–1820) composers such as Wolfgang Amadeus Mozart, Joseph Haydn, and Ludwig van Beethoven created widely admired representatives of absolute music,[8][9] including symphonies, string quartets and concertos. The subsequent Romantic music (1800–1910) focused instead on programmatic music, for which the art song, symphonic poem and various piano genres were important vessels. During this time virtuosity was celebrated, immensity was encouraged, while philosophy and nationalism were embedded—all aspects that converged in the operas of Richard Wagner. By the 20th century, stylistic unification gradually dissipated while the prominence of popular music greatly increased. Many composers actively avoided past techniques and genres in the lens of modernism, with some abandoning tonality in place of serialism, while others found new inspiration in folk melodies or impressionist sentiments. After World War II, for the first time audience members valued older music over contemporary works, a preference which has been catered to by the emergence and widespread availability of commercial recordings.[10] Trends of the mid-20th century to the present day include New Simplicity, New Complexity, Minimalism, Spectral music, and more recently Postmodern music and Postminimalism. Increasingly global, practitioners from the Americas, Africa and Asia have obtained crucial roles,[3] while symphony orchestras and opera houses now appear across the world.
 Both the English term ""classical"" and the German equivalent Klassik developed from the French classique, itself derived from the Latin word classicus, which originally referred to the highest class of Ancient Roman citizens.[11][n 1] In Roman usage, the term later became a means to distinguish revered literary figures;[11] the Roman author Aulus Gellius commended writers such as Demosthenes and Virgil as classicus.[13] By the Renaissance, the adjective had acquired a more general meaning: an entry in Randle Cotgrave's 1611 A Dictionarie of the French and English Tongues is among the earliest extant definitions, translating classique as ""classical, formall  [sic], orderlie, in due or fit ranke; also, approved, authenticall, chiefe, principall"".[11][14] The musicologist Daniel Heartz summarizes this into two definitions: 1) a ""formal discipline"" and 2) a ""model of excellence"".[11] Like Gellius, later Renaissance scholars who wrote in Latin used classicus in reference to writers of classical antiquity;[12][n 2] however, this meaning only gradually developed, and was for a while subordinate to the broader classical ideals of formality and excellence.[15] Literature and visual arts—for which substantial Ancient Greek and Roman examples existed—did eventually adopt the term ""classical"" as relating to classical antiquity, but virtually no music of that time was available to Renaissance musicians, limiting the connection between classical music and the Greco-Roman world.[15][n 3]
 It was in 18th-century England that the term 'classical' ""first came to stand for a particular canon of works in performance.""[15] London had developed a prominent public concert music scene, unprecedented and unmatched by other European cities.[11] The royal court had gradually lost its monopoly on music, in large part from instability that the Commonwealth of England's dissolution and the Glorious Revolution enacted on court musicians.[11][n 4] In 1672, the former court musician John Banister began giving popular public concerts at a London tavern;[n 5] his popularity rapidly inaugurated the prominence of public concerts in the London.[19] The conception of ""classical""—or more often ""ancient music""—emerged, which was still built on the principles of formality and excellence, and according to Heartz ""civic ritual, religion and moral activism figured significantly in this novel construction of musical taste"".[15] The performance of such music was specialized by the Academy of Ancient Music and later at the Concerts of Antient Music series, where the work of select 16th and 17th composers was featured,[20] especially George Frideric Handel.[15][n 6] In France, the reign of Louis XIV (r. 1638–1715) saw a cultural renaissance, by the end of which writers such as Molière, Jean de La Fontaine and Jean Racine were considered to have surpassed the achievements of classical antiquity.[21] They were thus characterized as ""classical"", as was the music of Jean-Baptiste Lully (and later Christoph Willibald Gluck), being designated as ""l'opéra française classique"".[21] In the rest of continental Europe, the abandonment of defining ""classical"" as analogous to the Greco-Roman World was slower, primarily because the formation of canonical repertoires was either minimal or exclusive to the upper classes.[15]
 Many European commentators of the early 19th century found new unification in their definition of classical music: to juxtapose the older composers Wolfgang Amadeus Mozart, Joseph Haydn, and (excluding some of his later works) Ludwig van Beethoven as ""classical"" against the emerging style of Romantic music.[22][23][24] These three composers in particular were grouped into the First Viennese School, sometimes called the ""Viennese classics"",[n 7] a coupling that remains problematic by reason of none of the three being born in Vienna and the minimal time Haydn and Mozart spent in the city.[25] While this was an often expressed characterization, it was not a strict one. In 1879 the composer Charles Kensington Salaman defined the following composers as classical: Bach, Handel, Haydn, Mozart, Beethoven, Weber, Spohr and Mendelssohn.[26] More broadly, some writers used the term ""classical"" to generally praise well-regarded outputs from various composers, particularly those who produced many works in an established genre.[11][n 8]
 The contemporary understanding of the term ""classical music"" remains vague and multifaceted.[31][32] Other terms such as ""art music"", ""canonic music"", ""cultivated music"" and ""serious music"" are largely synonymous.[33] The term ""classical music"" is often indicated or implied to concern solely the Western world,[34] and conversely, in many academic histories the term ""Western music"" excludes non-classical Western music.[35][n 9] Another complication lies in that ""classical music"" is sometimes used to describe non-Western art music exhibiting similar long-lasting and complex characteristics; examples include Indian classical music (i.e. Carnatic Music Hindustani music  and Odissi  Music), Gamelan music, and various styles of the court of Imperial China (see yayue for instance).[1] Thus in the later 20th century terms such as ""Western classical music"" and ""Western art music"" came in use to address this.[34] The musicologist Ralph P. Locke notes that neither term is ideal, as they create an ""intriguing complication"" when considering ""certain practitioners of Western-art music genres who come from non-Western cultures"".[37][n 10]
 Complexity in musical form and harmonic organization are typical traits of classical music.[1] The Oxford English Dictionary (OED) offers three definitions for the word ""classical"" in relation to music:[27]
 The last definition concerns what is now termed the Classical period, a specific stylistic era of European music from the second half of the 18th century to the beginning of the 19th century.[38]
 The Western classical tradition formally begins with music created by and for the early Christian Church.[39] It is probable that the early Church wished to disassociate itself from the predominant music of ancient Greece and Rome, as it was a reminder of the pagan religion it had persecuted and by which it had been persecuted.[39] As such, it remains unclear as to what extent the music of the Christian Church, and thus Western classical music as a whole, was influenced by preceding ancient music.[40] The general attitude towards music was adopted from the Ancient Greek and Roman music theorists and commentators.[41][n 11] Just as in Greco-Roman society, music was considered central to education; along with arithmetic, geometry and astronomy, music was included in the quadrivium, the four subjects of the upper division of a standard liberal arts education in the Middle Ages.[43] This high regard for music was first promoted by the scholars Cassiodorus, Isidore of Seville,[44] and particularly Boethius,[45] whose transmission and expansion on the perspectives of music from Pythagoras, Aristotle and Plato were crucial in the development of medieval musical thought.[46] However, scholars, medieval music theorists and composers regularly misinterpreted or misunderstood the writings of their Greek and Roman predecessors.[47] This was due to the complete absence of surviving Greco-Roman musical works available to medieval musicians,[47][n 12] to the extent that Isidore of Seville (c. 559 – 636) stated ""unless sounds are remembered by man, they perish, for they cannot be written down"", unaware of the systematic notational practices of Ancient Greece centuries before.[48][n 13] The musicologist Gustave Reese notes, however, that many Greco-Roman texts can still be credited as influential to Western classical music, since medieval musicians regularly read their works—regardless of whether they were doing so correctly.[47]
 However, there are some indisputable musical continuations from the ancient world.[49] Basic aspects such as monophony, improvisation and the dominance of text in musical settings are prominent in both early medieval and music of nearly all ancient civilizations.[50] Greek influences in particular include the church modes (which were descendants of developments by Aristoxenus and Pythagoras),[51] basic acoustical theory from pythagorean tuning,[40] as well as the central function of tetrachords.[52] Ancient Greek instruments such as the aulos (a reed instrument) and the lyre (a stringed instrument similar to a small harp) eventually led to several modern-day instruments of a symphonic orchestra.[53] However, Donald Jay Grout notes that attempting to create a direct evolutionary connection from the ancient music to early medieval is baseless, as it was almost solely influenced by Greco-Roman music theory, not performance or practice.[54]
 Medieval music includes Western European music from after the fall of the Western Roman Empire by 476 to about 1400. Monophonic chant, also called plainsong or Gregorian chant, was the dominant form until about 1100.[55] Christian monks developed the first forms of European musical notation in order to standardize liturgy throughout the Church.[56][57] Polyphonic (multi-voiced) music developed from monophonic chant throughout the late Middle Ages and into the Renaissance, including the more complex voicings of motets. During the earlier medieval period, the vocal music from the liturgical genre, predominantly Gregorian chant, was monophonic, using a single, unaccompanied vocal melody line.[58] Polyphonic vocal genres, which used multiple independent vocal melodies, began to develop during the high medieval era, becoming prevalent by the later 13th and early 14th century. Notable Medieval composers include Hildegard of Bingen, Léonin, Pérotin, Philippe de Vitry, Guillaume de Machaut, Francesco Landini, and Johannes Ciconia.
 Many medieval musical instruments still exist, but in different forms. Medieval instruments included the flute, the recorder and plucked string instruments like the lute. As well, early versions of the organ and fiddle (or vielle) existed. Medieval instruments in Europe had most commonly been used singly, often self accompanied with a drone note, or occasionally in parts. From at least as early as the 13th century through the 15th century there was a division of instruments into haut (loud, shrill, outdoor instruments) and bas (quieter, more intimate instruments).[59] A number of instrument have roots in Eastern predecessors that were adopted from the medieval Islamic world.[60] For example, the Arabic rebab is the ancestor of all European bowed string instruments, including the lira, rebec and violin.[61][62]
 The musical Renaissance era lasted from 1400 to 1600. It was characterized by greater use of instrumentation, multiple interweaving melodic lines, and the use of earlier forms of bass instruments. Social dancing became more widespread, so musical forms appropriate to accompanying dance began to standardize. It is in this time that the notation of music on a staff and other elements of musical notation began to take shape.[63] This invention made possible the separation of the composition of a piece of music from its transmission; without written music, transmission was oral, and subject to change every time it was transmitted. With a musical score, a work of music could be performed without the composer's presence.[64] The invention of the movable-type printing press in the 15th century had far-reaching consequences on the preservation and transmission of music.[65]
 Many instruments originated during the Renaissance; others were variations of, or improvements upon, instruments that had existed previously. Some have survived to the present day; others have disappeared, only to be re-created in order to perform music on period instruments. As in the modern day, instruments may be classified as brass, strings, percussion, and woodwind. Brass instruments in the Renaissance were traditionally played by professionals who were members of Guilds and they included the slide trumpet, the wooden cornet, the valveless trumpet and the sackbut. Stringed instruments included the viol, the rebec, the harp-like lyre, the hurdy-gurdy, the lute, the guitar, the cittern, the bandora, and the orpharion. Keyboard instruments with strings included the harpsichord and the clavichord. Percussion instruments include the triangle, the Jew's harp, the tambourine, the bells, the rumble-pot, and various kinds of drums. Woodwind instruments included the double-reed shawm (an early member of the oboe family), the reed pipe, the bagpipe, the transverse flute, the recorder, the dulcian, and the crumhorn. Simple pipe organs existed, but were largely confined to churches, although there were portable varieties.[66] Printing enabled the standardization of descriptions and specifications of instruments, as well as instruction in their use.[67]
 Vocal music in the Renaissance is noted for the flourishing of an increasingly elaborate polyphonic style. The principal liturgical forms which endured throughout the entire Renaissance period were masses and motets, with some other developments towards the end, especially as composers of sacred music began to adopt secular forms (such as the madrigal) for their own designs. Towards the end of the period, the early dramatic precursors of opera such as monody, the madrigal comedy, and the intermedio are seen. Around 1597, Italian composer Jacopo Peri wrote Dafne, the first work to be called an opera today. He also composed Euridice, the first opera to have survived to the present day.
 Notable Renaissance composers include Josquin des Prez, Giovanni Pierluigi da Palestrina, John Dunstaple, Johannes Ockeghem, Orlande de Lassus, Guillaume Du Fay, Gilles Binchois, Thomas Tallis, William Byrd, Giovanni Gabrieli, Carlo Gesualdo, John Dowland, Jacob Obrecht, Adrian Willaert, Jacques Arcadelt, and Cipriano de Rore.
 The common practice period is typically defined as the era between the formation and the dissolution of common-practice tonality.[citation needed] The term usually spans roughly two-and-a-half centuries, encompassing the Baroque, Classical, and Romantic periods.
 Baroque music is characterized by the use of complex tonal counterpoint and the use of a basso continuo, a continuous bass line. Music became more complex in comparison with the simple songs of all previous periods.[68] The beginnings of the sonata form took shape in the canzona, as did a more formalized notion of theme and variations. The tonalities of major and minor as means for managing dissonance and chromaticism in music took full shape.[69]
 During the Baroque era, keyboard music played on the harpsichord and pipe organ became increasingly popular, and the violin family of stringed instruments took the form generally seen today. Opera as a staged musical drama began to differentiate itself from earlier musical and dramatic forms, and vocal forms like the cantata and oratorio became more common.[70] For the first time, vocalists began adding ornamentals to the music.[68]
 The theories surrounding equal temperament began to be put in wider practice, as it enabled a wider range of chromatic possibilities in hard-to-tune keyboard instruments. Although J.S. Bach did not use equal temperament, changes in the temperaments from the then-common meantone system to various temperaments that made modulation between all keys musically acceptable made possible his Well-Tempered Clavier.[71]
 Baroque instruments included some instruments from the earlier periods (e.g., the hurdy-gurdy and recorder) and a number of new instruments (e.g., the oboe, bassoon, cello, contrabass and fortepiano). Some instruments from previous eras fell into disuse, such as the shawm, cittern, rackett, and the wooden cornet. The key Baroque instruments for strings included the violin, viol, viola, viola d'amore, cello, contrabass, lute, theorbo (which often played the basso continuo parts), mandolin, Baroque guitar, harp and hurdy-gurdy. Woodwinds included the Baroque flute, Baroque oboe, recorder and the bassoon. Brass instruments included the cornett, natural horn, natural trumpet, serpent and the trombone. Keyboard instruments included the clavichord, the tangent piano, the harpsichord, the pipe organ, and, later in the period, the fortepiano (an early version of the piano). Percussion instruments included the timpani, snare drum, tambourine and the castanets.
 One major difference between Baroque music and the classical era that followed it is that the types of instruments used in Baroque ensembles were much less standardized. A Baroque ensemble could include one of several different types of keyboard instruments (e.g., pipe organ or harpsichord),[72] additional stringed chordal instruments (e.g., a lute), bowed strings, woodwinds, and brass instruments, and an unspecified number of bass instruments performing the basso continuo,(e.g., a cello, contrabass, viola, bassoon, serpent, etc.).
 Vocal oeuvres of the Baroque era included suites such as oratorios and cantatas.[73][74] Secular music was less common, and was typically characterized only by instrumental music. Like Baroque art,[75] themes were generally sacred and for the purpose of a catholic setting.
 Important composers of this era include Johann Sebastian Bach, Antonio Vivaldi, George Frideric Handel, Johann Pachelbel, Henry Purcell, Claudio Monteverdi, Barbara Strozzi, Domenico Scarlatti, Georg Philipp Telemann, Arcangelo Corelli, Alessandro Scarlatti, Jean-Philippe Rameau, Jean-Baptiste Lully, and Heinrich Schütz.
 Though the term ""classical music"" includes all Western art music from the Medieval era to the early 2010s, the Classical Era was the period of Western art music from the 1750s to the early 1820s[76]—the era of Wolfgang Amadeus Mozart, Joseph Haydn, and Ludwig van Beethoven.
 The Classical era established many of the norms of composition, presentation, and style, and when the piano became the predominant keyboard instrument. The basic forces required for an orchestra became somewhat standardized (though they would grow as the potential of a wider array of instruments was developed). Chamber music grew to include ensembles with as many as 8-10 performers for serenades. Opera continued to develop, with regional styles in Italy, France, and German-speaking lands. The opera buffa, a form of comic opera, rose in popularity. The symphony came into its own as a musical form, and the concerto was developed as a vehicle for displays of virtuoso playing skill. Orchestras no longer required a harpsichord, and were often led by the lead violinist (now called the concertmaster).[77]
 Classical era musicians continued to use many of the instruments from the Baroque era, such as the cello, contrabass, recorder, trombone, timpani, fortepiano (the precursor to the modern piano) and organ. While some Baroque instruments fell into disuse e.g. the theorbo and rackett, many Baroque instruments were changed into the versions still in use today, such as the Baroque violin (which became the violin), Baroque oboe (which became the oboe) and Baroque trumpet, which transitioned to the regular valved trumpet. During the Classical era, the stringed instruments used in orchestra and chamber music such as string quartets were standardized as the four instruments which form the string section of the orchestra: the violin, viola, cello, and double bass. Baroque-era stringed instruments such as fretted, bowed viols were phased out. Woodwinds included the basset clarinet, basset horn, clarinette d'amour, the Classical clarinet, the chalumeau, the flute, oboe and bassoon. Keyboard instruments included the clavichord and the fortepiano. While the harpsichord was still used in basso continuo accompaniment in the 1750s and 1760s, it fell out of use at the end of the century. Brass instruments included the buccin, the ophicleide (a replacement for the bass serpent, which was the precursor of the tuba) and the natural horn.
 Wind instruments became more refined in the Classical era. While double-reed instruments like the oboe and bassoon became somewhat standardized in the Baroque, the clarinet family of single reeds was not widely used until Mozart expanded its role in orchestral, chamber, and concerto settings.[78]
 The music of the Romantic era, from roughly the first decade of the 19th century to the early 20th century, was characterized by increased attention to an extended melodic line, as well as expressive and emotional elements, paralleling romanticism in other art forms. Musical forms began to break from the Classical era forms (even as those were being codified), with free-form pieces like nocturnes, fantasias, and preludes being written where accepted ideas about the exposition and development of themes were ignored or minimized.[79] The music became more chromatic, dissonant, and tonally colorful, with tensions (with respect to accepted norms of the older forms) about key signatures increasing.[80] The art song (or Lied) came to maturity in this era, as did the epic scales of grand opera, ultimately transcended by Richard Wagner's Ring cycle.[81]
 In the 19th century, musical institutions emerged from the control of wealthy patrons, as composers and musicians could construct lives independent of the nobility. Increasing interest in music by the growing middle classes throughout western Europe spurred the creation of organizations for the teaching, performance, and preservation of music. The piano, which achieved its modern construction in this era (in part due to industrial advances in metallurgy) became widely popular with the middle class, whose demands for the instrument spurred many piano builders. Many symphony orchestras date their founding to this era.[80] Some musicians and composers were the stars of the day; some, like Franz Liszt and Niccolò Paganini, fulfilled both roles.[82]
 European cultural ideas and institutions began to follow colonial expansion into other parts of the world. There was also a rise, especially toward the end of the era, of nationalism in music (echoing, in some cases, political sentiments of the time), as composers such as Edvard Grieg, Nikolai Rimsky-Korsakov, and Antonín Dvořák echoed traditional music of their homelands in their compositions.[83]
 In the Romantic era, the modern piano, with a more powerful, sustained tone and a wider range took over from the more delicate-sounding fortepiano. In the orchestra, the existing Classical instruments and sections were retained (string section, woodwinds, brass, and percussion), but these sections were typically expanded to make a fuller, bigger sound. For example, while a Baroque orchestra may have had two double bass players, a Romantic orchestra could have as many as ten. ""As music grew more expressive, the standard orchestral palette just wasn't rich enough for many Romantic composers.""[84]
 The families of instruments used, especially in orchestras, grew larger; a process that climaxed in the early 20th century with very large orchestras used by late romantic and modernist composers. A wider array of percussion instruments began to appear. Brass instruments took on larger roles, as the introduction of rotary valves made it possible for them to play a wider range of notes. The size of the orchestra (typically around 40 in the Classical era) grew to be over 100.[80] Gustav Mahler's 1906 Symphony No. 8, for example, has been performed with over 150 instrumentalists and choirs of over 400.[85] New woodwind instruments were added, such as the contrabassoon, bass clarinet and piccolo and new percussion instruments were added, including xylophones, snare drums, celestas (a bell-like keyboard instrument), bells, and triangles,[84] large orchestral harps, and even wind machines for sound effects. Saxophones appear in some scores from the late 19th century onwards, usually featured as a solo instrument rather than as in integral part of the orchestra.
 The Wagner tuba, a modified member of the horn family, appears in Richard Wagner's cycle Der Ring des Nibelungen. It also has a prominent role in Anton Bruckner's Symphony No. 7 in E Major and is also used in several late romantic and modernist works by Richard Strauss, Béla Bartók, and others[86] Cornets appear regularly in 19th century scores, alongside trumpets which were regarded as less agile, at least until the end of the century.
 Prominent composers of this era include Ludwig van Beethoven, Pyotr Ilyich Tchaikovsky, Frédéric Chopin, Hector Berlioz, Franz Schubert, Robert Schumann, Felix Mendelssohn, Franz Liszt, Giuseppe Verdi, Richard Wagner, Johannes Brahms, Alexander Scriabin, Nikolai Medtner, Edvard Grieg, and Johann Strauss II. Gustav Mahler and Richard Strauss are commonly regarded as transitional composers whose music combines both late romantic and early modernist elements.
 Encompassing a wide variety of post-Romantic styles, modernist classical music includes late romantic, impressionist, expressionist, and neoclassical styles of composition. Modernism marked an era when many composers rejected certain values of the common practice period, such as traditional tonality, melody, instrumentation, and structure. Some music historians regard musical modernism as an era extending from about 1890 to 1930.[87][88] Others consider that modernism ended with one or the other of the two world wars.[89] Still other authorities claim that modernism is not associated with any historical era, but rather is ""an attitude of the composer; a living construct that can evolve with the times"".[90] Despite its decline in the last third of the 20th century, there remained at the end of the century an active core of composers who continued to advance the ideas and forms of modernism, such as Pierre Boulez, Pauline Oliveros, Toru Takemitsu, George Benjamin, Jacob Druckman, Brian Ferneyhough, George Perle, Wolfgang Rihm, Richard Wernick, Richard Wilson, and Ralph Shapey.[91]
 Two musical movements that were dominant during this time were the impressionist beginning around 1890 and the expressionist that started around 1908. It was a period of diverse reactions in challenging and reinterpreting older categories of music, innovations that lead to new ways of organizing and approaching harmonic, melodic, sonic, and rhythmic aspects of music, and changes in aesthetic worldviews in close relation to the larger identifiable period of modernism in the arts of the time. The operative word most associated with it is ""innovation"".[92] Its leading feature is a ""linguistic plurality"", which is to say that no single music genre ever assumed a dominant position.[93]
 The orchestra continued to grow during the early years modernist era, peaking in the first two decades of the 20th century. Saxophones that appeared only rarely during the 19th century became more commonly used as supplementary instruments, but never became core members of the orchestra. While appearing only as featured solo instruments in some works, for example Maurice Ravel's orchestration of Modest Mussorgsky's Pictures at an Exhibition and Sergei Rachmaninoff's Symphonic Dances, the saxophone is included in other works such as Sergei Prokofiev's Romeo and Juliet Suites 1 and 2 and many other works as a member of the orchestral ensemble. In some compositions such as Ravel's Boléro, two or more saxophones of different sizes are used to create an entire section like the other sections of the orchestra. The euphonium is featured in a few late Romantic and 20th century works, usually playing parts marked ""tenor tuba"", including Gustav Holst's The Planets, and Richard Strauss's Ein Heldenleben.
 Prominent composers of the early 20th century include Igor Stravinsky, Claude Debussy, Sergei Rachmaninoff, Sergei Prokofiev, Arnold Schoenberg, Nikos Skalkottas, Heitor Villa-Lobos, Anton Webern, Alban Berg, Cécile Chaminade, Paul Hindemith, Aram Khachaturian, George Gershwin, Amy Beach, Béla Bartók, and Dmitri Shostakovich, along with the aforementioned Mahler and Strauss as transitional figures who carried over from the 19th century.
 Postmodern music is a period of music that began as early as 1930 according to some authorities.[87][88] It shares characteristics with postmodernist art – that is, art that comes after and reacts against modernism.
 Some other authorities have more or less equated postmodern music with the ""contemporary music"" composed well after 1930, from the late 20th century through to the early 21st century.[94][95] Some of the diverse movements of the postmodern/contemporary era include the neoromantic, neomedieval, minimalist, and post minimalist.
 Contemporary classical music at the beginning of the 21st century was often considered to include all post-1945 musical forms.[96] A generation later, this term now properly refers to the music of today written by composers who are still alive; music that came into prominence in the mid-1970s. It includes different variations of modernist, postmodern, neoromantic, and pluralist music.[91]
 Performers who have studied classical music extensively are said to be ""classically trained"". This training may come from private lessons from instrument or voice teachers or from completion of a formal program offered by a Conservatory, college or university, such as a Bachelor of Music or Master of Music degree (which includes individual lessons from professors). In classical music, ""...extensive formal music education and training, often to postgraduate [Master's degree] level"" is required.[97]
 Performance of classical music repertoire requires a proficiency in sight-reading and ensemble playing, harmonic principles, strong ear training (to correct and adjust pitches by ear), knowledge of performance practice (e.g., Baroque ornamentation), and a familiarity with the style/musical idiom expected for a given composer or musical work (e.g., a Brahms symphony or a Mozart concerto).[citation needed]
 The key characteristic of European classical music that distinguishes it from popular music, folk music, and some other classical music traditions such as Indian classical music, is that the repertoire tends to be written down in musical notation, creating a musical part or score. This score typically determines details of rhythm, pitch, and, where two or more musicians (whether singers or instrumentalists) are involved, how the various parts are coordinated. The written quality of the music has enabled a high level of complexity within them: fugues, for instance, achieve a remarkable marriage of boldly distinctive melodic lines weaving in counterpoint yet creating a coherent harmonic logic. The use of written notation also preserves a record of the works and enables Classical musicians to perform music from many centuries ago.
 Although Classical music in the 2000s has lost most of its tradition for musical improvisation, from the Baroque era to the Romantic era, there are examples of performers who could improvise in the style of their era. In the Baroque era, organ performers would improvise preludes, keyboard performers playing harpsichord would improvise chords from the figured bass symbols beneath the bass notes of the basso continuo part and both vocal and instrumental performers would improvise musical ornaments.[98] Johann Sebastian Bach was particularly noted for his complex improvisations.[99] During the Classical era, the composer-performer Wolfgang Amadeus Mozart was noted for his ability to improvise melodies in different styles.[100] During the Classical era, some virtuoso soloists would improvise the cadenza sections of a concerto. During the Romantic era, Ludwig van Beethoven would improvise at the piano.[101]
 Almost all of the composers who are described in music textbooks on classical music and whose works are widely performed as part of the standard concert repertoire are male composers, even though there have been a large number of women composers throughout the history of classical music. Musicologist Marcia Citron has asked ""[w]hy is music composed by women so marginal to the standard 'classical' repertoire?""[102] Citron ""examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works"". She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed not to be notable as composers.[102] In the ""...Concise Oxford History of Music, Clara S[c]humann is one of the only [sic] female composers mentioned.""[103] Abbey Philips states that ""[d]uring the 20th century the women who were composing/playing gained far less attention than their male counterparts.""[103]
 Historically, major professional orchestras have been mostly or entirely composed of musicians who are men. Some of the earliest cases of women being hired in professional orchestras was in the position of harpist. The Vienna Philharmonic, for example, did not accept women to permanent membership until 1997, far later than the other orchestras ranked among the world's top five by Gramophone in 2008.[104][n 14] The last major orchestra to appoint a woman to a permanent position was the Berlin Philharmonic.[108] As late as February 1996, the Vienna Philharmonic's principal flute, Dieter Flury, told Westdeutscher Rundfunk that accepting women would be ""gambling with the emotional unity (emotionelle Geschlossenheit) that this organism currently has"".[109] In April 1996, the orchestra's press secretary wrote that ""compensating for the expected leaves of absence"" of maternity leave would be a problem.[110]
 In 2013, an article in Mother Jones stated that while ""[m]any prestigious orchestras have significant female membership—women outnumber men in the New York Philharmonic's violin section—and several renowned ensembles, including the National Symphony Orchestra, the Detroit Symphony, and the Minnesota Orchestra, are led by women violinists"", the double bass, brass, and percussion sections of major orchestras ""...are still predominantly male"".[111] A 2014 BBC article stated that the ""...introduction of 'blind' auditions, where a prospective instrumentalist performs behind a screen so that the judging panel can exercise no gender or racial prejudice, has seen the gender balance of traditionally male-dominated symphony orchestras gradually shift.""[112]
 Classical music has often incorporated elements or material from popular music of the composer's time. Examples include occasional music such as Brahms' use of student drinking songs in his Academic Festival Overture, genres exemplified by Kurt Weill's The Threepenny Opera, and the influence of jazz on early and mid-20th-century composers including Maurice Ravel, exemplified by the movement entitled ""Blues"" in his sonata for violin and piano.[113] Some postmodern, minimalist and postminimalist classical composers acknowledge a debt to popular music.[114][failed verification]
 George Gershwin's 1924 orchestral composition Rhapsody in Blue has been described as orchestral jazz or symphonic jazz. The composition combines elements of classical music with jazz-influenced effects.
 Numerous examples show influence in the opposite direction, including popular songs based on classical music, the use to which Pachelbel's Canon has been put since the 1970s, and the musical crossover phenomenon, where classical musicians have achieved success in the popular music arena.[115] In heavy metal, a number of lead guitarists (playing electric guitar), including Ritchie Blackmore and Randy Rhoads,[116] modeled their playing styles on Baroque or Classical-era instrumental music.[117]
 Composers of classical music have often made use of folk music (music created by musicians who are commonly not classically trained, often from a purely oral tradition). Some composers, like Dvořák and Smetana,[118] have used folk themes to impart a nationalist flavor to their work, while others like Bartók have used specific themes lifted whole from their folk-music origins.[119] Khachaturian widely incorporated into his work the folk music of his native Armenia, but also other ethnic groups of the Middle East and Eastern Europe.[120][121]
 Certain staples of classical music are often used commercially (either in advertising or in movie soundtracks). In television commercials, several passages have become clichéd, particularly the opening of Richard Strauss' Also sprach Zarathustra (made famous in the film 2001: A Space Odyssey) and the opening section ""O Fortuna"" of Carl Orff's Carmina Burana; other examples include the ""Dies irae"" from the Verdi Requiem, Edvard Grieg's ""In the Hall of the Mountain King"" from Peer Gynt, the opening bars of Beethoven's Symphony No. 5, Aram Khachaturian's ""Sabre Dance"", Wagner's ""Ride of the Valkyries"" from Die Walküre, Rimsky-Korsakov's ""Flight of the Bumblebee"", and excerpts of Aaron Copland's Rodeo.[citation needed] Several works from the Golden Age of Animation matched the action to classical music. Notable examples are Walt Disney's Fantasia, Tom and Jerry's Johann Mouse, and Warner Bros.' Rabbit of Seville and What's Opera, Doc?
 Similarly, movies and television often use standard, clichéd excerpts of classical music to convey refinement or opulence: some of the most-often heard pieces in this category include Bach's Cello Suite No. 1, Mozart's Eine kleine Nachtmusik, Vivaldi's Four Seasons, Mussorgsky's Night on Bald Mountain (as orchestrated by Rimsky-Korsakov), and Rossini's ""William Tell Overture"". Shawn Vancour argues the commercialization of classical music in the 1920s may have harmed the music industry.[122]
 During the 1990s, several research papers and popular books wrote on what came to be called the ""Mozart effect"": an observed temporary, small elevation of scores on spatial reasoning tests as a result of listening to Mozart's music. The approach has been popularized in a book by Don Campbell, and is based on an experiment published in Nature suggesting that listening to Mozart temporarily boosted students' IQ by 8 to 9 points.[123] This popularized version of the theory was expressed succinctly by the New York Times music columnist Alex Ross: ""researchers... have determined that listening to Mozart actually makes you smarter.""[124] Promoters marketed CDs claimed to induce the effect. Florida passed a law requiring toddlers in state-run schools to listen to classical music every day, and in 1998 the governor of Georgia budgeted $105,000 per year to provide every child born in Georgia with a tape or CD of classical music. One of the co-authors of the original studies of the Mozart effect commented ""I don't think it can hurt. I'm all for exposing children to wonderful cultural experiences. But I do think the money could be better spent on music education programs.""[125]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Gregorian chant', 'Franz Liszt and Niccolò Paganini', 'personalities and beliefs have fundamentally shaped its history', 'philosophy and nationalism', 'problematic'], 'answer_start': [], 'answer_end': []}"
"
 Jazz is a music genre that originated in the African-American communities of New Orleans, Louisiana, in the late 19th and early 20th centuries, with its roots in blues, ragtime, European harmony and African rhythmic rituals.[1][2][3][4][5][6] Since the 1920s Jazz Age, it has been recognized as a major form of musical expression in traditional and popular music. Jazz is characterized by swing and blue notes, complex chords, call and response vocals, polyrhythms and improvisation.
 As jazz spread around the world, it drew on national, regional, and local musical cultures, which gave rise to different styles. New Orleans jazz began in the early 1910s, combining earlier brass band marches, French quadrilles, biguine, ragtime and blues with collective polyphonic improvisation. However, jazz did not begin as a single musical tradition in New Orleans or elsewhere.[7] In the 1930s, arranged dance-oriented swing big bands, Kansas City jazz (a hard-swinging, bluesy, improvisational style), and gypsy jazz (a style that emphasized musette waltzes) were the prominent styles. Bebop emerged in the 1940s, shifting jazz from danceable popular music toward a more challenging ""musician's music"" which was played at faster tempos and used more chord-based improvisation. Cool jazz developed near the end of the 1940s, introducing calmer, smoother sounds and long, linear melodic lines.[8]
 The mid-1950s saw the emergence of hard bop, which introduced influences from rhythm and blues, gospel, and blues to small groups and particularly to saxophone and piano. Modal jazz developed in the late 1950s, using the mode, or musical scale, as the basis of musical structure and improvisation, as did free jazz, which explored playing without regular meter, beat and formal structures. Jazz-rock fusion appeared in the late 1960s and early 1970s, combining jazz improvisation with rock music's rhythms, electric instruments, and highly amplified stage sound. In the early 1980s, a commercial form of jazz fusion called smooth jazz became successful, garnering significant radio airplay. Other styles and genres abound in the 21st century, such as Latin and Afro-Cuban jazz.
 The origin of the word jazz has resulted in considerable research, and its history is well documented. It is believed to be related to jasm, a slang term dating back to 1860 meaning 'pep, energy'.[9][10] The earliest written record of the word is in a 1912 article in the Los Angeles Times in which a minor league baseball pitcher described a pitch which he called a 'jazz ball' ""because it wobbles and you simply can't do anything with it"".[9][10]
 The use of the word in a musical context was documented as early as 1915 in the Chicago Daily Tribune.[10][11] Its first documented use in a musical context in New Orleans was in a November 14, 1916, Times-Picayune article about ""jas bands"".[12] In an interview with National Public Radio, musician Eubie Blake offered his recollections of the slang connotations of the term, saying: ""When Broadway picked it up, they called it 'J-A-Z-Z'. It wasn't called that. It was spelled 'J-A-S-S'. That was dirty, and if you knew what it was, you wouldn't say it in front of ladies.""[13] The American Dialect Society named it the Word of the 20th Century.[14]
 Jazz is difficult to define because it encompasses a wide range of music spanning a period of over 100 years, from ragtime to rock-infused fusion. Attempts have been made to define jazz from the perspective of other musical traditions, such as European music history or African music. But critic Joachim-Ernst Berendt argues that its terms of reference and its definition should be broader,[15] defining jazz as a ""form of art music which originated in the United States through the confrontation of the Negro with European music""[16] and arguing that it differs from European music in that jazz has a ""special relationship to time defined as 'swing'"". Jazz involves ""a spontaneity and vitality of musical production in which improvisation plays a role"" and contains a ""sonority and manner of phrasing which mirror the individuality of the performing jazz musician"".[15]
 A broader definition that encompasses different eras of jazz has been proposed by Travis Jackson: ""it is music that includes qualities such as swing, improvising, group interaction, developing an 'individual voice', and being open to different musical possibilities"".[17] Krin Gibbard argued that ""jazz is a construct"" which designates ""a number of musics with enough in common to be understood as part of a coherent tradition"".[18] Duke Ellington, one of jazz's most famous figures, said, ""It's all music.""[19]
 Although jazz is considered difficult to define, in part because it contains many subgenres, improvisation is one of its defining elements. The centrality of improvisation is attributed to the influence of earlier forms of music such as blues, a form of folk music which arose in part from the work songs and field hollers of African-American slaves on plantations. These work songs were commonly structured around a repetitive call-and-response pattern, but early blues was also improvisational. Classical music performance is evaluated more by its fidelity to the musical score, with less attention given to interpretation, ornamentation, and accompaniment. The classical performer's goal is to play the composition as it was written. In contrast, jazz is often characterized by the product of interaction and collaboration, placing less value on the contribution of the composer, if there is one, and more on the performer.[20] The jazz performer interprets a tune in individual ways, never playing the same composition twice. Depending on the performer's mood, experience, and interaction with band members or audience members, the performer may change melodies, harmonies, and time signatures.[21]
 In early Dixieland, a.k.a. New Orleans jazz, performers took turns playing melodies and improvising countermelodies. In the swing era of the 1920s–'40s, big bands relied more on arrangements which were written or learned by ear and memorized. Soloists improvised within these arrangements. In the bebop era of the 1940s, big bands gave way to small groups and minimal arrangements in which the melody was stated briefly at the beginning and most of the piece was improvised. Modal jazz abandoned chord progressions to allow musicians to improvise even more. In many forms of jazz, a soloist is supported by a rhythm section of one or more chordal instruments (piano, guitar), double bass, and drums. The rhythm section plays chords and rhythms that outline the composition structure and complement the soloist.[22] In avant-garde and free jazz, the separation of soloist and band is reduced, and there is license, or even a requirement, for the abandoning of chords, scales, and meters.
 Since the emergence of bebop, forms of jazz that are commercially oriented or influenced by popular music have been criticized. According to Bruce Johnson, there has always been a ""tension between jazz as a commercial music and an art form"".[17] Regarding the Dixieland jazz revival of the 1940s, Black musicians rejected it as being shallow nostalgia entertainment for white audiences.[23][24] On the other hand, traditional jazz enthusiasts have dismissed bebop, free jazz, and jazz fusion as forms of debasement and betrayal. An alternative view is that jazz can absorb and transform diverse musical styles.[25] By avoiding the creation of norms, jazz allows avant-garde styles to emerge.[17]
 For some African Americans, jazz has drawn attention to African-American contributions to culture and history. For others, jazz is a reminder of ""an oppressive and racist society and restrictions on their artistic visions"".[26] Amiri Baraka argues that there is a ""white jazz"" genre that expresses whiteness.[27] White jazz musicians appeared in the Midwest and in other areas throughout the U.S. Papa Jack Laine, who ran the Reliance band in New Orleans in the 1910s, was called ""the father of white jazz"".[28] The Original Dixieland Jazz Band, whose members were white, were the first jazz group to record, and Bix Beiderbecke was one of the most prominent jazz soloists of the 1920s.[29] The Chicago Style was developed by white musicians such as Eddie Condon, Bud Freeman, Jimmy McPartland, and Dave Tough. Others from Chicago such as Benny Goodman and Gene Krupa became leading members of swing during the 1930s.[30] Many bands included both Black and white musicians. These musicians helped change attitudes toward race in the U.S.[31]
 Female jazz performers and composers have contributed to jazz throughout its history. Although Betty Carter, Ella Fitzgerald, Adelaide Hall, Billie Holiday, Peggy Lee, Abbey Lincoln, Anita O'Day, Dinah Washington, and Ethel Waters were recognized for their vocal talent, less familiar were bandleaders, composers, and instrumentalists such as pianist Lil Hardin Armstrong, trumpeter Valaida Snow, and songwriters Irene Higginbotham and Dorothy Fields. Women began playing instruments in jazz in the early 1920s, drawing particular recognition on piano.[32]
 When male jazz musicians were drafted during World War II, many all-female bands replaced them.[32] The International Sweethearts of Rhythm, which was founded in 1937, was a popular band that became the first all-female integrated band in the U.S. and the first to travel with the USO, touring Europe in 1945. Women were members of the big bands of Woody Herman and Gerald Wilson. Beginning in the 1950s, many women jazz instrumentalists were prominent, some sustaining long careers. Some of the most distinctive improvisers, composers, and bandleaders in jazz have been women.[33] Trombonist Melba Liston is acknowledged as the first female horn player to work in major bands and to make a real impact on jazz, not only as a musician but also as a respected composer and arranger, particularly through her collaborations with Randy Weston from the late 1950s into the 1990s.[34][35]
 Jewish Americans played a significant role in jazz. As jazz spread, it developed to encompass many different cultures, and the work of Jewish composers in Tin Pan Alley helped shape the many different sounds that jazz came to incorporate.[36]
 Jewish Americans were able to thrive in Jazz because of the probationary whiteness that they were allotted at the time.[37] George Bornstein wrote that African Americans were sympathetic to the plight of the Jewish American and vice versa. As disenfranchised minorities themselves, Jewish composers of popular music saw themselves as natural allies with African Americans.[38]
 The Jazz Singer with Al Jolson is one example of how Jewish Americans were able to bring jazz, music that African Americans developed, into popular culture.[39] Benny Goodman was a vital Jewish American to the progression of Jazz. Goodman was the leader of a racially integrated band named King of Swing. His jazz concert in the Carnegie Hall in 1938 was the first ever to be played there. The concert was described by Bruce Eder as ""the single most important jazz or popular music concert in history"".[40]
 Shep Fields also helped to popularize ""Sweet"" Jazz music through his appearances and Big band remote broadcasts from such landmark venues as Chicago's Palmer House, Broadway's Paramount Theater and the Starlight Roof at the famed Waldorf-Astoria Hotel. He entertained audiences with a light elegant musical style which remained popular with audiences for nearly three decades from the 1930s until the late 1950s.[41][42][43]
 Jazz originated in the late-19th to early-20th century.  It developed out of many forms of music, including blues, spirituals, hymns, marches, vaudeville song, ragtime, and dance music.[44] It also incorporated interpretations of American and European classical music, entwined with African and slave folk songs and the influences of West African culture.[45] Its composition and style have changed many times throughout the years with each performer's personal interpretation and improvisation, which is also one of the greatest appeals of the genre.[46]
 By the 18th century, slaves in the New Orleans area gathered socially at a special market, in an area which later became known as Congo Square, famous for its African dances.[47]
 By 1866, the Atlantic slave trade had brought nearly 400,000 Africans to North America.[48] The slaves came largely from West Africa and the greater Congo River basin and brought strong musical traditions with them.[49] The African traditions primarily use a single-line melody and call-and-response pattern, and the rhythms have a counter-metric structure and reflect African speech patterns.[50]
 An 1885 account says that they were making strange music (Creole) on an equally strange variety of 'instruments'—washboards, washtubs, jugs, boxes beaten with sticks or bones and a drum made by stretching skin over a flour-barrel.[4][51]
 Lavish festivals with African-based dances to drums were organized on Sundays at Place Congo, or Congo Square, in New Orleans until 1843.[52] There are historical accounts of other music and dance gatherings elsewhere in the southern United States. Robert Palmer said of percussive slave music:
 Usually such music was associated with annual festivals, when the year's crop was harvested and several days were set aside for celebration. As late as 1861, a traveler in North Carolina saw dancers dressed in costumes that included horned headdresses and cow tails and heard music provided by a sheepskin-covered ""gumbo box"", apparently a frame drum; triangles and jawbones furnished the auxiliary percussion. There are quite a few [accounts] from the southeastern states and Louisiana dating from the period 1820–1850. Some of the earliest [Mississippi] Delta settlers came from the vicinity of New Orleans, where drumming was never actively discouraged for very long and homemade drums were used to accompany public dancing until the outbreak of the Civil War.[53]
 Another influence came from the harmonic style of hymns of the church, which black slaves had learned and incorporated into their own music as spirituals.[54] The origins of the blues are undocumented, though they can be seen as the secular counterpart of the spirituals. However, as Gerhard Kubik points out, whereas the spirituals are homophonic, rural blues and early jazz ""was largely based on concepts of heterophony"".[55]
 During the early 19th century an increasing number of black musicians learned to play European instruments, particularly the violin, which they used to parody European dance music in their own cakewalk dances. In turn, European American minstrel show performers in blackface popularized the music internationally, combining syncopation with European harmonic accompaniment. In the mid-1800s the white New Orleans composer Louis Moreau Gottschalk adapted slave rhythms and melodies from Cuba and other Caribbean islands into piano salon music. New Orleans was the main nexus between the Afro-Caribbean and African American cultures.
 The Black Codes outlawed drumming by slaves, which meant that African drumming traditions were not preserved in North America, unlike in Cuba, Haiti, and elsewhere in the Caribbean. African-based rhythmic patterns were retained in the United States in large part through ""body rhythms"" such as stomping, clapping, and patting juba dancing.[56]
 In the opinion of jazz historian Ernest Borneman, what preceded New Orleans jazz before 1890 was ""Afro-Latin music"", similar to what was played in the Caribbean at the time.[57] A three-stroke pattern known in Cuban music as tresillo is a fundamental rhythmic figure heard in many different slave musics of the Caribbean, as well as the Afro-Caribbean folk dances performed in New Orleans Congo Square and Gottschalk's compositions (for example ""Souvenirs From Havana"" (1859)). Tresillo (shown below) is the most basic and most prevalent duple-pulse rhythmic cell in sub-Saharan African music traditions and the music of the African Diaspora.[58][59]
 Tresillo is heard prominently in New Orleans second line music and in other forms of popular music from that city from the turn of the 20th century to present.[60] ""By and large the simpler African rhythmic patterns survived in jazz ... because they could be adapted more readily to European rhythmic conceptions,"" jazz historian Gunther Schuller observed. ""Some survived, others were discarded as the Europeanization progressed.""[61]
 In the post-Civil War period (after 1865), African Americans were able to obtain surplus military bass drums, snare drums and fifes, and an original African-American drum and fife music emerged, featuring tresillo and related syncopated rhythmic figures.[62] This was a drumming tradition that was distinct from its Caribbean counterparts, expressing a uniquely African-American sensibility. ""The snare and bass drummers played syncopated cross-rhythms,"" observed the writer Robert Palmer, speculating that ""this tradition must have dated back to the latter half of the nineteenth century, and it could have not have developed in the first place if there hadn't been a reservoir of polyrhythmic sophistication in the culture it nurtured.""[56]
 African-American music began incorporating Afro-Cuban rhythmic motifs in the 19th century when the habanera (Cuban contradanza) gained international popularity.[63] Musicians from Havana and New Orleans would take the twice-daily ferry between both cities to perform, and the habanera quickly took root in the musically fertile Crescent City. John Storm Roberts states that the musical genre habanera ""reached the U.S. twenty years before the first rag was published.""[64] For the more than quarter-century in which the cakewalk, ragtime, and proto-jazz were forming and developing, the habanera was a consistent part of African-American popular music.[64]
 Habaneras were widely available as sheet music and were the first written music which was rhythmically based on an African motif (1803).[65] From the perspective of African-American music, the ""habanera rhythm"" (also known as ""congo""),[65] ""tango-congo"",[66] or tango.[67] can be thought of as a combination of tresillo and the backbeat.[68] The habanera was the first of many Cuban music genres which enjoyed periods of popularity in the United States and reinforced and inspired the use of tresillo-based rhythms in African-American music.
 New Orleans native Louis Moreau Gottschalk's piano piece ""Ojos Criollos (Danse Cubaine)"" (1860) was influenced by the composer's studies in Cuba: the habanera rhythm is clearly heard in the left hand.[58]: 125  In Gottschalk's symphonic work ""A Night in the Tropics"" (1859), the tresillo variant cinquillo appears extensively.[69] The figure was later used by Scott Joplin and other ragtime composers.
 Comparing the music of New Orleans with the music of Cuba, Wynton Marsalis observes that tresillo is the New Orleans ""clavé"", a Spanish word meaning ""code"" or ""key"", as in the key to a puzzle, or mystery.[70] Although the pattern is only half a clave, Marsalis makes the point that the single-celled figure is the guide-pattern of New Orleans music. Jelly Roll Morton called the rhythmic figure the Spanish tinge and considered it an essential ingredient of jazz.[71]
 The abolition of slavery in 1865 led to new opportunities for the education of freed African Americans. Although strict segregation limited employment opportunities for most blacks, many were able to find work in entertainment. Black musicians were able to provide entertainment in dances, minstrel shows, and in vaudeville, during which time many marching bands were formed. Black pianists played in bars, clubs, and brothels, as ragtime developed.[72][73]
 Ragtime appeared as sheet music, popularized by African-American musicians such as the entertainer Ernest Hogan, whose hit songs appeared in 1895. Two years later, Vess Ossman recorded a medley of these songs as a banjo solo known as ""Rag Time Medley"".[74][75] Also in 1897, the white composer William Krell published his ""Mississippi Rag"" as the first written piano instrumental ragtime piece, and Tom Turpin published his ""Harlem Rag"", the first rag published by an African-American.
 Classically trained pianist Scott Joplin produced his ""Original Rags"" in 1898 and, in 1899, had an international hit with ""Maple Leaf Rag"", a multi-strain ragtime march with four parts that feature recurring themes and a bass line with copious seventh chords. Its structure was the basis for many other rags, and the syncopations in the right hand, especially in the transition between the first and second strain, were novel at the time.[76] The last four measures of Scott Joplin's ""Maple Leaf Rag"" (1899) are shown below.
 African-based rhythmic patterns such as tresillo and its variants, the habanera rhythm and cinquillo, are heard in the ragtime compositions of Joplin and Turpin. Joplin's ""Solace"" (1909) is generally considered to be in the habanera genre:[77][78] both of the pianist's hands play in a syncopated fashion, completely abandoning any sense of a march rhythm. Ned Sublette postulates that the tresillo/habanera rhythm ""found its way into ragtime and the cakewalk,""[79] whilst Roberts suggests that ""the habanera influence may have been part of what freed black music from ragtime's European bass"".[80]
 In the northeastern United States, a ""hot"" style of playing ragtime had developed, notably James Reese Europe's symphonic Clef Club orchestra in New York City, which played a benefit concert at Carnegie Hall in 1912.[81][82] The Baltimore rag style of Eubie Blake influenced James P. Johnson's development of stride piano playing, in which the right hand plays the melody, while the left hand provides the rhythm and bassline.[83]
 In Ohio and elsewhere in the mid-west the major influence was ragtime, until about 1919. Around 1912, when the four-string banjo and saxophone came in, musicians began to improvise the melody line, but the harmony and rhythm remained unchanged. A contemporary account states that blues could only be heard in jazz in the gut-bucket cabarets, which were generally looked down upon by the Black middle-class.[84]
 Blues is the name given to both a musical form and a music genre,[85] which originated in African-American communities of primarily the Deep South of the United States at the end of the 19th century from their spirituals, work songs, field hollers, shouts and chants and rhymed simple narrative ballads.[86]
 The African use of pentatonic scales contributed to the development of blue notes in blues and jazz.[87] As Kubik explains:
 Many of the rural blues of the Deep South are stylistically an extension and merger of basically two broad accompanied song-style traditions in the west central Sudanic belt:
 W. C. Handy became interested in folk blues of the Deep South while traveling through the Mississippi Delta. In this folk blues form, the singer would improvise freely within a limited melodic range, sounding like a field holler, and the guitar accompaniment was slapped rather than strummed, like a small drum which responded in syncopated accents, functioning as another ""voice"".[89] Handy and his band members were formally trained African-American musicians who had not grown up with the blues, yet he was able to adapt the blues to a larger band instrument format and arrange them in a popular music form.
 Handy wrote about his adopting of the blues:
 The primitive southern Negro, as he sang, was sure to bear down on the third and seventh tone of the scale, slurring between major and minor. Whether in the cotton field of the Delta or on the Levee up St. Louis way, it was always the same. Till then, however, I had never heard this slur used by a more sophisticated Negro, or by any white man. I tried to convey this effect ... by introducing flat thirds and sevenths (now called blue notes) into my song, although its prevailing key was major ... , and I carried this device into my melody as well.[90] The publication of his ""Memphis Blues"" sheet music in 1912 introduced the 12-bar blues to the world (although Gunther Schuller argues that it is not really a blues, but ""more like a cakewalk"").[91] This composition, as well as his later ""St. Louis Blues"" and others, included the habanera rhythm,[92] and would become jazz standards. Handy's music career began in the pre-jazz era and contributed to the codification of jazz through the publication of some of the first jazz sheet music.
 The music of New Orleans, Louisiana had a profound effect on the creation of early jazz. In New Orleans, slaves could practice elements of their culture such as voodoo and playing drums.[93] Many early jazz musicians played in the bars and brothels of the red-light district around Basin Street called Storyville.[94] In addition to dance bands, there were marching bands which played at lavish funerals (later called jazz funerals). The instruments used by marching bands and dance bands became the instruments of jazz: brass, drums, and reeds tuned in the European 12-tone scale. Small bands contained a combination of self-taught and formally educated musicians, many from the funeral procession tradition. These bands traveled in black communities in the deep south. Beginning in 1914, Louisiana Creole and African-American musicians played in vaudeville shows which carried jazz to cities in the northern and western parts of the U.S.[95] Jazz became international in 1914, when the Creole Band with cornettist Freddie Keppard performed the first ever jazz concert outside the United States, at the Pantages Playhouse Theatre in Winnipeg, Canada.[96]
 In New Orleans, a white bandleader named Papa Jack Laine integrated blacks and whites in his marching band. He was known as ""the father of white jazz"" because of the many top players he employed, such as George Brunies, Sharkey Bonano, and future members of the Original Dixieland Jass Band. During the early 1900s, jazz was mostly performed in African-American and mulatto communities due to segregation laws. Storyville brought jazz to a wider audience through tourists who visited the port city of New Orleans.[97] Many jazz musicians from African-American communities were hired to perform in bars and brothels. These included Buddy Bolden and Jelly Roll Morton in addition to those from other communities, such as Lorenzo Tio and Alcide Nunez. Louis Armstrong started his career in Storyville[98] and found success in Chicago. Storyville was shut down by the U.S. government in 1917.[99]
 Cornetist Buddy Bolden played in New Orleans from 1895 to 1906. No recordings by him exist. His band is credited with creating the big four: the first syncopated bass drum pattern to deviate from the standard on-the-beat march.[100] As the example below shows, the second half of the big four pattern is the habanera rhythm.
 Afro-Creole pianist Jelly Roll Morton began his career in Storyville. Beginning in 1904, he toured with vaudeville shows to southern cities, Chicago, and New York City. In 1905, he composed ""Jelly Roll Blues"", which became the first jazz arrangement in print when it was published in 1915. It introduced more musicians to the New Orleans style.[101]
 Morton considered the tresillo/habanera, which he called the Spanish tinge, an essential ingredient of jazz.[102] ""Now in one of my earliest tunes, ""New Orleans Blues,"" you can notice the Spanish tinge. In fact, if you can't manage to put tinges of Spanish in your tunes, you will never be able to get the right seasoning, I call it, for jazz.""[71]
 An excerpt of ""New Orleans Blues"" is shown below. In the excerpt, the left hand plays the tresillo rhythm, while the right hand plays variations on cinquillo.
 Morton was a crucial innovator in the evolution from the early jazz form known as ragtime to jazz piano, and could perform pieces in either style; in 1938, Morton made a series of recordings for the Library of Congress in which he demonstrated the difference between the two styles. Morton's solos, however, were still close to ragtime, and were not merely improvisations over chord changes as in later jazz, but his use of the blues was of equal importance.
 Morton loosened ragtime's rigid rhythmic feeling, decreasing its embellishments and employing a swing feeling.[103] Swing is the most important and enduring African-based rhythmic technique used in jazz. An oft quoted definition of swing by Louis Armstrong is: ""if you don't feel it, you'll never know it.""[104] The New Harvard Dictionary of Music states that swing is: ""An intangible rhythmic momentum in jazz...Swing defies analysis; claims to its presence may inspire arguments."" The dictionary does nonetheless provide the useful description of triple subdivisions of the beat contrasted with duple subdivisions:[105] swing superimposes six subdivisions of the beat over a basic pulse structure or four subdivisions. This aspect of swing is far more prevalent in African-American music than in Afro-Caribbean music. One aspect of swing, which is heard in more rhythmically complex Diaspora musics, places strokes in-between the triple and duple-pulse ""grids"".[106]
 New Orleans brass bands are a lasting influence, contributing horn players to the world of professional jazz with the distinct sound of the city whilst helping black children escape poverty. The leader of New Orleans' Camelia Brass Band, D'Jalma Ganier, taught Louis Armstrong to play trumpet; Armstrong would then popularize the New Orleans style of trumpet playing, and then expand it. Like Jelly Roll Morton, Armstrong is also credited with the abandonment of ragtime's stiffness in favor of swung notes. Armstrong, perhaps more than any other musician, codified the rhythmic technique of swing in jazz and broadened the jazz solo vocabulary.[107]
 The Original Dixieland Jass Band made the music's first recordings early in 1917, and their ""Livery Stable Blues"" became the earliest released jazz record.[108][109][110][111][112][113][114] That year, numerous other bands made recordings featuring ""jazz"" in the title or band name, but most were ragtime or novelty records rather than jazz. In February 1918 during World War I, James Reese Europe's ""Hellfighters"" infantry band took ragtime to Europe,[115][116] then on their return recorded Dixieland standards including ""Darktown Strutters' Ball"".[81]
 From 1920 to 1933, Prohibition in the United States banned the sale of alcoholic drinks, resulting in illicit speakeasies which became lively venues of the ""Jazz Age"", hosting popular music, dance songs, novelty songs, and show tunes. Jazz began to get a reputation as immoral, and many members of the older generations saw it as a threat to the old cultural values by promoting the decadent values of the Roaring 20s. Henry van Dyke of Princeton University wrote, ""... it is not music at all. It's merely an irritation of the nerves of hearing, a sensual teasing of the strings of physical passion.""[117] The New York Times reported that Siberian villagers used jazz to scare away bears, but the villagers had used pots and pans; another story claimed that the fatal heart attack of a celebrated conductor was caused by jazz.[117]
 In 1919, Kid Ory's Original Creole Jazz Band of musicians from New Orleans began playing in San Francisco and Los Angeles, where in 1922 they became the first black jazz band of New Orleans origin to make recordings.[118][119] During the same year, Bessie Smith made her first recordings.[120] Chicago was developing ""Hot Jazz"", and King Oliver joined Bill Johnson. Bix Beiderbecke formed The Wolverines in 1924.
 Despite its Southern black origins, there was a larger market for jazzy dance music played by white orchestras. In 1918, Paul Whiteman and his orchestra became a hit in San Francisco. He signed a contract with Victor and became the top bandleader of the 1920s, giving hot jazz a white component, hiring white musicians such as Bix Beiderbecke, Jimmy Dorsey, Tommy Dorsey, Frankie Trumbauer, and Joe Venuti. In 1924, Whiteman commissioned George Gershwin's Rhapsody in Blue, which was premiered by his orchestra. Jazz began to be recognized as a notable musical form. Olin Downes, reviewing the concert in The New York Times, wrote, ""This composition shows extraordinary talent, as it shows a young composer with aims that go far beyond those of his ilk, struggling with a form of which he is far from being master. ... In spite of all this, he has expressed himself in a significant and, on the whole, highly original form. ... His first theme ... is no mere dance-tune ... it is an idea, or several ideas, correlated and combined in varying and contrasting rhythms that immediately intrigue the listener.""[121]
 After Whiteman's band successfully toured Europe, huge hot jazz orchestras in theater pits caught on with other whites, including Fred Waring, Jean Goldkette, and Nathaniel Shilkret. According to Mario Dunkel, Whiteman's success was based on a ""rhetoric of domestication"" according to which he had elevated and rendered valuable (read ""white"") a previously inchoate (read ""black"") kind of music.[122]
 Whiteman's success caused black artists to follow suit, including Earl Hines (who opened in The Grand Terrace Cafe in Chicago in 1928), Washington, D.C.-native Duke Ellington (who opened at the Cotton Club in Harlem in 1927), Lionel Hampton, Fletcher Henderson, Claude Hopkins, and Don Redman, with Henderson and Redman developing the ""talking to one another"" formula for ""hot"" swing music.[123]
 In 1924, Louis Armstrong joined the Fletcher Henderson dance band for a year, as featured soloist. By 1924, one of Armstrong's favorite ""Sweet Jazz"" Big bands was also formed in Canada by Guy Lombardo. His Royal Canadians Orchestra specialized in performances of ""the Sweetest music this side of Heaven"" which transcended racial boundaries.[124][125]  The original New Orleans style was polyphonic, with theme variation and simultaneous collective improvisation. Armstrong was a master of his hometown style, but by the time he joined Henderson's band, he was already a trailblazer in a new phase of jazz, with its emphasis on arrangements and soloists. Armstrong's solos went well beyond the theme-improvisation concept and extemporized on chords, rather than melodies. According to Schuller, by comparison, the solos by Armstrong's bandmates (including a young Coleman Hawkins), sounded ""stiff, stodgy"", with ""jerky rhythms and a grey undistinguished tone quality"".[126] The following example shows a short excerpt of the straight melody of ""Mandy, Make Up Your Mind"" by George W. Meyer and Arthur Johnston (top), compared with Armstrong's solo improvisations (below) (recorded 1924).[127] Armstrong's solos were a significant factor in making jazz a true 20th-century language. After leaving Henderson's group, Armstrong formed his Hot Five band, where he popularized scat singing.[128]
 The 1930s belonged to popular swing big bands, in which some virtuoso soloists became as famous as the band leaders. Key figures in developing the ""big"" jazz band included bandleaders and arrangers Count Basie, Cab Calloway, Jimmy and Tommy Dorsey, Duke Ellington, Benny Goodman, Fletcher Henderson, Earl Hines, Harry James, Jimmie Lunceford, Glenn Miller and Artie Shaw. Although it was a collective sound, swing also offered individual musicians a chance to ""solo"" and improvise melodic, thematic solos which could at times be complex ""important"" music.
 Over time, social strictures regarding racial segregation began to relax in America: white bandleaders began to recruit black musicians and black bandleaders white ones. In the mid-1930s, Benny Goodman hired pianist Teddy Wilson, vibraphonist Lionel Hampton and guitarist Charlie Christian to join small groups. In the 1930s, Kansas City Jazz as exemplified by tenor saxophonist Lester Young marked the transition from big bands to the bebop influence of the 1940s. An early 1940s style known as ""jumping the blues"" or jump blues used small combos, uptempo music and blues chord progressions, drawing on boogie-woogie from the 1930s.
 While swing was reaching the height of its popularity, Duke Ellington spent the late 1920s and 1930s in Washington, D.C's jazz scene, developing an innovative musical idiom for his orchestra. Abandoning the conventions of swing, he experimented with orchestral sounds, harmony, and musical form with complex compositions that still translated well for popular audiences; some of his tunes became hits, and his own popularity spanned from the United States to Europe.[129]
 Ellington called his music American Music, rather than jazz, and liked to describe those who impressed him as ""beyond category"".[130] These included many musicians from his orchestra, some of whom are considered among the best in jazz in their own right, but it was Ellington who melded them into one of the most popular jazz orchestras in the history of jazz. He often composed for the style and skills of these individuals, such as ""Jeep's Blues"" for Johnny Hodges, ""Concerto for Cootie"" for Cootie Williams (which later became ""Do Nothing Till You Hear from Me"" with Bob Russell's lyrics), and ""The Mooche"" for Tricky Sam Nanton and Bubber Miley. He also recorded compositions written by his bandsmen, such as Juan Tizol's ""Caravan"" and ""Perdido"", which brought the ""Spanish Tinge"" to big-band jazz. Several members of the orchestra remained with him for several decades. The band reached a creative peak in the early 1940s, when Ellington and a small hand-picked group of his composers and arrangers wrote for an orchestra of distinctive voices who displayed tremendous creativity.[131]
 As only a limited number of American jazz records were released in Europe, European jazz traces many of its roots to American artists such as James Reese Europe, Paul Whiteman, and Lonnie Johnson, who visited Europe during and after World War I. It was their live performances which inspired European audiences' interest in jazz, as well as the interest in all things American (and therefore exotic) which accompanied the economic and political woes of Europe during this time.[132] The beginnings of a distinct European style of jazz began to emerge in this interwar period.
 British jazz began with a tour by the Original Dixieland Jazz Band in 1919. In 1926, Fred Elizalde and His Cambridge Undergraduates began broadcasting on the BBC. Thereafter jazz became an important element in many leading dance orchestras, and jazz instrumentalists became numerous.[133]
 This style entered full swing in France with the Quintette du Hot Club de France, which began in 1934. Much of this French jazz was a combination of African-American jazz and the symphonic styles in which French musicians were well-trained; in this, it is easy to see the inspiration taken from Paul Whiteman since his style was also a fusion of the two.[134] Belgian guitarist Django Reinhardt popularized gypsy jazz, a mix of 1930s American swing, French dance hall ""musette"", and Eastern European folk with a languid, seductive feel; the main instruments were steel stringed guitar, violin, and double bass. Solos pass from one player to another as guitar and bass form the rhythm section. Some researchers believe Eddie Lang and Joe Venuti pioneered the guitar-violin partnership characteristic of the genre,[135] which was brought to France after they had been heard live or on Okeh Records in the late 1920s.[136]
 The outbreak of World War II marked a turning point for jazz. The swing-era jazz of the previous decade had challenged other popular music as being representative of the nation's culture, with big bands reaching the height of the style's success by the early 1940s; swing acts and big bands traveled with U.S. military overseas to Europe, where it also became popular.[137] Stateside, however, the war presented difficulties for the big-band format: conscription shortened the number of musicians available; the military's need for shellac (commonly used for pressing gramophone records) limited record production; a shortage of rubber (also due to the war effort) discouraged bands from touring via road travel; and a demand by the musicians' union for a commercial recording ban limited music distribution between 1942 and 1944.[138]
 Many of the big bands who were deprived of experienced musicians because of the war effort began to enlist young players who were below the age for conscription, as was the case with saxophonist Stan Getz's entry in a band as a teenager.[139] This coincided with a nationwide resurgence in the Dixieland style of pre-swing jazz; performers such as clarinetist George Lewis, cornetist Bill Davison, and trombonist Turk Murphy were hailed by conservative jazz critics as more authentic than the big bands.[138] Elsewhere, with the limitations on recording, small groups of young musicians developed a more uptempo, improvisational style of jazz,[137] collaborating and experimenting with new ideas for melodic development, rhythmic language, and harmonic substitution, during informal, late-night jam sessions hosted in small clubs and apartments. Key figures in this development were largely based in New York and included pianists Thelonious Monk and Bud Powell, drummers Max Roach and Kenny Clarke, saxophonist Charlie Parker, and trumpeter Dizzy Gillespie.[138] This musical development became known as bebop.[137]
 Bebop and subsequent post-war jazz developments featured a wider set of notes, played in more complex patterns and at faster tempos than previous jazz.[139] According to Clive James, bebop was ""the post-war musical development which tried to ensure that jazz would no longer be the spontaneous sound of joy ... Students of race relations in America are generally agreed that the exponents of post-war jazz were determined, with good reason, to present themselves as challenging artists rather than tame entertainers.""[140] The end of the war marked ""a revival of the spirit of experimentation and musical pluralism under which it had been conceived"", along with ""the beginning of a decline in the popularity of jazz music in America"", according to American academic Michael H. Burchett.[137]
 With the rise of bebop and the end of the swing era after the war, jazz lost its cachet as pop music. Vocalists of the famous big bands moved on to being marketed and performing as solo pop singers; these included Frank Sinatra, Peggy Lee, Dick Haymes, and Doris Day.[139] Older musicians who still performed their pre-war jazz, such as Armstrong and Ellington, were gradually viewed in the mainstream as passé. Other younger performers, such as singer Big Joe Turner and saxophonist Louis Jordan, who were discouraged by bebop's increasing complexity, pursued more lucrative endeavors in rhythm and blues, jump blues, and eventually rock and roll.[137] Some, including Gillespie, composed intricate yet danceable pieces for bebop musicians in an effort to make them more accessible, but bebop largely remained on the fringes of American audiences' purview. ""The new direction of postwar jazz drew a wealth of critical acclaim, but it steadily declined in popularity as it developed a reputation as an academic genre that was largely inaccessible to mainstream audiences"", Burchett said. ""The quest to make jazz more relevant to popular audiences, while retaining its artistic integrity, is a constant and prevalent theme in the history of postwar jazz.""[137] During its swing period, jazz had been an uncomplicated musical scene; according to Paul Trynka, this changed in the post-war years:
 Suddenly jazz was no longer straightforward. There was bebop and its variants, there was the last gasp of swing, there were strange new brews like the progressive jazz of Stan Kenton, and there was a completely new phenomenon called revivalism – the rediscovery of jazz from the past, either on old records or performed live by aging players brought out of retirement. From now on it was no good saying that you liked jazz, you had to specify what kind of jazz. And that is the way it has been ever since, only more so. Today, the word 'jazz' is virtually meaningless without further definition.[139] In the early 1940s, bebop-style performers began to shift jazz from danceable popular music toward a more challenging ""musician's music"". The most influential bebop musicians included saxophonist Charlie Parker, pianists Bud Powell and Thelonious Monk, trumpeters Dizzy Gillespie and Clifford Brown, and drummer Max Roach. Divorcing itself from dance music, bebop established itself more as an art form, thus lessening its potential popular and commercial appeal.
 Composer Gunther Schuller wrote: ""In 1943 I heard the great Earl Hines band which had Bird in it and all those other great musicians. They were playing all the flatted fifth chords and all the modern harmonies and substitutions and Dizzy Gillespie runs in the trumpet section work. Two years later I read that that was 'bop' and the beginning of modern jazz ... but the band never made recordings.""[141]
 Dizzy Gillespie wrote: ""People talk about the Hines band being 'the incubator of bop' and the leading exponents of that music ended up in the Hines band. But people also have the erroneous impression that the music was new. It was not. The music evolved from what went before. It was the same basic music. The difference was in how you got from here to here to here...naturally each age has got its own shit.""[142]
 Since bebop was meant to be listened to, not danced to, it could use faster tempos. Drumming shifted to a more elusive and explosive style, in which the ride cymbal was used to keep time while the snare and bass drum were used for accents. This led to a highly syncopated music with a linear rhythmic complexity.[143]
 Bebop musicians employed several harmonic devices which were not previously typical in jazz, engaging in a more abstracted form of chord-based improvisation. Bebop scales are traditional scales with an added chromatic passing note;[144] bebop also uses ""passing"" chords, substitute chords, and altered chords. New forms of chromaticism and dissonance were introduced into jazz, and the dissonant tritone (or ""flatted fifth"") interval became the ""most important interval of bebop""[145] Chord progressions for bebop tunes were often taken directly from popular swing-era tunes and reused with a new and more complex melody or reharmonized with more complex chord progressions to form new compositions, a practice which was already well-established in earlier jazz, but came to be central to the bebop style. Bebop made use of several relatively common chord progressions, such as blues (at base, I–IV–V, but often infused with ii–V motion) and ""rhythm changes"" (I-vi-ii-V) – the chords to the 1930s pop standard ""I Got Rhythm"". Late bop also moved towards extended forms that represented a departure from pop and show tunes.
 The harmonic development in bebop is often traced back to a moment experienced by Charlie Parker while performing ""Cherokee"" at Clark Monroe's Uptown House, New York, in early 1942. ""I'd been getting bored with the stereotyped changes that were being used...and I kept thinking there's bound to be something else. I could hear it sometimes. I couldn't play it...I was working over 'Cherokee,' and, as I did, I found that by using the higher intervals of a chord as a melody line and backing them with appropriately related changes, I could play the thing I'd been hearing. It came alive.""[146] Gerhard Kubik postulates that harmonic development in bebop sprang from blues and African-related tonal sensibilities rather than 20th-century Western classical music. ""Auditory inclinations were the African legacy in [Parker's] life, reconfirmed by the experience of the blues tonal system, a sound world at odds with the Western diatonic chord categories. Bebop musicians eliminated Western-style functional harmony in their music while retaining the strong central tonality of the blues as a basis for drawing upon various African matrices.""[146]
 Samuel Floyd states that blues was both the bedrock and propelling force of bebop, bringing about a new harmonic conception using extended chord structures that led to unprecedented harmonic and melodic variety, a developed and even more highly syncopated, linear rhythmic complexity and a melodic angularity in which the blue note of the fifth degree was established as an important melodic-harmonic device; and reestablishment of the blues as the primary organizing and functional principle.[143] Kubik wrote:
 While for an outside observer, the harmonic innovations in bebop would appear to be inspired by experiences in Western ""serious"" music, from Claude Debussy to Arnold Schoenberg, such a scheme cannot be sustained by the evidence from a cognitive approach. Claude Debussy did have some influence on jazz, for example, on Bix Beiderbecke's piano playing. And it is also true that Duke Ellington adopted and reinterpreted some harmonic devices in European contemporary music. West Coast jazz would run into such debts as would several forms of cool jazz, but bebop has hardly any such debts in the sense of direct borrowings. On the contrary, ideologically, bebop was a strong statement of rejection of any kind of eclecticism, propelled by a desire to activate something deeply buried in self. Bebop then revived tonal-harmonic ideas transmitted through the blues and reconstructed and expanded others in a basically non-Western harmonic approach. The ultimate significance of all this is that the experiments in jazz during the 1940s brought back to African-American music several structural principles and techniques rooted in African traditions.[147] These divergences from the jazz mainstream of the time met a divided, sometimes hostile response among fans and musicians, especially swing players who bristled at the new harmonic sounds. To hostile critics, bebop seemed filled with ""racing, nervous phrases"".[148] But despite the friction, by the 1950s bebop had become an accepted part of the jazz vocabulary.
 The general consensus among musicians and musicologists is that the first original jazz piece to be overtly based in clave was ""Tanga"" (1943), composed by Cuban-born Mario Bauza and recorded by Machito and his Afro-Cubans in New York City. ""Tanga"" began as a spontaneous descarga (Cuban jam session), with jazz solos superimposed on top.[149]
 This was the birth of Afro-Cuban jazz. The use of clave brought the African timeline, or key pattern, into jazz. Music organized around key patterns convey a two-celled (binary) structure, which is a complex level of African cross-rhythm.[150] Within the context of jazz, however, harmony is the primary referent, not rhythm. The harmonic progression can begin on either side of clave, and the harmonic ""one"" is always understood to be ""one"". If the progression begins on the ""three-side"" of clave, it is said to be in 3–2 clave (shown below). If the progression begins on the ""two-side"", it is in 2–3 clave.[151]
 Mario Bauzá introduced bebop innovator Dizzy Gillespie to Cuban conga drummer and composer Chano Pozo. Gillespie and Pozo's brief collaboration produced some of the most enduring Afro-Cuban jazz standards. ""Manteca"" (1947) is the first jazz standard to be rhythmically based on clave. According to Gillespie, Pozo composed the layered, contrapuntal guajeos (Afro-Cuban ostinatos) of the A section and the introduction, while Gillespie wrote the bridge. Gillespie recounted: ""If I'd let it go like [Chano] wanted it, it would have been strictly Afro-Cuban all the way. There wouldn't have been a bridge. I thought I was writing an eight-bar bridge, but ... I had to keep going and ended up writing a sixteen-bar bridge.""[152] The bridge gave ""Manteca"" a typical jazz harmonic structure, setting the piece apart from Bauza's modal ""Tanga"" of a few years earlier.
 Gillespie's collaboration with Pozo brought specific African-based rhythms into bebop. While pushing the boundaries of harmonic improvisation, cu-bop also drew from African rhythm. Jazz arrangements with a Latin A section and a swung B section, with all choruses swung during solos, became common practice with many Latin tunes of the jazz standard repertoire. This approach can be heard on pre-1980 recordings of ""Manteca"", ""A Night in Tunisia"", ""Tin Tin Deo"", and ""On Green Dolphin Street"".
 Another jazz composition critical to the development of Afro-Cuban jazz was Bud Powell's ""Un Poco Loco,"" recorded with Curley Russell on bass and Max Roach on drums. Noted for its ""frenetic energy"" and ""clanging cowbell and polyrhythmic accompaniment,""[153] the composition combined Afro-Cuban rhythm with polytonality and preceded further use of modality and avant-garde harmony in Latin jazz.[154]
 Cuban percussionist Mongo Santamaria first recorded his composition ""Afro Blue"" in 1959.[155]
""Afro Blue"" was the first jazz standard built upon a typical African three-against-two (3:2) cross-rhythm, or hemiola.[156] The piece begins with the bass repeatedly playing 6 cross-beats per each measure of 128, or 6 cross-beats per 4 main beats—6:4 (two cells of 3:2).
 The following example shows the original ostinato ""Afro Blue"" bass line. The cross noteheads indicate the main beats (not bass notes).
 When John Coltrane covered ""Afro Blue"" in 1963, he inverted the metric hierarchy, interpreting the tune as a 34 jazz waltz with duple cross-beats superimposed (2:3). Originally a B♭ pentatonic blues, Coltrane expanded the harmonic structure of ""Afro Blue"".
 Perhaps the most respected Afro-cuban jazz combo of the late 1950s was vibraphonist Cal Tjader's band. Tjader had Mongo Santamaria, Armando Peraza, and Willie Bobo on his early recording dates.
 In the late 1940s, there was a revival of Dixieland, harking back to the contrapuntal New Orleans style. This was driven in large part by record company reissues of jazz classics by the Oliver, Morton, and Armstrong bands of the 1930s. There were two types of musicians involved in the revival: the first group was made up of those who had begun their careers playing in the traditional style and were returning to it (or continuing what they had been playing all along), such as Bob Crosby's Bobcats, Max Kaminsky, Eddie Condon, and Wild Bill Davison.[157] Most of these players were originally Midwesterners, although there were a small number of New Orleans musicians involved. The second group of revivalists consisted of younger musicians, such as those in the Lu Watters band, Conrad Janis, and Ward Kimball and his Firehouse Five Plus Two Jazz Band. By the late 1940s, Louis Armstrong's Allstars band became a leading ensemble. Through the 1950s and 1960s, Dixieland was one of the most commercially popular jazz styles in the US, Europe, and Japan, although critics paid little attention to it.[157]
 Hard bop is an extension of bebop (or ""bop"") music that incorporates influences from blues, rhythm and blues, and gospel, especially in saxophone and piano playing. Hard bop was developed in the mid-1950s, coalescing in 1953 and 1954; it developed partly in response to the vogue for cool jazz in the early 1950s and paralleled the rise of rhythm and blues. It has been described as ""funky"" and can be considered a relative of soul jazz.[158] Some elements of the genre were simplified from their bebop roots.[159]
 Miles Davis' 1954 performance of ""Walkin'"" at the first Newport Jazz Festival introduced the style to the jazz world.[160] Further leaders of hard bop's development included the Clifford Brown/Max Roach Quintet, Art Blakey's Jazz Messengers, the Horace Silver Quintet, and trumpeters Lee Morgan and Freddie Hubbard. The late 1950s to early 1960s saw hard boppers form their own bands as a new generation of blues- and bebop-influenced musicians entered the jazz world, from pianists Wynton Kelly and Tommy Flanagan[161] to saxophonists Joe Henderson and Hank Mobley. Coltrane, Johnny Griffin, Mobley, and Morgan all participated on the album A Blowin' Session (1957), considered by Al Campbell to have been one of the high points of the hard bop era.[162]
 Hard bop was prevalent within jazz for about a decade spanning from 1955 to 1965,[161] but has remained highly influential on mainstream[159] or ""straight-ahead"" jazz. It went into decline in the late 1960s through the 1970s due to the emergence of other styles such as jazz fusion, but again became influential following the Young Lions Movement and the emergence of neo-bop.[159]
 Modal jazz is a development which began in the later 1950s which takes the mode, or musical scale, as the basis of musical structure and improvisation. Previously, a solo was meant to fit into a given chord progression, but with modal jazz, the soloist creates a melody using one (or a small number of) modes. The emphasis is thus shifted from harmony to melody:[163] ""Historically, this caused a seismic shift among jazz musicians, away from thinking vertically (the chord), and towards a more horizontal approach (the scale)"",[164] explained pianist Mark Levine.
 The modal theory stems from a work by George Russell. Miles Davis introduced the concept to the greater jazz world with Kind of Blue (1959), an exploration of the possibilities of modal jazz which would become the best selling jazz album of all time. In contrast to Davis' earlier work with hard bop and its complex chord progression and improvisation, Kind of Blue was composed as a series of modal sketches in which the musicians were given scales that defined the parameters of their improvisation and style.[165]
 ""I didn't write out the music for Kind of Blue, but brought in sketches for what everybody was supposed to play because I wanted a lot of spontaneity,""[166] recalled Davis. The track ""So What"" has only two chords: D-7 and E♭-7.[167]
 Other innovators in this style include Jackie McLean,[168] and two of the musicians who had also played on Kind of Blue: John Coltrane and Bill Evans.
 Free jazz, and the related form of avant-garde jazz, broke through into an open space of ""free tonality"" in which meter, beat, and formal symmetry all disappeared, and a range of world music from India, Africa, and Arabia were melded into an intense, even religiously ecstatic or orgiastic style of playing.[169] While loosely inspired by bebop, free jazz tunes gave players much more latitude; the loose harmony and tempo was deemed controversial when this approach was first developed. The bassist Charles Mingus is also frequently associated with the avant-garde in jazz, although his compositions draw from myriad styles and genres.
 The first major stirrings came in the 1950s with the early work of Ornette Coleman (whose 1960 album Free Jazz: A Collective Improvisation coined the term) and Cecil Taylor. In the 1960s, exponents included Albert Ayler, Gato Barbieri, Carla Bley, Don Cherry, Larry Coryell, John Coltrane, Bill Dixon, Jimmy Giuffre, Steve Lacy, Michael Mantler, Sun Ra, Roswell Rudd, Pharoah Sanders, and John Tchicai. In developing his late style, Coltrane was especially influenced by the dissonance of Ayler's trio with bassist Gary Peacock and drummer Sunny Murray, a rhythm section honed with Cecil Taylor as leader. In November 1961, Coltrane played a gig at the Village Vanguard, which resulted in the classic Chasin' the 'Trane, which DownBeat magazine panned as ""anti-jazz"". On his 1961 tour of France, he was booed, but persevered, signing with the new Impulse! Records in 1960 and turning it into ""the house that Trane built"", while championing many younger free jazz musicians, notably Archie Shepp, who often played with trumpeter Bill Dixon, who organized the 4-day ""October Revolution in Jazz"" in Manhattan in 1964, the first free jazz festival.
 A series of recordings with the Classic Quartet in the first half of 1965 show Coltrane's playing becoming increasingly abstract, with greater incorporation of devices like multiphonics, utilization of overtones, and playing in the altissimo register, as well as a mutated return to Coltrane's sheets of sound. In the studio, he all but abandoned his soprano to concentrate on the tenor saxophone. In addition, the quartet responded to the leader by playing with increasing freedom. The group's evolution can be traced through the recordings The John Coltrane Quartet Plays, Living Space and Transition (both June 1965), New Thing at Newport (July 1965), Sun Ship (August 1965), and First Meditations (September 1965).
 In June 1965, Coltrane and 10 other musicians recorded Ascension, a 40-minute-long piece without breaks that included adventurous solos by young avant-garde musicians as well as Coltrane, and was controversial primarily for the collective improvisation sections that separated the solos. Dave Liebman later called it ""the torch that lit the free jazz thing"". After recording with the quartet over the next few months, Coltrane invited Pharoah Sanders to join the band in September 1965. While Coltrane used over-blowing frequently as an emotional exclamation-point, Sanders would opt to overblow his entire solo, resulting in a constant screaming and screeching in the altissimo range of the instrument.
 Free jazz was played in Europe in part because musicians such as Ayler, Taylor, Steve Lacy, and Eric Dolphy spent extended periods of time there, and European musicians such as Michael Mantler and John Tchicai traveled to the U.S. to experience American music firsthand. European contemporary jazz was shaped by Peter Brötzmann, John Surman, Krzysztof Komeda, Zbigniew Namysłowski, Tomasz Stanko, Lars Gullin, Joe Harriott, Albert Mangelsdorff, Kenny Wheeler, Graham Collier, Michael Garrick and Mike Westbrook. They were eager to develop approaches to music that reflected their heritage.
 Since the 1960s, creative centers of jazz in Europe have developed, such as the creative jazz scene in Amsterdam. Following the work of drummer Han Bennink and pianist Misha Mengelberg, musicians started to explore by improvising collectively until a form (melody, rhythm, a famous song) is found Jazz critic Kevin Whitehead documented the free jazz scene in Amsterdam and some of its main exponents such as the ICP (Instant Composers Pool) orchestra in his book New Dutch Swing. Since the 1990s Keith Jarrett has defended free jazz from criticism. British writer Stuart Nicholson has argued European contemporary jazz has an identity different from American jazz and follows a different trajectory.[170]
 Latin jazz is jazz that employs Latin American rhythms and is generally understood to have a more specific meaning than simply jazz from Latin America. A more precise term might be Afro-Latin jazz, as the jazz subgenre typically employs rhythms that either have a direct analog in Africa or exhibit an African rhythmic influence beyond what is ordinarily heard in other jazz. The two main categories of Latin jazz are Afro-Cuban jazz and Brazilian jazz.
 In the 1960s and 1970s, many jazz musicians had only a basic understanding of Cuban and Brazilian music, and jazz compositions which used Cuban or Brazilian elements were often referred to as ""Latin tunes"", with no distinction between a Cuban son montuno and a Brazilian bossa nova. Even as late as 2000, in Mark Gridley's Jazz Styles: History and Analysis, a bossa nova bass line is referred to as a ""Latin bass figure"".[171] It was not uncommon during the 1960s and 1970s to hear a conga playing a Cuban tumbao while the drumset and bass played a Brazilian bossa nova pattern. Many jazz standards such as ""Manteca"", ""On Green Dolphin Street"" and ""Song for My Father"" have a ""Latin"" A section and a swung B section. Typically, the band would only play an even-eighth ""Latin"" feel in the A section of the head and swing throughout all of the solos. Latin jazz specialists like Cal Tjader tended to be the exception. For example, on a 1959 live Tjader recording of ""A Night in Tunisia"", pianist Vince Guaraldi soloed through the entire form over an authentic mambo.[172]
 For most of its history, Afro-Cuban jazz had been a matter of superimposing jazz phrasing over Cuban rhythms. But by the end of the 1970s, a new generation of New York City musicians had emerged who were fluent in both salsa dance music and jazz, leading to a new level of integration of jazz and Cuban rhythms. This era of creativity and vitality is best represented by the Gonzalez brothers Jerry (congas and trumpet) and Andy (bass).[173] During 1974–1976, they were members of one of Eddie Palmieri's most experimental salsa groups: salsa was the medium, but Palmieri was stretching the form in new ways. He incorporated parallel fourths, with McCoy Tyner-type vamps. The innovations of Palmieri, the Gonzalez brothers and others led to an Afro-Cuban jazz renaissance in New York City.
 This occurred in parallel with developments in Cuba[174] The first Cuban band of this new wave was Irakere. Their ""Chékere-son"" (1976) introduced a style of ""Cubanized"" bebop-flavored horn lines that departed from the more angular guajeo-based lines which were typical of Cuban popular music and Latin jazz up until that time. It was based on Charlie Parker's composition ""Billie's Bounce"", jumbled together in a way that fused clave and bebop horn lines.[175] In spite of the ambivalence of some band members towards Irakere's Afro-Cuban folkloric / jazz fusion, their experiments forever changed Cuban jazz: their innovations are still heard in the high level of harmonic and rhythmic complexity in Cuban jazz and in the jazzy and complex contemporary form of popular dance music known as timba.
 Brazilian jazz, such as bossa nova, is derived from samba, with influences from jazz and other 20th-century classical and popular music styles. Bossa is generally moderately paced, with melodies sung in Portuguese or English, whilst the related jazz-samba is an adaptation of street samba into jazz.
 The bossa nova style was pioneered by Brazilians João Gilberto and Antônio Carlos Jobim and was made popular by Elizete Cardoso's recording of ""Chega de Saudade"" on the Canção do Amor Demais LP. Gilberto's initial releases, and the 1959 film Black Orpheus, achieved significant popularity in Latin America; this spread to North America via visiting American jazz musicians. The resulting recordings by Charlie Byrd and Stan Getz cemented bossa nova's popularity and led to a worldwide boom, with 1963's Getz/Gilberto, numerous recordings by famous jazz performers such as Ella Fitzgerald and Frank Sinatra, and the eventual entrenchment of the bossa nova style as a lasting influence in world music.
 Brazilian percussionists such as Airto Moreira and Naná Vasconcelos also influenced jazz internationally by introducing Afro-Brazilian folkloric instruments and rhythms into a wide variety of jazz styles, thus attracting a greater audience to them.[176][177][178]
 While bossa nova has been labeled as jazz by music critics, namely those from outside of Brazil, it has been rejected by many prominent bossa nova musicians such as Jobim, who once said ""Bossa nova is not Brazilian jazz.""[179][180]
 The first jazz standard composed by a non-Latino to use an overt African 128 cross-rhythm was Wayne Shorter's ""Footprints"" (1967).[181] On the version recorded on Miles Smiles by Miles Davis, the bass switches to a 44 tresillo figure at 2:20. ""Footprints"" is not, however, a Latin jazz tune: African rhythmic structures are accessed directly by Ron Carter (bass) and Tony Williams (drums) via the rhythmic sensibilities of swing. Throughout the piece, the four beats, whether sounded or not, are maintained as the temporal referent. The following example shows the 128 and 44 forms of the bass line. The slashed noteheads indicate the main beats (not bass notes), where one ordinarily taps their foot to ""keep time"".
 The use of pentatonic scales was another trend associated with Africa. The use of pentatonic scales in Africa probably goes back thousands of years.[182]
 McCoy Tyner perfected the use of the pentatonic scale in his solos,[183] and also used parallel fifths and fourths, which are common harmonies in West Africa.[184]
 The minor pentatonic scale is often used in blues improvisation, and like a blues scale, a minor pentatonic scale can be played over all of the chords in a blues. The following pentatonic lick was played over blues changes by Joe Henderson on Horace Silver's ""African Queen"" (1965).[185]
 Jazz pianist, theorist, and educator Mark Levine refers to the scale generated by beginning on the fifth step of a pentatonic scale as the V pentatonic scale.[186]
 Levine points out that the V pentatonic scale works for all three chords of the standard II–V–I jazz progression.[187] This is a very common progression, used in pieces such as Miles Davis' ""Tune Up"". The following example shows the V pentatonic scale over a II–V–I progression.[188]
 Accordingly, John Coltrane's ""Giant Steps"" (1960), with its 26 chords per 16 bars, can be played using only three pentatonic scales. Coltrane studied Nicolas Slonimsky's Thesaurus of Scales and Melodic Patterns, which contains material that is virtually identical to portions of ""Giant Steps"".[189] The harmonic complexity of ""Giant Steps"" is on the level of the most advanced 20th-century art music. Superimposing the pentatonic scale over ""Giant Steps"" is not merely a matter of harmonic simplification, but also a sort of ""Africanizing"" of the piece, which provides an alternate approach for soloing. Mark Levine observes that when mixed in with more conventional ""playing the changes"", pentatonic scales provide ""structure and a feeling of increased space"".[190]
 As noted above, jazz has incorporated from its inception aspects of African-American sacred music including spirituals and hymns. Secular jazz musicians often performed renditions of spirituals and hymns as part of their repertoire or isolated compositions such as ""Come Sunday"", part of ""Black and Beige Suite"" by Duke Ellington. Later many other jazz artists borrowed from black gospel music. However, it was only after World War II that a few jazz musicians began to compose and perform extended works intended for religious settings or as religious expression. Since the 1950s, sacred and liturgical music has been performed and recorded by many prominent jazz composers and musicians.[191] The ""Abyssinian Mass"" by Wynton Marsalis (Blueengine Records, 2016) is a recent example.
 Relatively little has been written about sacred and liturgical jazz. In a 2013 doctoral dissertation, Angelo Versace examined the development of sacred jazz in the 1950s using disciplines of musicology and history. He noted that the traditions of black gospel music and jazz were combined in the 1950s to produce a new genre, ""sacred jazz"".[192] Versace maintained that the religious intent separates sacred from secular jazz. Most prominent in initiating the sacred jazz movement were pianist and composer Mary Lou Williams, known for her jazz masses in the 1950s and Duke Ellington. Prior to his death in 1974 in response to contacts from Grace Cathedral in San Francisco, Duke Ellington wrote three Sacred Concerts: 1965 – A Concert of Sacred Music; 1968 – Second Sacred Concert; 1973 – Third Sacred Concert.
 The most prominent form of sacred and liturgical jazz is the jazz mass. Although most often performed in a concert setting rather than church worship setting, this form has many examples. An eminent example of composers of the jazz mass was Mary Lou Williams. Williams converted to Catholicism in 1957, and proceeded to compose three masses in the jazz idiom.[193] One was composed in 1968 to honor the recently assassinated Martin Luther King Jr. and the third was commissioned by a pontifical commission. It was performed once in 1975 in St Patrick's Cathedral in New York City. However the Catholic Church has not embraced jazz as appropriate for worship. In 1966 Joe Masters recorded ""Jazz Mass"" for Columbia Records. A jazz ensemble was joined by soloists and choir using the English text of the Roman Catholic Mass.[194] Other examples include ""Jazz Mass in Concert"" by Lalo Schiffrin (Aleph Records, 1998, UPC 0651702632725) and ""Jazz Mass"" by Vince Guaraldi (Fantasy Records, 1965). In England, classical composer Will Todd recorded his ""Jazz Missa Brevis"" with a jazz ensemble, soloists and the St Martin's Voices on a 2018 Signum Records release, ""Passion Music/Jazz Missa Brevis"" also released as ""Mass in Blue"", and jazz organist James Taylor composed ""The Rochester Mass"" (Cherry Red Records, 2015).[195] In 2013, Versace put forth bassist Ike Sturm and New York composer Deanna Witkowski as contemporary exemplars of sacred and liturgical jazz.[192]
 In the late 1960s and early 1970s, the hybrid form of jazz-rock fusion was developed by combining jazz improvisation with rock rhythms, electric instruments and the highly amplified stage sound of rock musicians such as Jimi Hendrix and Frank Zappa. Jazz fusion often uses mixed meters, odd time signatures, syncopation, complex chords, and harmonies.
 According to AllMusic:
 ... until around 1967, the worlds of jazz and rock were nearly completely separate. [However, ...] as rock became more creative and its musicianship improved, and as some in the jazz world became bored with hard bop and did not want to play strictly avant-garde music, the two different idioms began to trade ideas and occasionally combine forces.[196] In 1969, Davis fully embraced the electric instrument approach to jazz with In a Silent Way, which can be considered his first fusion album. Composed of two side-long suites edited heavily by producer Teo Macero, this quiet, static album would be equally influential to the development of ambient music.
 As Davis recalls:
 The music I was really listening to in 1968 was James Brown, the great guitar player Jimi Hendrix, and a new group who had just come out with a hit record, ""Dance to the Music"", Sly and the Family Stone ... I wanted to make it more like rock. When we recorded In a Silent Way I just threw out all the chord sheets and told everyone to play off of that.[197] Two contributors to In a Silent Way also joined organist Larry Young to create one of the early acclaimed fusion albums: Emergency! (1969) by The Tony Williams Lifetime.
 Weather Report's self-titled electronic and psychedelic Weather Report debut album caused a sensation in the jazz world on its arrival in 1971, thanks to the pedigree of the group's members (including percussionist Airto Moreira), and their unorthodox approach to music. The album featured a softer sound than would be the case in later years (predominantly using acoustic bass with Shorter exclusively playing soprano saxophone, and with no synthesizers involved), but is still considered a classic of early fusion. It built on the avant-garde experiments which Joe Zawinul and Shorter had pioneered with Miles Davis on Bitches Brew, including an avoidance of head-and-chorus composition in favor of continuous rhythm and movement – but took the music further. To emphasize the group's rejection of standard methodology, the album opened with the inscrutable avant-garde atmospheric piece ""Milky Way"", which featured by Shorter's extremely muted saxophone inducing vibrations in Zawinul's piano strings while the latter pedaled the instrument. DownBeat described the album as ""music beyond category"", and awarded it Album of the Year in the magazine's polls that year.
 Weather Report's subsequent releases were creative funk-jazz works.[198]
 Although some jazz purists protested against the blend of jazz and rock, many jazz innovators crossed over from the contemporary hard bop scene into fusion. As well as the electric instruments of rock (such as electric guitar, electric bass, electric piano and synthesizer keyboards), fusion also used the powerful amplification, ""fuzz"" pedals, wah-wah pedals and other effects that were used by 1970s-era rock bands. Notable performers of jazz fusion included Miles Davis, Eddie Harris, keyboardists Joe Zawinul, Chick Corea, and Herbie Hancock, vibraphonist Gary Burton, drummer Tony Williams, violinist Jean-Luc Ponty, guitarists Larry Coryell, Al Di Meola, John McLaughlin, Ryo Kawasaki, and Frank Zappa, saxophonist Wayne Shorter and bassists Jaco Pastorius and Stanley Clarke. Jazz fusion was also popular in Japan, where the band Casiopea released more than thirty fusion albums.
 According to jazz writer Stuart Nicholson, ""just as free jazz appeared on the verge of creating a whole new musical language in the 1960s ... jazz-rock briefly suggested the promise of doing the same"" with albums such as Williams' Emergency! (1970) and Davis' Agharta (1975), which Nicholson said ""suggested the potential of evolving into something that might eventually define itself as a wholly independent genre quite apart from the sound and conventions of anything that had gone before."" This development was stifled by commercialism, Nicholson said, as the genre ""mutated into a peculiar species of jazz-inflected pop music that eventually took up residence on FM radio"" at the end of the 1970s.[199]
 Although jazz-rock fusion reached the height of its popularity in the 1970s, the use of electronic instruments and rock-derived musical elements in jazz continued in the 1990s and 2000s. Musicians using this approach include Pat Metheny, John Abercrombie, John Scofield and the Swedish group e.s.t. Since the beginning of the 1990s, electronic music had significant technical improvements that popularized and created new possibilities for the genre. Jazz elements such as improvisation, rhythmic complexities and harmonic textures were introduced to the genre and consequently had a big impact in new listeners and in some ways kept the versatility of jazz relatable to a newer generation that did not necessarily relate to what the traditionalists call real jazz (bebop, cool and modal jazz).[200] Artists such as Squarepusher, Aphex Twin, Flying Lotus and sub genres like IDM, drum 'n' bass, jungle and techno ended up incorporating a lot of these elements.[201] Squarepusher being cited as one big influence for jazz performers drummer Mark Guiliana and pianist Brad Mehldau, showing the correlations between jazz and electronic music are a two-way street.[202]
 By the mid-1970s, the sound known as jazz-funk had developed, characterized by a strong back beat (groove), electrified sounds[203] and, often, the presence of electronic analog synthesizers. Jazz-funk also draws influences from traditional African music, Afro-Cuban rhythms and Jamaican reggae, notably Kingston bandleader Sonny Bradshaw. Another feature is the shift of emphasis from improvisation to composition: arrangements, melody and overall writing became important. The integration of funk, soul, and R&B music into jazz resulted in the creation of a genre whose spectrum is wide and ranges from strong jazz improvisation to soul, funk or disco with jazz arrangements, jazz riffs and jazz solos, and sometimes soul vocals.[204]
 Early examples are Herbie Hancock's Headhunters band and Miles Davis' On the Corner album, which, in 1972, began Davis' foray into jazz-funk and was, he claimed, an attempt at reconnecting with the young black audience which had largely forsaken jazz for rock and funk. While there is a discernible rock and funk influence in the timbres of the instruments employed, other tonal and rhythmic textures, such as the Indian tambora and tablas and Cuban congas and bongos, create a multi-layered soundscape. The album was a culmination of sorts of the musique concrète approach that Davis and producer Teo Macero had begun to explore in the late 1960s.
 The 1980s saw something of a reaction against the fusion and free jazz that had dominated the 1970s. Trumpeter Wynton Marsalis emerged early in the decade, and strove to create music within what he believed was the tradition, rejecting both fusion and free jazz and creating extensions of the small and large forms initially pioneered by artists such as Louis Armstrong and Duke Ellington, as well as the hard bop of the 1950s. It is debatable whether Marsalis' critical and commercial success was a cause or a symptom of the reaction against Fusion and Free Jazz and the resurgence of interest in the kind of jazz pioneered in the 1960s (particularly modal jazz and post-bop); nonetheless there were many other manifestations of a resurgence of traditionalism, even if fusion and free jazz were by no means abandoned and continued to develop and evolve.
 For example, several musicians who had been prominent in the fusion genre during the 1970s began to record acoustic jazz once more, including Chick Corea and Herbie Hancock. Other musicians who had experimented with electronic instruments in the previous decade had abandoned them by the 1980s; for example, Bill Evans, Joe Henderson, and Stan Getz. Even the 1980s music of Miles Davis, although certainly still fusion, adopted a far more accessible and recognizably jazz-oriented approach than his abstract work of the mid-1970s, such as a return to a theme-and-solos approach.
 
A similar reaction[vague] took place against free jazz. According to Ted Gioia: the very leaders of the avant garde started to signal a retreat from the core principles of free jazz. Anthony Braxton began recording standards over familiar chord changes. Cecil Taylor played duets in concert with Mary Lou Williams, and let her set out structured harmonies and familiar jazz vocabulary under his blistering keyboard attack. And the next generation of progressive players would be even more accommodating, moving inside and outside the changes without thinking twice. Musicians such as David Murray or Don Pullen may have felt the call of free-form jazz, but they never forgot all the other ways one could play African-American music for fun and profit.[205] Pianist Keith Jarrett—whose bands of the 1970s had played only original compositions with prominent free jazz elements—established his so-called 'Standards Trio' in 1983, which, although also occasionally exploring collective improvisation, has primarily performed and recorded jazz standards. Chick Corea similarly began exploring jazz standards in the 1980s, having neglected them for the 1970s.
 In 1987, the United States House of Representatives and Senate passed a bill proposed by Democratic Representative John Conyers Jr. to define jazz as a unique form of American music, stating ""jazz is hereby designated as a rare and valuable national American treasure to which we should devote our attention, support and resources to make certain it is preserved, understood and promulgated."" It passed in the House on September 23, 1987, and in the Senate on November 4, 1987.[206]
 In 2001, Ken Burns's documentary Jazz premiered on PBS, featuring Wynton Marsalis and other experts reviewing the entire history of American jazz to that time. It received some criticism, however, for its failure to reflect the many distinctive non-American traditions and styles in jazz that had developed, and its limited representation of US developments in the last quarter of the 20th century.
 The emergence of young jazz talent beginning to perform in older, established musicians' groups further impacted the resurgence of traditionalism in the jazz community. In the 1970s, the groups of Betty Carter and Art Blakey and the Jazz Messengers retained their conservative jazz approaches in the midst of fusion and jazz-rock, and in addition to difficulty booking their acts, struggled to find younger generations of personnel to authentically play traditional styles such as hard bop and bebop. In the late 1970s, however, a resurgence of younger jazz players in Blakey's band began to occur. This movement included musicians such as Valery Ponomarev and Bobby Watson, Dennis Irwin and James Williams. In the 1980s, in addition to Wynton and Branford Marsalis, the emergence of pianists in the Jazz Messengers such as Donald Brown, Mulgrew Miller, and later, Benny Green, bassists such as Charles Fambrough, Lonnie Plaxico (and later, Peter Washington and Essiet Essiet) horn players such as Bill Pierce, Donald Harrison and later Javon Jackson and Terence Blanchard emerged as talented jazz musicians, all of whom made significant contributions in the 1990s and 2000s.
 The young Jazz Messengers' contemporaries, including Roy Hargrove, Marcus Roberts, Wallace Roney and Mark Whitfield were also influenced by Wynton Marsalis's emphasis toward jazz tradition. These younger rising stars rejected avant-garde approaches and instead championed the acoustic jazz sound of Charlie Parker, Thelonious Monk and early recordings of the first Miles Davis quintet. This group of ""Young Lions"" sought to reaffirm jazz as a high art tradition comparable to the discipline of classical music.[207]
 In addition, Betty Carter's rotation of young musicians in her group foreshadowed many of New York's preeminent traditional jazz players later in their careers. Among these musicians were Jazz Messenger alumni Benny Green, Branford Marsalis and Ralph Peterson Jr., as well as Kenny Washington, Lewis Nash, Curtis Lundy, Cyrus Chestnut, Mark Shim, Craig Handy, Greg Hutchinson and Marc Cary, Taurus Mateen and Geri Allen. O.T.B. ensemble included a rotation of young jazz musicians such as Kenny Garrett, Steve Wilson, Kenny Davis, Renee Rosnes, Ralph Peterson Jr., Billy Drummond, and Robert Hurst.[208]
 Starting in the 1990s, a number of players from largely straight-ahead or post-bop backgrounds emerged as a result of the rise of neo-traditionalist jazz, including pianists Jason Moran and Vijay Iyer, guitarist Kurt Rosenwinkel, vibraphonist Stefon Harris, trumpeters Roy Hargrove and Terence Blanchard, saxophonists Chris Potter and Joshua Redman, clarinetist Ken Peplowski and bassist Christian McBride.
 In the early 1980s, a commercial form of jazz fusion called ""pop fusion"" or ""smooth jazz"" became successful, garnering significant radio airplay in ""quiet storm"" time slots at radio stations in urban markets across the U.S. This helped to establish or bolster the careers of vocalists including Al Jarreau, Anita Baker, Chaka Khan, and Sade, as well as saxophonists including Grover Washington Jr., Kenny G, Kirk Whalum, Boney James, and David Sanborn. In general, smooth jazz is downtempo (the most widely played tracks are of 90–105 beats per minute), and has a lead melody-playing instrument (saxophone, especially soprano and tenor, and legato electric guitar are popular).
 In his Newsweek article ""The Problem With Jazz Criticism"",[209] Stanley Crouch considers Miles Davis' playing of fusion to be a turning point that led to smooth jazz. Critic Aaron J. West has countered the often negative perceptions of smooth jazz, stating:
 I challenge the prevalent marginalization and malignment of smooth jazz in the standard jazz narrative. Furthermore, I question the assumption that smooth jazz is an unfortunate and unwelcomed evolutionary outcome of the jazz-fusion era. Instead, I argue that smooth jazz is a long-lived musical style that merits multi-disciplinary analyses of its origins, critical dialogues, performance practice, and reception.[210] Acid jazz developed in the UK in the 1980s and 1990s, influenced by jazz-funk and electronic dance music. Acid jazz often contains various types of electronic composition (sometimes including sampling or live DJ cutting and scratching), but it is just as likely to be played live by musicians, who often showcase jazz interpretation as part of their performance. Richard S. Ginell of AllMusic considers Roy Ayers ""one of the prophets of acid jazz"".[211]
 Nu jazz is influenced by jazz harmony and melodies, and there are usually no improvisational aspects. It can be very experimental in nature and can vary widely in sound and concept. It ranges from the combination of live instrumentation with the beats of jazz house (as exemplified by St Germain, Jazzanova, and Fila Brazillia) to more band-based improvised jazz with electronic elements (for example, The Cinematic Orchestra, Kobol and the Norwegian ""future jazz"" style pioneered by Bugge Wesseltoft, Jaga Jazzist, and Nils Petter Molvær).
 Jazz rap developed in the late 1980s and early 1990s and incorporates jazz influences into hip hop. In 1988, Gang Starr released the debut single ""Words I Manifest"", which sampled Dizzy Gillespie's 1962 ""Night in Tunisia"", and Stetsasonic released ""Talkin' All That Jazz"", which sampled Lonnie Liston Smith. Gang Starr's debut LP No More Mr. Nice Guy (1989) and their 1990 track ""Jazz Thing"" sampled Charlie Parker and Ramsey Lewis. The groups which made up the Native Tongues Posse tended toward jazzy releases: these include the Jungle Brothers' debut Straight Out the Jungle (1988), and A Tribe Called Quest's People's Instinctive Travels and the Paths of Rhythm (1990) and The Low End Theory (1991). Rap duo Pete Rock & CL Smooth incorporated jazz influences on their 1992 debut Mecca and the Soul Brother. Rapper Guru's Jazzmatazz series began in 1993 using jazz musicians during the studio recordings.
 Although jazz rap had achieved little mainstream success, Miles Davis' final album Doo-Bop (released posthumously in 1992) was based on hip hop beats and collaborations with producer Easy Mo Bee. Davis' ex-bandmate Herbie Hancock also absorbed hip-hop influences in the mid-1990s, releasing the album Dis Is Da Drum in 1994.
 The mid-2010s saw an increased influence of R&B, hip-hop, and pop music on jazz. In 2015, Kendrick Lamar released his third studio album, To Pimp a Butterfly. The album heavily featured prominent contemporary jazz artists such as Thundercat[212] and redefined jazz rap with a larger focus on improvisation and live soloing rather than simply sampling. In that same year, saxophonist Kamasi Washington released his nearly three-hour long debut, The Epic. Its hip-hop inspired beats and R&B vocal interludes was not only acclaimed by critics for being innovative in keeping jazz relevant,[213] but also sparked a small resurgence in jazz on the internet.
 The relaxation of orthodoxy which was concurrent with post-punk in London and New York City led to a new appreciation of jazz. In London, the Pop Group began to mix free jazz and dub reggae into their brand of punk rock.[214] In New York, No Wave took direct inspiration from both free jazz and punk. Examples of this style include Lydia Lunch's Queen of Siam,[215] Gray, the work of James Chance and the Contortions (who mixed Soul with free jazz and punk)[215] and the Lounge Lizards[215] (the first group to call themselves ""punk jazz"").
 John Zorn took note of the emphasis on speed and dissonance that was becoming prevalent in punk rock, and incorporated this into free jazz with the release of the Spy vs. Spy album in 1986, a collection of Ornette Coleman tunes done in the contemporary thrashcore style.[216] In the same year, Sonny Sharrock, Peter Brötzmann, Bill Laswell, and Ronald Shannon Jackson recorded the first album under the name Last Exit, a similarly aggressive blend of thrash and free jazz.[217] These developments are the origins of jazzcore, the fusion of free jazz with hardcore punk.
 The M-Base movement started in the 1980s, when a loose collective of young African-American musicians in New York which included Steve Coleman, Greg Osby, and Gary Thomas developed a complex but grooving[218] sound.
 In the 1990s, most M-Base participants turned to more conventional music, but Coleman, the most active participant, continued developing his music in accordance with the M-Base concept.[219]
 Coleman's audience decreased, but his music and concepts influenced many musicians, according to pianist Vijay Iver and critic Ben Ratlifff of The New York Times.[220][221]
 M-Base changed from a movement of a loose collective of young musicians to a kind of informal Coleman ""school"",[222] with a much advanced but already originally implied concept.[223] Steve Coleman's music and M-Base concept gained recognition as ""next logical step"" after Charlie Parker, John Coltrane, and Ornette Coleman.[224]
 Since the 1990s, jazz has been characterized by a pluralism in which no one style dominates, but rather a wide range of styles and genres are popular. Individual performers often play in a variety of styles, sometimes in the same performance. Pianist Brad Mehldau and The Bad Plus have explored contemporary rock music within the context of the traditional jazz acoustic piano trio, recording instrumental jazz versions of songs by rock musicians. The Bad Plus have also incorporated elements of free jazz into their music. A firm avant-garde or free jazz stance has been maintained by some players, such as saxophonists Greg Osby and Charles Gayle, while others, such as James Carter, have incorporated free jazz elements into a more traditional framework.
 Harry Connick Jr. began his career playing stride piano and the Dixieland jazz of his home, New Orleans, beginning with his first recording when he was 10 years old.[225] Some of his earliest lessons were at the home of pianist Ellis Marsalis.[226] Connick had success on the pop charts after recording the soundtrack to the movie When Harry Met Sally, which sold over two million copies.[225] Crossover success has also been achieved by Diana Krall, Norah Jones, Cassandra Wilson, Kurt Elling, and Jamie Cullum.
 Additionally, the era saw the release of recordings and videos from the previous century, such as a Just Jazz tape broadcast by a band led by Gene Ammons[227] and studio archives such as Just Coolin' by Art Blakey and the Jazz Messengers.[228]
 An internet-aided trend of 2010's jazz was that of extreme reharmonization, inspired by both virtuosic players known for their speed and rhythm such as Art Tatum, as well as players known for their ambitious voicings and chords such as Bill Evans. Supergroup Snarky Puppy adopted this trend, allowing players like Cory Henry[229] to shape the grooves and harmonies of modern jazz soloing. YouTube phenomenon Jacob Collier also gained recognition for his ability to play an incredibly large number of instruments and his ability to use microtones, advanced polyrhythms, and blend a spectrum of genres in his largely homemade production process.[230][231]
 Other jazz musicians gained popularity through social media during the 2010s and 2020s. These included Joan Chamorro, a bassist and bandleader based in Barcelona whose big band and jazz combo videos have received tens of millions of views on YouTube,[232] and Emmet Cohen, who broadcast a series of performances live from New York starting in March 2020.[233]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['musicology and history', 'Eddie Lang and Joe Venuti', 'The quest to make jazz more relevant to popular audiences', 'African rhythmic rituals', 'unorthodox'], 'answer_start': [], 'answer_end': []}"
"
 
 Rock is a broad genre of popular music that originated as ""rock and roll"" in the United States in the late 1940s and early 1950s, developing into a range of different styles from the mid-1960s, particularly in the United States and the United Kingdom.[3] It has its roots in 1940s and 1950s rock and roll, a style that drew directly from the blues and rhythm and blues genres of African-American music and from country music. Rock also drew strongly from genres such as electric blues and folk, and incorporated influences from jazz and other musical styles. For instrumentation, rock has centered on the electric guitar, usually as part of a rock group with electric bass guitar, drums, and one or more singers. Usually, rock is song-based music with a 44 time signature using a verse–chorus form, but the genre has become extremely diverse. Like pop music, lyrics often stress romantic love but also address a wide variety of other themes that are frequently social or political. Rock was the most popular genre of music in the U.S. and much of the Western world from the 1950s to the 2010s.
 Rock musicians in the mid-1960s began to advance the album ahead of the single as the dominant form of recorded music expression and consumption, with the Beatles at the forefront of this development. Their contributions lent the genre a cultural legitimacy in the mainstream and initiated a rock-informed album era in the music industry for the next several decades. By the late 1960s ""classic rock""[3] period, a number of distinct rock music subgenres had emerged, including hybrids like blues rock, folk rock, country rock, southern rock, raga rock, and jazz rock, which contributed to the development of psychedelic rock, influenced by the countercultural psychedelic and hippie scene. New genres that emerged included progressive rock, which extended artistic elements, and glam rock, which highlighted showmanship and visual style. In the second half of the 1970s, punk rock reacted by producing stripped-down, energetic social and political critiques. Punk was an influence in the 1980s on new wave, post-punk and eventually alternative rock.
 From the 1990s, alternative rock began to dominate rock music and break into the mainstream in the form of grunge, Britpop, and indie rock. Further fusion subgenres have since emerged, including pop-punk, electronic rock, rap rock, and rap metal. Some movements were conscious attempts to revisit rock's history, including the garage rock/post-punk revival in the 2000s. Since the 2010s, rock has lost its position as the pre-eminent popular music genre in world culture, but remains commercially successful. The increased influence of hip-hop and electronic dance music can be seen in rock music, notably in the techno-pop scene of the early 2010s and the pop-punk-hip-hop revival of the 2020s.
 
Rock has also embodied and served as the vehicle for cultural and social movements, leading to major subcultures including mods and rockers in the U.K., the hippie movement and the wider Western counterculture movement that spread out from San Francisco in the U.S. in the 1960s, the latter of which continues to this day. Similarly, 1970s punk culture spawned the goth, punk, and emo subcultures. Inheriting the folk tradition of the protest song, rock music has been associated with political activism, as well as changes in social attitudes to race, sex, and drug use, and is often seen as an expression of youth revolt against adult consumerism and conformity. At the same time, it has been commercially highly successful, leading to accusations of selling out. A good definition of rock, in fact, is that it's popular music that to a certain degree doesn't care if it's popular.
 —Bill Wyman in Vulture (2016)[4] The sound of rock is traditionally centered on the amplified electric guitar, which emerged in its modern form in the 1950s with the popularity of rock and roll.[5] It was also greatly influenced by the sounds of electric blues guitarists.[6] The sound of an electric guitar in rock music is typically supported by an electric bass guitar, which pioneered in jazz music in the same era,[7] and by percussion produced from a drum kit that combines drums and cymbals.[8] This trio of instruments has often been complemented by the inclusion of other instruments, particularly keyboards such as the piano, the Hammond organ, and the synthesizer.[9] The basic rock instrumentation was derived from the basic blues band instrumentation (prominent lead guitar, second chordal instrument, bass, and drums).[6] A group of musicians performing rock music is termed as a rock band or a rock group. Furthermore, it typically consists of between three (the power trio) and five members. Classically, a rock band takes the form of a quartet whose members cover one or more roles, including vocalist, lead guitarist, rhythm guitarist, bass guitarist, drummer, and often keyboard player or other instrumentalist.[10]
 Rock music is traditionally built on a foundation of simple syncopated rhythms in a 44 meter, with a repetitive snare drum back beat on beats two and four.[11] Melodies often originate from older musical modes such as the Dorian and Mixolydian, as well as major and minor modes. Harmonies range from the common triad to parallel perfect fourths and fifths and dissonant harmonic progressions.[11] Since the late 1950s,[12] and particularly from the mid-1960s onwards, rock music often used the verse–chorus structure derived from blues and folk music, but there has been considerable variation from this model.[13] Critics have stressed the eclecticism and stylistic diversity of rock.[14] Because of its complex history and its tendency to borrow from other musical and cultural forms, it has been argued that ""it is impossible to bind rock music to a rigidly delineated musical definition.""[15] In the opinion of music journalist Robert Christgau, ""the best rock jolts folk-art virtues—directness, utility, natural audience—into the present with shots of modern technology and modernist dissociation"".[16]
 Rock and roll was conceived as an outlet for adolescent yearnings ... To make rock and roll is also an ideal way to explore intersections of sex, love, violence, and fun, to broadcast the delights and limitations of the regional, and to deal with the depredations and benefits of mass culture itself.
 —Robert Christgau in Christgau's Record Guide (1981)[17] Unlike many earlier styles of popular music, rock lyrics have dealt with a wide range of themes, including romantic love, sex, rebellion against ""The Establishment"", social concerns, and life styles.[11] These themes were inherited from a variety of sources such as the Tin Pan Alley pop tradition, folk music, and rhythm and blues.[18] Christgau characterizes rock lyrics as a ""cool medium"" with simple diction and repeated refrains, and asserts that rock's primary ""function"" ""pertains to music, or, more generally, noise.""[19] The predominance of white, male, and often middle class musicians in rock music has often been noted,[20] and rock has been seen as an appropriation of Black musical forms for a young, white and largely male audience.[21] As a result, it has also been seen to articulate the concerns of this group in both style and lyrics.[22] Christgau, writing in 1972, said in spite of some exceptions, ""rock and roll usually implies an identification of male sexuality and aggression"".[23]
 Since the term ""rock"" started being used in preference to ""rock and roll"" from the late-1960s, it has usually been contrasted with pop music, with which it has shared many characteristics, but from which it is often distanced by an emphasis on musicianship, live performance, and a focus on serious and progressive themes as part of an ideology of authenticity that is frequently combined with an awareness of the genre's history and development.[24] According to Simon Frith, rock was ""something more than pop, something more than rock and roll"" and ""[r]ock musicians combined an emphasis on skill and technique with the romantic concept of art as artistic expression, original and sincere"".[24]
 In the new millennium, the term rock has occasionally been used as a blanket term including forms like pop music, reggae music, soul music, and even hip hop, which it has been influenced with but often contrasted through much of its history.[25] Christgau has used the term broadly to refer to popular and semipopular music that caters to his sensibility as ""a rock-and-roller"", including a fondness for a good beat, a meaningful lyric with some wit, and the theme of youth, which holds an ""eternal attraction"" so objective ""that all youth music partakes of sociology and the field report."" Writing in Christgau's Record Guide: The '80s (1990), he said this sensibility is evident in the music of folk singer-songwriter Michelle Shocked, rapper LL Cool J, and synth-pop duo Pet Shop Boys—""all kids working out their identities""—as much as it is in the music of Chuck Berry, the Ramones, and the Replacements.[26]
 The foundations of rock music are in rock and roll, which originated in the United States during the late 1940s and early 1950s, and quickly spread to much of the rest of the world. Its immediate origins lay in a melding of various black musical genres of the time, including rhythm and blues and gospel music, with country and western.[27]
 Debate surrounds the many recordings which have been suggested as ""the first rock and roll record"". Contenders include ""Strange Things Happening Every Day"" by Sister Rosetta Tharpe (1944);[28] ""That's All Right"" by Arthur Crudup (1946);[29] ""The House of Blue Lights"" by Ella Mae Morse and Freddie Slack (1946);[30] Wynonie Harris' ""Good Rocking Tonight"" (1948);[31] Goree Carter's ""Rock Awhile"" (1949);[32] Jimmy Preston's ""Rock the Joint"" (1949), which was later covered by Bill Haley & His Comets in 1952;[33] and ""Rocket 88"" by Jackie Brenston and his Delta Cats (in fact, Ike Turner and his band the Kings of Rhythm), recorded by Sam Phillips for Chess Records in 1951.[34]
 In 1951, Cleveland, Ohio disc jockey Alan Freed began playing rhythm and blues music (then termed ""race music"") for a multi-racial audience, and is credited with first using the phrase ""rock and roll"" to describe the music.[35] Four years later, Bill Haley's ""Rock Around the Clock"" (1954) became the first rock and roll song to top Billboard magazine's main sales and airplay charts, and opened the door worldwide for this new wave of popular culture.[36][37] Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Fats Domino, Little Richard, Jerry Lee Lewis, and Gene Vincent.[34] Soon rock and roll was the major force in American record sales and crooners, such as Eddie Fisher, Perry Como, and Patti Page, who had dominated the previous decade of popular music, found their access to the pop charts significantly curtailed.[38]
 Rock and roll has been seen as leading to a number of distinct subgenres, including rockabilly, combining rock and roll with ""hillbilly"" country music, which was usually played and recorded in the mid-1950s by white singers such as Carl Perkins, Jerry Lee Lewis, Buddy Holly and with the greatest commercial success, Elvis Presley.[39] Hispanic and Latino American movements in rock and roll, which would eventually lead to the success of Latin rock and Chicano rock within the US, began to rise in the Southwest; with rock and roll standard musician Ritchie Valens and even those within other heritage genres, such as Al Hurricane along with his brothers Tiny Morrie and Baby Gaby as they began combining rock and roll with country-western within traditional New Mexico music.[40] In addition, the 1950s saw the growth in popularity of the electric guitar, and the development of a specifically rock and roll style of playing through such exponents as Chuck Berry, Link Wray, and Scotty Moore.[41] The use of distortion, pioneered by Western swing guitarists such as Junior Barnard[42] and Eldon Shamblin was popularized by Chuck Berry in the mid-1950s.[43] The use of power chords, pioneered by Francisco Tárrega and Heitor Villa-Lobos in the 19th century and later on by Willie Johnson and Pat Hare in the early 1950s, was popularized by Link Wray in the late 1950s.[44]
 Commentators have traditionally perceived a decline of rock and roll in the late 1950s and early 1960s. By 1959, the death of Buddy Holly, the Big Bopper and Ritchie Valens in a plane crash, the departure of Elvis for the army, the retirement of Little Richard to become a preacher, prosecutions of Jerry Lee Lewis and Chuck Berry and the breaking of the payola scandal (which implicated major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs), gave a sense that the rock and roll era established at that point had come to an end.[45]
 Rock quickly spread out from its origins in the US, associated with the rapid Americanization that was taking place globally in the aftermath of the Second world war.[46] Cliff Richard is credited with one of the first rock and roll hits outside of North America with ""Move It"" (1959), effectively ushering in the sound of British rock.[47] Several artists, most prominently Tommy Steele from the UK, found success with covers of major American rock and roll hits before the recordings could spread internationally, often translating them into local languages where appropriate.[48][49] Steele in particular toured Britain, Scandinavia, Australia, the USSR and South Africa from 1955 to 1957, influencing the globalisation of rock.[48] Johnny O'Keefe's 1958 record ""Wild One"" was one of the earliest Australian rock and roll hits.[50] By the late 1950s, as well as in the American-influenced Western world, rock was popular in communist states such as Yugoslavia,[51] and the USSR,[52] as well as in regions such as South America.[49]
 In the late 1950s and early 1960s, American blues music and blues rock artists, who had been surpassed by the rise of rock and roll in the US, found new popularity in the UK, visiting with successful tours.[53] Lonnie Donegan's 1955 hit ""Rock Island Line"" was a major influence and helped to develop the trend of skiffle music groups throughout the country, many of which, including John Lennon's Quarrymen (later the Beatles), moved on to play rock and roll.[54] While former rock and roll market in the US was becoming dominated by lightweight pop and ballads, British rock groups at clubs and local dances were developing a style more strongly influenced by blues-rock pioneers, and were starting to play with an intensity and drive seldom found in white American acts;[55] this influence would go on to shape the future of rock music through the British Invasion.[53]
 The term pop has been used since the early 20th century to refer to popular music in general, but from the mid-1950s it began to be used for a distinct genre, aimed at a youth market, often characterized as a softer alternative to rock and roll.[56][57] From about 1967, it was increasingly used in opposition to the term rock music, to describe a form that was more commercial, ephemeral and accessible.[24] In contrast rock music was seen as focusing on extended works, particularly albums, was often associated with particular sub-cultures (like the counterculture of the 1960s), placed an emphasis on artistic values and ""authenticity"", stressed live performance and instrumental or vocal virtuosity and was often seen as encapsulating progressive developments rather than simply reflecting existing trends.[24][56][57][58] Nevertheless, much pop and rock music has been very similar in sound, instrumentation and even lyrical content.[nb 1]
 The period of the later 1950s and early 1960s has traditionally been seen as an era of hiatus for rock and roll.[62] More recently some authors[weasel words] have emphasised important innovations and trends in this period without which future developments would not have been possible.[63][64] While early rock and roll, particularly through the advent of rockabilly, saw the greatest commercial success for male and white performers, in this era, the genre was dominated by black and female artists. Rock and roll had not disappeared entirely from music at the end of the 1950s and some of its energy can be seen in the various dance crazes of the early 1960s, started by Chubby Checker's record ""The Twist"" (1960).[64][nb 2] Some music historians have also pointed to important and innovative technical developments that built on rock and roll in this period, including the electronic treatment of sound by such innovators as Joe Meek, and the elaborate production methods of the Wall of Sound pursued by Phil Spector.[64]
 The instrumental rock and roll of performers such as Duane Eddy, Link Wray and the Ventures was further developed by Dick Dale, who added distinctive ""wet"" reverb, rapid alternate picking, and Middle Eastern and Mexican influences. He produced the regional hit ""Let's Go Trippin'"" in 1961 and launched the surf music craze, following up with songs like ""Misirlou"" (1962).[68] Like Dale and his Del-Tones, most early surf bands were formed in Southern California, including the Bel-Airs, the Challengers, and Eddie & the Showmen.[68] The Chantays scored a top ten national hit with ""Pipeline"" in 1963 and probably the best-known surf tune was 1963's ""Wipe Out"", by the Surfaris, which hit number 2 and number 10 on the Billboard charts in 1965.[69] Surf rock was also popular in Europe during this time, with the British group the Shadows scoring hits in the early 1960s with instrumentals such as ""Apache"" and ""Kon-Tiki"", while Swedish surf group the Spotnicks saw success in both Sweden and Britain.
 Surf music achieved its greatest commercial success as vocal pop music, particularly the work of the Beach Boys, formed in 1961 in Southern California. Their early albums included both instrumental surf rock (among them covers of music by Dick Dale) and vocal songs, drawing on rock and roll and doo wop and the close harmonies of vocal pop acts like the Four Freshmen.[70] The Beach Boys first chart hit, ""Surfin'"" in 1962 reached the Billboard top 100 and helped make the surf music craze a national phenomenon.[71] It is often argued that the surf music craze and the careers of almost all surf acts was effectively ended by the arrival of the British Invasion from 1964, because most surf music hits were recorded and released between 1961 and 1965.[72][nb 3]
 By the end of 1962, what would become the British rock scene had started with beat groups like the Beatles, Gerry & the Pacemakers and the Searchers from Liverpool and Freddie and the Dreamers, Herman's Hermits and the Hollies from Manchester. They drew on a wide range of American influences including 1950s rock and roll, soul, rhythm and blues, and surf music,[73] initially reinterpreting standard American tunes and playing for dancers. Bands like the Animals from Newcastle and Them from Belfast,[74] and particularly those from London like the Rolling Stones and the Yardbirds, were much more directly influenced by rhythm and blues and later blues music.[75] Soon these groups were composing their own material, combining US forms of music and infusing it with a high energy beat. Beat bands tended towards ""bouncy, irresistible melodies"", while early British blues acts tended towards less sexually innocent, more aggressive songs, often adopting an anti-establishment stance. There was, however, particularly in the early stages, considerable musical crossover between the two tendencies.[76] By 1963, led by the Beatles, beat groups had begun to achieve national success in Britain, soon to be followed into the charts by the more rhythm and blues focused acts.[77]
 ""I Want to Hold Your Hand"" was the Beatles' first number one hit on the Billboard Hot 100,[78] spending seven weeks at the top and a total of 15 weeks on the chart.[79][80] Their first appearance on The Ed Sullivan Show on 9 February 1964, drawing an estimated 73 million viewers (at the time a record for an American television program) is considered a milestone in American pop culture. During the week of 4 April 1964, the Beatles held 12 positions on the Billboard Hot 100 singles chart, including the entire top five. The Beatles went on to become the biggest selling rock band of all time and they were followed into the US charts by numerous British bands.[76] During the next two years British acts dominated their own and the US charts with Peter and Gordon, the Animals,[81] Manfred Mann, Petula Clark,[81] Freddie and the Dreamers, Wayne Fontana and the Mindbenders, Herman's Hermits, the Rolling Stones,[82] the Troggs, and Donovan[83] all having one or more number one singles.[79] Other major acts that were part of the invasion included the Kinks and the Dave Clark Five.[84][85]
 The British Invasion helped internationalize the production of rock and roll, opening the door for subsequent British (and Irish) performers to achieve international success.[86] In America it arguably spelled the end of instrumental surf music, vocal girl groups and (for a time) the teen idols, that had dominated the American charts in the late 1950s and 1960s.[87] It dented the careers of established R&B acts like Fats Domino and Chubby Checker and even temporarily derailed the chart success of surviving rock and roll acts, including Elvis.[88] The British Invasion also played a major part in the rise of a distinct genre of rock music, and cemented the primacy of the rock group, based on guitars and drums and producing their own material as singer-songwriters.[89] Following the example set by the Beatles' 1965 LP Rubber Soul in particular, other British rock acts released rock albums intended as artistic statements in 1966, including the Rolling Stones' Aftermath, the Beatles' own Revolver, and the Who's A Quick One, as well as American acts in the Beach Boys (Pet Sounds) and Bob Dylan (Blonde on Blonde).[90]
 Garage rock was a raw form of rock music, particularly prevalent in North America in the mid-1960s and so called because of the perception that it was rehearsed in the suburban family garage.[91][92] Garage rock songs often revolved around the traumas of high school life, with songs about ""lying girls"" and unfair social circumstances being particularly common.[93] The lyrics and delivery tended to be more aggressive than was common at the time, often with growled or shouted vocals that dissolved into incoherent screaming.[91] They ranged from crude one-chord music (like the Seeds) to near-studio musician quality (including the Knickerbockers, the Remains, and the Fifth Estate). There were also regional variations in many parts of the country with flourishing scenes particularly in California and Texas.[93] The Pacific Northwest states of Washington and Oregon had perhaps[according to whom?] the most defined regional sound.[94]
 The style had been evolving from regional scenes as early as 1958. ""Tall Cool One"" (1959) by the Wailers and ""Louie Louie"" by the Kingsmen (1963) are mainstream examples of the genre in its formative stages.[95] By 1963, garage band singles were creeping into the national charts in greater numbers, including Paul Revere and the Raiders (Boise),[96] the Trashmen (Minneapolis)[97] and the Rivieras (South Bend, Indiana).[98] Other influential garage bands, such as the Sonics (Tacoma, Washington), never reached the Billboard Hot 100.[99]
 The British Invasion greatly influenced garage bands, providing them with a national audience, leading many (often surf or hot rod groups) to adopt a British influence, and encouraging many more groups to form.[93] Thousands of garage bands were extant in the United States and Canada during the era and hundreds produced regional hits.[93] Despite scores of bands being signed to major or large regional labels, most were commercial failures. It is generally agreed that garage rock peaked both commercially and artistically around 1966.[93] By 1968 the style largely disappeared from the national charts and at the local level as amateur musicians faced college, work or the draft.[93] New styles had evolved to replace garage rock.[93][nb 4]
 Although the first impact of the British Invasion on American popular music was through beat and R&B based acts, the impetus was soon taken up by a second wave of bands that drew their inspiration more directly from American blues, including the Rolling Stones and the Yardbirds.[101] British blues musicians of the late 1950s and early 1960s had been inspired by the acoustic playing of figures such as Lead Belly, who was a major influence on the Skiffle craze, and Robert Johnson.[102] Increasingly they adopted a loud amplified sound, often centered on the electric guitar, based on the Chicago blues, particularly after the tour of Britain by Muddy Waters in 1958, which prompted Cyril Davies and guitarist Alexis Korner to form the band Blues Incorporated.[103] The band involved and inspired many of the figures of the subsequent British blues boom, including members of the Rolling Stones and Cream, combining blues standards and forms with rock instrumentation and emphasis.[55]
 The other key focus for British blues was John Mayall; his band, the Bluesbreakers, included Eric Clapton (after Clapton's departure from the Yardbirds) and later Peter Green. Particularly significant was the release of Blues Breakers with Eric Clapton (Beano) album (1966), considered one of the seminal British blues recordings and the sound of which was much emulated in both Britain and the United States.[104] Eric Clapton went on to form supergroups Cream, Blind Faith, and Derek and the Dominos, followed by an extensive solo career that helped bring blues rock into the mainstream.[103] Green, along with the Bluesbreaker's rhythm section Mick Fleetwood and John McVie, formed Peter Green's Fleetwood Mac, who enjoyed some of the greatest commercial success in the genre.[103] In the late 1960s Jeff Beck, also an alumnus of the Yardbirds, moved blues rock in the direction of heavy rock with his band, the Jeff Beck Group.[103] The last Yardbirds guitarist was Jimmy Page, who went on to form The New Yardbirds which rapidly became Led Zeppelin. Many of the songs on their first three albums, and occasionally later in their careers, were expansions on traditional blues songs.[103]
 In America, blues rock had been pioneered in the early 1960s by guitarist Lonnie Mack,[105] but the genre began to take off in the mid-1960s as acts developed a sound similar to British blues musicians. Key acts included Paul Butterfield (whose band acted like Mayall's Bluesbreakers in Britain as a starting point for many successful musicians), Canned Heat, the early Jefferson Airplane, Janis Joplin, Johnny Winter, the J. Geils Band and Jimi Hendrix with his power trios, the Jimi Hendrix Experience (which included two British members, and was founded in Britain), and Band of Gypsys, whose guitar virtuosity and showmanship would be among the most emulated of the decade.[103] Blues rock bands from the southern states, like the Allman Brothers Band, Lynyrd Skynyrd, and ZZ Top, incorporated country elements into their style to produce the distinctive genre Southern rock.[106]
 Early blues rock bands often emulated jazz, playing long, involved improvisations, which would later be a major element of progressive rock. From about 1967 bands like Cream and the Jimi Hendrix Experience had moved away from purely blues-based music into psychedelia.[107] By the 1970s, blues rock had become heavier and more riff-based, exemplified by the work of Led Zeppelin and Deep Purple, and the lines between blues rock and hard rock ""were barely visible"",[107] as bands began recording rock-style albums.[107] The genre was continued in the 1970s by figures such as George Thorogood and Pat Travers,[103] but, particularly on the British scene (except perhaps for the advent of groups such as Status Quo and Foghat who moved towards a form of high energy and repetitive boogie rock), bands became focused on heavy metal innovation, and blues rock began to slip out of the mainstream.[108]
 By the 1960s, the scene that had developed out of the American folk music revival had grown to a major movement, using traditional music and new compositions in a traditional style, usually on acoustic instruments.[109] In America the genre was pioneered by figures such as Woody Guthrie and Pete Seeger and often identified with progressive or labor politics.[109] In the early sixties figures such as Joan Baez and Bob Dylan had come to the fore in this movement as singer-songwriters.[110] Dylan had begun to reach a mainstream audience with hits including ""Blowin' in the Wind"" (1963) and ""Masters of War"" (1963), which brought ""protest songs"" to a wider public,[111] but, although beginning to influence each other, rock and folk music had remained largely separate genres, often with mutually exclusive audiences.[112]
 Early attempts to combine elements of folk and rock included the Animals' ""House of the Rising Sun"" (1964), which was the first commercially successful folk song to be recorded with rock and roll instrumentation[113] and the Beatles ""I'm a Loser"" (1964), arguably the first Beatles song to be influenced directly by Dylan.[114] The folk rock movement is usually thought to have taken off with the Byrds' recording of Dylan's ""Mr. Tambourine Man"" which topped the charts in 1965.[112] With members who had been part of the café-based folk scene in Los Angeles, the Byrds adopted rock instrumentation, including drums and 12-string Rickenbacker guitars, which became a major element in the sound of the genre.[112] Later that year Dylan adopted electric instruments, much to the outrage of many folk purists, with his ""Like a Rolling Stone"" becoming a US hit single.[112] According to Ritchie Unterberger, Dylan (even before his adoption of electric instruments) influenced rock musicians like the Beatles, demonstrating ""to the rock generation in general that an album could be a major standalone statement without hit singles"", such as on The Freewheelin' Bob Dylan (1963).[115]
 Folk rock particularly took off in California, where it led acts like the Mamas & the Papas and Crosby, Stills, and Nash to move to electric instrumentation, and in New York, where it spawned performers including the Lovin' Spoonful and Simon and Garfunkel, with the latter's acoustic ""The Sounds of Silence"" (1965) being remixed with rock instruments to be the first of many hits.[112] These acts directly influenced British performers like Donovan and Fairport Convention.[112] In 1969 Fairport Convention abandoned their mixture of American covers and Dylan-influenced songs to play traditional English folk music on electric instruments.[116] This British folk-rock was taken up by bands including Pentangle, Steeleye Span and the Albion Band, which in turn prompted Irish groups like Horslips and Scottish acts like the JSD Band, Spencer's Feat and later Five Hand Reel, to use their traditional music to create a brand of Celtic rock in the early 1970s.[117]
 Folk-rock reached its peak of commercial popularity in the period 1967–68, before many acts moved off in a variety of directions, including Dylan and the Byrds, who began to develop country rock.[118] However, the hybridization of folk and rock has been seen as having a major influence on the development of rock music, bringing in elements of psychedelia, and helping to develop the ideas of the singer-songwriter, the protest song, and concepts of ""authenticity"".[112][119]
 Psychedelic music's LSD-inspired vibe began in the folk scene.[120] The first group to advertise themselves as psychedelic rock were the 13th Floor Elevators from Texas.[120] The Beatles introduced many of the major elements of the psychedelic sound to audiences in this period, such as guitar feedback, the Indian sitar and backmasking sound effects.[121] Psychedelic rock particularly took off in California's emerging music scene as groups followed the Byrds' shift from folk to folk rock from 1965.[121] The psychedelic lifestyle, which revolved around hallucinogenic drugs, had already developed in San Francisco and particularly prominent products of the scene were Big Brother and the Holding Company, the Grateful Dead and Jefferson Airplane.[121][122] The Jimi Hendrix Experience's lead guitarist, Jimi Hendrix did
extended distorted, feedback-filled jams which became a key feature of psychedelia.[121] Psychedelic rock reached its apogee in the last years of the decade. 1967 saw the Beatles release their definitive psychedelic statement in Sgt. Pepper's Lonely Hearts Club Band, including the controversial track ""Lucy in the Sky with Diamonds"", the Rolling Stones responded later that year with Their Satanic Majesties Request,[121] and Pink Floyd debuted with The Piper at the Gates of Dawn. Key recordings included Jefferson Airplane's Surrealistic Pillow and the Doors' self-titled debut album. These trends peaked in the 1969 Woodstock festival, which saw performances by most of the major psychedelic acts.[121]
 Sgt. Pepper was later regarded as the greatest album of all time and a starting point for the album era, during which rock music transitioned from the singles format to albums and achieved cultural legitimacy in the mainstream.[123] Led by the Beatles in the mid-1960s,[124] rock musicians advanced the LP as the dominant form of recorded music expression and consumption, initiating a rock-informed album era in the music industry for the next several decades.[125]
 Progressive rock, a term sometimes used interchangeably with art rock, moved beyond established musical formulas by experimenting with different instruments, song types, and forms.[126] From the mid-1960s the Left Banke, the Beatles, the Rolling Stones and the Beach Boys, had pioneered the inclusion of harpsichords, wind, and string sections on their recordings to produce a form of Baroque rock and can be heard in singles like Procol Harum's ""A Whiter Shade of Pale"" (1967), with its Bach-inspired introduction.[127] The Moody Blues used a full orchestra on their album Days of Future Passed (1967) and subsequently created orchestral sounds with synthesizers.[126] Classical orchestration, keyboards, and synthesizers were a frequent addition to the established rock format of guitars, bass, and drums in subsequent progressive rock.[128]
 Instrumentals were common, while songs with lyrics were sometimes conceptual, abstract, or based in fantasy and science fiction.[129] The Pretty Things' SF Sorrow (1968), the Kinks' Arthur (Or the Decline and Fall of the British Empire) (1969), and the Who's Tommy (1969) introduced the format of rock operas and opened the door to concept albums, often telling an epic story or tackling a grand overarching theme.[130] King Crimson's 1969 début album, In the Court of the Crimson King, which mixed powerful guitar riffs and mellotron, with jazz and symphonic music, is often taken as the key recording in progressive rock, helping the widespread adoption of the genre in the early 1970s among existing blues-rock and psychedelic bands, as well as newly formed acts.[126] The vibrant Canterbury scene saw acts following Soft Machine from psychedelia, through jazz influences, toward more expansive hard rock, including Caravan, Hatfield and the North, Gong, and National Health.[131] The French group Magma around drummer Christian Vander almost single-handedly created the new music genre zeuhl with their first albums in the early 1970s.[132]
 Greater commercial success was enjoyed by Pink Floyd, who also moved away from psychedelia after the departure of Syd Barrett in 1968, with The Dark Side of the Moon (1973), seen as a masterpiece of the genre, becoming one of the best-selling albums of all time.[133] There was an emphasis on instrumental virtuosity, with Yes showcasing the skills of both guitarist Steve Howe and keyboard player Rick Wakeman, while Emerson, Lake & Palmer were a supergroup who produced some of the genre's most technically demanding work.[126] Jethro Tull and Genesis both pursued very different, but distinctly English, brands of music.[134] Renaissance, formed in 1969 by ex-Yardbirds Jim McCarty and Keith Relf, evolved into a high-concept band featuring the three-octave voice of Annie Haslam.[135] Most British bands depended on a relatively small cult following, but a handful, including Pink Floyd, Genesis, and Jethro Tull, managed to produce top ten singles at home and break the American market.[136] The American brand of progressive rock varied from the eclectic and innovative Frank Zappa, Captain Beefheart and Blood, Sweat & Tears,[137] to more pop rock orientated bands like Boston, Foreigner, Kansas, Journey, and Styx.[126] These, beside British bands Supertramp and ELO, all demonstrated a prog rock influence and while ranking among the most commercially successful acts of the 1970s, heralding the era of pomp or arena rock, which would last until the costs of complex shows (often with theatrical staging and special effects), would be replaced by more economical rock festivals as major live venues in the 1990s.[citation needed]
 The instrumental strand of the genre resulted in albums like Mike Oldfield's Tubular Bells (1973), the first record, and worldwide hit, for the Virgin Records label, which became a mainstay of the genre.[126] Instrumental rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Can, and Faust to circumvent the language barrier.[138] Their synthesiser-heavy ""krautrock"", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent electronic rock.[126] With the advent of punk rock and technological changes in the late 1970s, progressive rock was increasingly dismissed as pretentious and overblown.[139][140] Many bands broke up, but some, including Genesis, ELP, Yes, and Pink Floyd, regularly scored top ten albums with successful accompanying worldwide tours.[100] Some bands which emerged in the aftermath of punk, such as Siouxsie and the Banshees, Ultravox, and Simple Minds, showed the influence of progressive rock, as well as their more usually recognized punk influences.[141]
 In the late 1960s, jazz-rock emerged as a distinct subgenre out of the blues-rock, psychedelic, and progressive rock scenes, mixing the power of rock with the musical complexity and improvisational elements of jazz. AllMusic states that the term jazz-rock ""may refer to the loudest, wildest, most electrified fusion bands from the jazz camp, but most often it describes performers coming from the rock side of the equation."" Jazz-rock ""...generally grew out of the most artistically ambitious rock subgenres of the late '60s and early '70s"", including the singer-songwriter movement.[142] Many early US rock and roll musicians had begun in jazz and carried some of these elements into the new music. In Britain the subgenre of blues rock, and many of its leading figures, like Ginger Baker and Jack Bruce of the Eric Clapton-fronted band Cream, had emerged from the British jazz scene. Often highlighted as the first true jazz-rock recording is the only album by the relatively obscure New York–based the Free Spirits with Out of Sight and Sound (1966). The first group of bands to self-consciously use the label were R&B oriented white rock bands that made use of jazzy horn sections, like Electric Flag, Blood, Sweat & Tears and Chicago, to become some of the most commercially successful acts of the later 1960s and the early 1970s.[143]
 British acts to emerge in the same period from the blues scene, to make use of the tonal and improvisational aspects of jazz, included Nucleus[144] and the Graham Bond and John Mayall spin-off Colosseum. From the psychedelic rock and the Canterbury scenes came Soft Machine, who, it has been suggested, produced one of the artistically successfully fusions of the two genres. Perhaps the most critically acclaimed fusion came from the jazz side of the equation, with Miles Davis, particularly influenced by the work of Hendrix, incorporating rock instrumentation into his sound for the album Bitches Brew (1970). It was a major influence on subsequent rock-influenced jazz artists, including Herbie Hancock, Chick Corea and Weather Report.[143] The genre began to fade in the late 1970s, as a mellower form of fusion began to take its audience,[142] but acts like Steely Dan,[142] Frank Zappa and Joni Mitchell recorded significant jazz-influenced albums in this period, and it has continued to be a major influence on rock music.[143]
 Reflecting on developments that occurred in rock music in the early 1970s, Robert Christgau wrote in Christgau's Record Guide: Rock Albums of the Seventies (1981):[17]
 The decade is, of course, an arbitrary schema itself—time doesn't just execute a neat turn toward the future every ten years. But like a lot of artificial concepts—money, say—the category does take on a reality of its own once people figure out how to put it to work. ""The '60s are over,"" a slogan one only began to hear in 1972 or so, mobilized all those eager to believe that idealism had become passe, and once they were mobilized, it had. In popular music, embracing the '70s meant both an elitist withdrawal from the messy concert and counterculture scene and a profiteering pursuit of the lowest common denominator in FM radio and album rock. Rock saw greater commodification during this decade, turning into a multibillion-dollar industry and doubling its market while, as Christgau noted, suffering a significant ""loss of cultural prestige"". ""Maybe the Bee Gees became more popular than the Beatles, but they were never more popular than Jesus"", he said. ""Insofar as the music retained any mythic power, the myth was self-referential – there were lots of songs about the rock and roll life but very few about how rock could change the world, except as a new brand of painkiller ... In the '70s the powerful took over, as rock industrialists capitalized on the national mood to reduce potent music to an often reactionary species of entertainment—and to transmute rock's popular base from the audience to market.""[17]
 Roots rock is the term now used to describe a move away from what some saw as the excesses of the psychedelic scene, to a more basic form of rock and roll that incorporated its original influences, particularly blues, country and folk music, leading to the creation of country rock and Southern rock.[145] In 1966 Bob Dylan went to Nashville to record the album Blonde on Blonde.[146] This, and subsequent more clearly country-influenced albums, such as Nashville Skyline, have been seen as creating the genre of country folk, a route pursued by a number of largely acoustic folk musicians.[146] Other acts that followed the back-to-basics trend were the Canadian group the Band and the California-based Creedence Clearwater Revival, both of which mixed basic rock and roll with folk, country and blues, to be among the most successful and influential bands of the late 1960s.[147] The same movement saw the beginning of the recording careers of Californian solo artists like Ry Cooder, Bonnie Raitt and Lowell George,[148] and influenced the work of established performers such as the Rolling Stones' Beggar's Banquet (1968) and the Beatles' Let It Be (1970).[121] Reflecting on this change of trends in rock music over the past few years, Christgau wrote in his June 1970 ""Consumer Guide"" column that this ""new orthodoxy"" and ""cultural lag"" abandoned improvisatory, studio-ornamented productions in favor of an emphasis on ""tight, spare instrumentation"" and song composition: ""Its referents are '50s rock, country music, and rhythm-and-blues, and its key inspiration is the Band.""[149]
 In 1968, Gram Parsons recorded Safe at Home with the International Submarine Band, arguably the first true country rock album.[150] Later that year he joined the Byrds for Sweetheart of the Rodeo (1968), generally considered one of the most influential recordings in the genre.[150] The Byrds continued in the same vein, but Parsons left to be joined by another ex-Byrds member Chris Hillman in forming the Flying Burrito Brothers who helped establish the respectability and parameters of the genre, before Parsons departed to pursue a solo career.[150] Bands in California that adopted country rock included Hearts and Flowers, Poco, New Riders of the Purple Sage,[150] the Beau Brummels,[150] and the Nitty Gritty Dirt Band.[151] Some performers also enjoyed a renaissance by adopting country sounds, including: the Everly Brothers; one-time teen idol Rick Nelson who became the frontman for the Stone Canyon Band; former Monkee Mike Nesmith who formed the First National Band; and Neil Young.[150] The Dillards were, unusually, a country act, who moved towards rock music.[150] The greatest commercial success for country rock came in the 1970s, with artists including the Doobie Brothers, Emmylou Harris, Linda Ronstadt and the Eagles (made up of members of the Burritos, Poco, and Stone Canyon Band), who emerged as one of the most successful rock acts of all time, producing albums that included Hotel California (1976).[152]
 The founders of Southern rock are usually thought to be the Allman Brothers Band, who developed a distinctive sound, largely derived from blues rock, but incorporating elements of boogie, soul, and country in the early 1970s.[106] The most successful act to follow them were Lynyrd Skynyrd, who helped establish the ""Good ol' boy"" image of the subgenre and the general shape of 1970s' guitar rock.[106] Their successors included the fusion/progressive instrumentalists Dixie Dregs, the more country-influenced Outlaws, funk/R&B-leaning Wet Willie and (incorporating elements of R&B and gospel) the Ozark Mountain Daredevils.[106] After the loss of original members of the Allmans and Lynyrd Skynyrd, the genre began to fade in popularity in the late 1970s, but was sustained the 1980s with acts like .38 Special, Molly Hatchet and the Marshall Tucker Band.[106]
 Glam rock emerged from the English psychedelic and art rock scenes of the late 1960s and can be seen as both an extension of and reaction against those trends.[153] Musically diverse, varying between the simple rock and roll revivalism of figures like Alvin Stardust to the complex art rock of Roxy Music, and can be seen as much as a fashion as a musical subgenre.[153] Visually it was a mesh of various styles, ranging from 1930s Hollywood glamor, through 1950s pin-up sex appeal, pre-war Cabaret theatrics, Victorian literary and symbolist styles, science fiction, to ancient and occult mysticism and mythology; manifesting itself in outrageous clothes, makeup, hairstyles, and platform-soled boots.[154] Glam is most noted for its sexual and gender ambiguity and representations of androgyny, beside extensive use of theatrics.[155] It was prefigured by the showmanship and gender-identity manipulation of American acts such as the Cockettes and Alice Cooper.[156]
 The origins of glam rock are associated with Marc Bolan, who had renamed his folk duo to T. Rex and taken up electric instruments by the end of the 1960s. Often cited as the moment of inception is his appearance on the BBC music show Top of the Pops in March 1971 wearing glitter and satins, to perform what would be his second UK Top 10 hit (and first UK Number 1 hit), ""Hot Love"".[157] From 1971, already a minor star, David Bowie developed his Ziggy Stardust persona, incorporating elements of professional make up, mime and performance into his act.[158] These performers were soon followed in the style by acts including Roxy Music, Sweet, Slade, Mott the Hoople, Mud and Alvin Stardust.[158] While highly successful in the single charts in the United Kingdom, very few of these musicians were able to make a serious impact in the United States; Bowie was the major exception becoming an international superstar and prompting the adoption of glam styles among acts like Lou Reed, Iggy Pop, New York Dolls and Jobriath, often known as ""glitter rock"" and with a darker lyrical content than their British counterparts.[159] In the UK the term glitter rock was most often used to refer to the extreme version of glam pursued by Gary Glitter and his support musicians the Glitter Band, who between them achieved eighteen top ten singles in the UK between 1972 and 1976.[160] A second wave of glam rock acts, including Suzi Quatro, Roy Wood's Wizzard and Sparks, dominated the British single charts from about 1974 to 1976.[158] Existing acts, some not usually considered central to the genre, also adopted glam styles, including Rod Stewart, Elton John, Queen and, for a time, even the Rolling Stones.[158] It was also a direct influence on acts that rose to prominence later, including Kiss and Adam Ant, and less directly on the formation of gothic rock and glam metal as well as on punk rock, which helped end the fashion for glam from about 1976.[159] Glam has since enjoyed sporadic modest revivals through bands such as Chainsaw Kittens, the Darkness[161] and in R&B crossover act Prince.[162]
 After the early successes of Latin rock in the 1960s, Chicano musicians like Carlos Santana and Al Hurricane continued to have successful careers throughout the 1970s. Santana opened the decade with success in his 1970 single ""Black Magic Woman"" on the Abraxas album.[163] His third album Santana III yielded the single ""No One to Depend On"", and his fourth album Caravanserai experimented with his sound to mixed reception.[164][165] He later released a series of four albums that all achieved gold status: Welcome, Borboletta, Amigos, and Festivál. Al Hurricane continued to mix his rock music with New Mexico music, though he was also experimenting more heavily with Jazz music, which led to several successful singles, especially on his Vestido Mojado album, including the eponymous ""Vestido Mojado"", as well as ""Por Una Mujer Casada"" and ""Puño de Tierra""; his brothers had successful New Mexico music singles in ""La Del Moño Colorado"" by Tiny Morrie and ""La Cumbia De San Antone"" by Baby Gaby.[166] Al Hurricane Jr. also began his successful rock-infused New Mexico music recording career in the 1970s, with his 1976 rendition of ""Flor De Las Flores"".[167][168] Los Lobos gained popularity at this time, with their first album Los Lobos del Este de Los Angeles in 1977.
 A strange time, 1971—although rock's balkanization into genres was well underway, it was often hard to tell one catch-phrase from the next. ""Art-rock"" could mean anything from the Velvets to the Moody Blues, and although Led Zeppelin was launched and Black Sabbath celebrated, ""heavy metal"" remained an amorphous concept.
 —Robert Christgau[169] From the late 1960s it became common to divide mainstream rock music into soft and hard rock. Soft rock was often derived from folk rock, using acoustic instruments and putting more emphasis on melody and harmonies.[170] Major artists included Carole King, Cat Stevens and James Taylor.[170] It reached its commercial peak in the mid- to late 1970s with acts like Billy Joel, America and the reformed Fleetwood Mac, whose Rumours (1977) was the best-selling album of the decade.[171] In contrast, hard rock was more often derived from blues-rock and was played louder and with more intensity.[172] It often emphasised the electric guitar, both as a rhythm instrument using simple repetitive riffs and as a solo lead instrument, and was more likely to be used with distortion and other effects.[172] Key acts included British Invasion bands like the Kinks, as well as psychedelic era performers like Cream, Jimi Hendrix and the Jeff Beck Group.[172] Hard rock-influenced bands that enjoyed international success in the later 1970s included Queen,[173] Thin Lizzy,[174] Aerosmith, AC/DC,[172] and Van Halen.
 From the late 1960s the term ""heavy metal"" began to be used to describe some hard rock played with even more volume and intensity, first as an adjective and by the early 1970s as a noun.[175] The term was first used in music in Steppenwolf's ""Born to Be Wild"" (1967) and began to be associated with pioneer bands like San Francisco's Blue Cheer, Cleveland's James Gang and Michigan's Grand Funk Railroad.[176] By 1970 three key British bands had developed the characteristic sounds and styles which would help shape the subgenre. Led Zeppelin added elements of fantasy to their riff laden blues-rock, Deep Purple brought in symphonic and medieval interests from their progressive rock phase and Black Sabbath introduced facets of the gothic and modal harmony, helping to produce a ""darker"" sound.[177] These elements were taken up by a ""second generation"" of heavy metal bands into the late 1970s, including: Judas Priest, UFO, Motörhead and Rainbow from Britain; Kiss, Ted Nugent, and Blue Öyster Cult from the US; Rush from Canada and Scorpions from Germany, all marking the expansion in popularity of the subgenre.[177] Despite a lack of airplay and very little presence on the singles charts, late-1970s heavy metal built a considerable following, particularly among adolescent working-class males in North America and Europe.[178] In the 1980s, bands such as Bon Jovi, Guns N' Roses, Skid Row and Def Leppard saw mainstream success, with hard rock and a fusion of hard rock with pop. During the 1990s, hard rock saw a slight decline in popularity, save for some major hits like Guns N' Roses' November Rain. But in the early 2000s, Bon Jovi's ""It's My Life"" saw a huge increase in popularity of rock and pop rock and helped introduce the genres to a newer fanbase.
 Rock, mostly the heavy metal genre, has been criticized by some Christian leaders, who have condemned it as immoral, anti-Christian and even satanic.[179] However, Christian rock began to develop in the late 1960s, particularly out of the Jesus movement beginning in Southern California, and emerged as a subgenre in the 1970s with artists like Larry Norman, usually seen as the first major ""star"" of Christian rock.[180] The genre was mostly a phenomenon in the United States.[181] Many Christian rock performers have ties to the contemporary Christian music scene. Starting in the 1980s Christian pop performers have had some mainstream success. While these artists were largely acceptable in Christian communities, the adoption of heavy rock and glam metal styles by bands like Stryper, who achieved considerable mainstream success in the 1980s, was more controversial.[182][183] From the 1990s there were increasing numbers of acts who attempted to avoid the Christian band label, preferring to be seen as groups who were also Christians, including P.O.D.[184]
 American working-class oriented heartland rock, characterized by a straightforward musical style, and a concern with the lives of ordinary, blue-collar American people, developed in the second half of the 1970s. The term heartland rock was first used to describe Midwestern arena rock groups like Kansas, REO Speedwagon and Styx, but which came to be associated with a more socially concerned form of roots rock more directly influenced by folk, country and rock and roll.[185] It has been seen as an American Midwest and Rust Belt counterpart to West Coast country rock and the Southern rock of the American South.[186] Led by figures who had initially been identified with punk and New Wave, it was most strongly influenced by acts such as Bob Dylan, the Byrds, Creedence Clearwater Revival and Van Morrison, and the basic rock of 1960s garage and the Rolling Stones.[187]
 Exemplified by the commercial success of singer songwriters Bruce Springsteen, Bob Seger, and Tom Petty, along with less widely known acts such as Southside Johnny and the Asbury Jukes and Joe Grushecky and the Houserockers, it was partly a reaction to post-industrial urban decline in the East and Mid-West, often dwelling on issues of social disintegration and isolation, beside a form of good-time rock and roll revivalism.[187] The genre reached its commercial, artistic and influential peak in the mid-1980s, with Springsteen's Born in the USA (1984), topping the charts worldwide and spawning a series of top ten singles, together with the arrival of artists including John Mellencamp, Steve Earle and more gentle singer-songwriters such as Bruce Hornsby.[187] It can also be heard as an influence on artists as diverse as Billy Joel,[188] Kid Rock[189] and the Killers.[190]
 Heartland rock faded away as a recognized genre by the early 1990s, as rock music in general, and blue-collar and white working class themes in particular, lost influence with younger audiences, and as heartland's artists turned to more personal works.[187] Many heartland rock artists continue to record today with critical and commercial success, most notably Bruce Springsteen, Tom Petty, and John Mellencamp, although their works have become more personal and experimental and no longer fit easily into a single genre. Newer artists whose music would perhaps have been labeled heartland rock had it been released in the 1970s or 1980s, such as Missouri's Bottle Rockets and Illinois' Uncle Tupelo, often find themselves labeled alt-country.[191]
 Punk rock was developed between 1974 and 1976 in the United States and the United Kingdom. Rooted in garage rock and other forms of what is now known as protopunk music, punk rock bands eschewed the perceived excesses of mainstream 1970s rock.[192] They created fast, hard-edged music, typically with short songs, stripped-down instrumentation, and often political, anti-establishment lyrics. Punk embraces a DIY (do it yourself) ethic, with many bands self-producing their recordings and distributing them through informal channels.[193]
 By late 1976, acts such as the Ramones and Patti Smith, in New York City, and the Sex Pistols and the Clash, in London, were recognized as the vanguard of a new musical movement.[192] The following year saw punk rock spreading around the world. Punk quickly became a major cultural phenomenon in the UK. The Sex Pistols' live TV skirmish with Bill Grundy on 1 December 1976, was the watershed moment in British punk's transformation into a major media phenomenon, even as some stores refused to stock the records and radio airplay was hard to come by.[194] In May 1977, the Sex Pistols achieved new heights of controversy (and number two on the singles chart) with a song that referenced Queen Elizabeth II, ""God Save the Queen"", during her Silver Jubilee.[195] For the most part, punk took root in local scenes that tended to reject association with the mainstream. An associated punk subculture emerged, expressing youthful rebellion and characterized by distinctive clothing styles and a variety of anti-authoritarian ideologies.[196]
 By the beginning of the 1980s, faster, more aggressive styles such as hardcore and Oi! had become the predominant mode of punk rock.[197] This has resulted in several evolved strains of hardcore punk, such as D-beat (a distortion-heavy subgenre influenced by the UK band Discharge), anarcho-punk (such as Crass), grindcore (such as Napalm Death), and crust punk.[198] Musicians identifying with or inspired by punk also pursued a broad range of other variations, giving rise to New wave, post-punk and the alternative rock movement.[192]
 Although punk rock was a significant social and musical phenomenon, it achieved less in the way of record sales (being distributed by small specialty labels such as Stiff Records),[199] or American radio airplay (as the radio scene continued to be dominated by mainstream formats such as disco and album-oriented rock).[200] Punk rock had attracted devotees from the art and collegiate world and soon bands sporting a more literate, arty approach, such as Talking Heads and Devo began to infiltrate the punk scene; in some quarters the description ""new wave"" began to be used to differentiate these less overtly punk bands.[201] Record executives, who had been mostly mystified by the punk movement, recognized the potential of the more accessible new wave acts and began aggressively signing and marketing any band that could claim a remote connection to punk or new wave.[202] Many of these bands, such as the Cars and the Go-Go's can be seen as pop bands marketed as new wave;[203] other existing acts, including the Police, the Pretenders and Elvis Costello, used the new wave movement as the springboard for relatively long and critically successful careers,[204] while ""skinny tie"" bands exemplified by the Knack,[205] or the photogenic Blondie, began as punk acts and moved into more commercial territory.[206]
 Between 1979 and 1985, influenced by Kraftwerk, Yellow Magic Orchestra, David Bowie and Gary Numan, British new wave went in the direction of such New Romantics as Spandau Ballet, Ultravox, Japan, Duran Duran, A Flock of Seagulls, Culture Club, Talk Talk and the Eurythmics, sometimes using the synthesizer to replace all other instruments.[207] This period coincided with the rise of MTV and led to a great deal of exposure for this brand of synth-pop, creating what has been characterised as a second British Invasion.[208] Some more traditional rock bands adapted to the video age and profited from MTV's airplay, most obviously Dire Straits, whose ""Money for Nothing"" gently poked fun at the station, despite the fact that it had helped make them international stars,[209] but in general, guitar-oriented rock was commercially eclipsed.[210]
 If hardcore most directly pursued the stripped down aesthetic of punk, and new wave came to represent its commercial wing, post-punk emerged in the later 1970s and early 1980s as its more artistic and challenging side. Major influences beside punk bands were the Velvet Underground, Frank Zappa and Captain Beefheart, and the New York-based no wave scene which placed an emphasis on performance, including bands such as James Chance and the Contortions, DNA and Sonic Youth.[211] Early contributors to the genre included the US bands Pere Ubu, Devo, the Residents and Talking Heads.[211]
 The first wave of British post-punk included Gang of Four, Siouxsie and the Banshees and Joy Division, who placed less emphasis on art than their US counterparts and more on the dark emotional qualities of their music.[211] Bands like Siouxsie and the Banshees, Bauhaus, the Cure, and the Sisters of Mercy, moved increasingly in this direction to found Gothic rock, which had become the basis of a major sub-culture by the early 1980s.[212] Similar emotional territory was pursued by Australian acts like the Birthday Party and Nick Cave.[211] Members of Bauhaus and Joy Division explored new stylistic territory as Love and Rockets and New Order respectively.[211] Another early post-punk movement was the industrial music[213] developed by British bands Throbbing Gristle and Cabaret Voltaire, and New York-based Suicide, using a variety of electronic and sampling techniques that emulated the sound of industrial production and which would develop into a variety of forms of post-industrial music in the 1980s.[214]
 The second generation of British post-punk bands that broke through in the early 1980s, including the Fall, the Pop Group, the Mekons, Echo and the Bunnymen and the Teardrop Explodes, tended to move away from dark sonic landscapes.[211] Arguably the most successful band to emerge from post-punk was Ireland's U2, who incorporated elements of religious imagery together with political commentary into their often anthemic music, and by the late 1980s had become one of the biggest bands in the world.[215] Although many post-punk bands continued to record and perform, it declined as a movement in the mid-1980s as acts disbanded or moved off to explore other musical areas, but it has continued to influence the development of rock music and has been seen as a major element in the creation of the alternative rock movement.[216]
 The term alternative rock was coined in the early 1980s to describe rock artists who did not fit into the mainstream genres of the time. Bands dubbed ""alternative"" had no unified style, but were all seen as distinct from mainstream music. Alternative bands were linked by their collective debt to punk rock, through hardcore, New Wave or the post-punk movements.[217] Important alternative rock bands of the 1980s in the US included R.E.M., Hüsker Dü, Jane's Addiction, Sonic Youth, and the Pixies,[217] and in the UK the Cure, New Order, the Jesus and Mary Chain, and the Smiths.[218] Artists were largely confined to independent record labels, building an extensive underground music scene based on college radio, fanzines, touring, and word-of-mouth.[219] They rejected the dominant synth-pop of the early 1980s, marking a return to group-based guitar rock.[220][221][222]
 Few of these early bands achieved mainstream success, although exceptions to this rule include R.E.M., the Smiths, and the Cure. Despite a general lack of spectacular album sales, the original alternative rock bands exerted a considerable influence on the generation of musicians who came of age in the 1980s and ended up breaking through to mainstream success in the 1990s. Styles of alternative rock in the US during the 1980s included jangle pop, associated with the early recordings of R.E.M., which incorporated the ringing guitars of mid-1960s pop and rock, and college rock, used to describe alternative bands that began in the college circuit and college radio, including acts such as 10,000 Maniacs and the Feelies.[217] In the UK, Gothic rock was dominant in the early 1980s, but by the end of the decade, indie or dream pop[223] like Primal Scream, Bogshed, Half Man Half Biscuit and the Wedding Present, and what were dubbed shoegaze bands like My Bloody Valentine, Slowdive, Ride and Lush entered.[224] Particularly vibrant was the Madchester scene, producing such bands as Happy Mondays, Inspiral Carpets and the Stone Roses.[218][225] The next decade would see the success of grunge in the US and Britpop in the UK, bringing alternative rock into the mainstream.
 Disaffected by commercialized and highly produced pop and rock in the mid-1980s, bands in Washington state (particularly in the Seattle area) formed a new style of rock which sharply contrasted with the mainstream music of the time.[226] The developing genre came to be known as ""grunge"", a term descriptive of the dirty sound of the music and the unkempt appearance of most musicians, who actively rebelled against the over-groomed images of other artists.[226] Grunge fused elements of hardcore punk and heavy metal into a single sound, and made heavy use of guitar distortion, fuzz, and feedback.[226] The lyrics were typically apathetic and angst-filled, and often concerned themes such as social alienation and entrapment, although it was also known for its dark humor and parodies of commercial rock.[226]
 Bands such as Green River, Soundgarden, Melvins, and Skin Yard pioneered the genre, with Mudhoney becoming the most successful by the end of the decade. Grunge remained largely a local phenomenon until 1991, when Nirvana's album Nevermind became a huge success, containing the anthemic song ""Smells Like Teen Spirit"".[227] Nevermind was more melodic than its predecessors, by signing to Geffen Records the band was one of the first to employ traditional corporate promotion and marketing mechanisms such as an MTV video, in store displays and the use of radio ""consultants"" who promoted airplay at major mainstream rock stations. During 1991 and 1992, other grunge albums such as Pearl Jam's Ten, Soundgarden's Badmotorfinger, and Alice in Chains' Dirt, along with the Temple of the Dog album featuring members of Pearl Jam and Soundgarden, became among the 100 top-selling albums.[228] Major record labels signed most of the remaining grunge bands in Seattle, while a second influx of acts moved to the city in the hope of success.[229] However, with the death of Kurt Cobain and the subsequent break-up of Nirvana in 1994, touring problems for Pearl Jam and the departure of Alice in Chains' lead singer Layne Staley in 1998, the genre began to decline, partly to be overshadowed by Britpop and more commercial sounding post-grunge.[230]
 Britpop emerged from the British alternative rock scene of the early 1990s and was characterised by bands particularly influenced by British guitar music of the 1960s and 1970s.[218] The Smiths were a major influence, as were bands of the Madchester scene, which had dissolved in the early 1990s.[86] The movement has been seen partly as a reaction against various US-based, musical and cultural trends in the late 1980s and early 1990s, particularly the grunge phenomenon and as a reassertion of a British rock identity.[218] Britpop was varied in style, but often used catchy tunes and hooks, beside lyrics with particularly British concerns and the adoption of the iconography of the 1960s British Invasion, including the symbols of British identity previously used by the mods.[231] It was launched around 1993 with releases by groups such as Suede and Blur, who were soon joined by others including Oasis, Pulp, Supergrass, and Elastica, who produced a series of successful albums and singles.[218] For a while the contest between Blur and Oasis was built by the popular press into the ""Battle of Britpop"", initially won by Blur, but with Oasis achieving greater long-term and international success, directly influencing later Britpop bands, such as Ocean Colour Scene and Kula Shaker.[232] Britpop groups brought British alternative rock into the mainstream and formed the backbone of a larger British cultural movement known as Cool Britannia.[233] Although its more popular bands, particularly Blur and Oasis, were able to spread their commercial success overseas, especially to the United States, the movement had largely fallen apart by the end of the decade.[218]
 The term post-grunge was coined for the generation of bands that followed the emergence into the mainstream and subsequent hiatus of the Seattle grunge bands. Post-grunge bands emulated their attitudes and music, but with a more radio-friendly commercially oriented sound.[230] Often they worked through the major labels and came to incorporate diverse influences from jangle pop, pop-punk, alternative metal or hard rock.[230] The term post-grunge originally was meant to be pejorative, suggesting that they were simply musically derivative, or a cynical response to an ""authentic"" rock movement.[234] Originally, grunge bands that emerged when grunge was mainstream and were suspected of emulating the grunge sound were pejoratively labelled as post-grunge.[234] From 1994, former Nirvana drummer Dave Grohl's new band, the Foo Fighters, helped popularize the genre and define its parameters.[235]
 Some post-grunge bands, like Candlebox, were from Seattle, but the subgenre was marked by a broadening of the geographical base of grunge, with bands like Los Angeles' Audioslave, and Georgia's Collective Soul and beyond the US to Australia's Silverchair and Britain's Bush, who all cemented post-grunge as one of the most commercially viable subgenres of the late 1990s.[217][230] Although male bands predominated post-grunge, female solo artist Alanis Morissette's 1995 album Jagged Little Pill, labelled as post-grunge, also became a multi-platinum hit.[236] Post-grunge morphed during the late 1990s as post-grunge bands like Creed and Nickelback emerged.[234] Bands like Creed and Nickelback took post-grunge into the 21st century with considerable commercial success, abandoning most of the angst and anger of the original movement for more conventional anthems, narratives and romantic songs, and were followed in this vein by newer acts including Shinedown, Seether, 3 Doors Down and Puddle of Mudd.[234]
 The origins of 1990s pop-punk can be seen in the more song-oriented bands of the 1970s punk movement like Buzzcocks and the Clash, commercially successful new wave acts such as the Jam and the Undertones, and the more hardcore-influenced elements of alternative rock in the 1980s.[237] Pop-punk tends to use power-pop melodies and chord changes with speedy punk tempos and loud guitars.[238] Punk music provided the inspiration for some California-based bands on independent labels in the early 1990s, including Rancid and Green Day.[237] In 1994 Green Day moved to a major label and produced the album Dookie, which found a new, largely teenage, audience and proved a surprise diamond-selling success, leading to a series of hit singles, including two number ones in the US.[217] They were soon followed by the eponymous debut from Weezer, which spawned three top ten singles in the US.[239] This success opened the door for the multi-platinum sales of metallic punk band the Offspring with Smash (1994).[217] This first wave of pop punk reached its commercial peak with Green Day's Nimrod (1997) and the Offspring's Americana (1998).[240]
 A second wave of pop-punk was spearheaded by Blink-182, with their breakthrough album Enema of the State (1999), followed by bands such as Good Charlotte, Simple Plan and Sum 41, who made use of humour in their videos and had a more radio-friendly tone to their music, while retaining the speed, some of the attitude and even the look of 1970s punk.[237] Later pop-punk bands, including All Time Low, the All-American Rejects and Fall Out Boy, had a sound that has been described as closer to 1980s hardcore, while still achieving commercial success.[237]
 In the 1980s the terms indie rock and alternative rock were used interchangeably.[241] By the mid-1990s, as elements of the movement began to attract mainstream interest, particularly grunge and then Britpop, post-grunge and pop-punk, the term alternative began to lose its meaning.[241] Those bands following the less commercial contours of the scene were increasingly referred to by the label indie.[241] They characteristically attempted to retain control of their careers by releasing albums on their own or small independent labels, while relying on touring, word-of-mouth, and airplay on independent or college radio stations for promotion.[241] Linked by an ethos more than a musical approach, the indie rock movement encompassed a wide range of styles, from hard-edged, grunge-influenced bands like the Cranberries and Superchunk, through do-it-yourself experimental bands like Pavement, to punk-folk singers such as Ani DiFranco.[217][218] It has been noted that indie rock has a relatively high proportion of female artists compared with preceding rock genres, a tendency exemplified by the development of feminist-informed Riot grrrl music.[242] Many countries have developed an extensive local indie scene, flourishing with bands with enough popularity to survive inside the respective country, but virtually unknown outside them.[243]
 By the end of the 1990s many recognisable subgenres, most with their origins in the late 1980s alternative movement, were included under the umbrella of indie. Lo-fi eschewed polished recording techniques for a D.I.Y. ethos and was spearheaded by Beck, Sebadoh and Pavement.[217] The work of Talk Talk and Slint helped inspire both post rock, an experimental style influenced by jazz and electronic music, pioneered by Bark Psychosis and taken up by acts such as Tortoise, Stereolab, and Laika,[244][245] as well as leading to more dense and complex, guitar-based math rock, developed by acts like Polvo and Chavez.[246] Space rock looked back to progressive roots, with drone heavy and minimalist acts like Spacemen 3, the two bands created out of its split, Spectrum and Spiritualized, and later groups including Flying Saucer Attack, Godspeed You! Black Emperor and Quickspace.[247] In contrast, Sadcore emphasised pain and suffering through melodic use of acoustic and electronic instrumentation in the music of bands like American Music Club and Red House Painters,[248] while the revival of baroque pop reacted against lo-fi and experimental music by placing an emphasis on melody and classical instrumentation, with artists like Arcade Fire, Belle and Sebastian and Rufus Wainwright.[249]
 Alternative metal emerged from the hardcore scene of alternative rock in the US in the later 1980s, but gained a wider audience after grunge broke into the mainstream in the early 1990s.[250] Early alternative metal bands mixed a wide variety of genres with hardcore and heavy metal sensibilities, with acts like Jane's Addiction and Primus using progressive rock, Soundgarden and Corrosion of Conformity using garage punk, the Jesus Lizard and Helmet mixing noise rock, Ministry and Nine Inch Nails influenced by industrial music, Monster Magnet moving into psychedelia, Pantera, Sepultura and White Zombie creating groove metal, while Biohazard, Limp Bizkit and Faith No More turned to hip hop and rap.[250]
 Hip hop had gained attention from rock acts in the early 1980s, including the Clash with ""The Magnificent Seven"" (1980) and Blondie with ""Rapture"" (1980).[251][252] Early crossover acts included Run DMC and the Beastie Boys.[253] Detroit rapper Esham became known for his ""acid rap"" style, which fused rapping with a sound that was often based in rock and heavy metal.[254][255] Rappers who sampled rock songs included Ice-T, the Fat Boys, LL Cool J, Public Enemy and Whodini.[256] The mixing of thrash metal and rap was pioneered by Anthrax on their 1987 comedy-influenced single ""I'm the Man"".[256]
 In 1990, Faith No More broke into the mainstream with their single ""Epic"", often seen as the first truly successful combination of heavy metal with rap.[257] This paved the way for the success of existing bands like 24-7 Spyz and Living Colour, and new acts including Rage Against the Machine and Red Hot Chili Peppers, who all fused rock and hip hop among other influences.[256][258] Among the first wave of performers to gain mainstream success as rap rock were 311,[259] Bloodhound Gang,[260] and Kid Rock.[261] A more metallic sound – nu metal – was pursued by bands including Limp Bizkit, Korn and Slipknot.[256] Later in the decade this style, which contained a mix of grunge, punk, metal, rap and turntable scratching, spawned a wave of successful bands like Linkin Park, P.O.D. and Staind, who were often classified as rap metal or nu metal, the first of which are the best-selling band of the genre.[262]
 In 2001, nu metal reached its peak with albums like Staind's Break the Cycle, P.O.D's Satellite, Slipknot's Iowa and Linkin Park's Hybrid Theory. New bands also emerged like Disturbed, Godsmack and Papa Roach, whose major label début Infest became a platinum hit.[263] Korn's long-awaited fifth album Untouchables, and Papa Roach's second album Lovehatetragedy, did not sell as well as their previous releases, while nu metal bands were played more infrequently on rock radio stations and MTV began focusing on pop punk and emo.[264] Since then, many bands have changed to a more conventional hard rock, heavy metal, or electronic music sound.[264]
 From about 1997, as dissatisfaction grew with the concept of Cool Britannia, and Britpop as a movement began to dissolve, emerging bands began to avoid the Britpop label while still producing music derived from it.[265][266] Many of these bands tended to mix elements of British traditional rock (or British trad rock),[267] particularly the Beatles, Rolling Stones and Small Faces,[268] with American influences, including post-grunge.[269][270] Drawn from across the United Kingdom (with several important bands emerging from the north of England, Scotland, Wales and Northern Ireland), the themes of their music tended to be less parochially centered on British, English and London life and more introspective than had been the case with Britpop at its height.[271][272] This, beside a greater willingness to engage with the American press and fans, may have helped some of them in achieving international success.[273] Several alternative bands that had enjoyed some success during the mid-1990s, but did not find major commercial success until the late 1990s included the Verve and Radiohead. After the decline of Britpop they began to gain more critical and popular attention. The Verve's album Urban Hymns (1997) was a worldwide hit, and Radiohead achieved near-universal critical acclaim with their experimental third album OK Computer (1997), as well as its follow-up Kid A (2000).
 Post-Britpop bands have been seen as presenting the image of the rock star as an ordinary person and their increasingly melodic music was criticised for being bland or derivative.[274] Post-Britpop bands like Travis from The Man Who (1999), Stereophonics from Performance and Cocktails (1999), Feeder from Echo Park (2001), and particularly Coldplay from their debut album Parachutes (2000), achieved much wider international success than most of the Britpop groups that had preceded them, and were some of the most commercially successful acts of the late 1990s and early 2000s, arguably providing a launchpad for the subsequent garage rock revival and post-punk revival, which has also been seen as a reaction to their introspective brand of rock.[270][275][276][277]
 Post-hardcore developed in the US, particularly in the Chicago and Washington, DC areas, in the early to mid-1980s, with bands that were inspired by the do-it-yourself ethics and guitar-heavy music of hardcore punk, but influenced by post-punk, adopting longer song formats, more complex musical structures and sometimes more melodic vocal styles.[278]
 Emo also emerged from the hardcore scene in 1980s Washington, D.C., initially as ""emocore"", used as a term to describe bands who favored expressive vocals over the more common abrasive, barking style.[279] The early emo scene operated as an underground, with short-lived bands releasing small-run vinyl records on tiny independent labels.[279] Emo broke into mainstream culture in the early 2000s with the platinum-selling success of Jimmy Eat World's Bleed American (2001) and Dashboard Confessional's The Places You Have Come to Fear the Most (2003).[280] The new emo had a much more mainstream sound than in the 1990s and a far greater appeal amongst adolescents than its earlier incarnations.[280] At the same time, use of the term emo expanded beyond the musical genre, becoming associated with fashion, a hairstyle and any music that expressed emotion.[281] By 2003 post-hardcore bands had also caught the attention of major labels and began to enjoy mainstream success in the album charts.[citation needed] A number of these bands were seen as a more aggressive offshoot of emo and given the often vague label of screamo.[282]
 In the early 2000s, a new group of bands that played a stripped down and back-to-basics version of guitar rock, emerged into the mainstream. They were variously characterised as part of a garage rock, post-punk or New Wave revival.[283][284][285][286] Because the bands came from across the globe, cited diverse influences (from traditional blues, through New Wave to grunge), and adopted differing styles of dress, their unity as a genre has been disputed.[287] There had been attempts to revive garage rock and elements of punk in the 1980s and 1990s and by 2000 scenes had grown up in several countries.[288]
 The commercial breakthrough from these scenes was led by four bands: the Strokes, who emerged from the New York club scene with their début album Is This It (2001); the White Stripes, from Detroit, with their third album White Blood Cells (2001); the Hives from Sweden after their compilation album Your New Favourite Band (2001); and the Vines from Australia with Highly Evolved (2002).[289] They were christened by the media as the ""The"" bands, and dubbed ""The saviours of rock 'n' roll"", leading to accusations of hype.[290] A second wave of bands that gained international recognition due to the movement included Black Rebel Motorcycle Club, the Killers, Interpol and Kings of Leon from the US,[291] the Libertines, Arctic Monkeys, Bloc Party, Kaiser Chiefs and Franz Ferdinand from the UK,[292] Jet and Wolfmother from Australia,[293] and the Datsuns and the D4 from New Zealand.[294]
 In the 2000s, as computer technology became more accessible and music software advanced, it became possible to create high quality music using little more than a single laptop computer.[295] This resulted in a massive increase in the amount of home-produced electronic music available to the general public via the expanding internet,[296] and new forms of performance such as laptronica[295] and live coding.[297] These techniques also began to be used by existing bands and by developing genres that mixed rock with digital techniques and sounds, including indie electronic, electroclash, dance-punk and new rave.[citation needed]
 During the 2010s, rock music declined from its position as the major popular music genre, now sharing with electronic dance and hip hop, the latter of which had surpassed it as the most consumed musical genre in the United States by 2017.[298][299][300] The rise of streaming and the advent of technology, which changed approaches toward music creation, were cited as major factors.[301] Ken Partridge of Genius suggested that hip-hop became more popular because it is a more transformative genre and does not need to rely on past sounds and that there is a direct connection to the stagnation of rock music and changing social attitudes during the 2010s.[299] Bill Flanagan, in a 2016 opinion piece for The New York Times, compared the state of rock during this period to the state of jazz in the early 1980s, ""slowing down and looking back.""[302]
 The rock bands which had chart success in the 2010s were mostly associated with the trends that had been popular in the 2000s and earlier decades rather than reflecting new scenes and sounds.[303] Some pop rock and hard rock bands continued to see commercial success during this period, including Ghost, Maroon 5, Twenty One Pilots, Fall Out Boy, Imagine Dragons, Halestorm, Panic! at the Disco, Black Veil Brides, Greta Van Fleet, The Black Keys and Måneskin.[304][305][306] Outside of the charts, the commercialisation of rock festivals was a major theme of the decade, with both global megafestivals such as Coachella, Glastonbury and Roskilde, and smaller-scale local festivals expanding.[307]
 In 2020, the COVID-19 pandemic brought extreme changes to the rock scene worldwide. Restrictions, such as quarantine rules, caused widespread cancellations and postponements of concerts, tours, festivals, album releases, award ceremonies, and competitions.[308][309][310][311][312] Some artists resorted to giving online performances to keep their careers active.[313] Another scheme to circumvent the quarantine limitations was used at a concert of Danish rock musician Mads Langer: the audience watched the performance from inside their cars, much like in a drive-in theater.[314] Musically, the pandemic led to a surge in new releases from the slower, less energetic, and more acoustic subgenres of rock music.[315][316] The industry raised funds to help itself through efforts such as Crew Nation, a relief fund for live music crews organised by Livenation.[317]
 Psychedelic and progressive styles in rock would see a resurgence in popularity during the 2010s and 2020s. Some of the most notable acts in neo-psychedelia originated in Australia; Kevin Parker’s Tame Impala released the single “Elephant” in 2012, which became a hit on alternative radio in various countries, and would be followed by the release of critically-acclaimed albums by Parker such as Lonerism (2012) and Currents (2015).[318][319][320] This new style of Australian psychedelic music not only echoed the psychedelic and progressive rock acts of the ‘60s and '70s, but also incorporated new and unique musical influences from various subgenres of rock, heavy metal, EDM, and world music.[321] A 2014 article in The Guardian described Australia as a place where “independently minded rock bands are free to develop at their own pace”.[322] Other Australian psychedelic and progressive revival acts of the 2010s and 2020s include King Gizzard & the Lizard Wizard, Psychedelic Porn Crumpets, Rolling Blackouts Coastal Fever, Bananagun, Jay Watson, The Murlocs, Stonefield, and Tropical Fuck Storm.[323][324]
 Psychedelic trends in rock have also seen a resurgence in Europe, with European and American stoner rock groups such as Uncle Acid & the Deadbeats, Graveyard, Kadavar, All Them Witches, and True Widow performing a heavier, more riff-based version of neo-psychedelia containing stronger blues and metal influences.[325] Europe has been described as ""really good"" for new psychedelic music, with many American stoner rock bands choosing to tour Europe as opposed to North America.[326]
 At the start of the 2020s, recording artists in both pop and rap music released popular pop-punk-influenced recordings, many of them produced or assisted by Blink-182 drummer Travis Barker. Representing a commercial resurgence for the genre, these acts included Machine Gun Kelly, Willow Smith, Trippie Redd, Halsey, Yungblud, and Olivia Rodrigo. The popularity of the social media platform TikTok helped spark nostalgia for the angst-driven musical style among young listeners during the pandemic. Among the most successful of these releases have been Machine Gun Kelly's 2020 album Tickets to My Downfall, which topped the Billboard 200, and Rodrigo's number-one hit single ""Good 4 U"" (2021).[327]
 In the mid-to-late 2010s and early 2020s, a new wave of post-punk bands from Britain and Ireland emerged. The groups in this scene have been described with the term ""Crank Wave"" by NME and The Quietus in 2019, and as ""Post-Brexit New Wave"" by NPR writer Matthew Perpetua in 2021.[328][329][330] Artists that have been identified as part of the style include Black Midi, Wet Leg, Squid, Black Country, New Road, Dry Cleaning, Shame, Sleaford Mods, Fontaines D.C., The Murder Capital, Idles and Yard Act.[328][329][330][331] Post-punk artists that attained prominence in the 2010s and early 2020s from other countries besides the UK included Parquet Courts, Protomartyr and Geese (United States), Preoccupations (Canada), Iceage (Denmark), and Viagra Boys (Sweden).[332][333][334]
 Different subgenres of rock were adopted by, and became central to, the identity of a large number of sub-cultures. In the 1950s and 1960s, respectively, British youths adopted the Teddy Boy and Rocker subcultures, which revolved around US rock and roll.[335] The counterculture of the 1960s was closely associated with psychedelic rock.[335] The mid-late 1970s punk subculture began in the US, but it was given a distinctive look by British designer Vivienne Westwood, a look which spread worldwide.[336] Out of the punk scene, the Goth and Emo subcultures grew, both of which presented distinctive visual styles.[337]
 When an international rock culture developed, it supplanted cinema as the major sources of fashion influence.[338] Paradoxically, followers of rock music have often mistrusted the world of fashion, which has been seen as elevating image above substance.[338] Rock fashions have been seen as combining elements of different cultures and periods, as well as expressing divergent views on sexuality and gender, and rock music in general has been noted and criticised for facilitating greater sexual freedom.[338][339] Rock has also been associated with various forms of drug use, including the amphetamines taken by mods in the early to mid-1960s, through the LSD, mescaline, hashish and other hallucinogenic drugs linked with psychedelic rock in the mid-late 1960s and early 1970s; and sometimes to cannabis, cocaine and heroin, all of which have been eulogised in song.[340][341]
 Rock has been credited with changing attitudes to race by opening up African-American culture to white audiences; but at the same time, rock has been accused of appropriating and exploiting that culture.[342][343] While rock music has absorbed many influences and introduced Western audiences to different musical traditions,[344] the global spread of rock music has been interpreted as a form of cultural imperialism.[345] Rock music inherited the folk tradition of protest song, making political statements on subjects such as war, religion, poverty, civil rights, justice and the environment.[346] Political activism reached a mainstream peak with the ""Do They Know It's Christmas?"" single (1984) and Live Aid concert for Ethiopia in 1985, which, while raising awareness of world poverty and funds for aid, have also been criticised (along with similar events), for providing a stage for self-aggrandisement and increased profits for the rock stars involved.[347]
 Since its early development, rock music has been associated with rebellion against social and political norms, most in early rock and roll's rejection of an adult-dominated culture, the counterculture's rejection of consumerism and conformity and punk's rejection of all forms of social convention,[348] however, it can also be seen as providing a means of commercial exploitation of such ideas and of diverting youth away from political action.[349][350]
 Professional women instrumentalists are uncommon in rock genres such as heavy metal although bands such as Within Temptation have featured women as lead singers with men playing instruments. According to Schaap and Berkers, ""playing in a band is a male homosocial activity, that is, learning to play in a band is a peer-based ... experience, shaped by existing sex-segregated friendship networks.[351] They note that rock music ""is often defined as a form of male rebellion vis-à-vis female bedroom culture.""[352] (The theory of ""bedroom culture"" argues that society influences girls to not engage in crime and deviance by virtually trapping them in their bedroom; it was identified by a sociologist named Angela McRobbie.) In popular music, there has been a gendered ""distinction between public (male) and private (female) participation"" in music.[352] ""Several scholars have argued that men exclude women from bands or from the bands' rehearsals, recordings, performances, and other social activities"".[353] ""Women are regarded as passive and private consumers of slick, prefabricated – hence, inferior – pop music ..., excluding them from participating as high status rock musicians"".[353] One of the reasons that there are mixed gender bands is that ""bands operate as tight-knit units in which homosocial solidarity – social bonds between people of the same sex ...  – plays a crucial role"".[353] In the 1960s rock music scene, ""singing was sometimes an acceptable pastime for a girl, but playing an instrument ... simply wasn't done"".[354]
 ""The rebellion of rock music was a male rebellion; the women – often, in the 1950s and '60s, girls in their teens – in rock sang songs as personæ dependent on their macho boyfriends ..."". Philip Auslander says that ""Although there were many women in rock by the late 1960s, most performed only as singers, a feminine position in popular music"". Though some women played instruments in American all-female garage rock bands, none of these bands achieved more than regional success. So they ""did not provide viable templates for women's on-going participation in rock"".[355] In relation to the gender composition of heavy metal bands, it has been said that ""[h]eavy metal performers are almost exclusively male""[356] ""...at least until the mid-1980s""[357] apart from ""...exceptions such as Girlschool"".[356] However, ""...now [in the 2010s] maybe more than ever–strong metal women have put up their dukes and got down to it"",[358] ""carv[ing] out a considerable place for [them]selves.""[359] When Suzi Quatro emerged in 1973, ""no other prominent female musician worked in rock simultaneously as a singer, instrumentalist, songwriter, and bandleader"".[355] According to Auslander, she was ""kicking down the male door in rock and roll and proving that a female musician ... and this is a point I am extremely concerned about ... could play as well if not better than the boys"".[355]
 An all-female band is a musical group in genres such as rock and blues which is composed of female musicians. This is distinct from a girl group, in which the female members are vocalists, though this terminology is not universally followed.[360]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['social alienation and entrapment', 'George Thorogood and Pat Travers', 'the garage rock/post-punk revival in the 2000s', 'social alienation and entrapment', 'social alienation and entrapment'], 'answer_start': [], 'answer_end': []}"
"Pop music is a genre of popular music that originated in its modern form during the mid-1950s in the United States and the United Kingdom.[4] During the 1950s and 1960s, pop music encompassed rock and roll and the youth-oriented styles it influenced. Rock and pop music remained roughly synonymous until the late 1960s, after which pop became associated with music that was more commercial, ephemeral, and accessible.
 Identifying factors of pop music usually include repeated choruses and hooks, short to medium-length songs written in a basic format (often the verse–chorus structure), and rhythms or tempos that can be easily danced to. Much of pop music also borrows elements from other styles such as rock, urban, dance, Latin, and country.
 The terms popular music and pop music are often used interchangeably, although the former more accurately describes all music that is popular and includes many disparate styles. Although much of the music that appears on record charts is considered to be pop music, the genre is distinguished from chart music.
 David Hatch and Stephen Millward describe pop music as ""a body of music which is distinguishable from popular, jazz, and folk music"".[9]
According to Pete Seeger, pop music is ""professional music which draws upon both folk music and fine arts music"".[3]
David Boyle, a music researcher, states pop music as any type of music that a person has been exposed to by the mass media.[10] Most individuals think that pop music is just the singles charts and not the sum of all chart music. The music charts contain songs from a variety of sources, including classical, jazz, rock, and novelty songs. As a genre, pop music is seen to exist and develop separately.[11] Therefore, the term ""pop music"" may be used to describe a distinct genre, designed to appeal to all, often characterized as ""instant singles-based music aimed at teenagers"" in contrast to rock music as ""album-based music for adults"".[4][13]
 Pop music continuously evolves along with the term's definition. According to music writer Bill Lamb, popular music is defined as ""the music since industrialization in the 1800s that is most in line with the tastes and interests of the urban middle class.""[14] The term ""pop song"" was first used in 1926, in the sense of a piece of music ""having popular appeal"".[15] Hatch and Millward indicate that many events in the history of recording in the 1920s can be seen as the birth of the modern pop music industry, including in country, blues, and hillbilly music.[16]
 According to the website of The New Grove Dictionary of Music and Musicians, the term ""pop music"" ""originated in Britain in the mid-1950s as a description for rock and roll and the new youth music styles that it influenced"".[2] The Oxford Dictionary of Music states that while pop's ""earlier meaning meant concerts appealing to a wide audience [...] since the late 1950s, however, pop has had the special meaning of non-classical mus[ic], usually in the form of songs, performed by such artists as The Beatles, The Rolling Stones, ABBA, etc.""[17] Grove Music Online also states that ""[...] in the early 1960s, [the term] 'pop music' competed terminologically with beat music [in England], while in the US its coverage overlapped (as it still does) with that of 'rock and roll'"".[2]
 
From about 1967, the term ""pop music"" was increasingly used in opposition to the term rock music, a division that gave generic significance to both terms.[18] While rock aspired to authenticity and an expansion of the possibilities of popular music,[18] pop was more commercial, ephemeral, and accessible.[19] According to British musicologist Simon Frith, pop music is produced ""as a matter of enterprise not art"", and is ""designed to appeal to everyone"" but ""doesn't come from any particular place or mark off any particular taste"". Frith adds that it is ""not driven by any significant ambition except profit and commercial reward [...] and, in musical terms, it is essentially conservative"". It is, ""provided from on high (by record companies, radio programmers, and concert promoters) rather than being made from below (...) Pop is not a do-it-yourself music but is professionally produced and packaged"".[4]  According to Frith, characteristics of pop music include an aim of appealing to a general audience, rather than to a particular sub-culture or ideology, and an emphasis on craftsmanship rather than formal ""artistic"" qualities.[4] Besides, Frith also offers three identifying characteristics of pop music: light entertainment, commercial imperatives, and personal identification. Pop music grew out of a light entertainment and easy listening tradition.[22] Pop music is more conservative than other music genres such as folk, blues, country, and tradition. Many pop songs do not contain themes of resistance, opposition, or political themes, rather focusing more on love and relationships.  Therefore, pop music does not challenge its audiences socially, and does not cause political activism. Frith also said the main purpose of pop music is to create revenue. It is not a medium of free articulation of the people. Instead, pop music seeks to supply the nature of personal desire and achieve the instant empathy with cliche personalities, stereotypes, and melodrama that appeals to listeners. It is mostly about how much revenue pop music makes for record companies.[23] Music scholar Timothy Warner said pop music typically has an emphasis on recording, production, and technology, rather than live performance; a tendency to reflect existing trends rather than progressive developments; and seeks to encourage dancing or uses dance-oriented rhythms.[19]
 The main medium of pop music is the song, often between two and a half and three and a half minutes in length, generally marked by a consistent and noticeable rhythmic element, a mainstream style and a simple traditional structure.[26] The structure of many popular songs is that of a verse and a chorus, the chorus serving as the portion of the track that is designed to stick in the ear through simple repetition both musically and lyrically. The chorus is often where the music builds towards and is often preceded by ""the drop"" where the bass and drum parts ""drop out"".[27] Common variants include the verse-chorus form and the thirty-two-bar form, with a focus on melodies and catchy hooks, and a chorus that contrasts melodically, rhythmically and harmonically with the verse.[28] The beat and the melodies tend to be simple, with limited harmonic accompaniment.[29] The lyrics of modern pop songs typically focus on simple themes – often love and romantic relationships – although there are notable exceptions.[4]
 Harmony and chord progressions in pop music are often ""that of classical European tonality, only more simple-minded.""[30] Clichés include the barbershop quartet-style harmony (i.e. ii – V – I) and blues scale-influenced harmony.[31] There was a lessening of the influence of traditional views of the circle of fifths between the mid-1950s and the late 1970s, including less predominance for the dominant function.[32]
 In October 2023, Billboard compiled a list of ""the 500 best pop songs"". In doing so, they noted the difficulty of defining ""pop songs"":
 One of the reasons pop can be hard to summarize is because there’s no real sonic or musical definition to it. There are common elements to a lot of the biggest pop songs, but at the end of the day, ""pop"" means ""popular"" first and foremost, and just about any song that becomes popular enough...can be considered a pop song.[33] In the 1940s, improved microphone design allowed a more intimate singing style and, ten or twenty years later, inexpensive and more durable 45 rpm records for singles ""revolutionized the manner in which pop has been disseminated"", which helped to move pop music to ""a record/radio/film star system"".[35] Another technological change was the widespread availability of television in the 1950s with televised performances, which meant that ""pop stars had to have a visual presence"".[35] In the 1960s, the introduction of inexpensive, portable transistor radios meant that teenagers in the developed world could listen to music outside of the home.[35] By the early 1980s, the promotion of pop music had been greatly affected by the rise of music television channels like MTV, which ""favoured those artists such as Michael Jackson and Madonna who had a strong visual appeal"".[35]
 Multi-track recording (from the 1960s) and digital sampling (from the 1980s) have also been used as methods for the creation and elaboration of pop music.[4] During the mid-1960s, pop music made repeated forays into new sounds, styles, and techniques that inspired public discourse among its listeners. The word ""progressive"" was frequently used, and it was thought that every song and single was to be a ""progression"" from the last.[36] Music critic Simon Reynolds writes that beginning with 1967, a divide would exist between ""progressive"" pop and ""mass/chart"" pop, a separation which was ""also, broadly, one between boys and girls, middle-class and working-class.""[37]
 The latter half of the 20th century included a large-scale trend in American culture in which the boundaries between art and pop music were increasingly blurred.[38] Between 1950 and 1970, there was a debate of pop versus art.[39] Since then, certain music publications have embraced the music's legitimacy, a trend referred to as ""poptimism"".[39]
 Throughout its development, pop music has absorbed influences from other genres of popular music. Early pop music drew on traditional pop, an American counterpart to German Schlager and French Chanson, however compared to the pop of European countries, traditional pop originally emphasized influences ranging from Tin Pan Alley songwriting, Broadway theatre, and show tunes. As the genre evolved more influences ranging from classical, folk, rock, country, electronic music, and other popular genres became more prominent. In 2016, a Scientific Reports study that examined over 464,000 recordings of popular music recorded between 1955 and 2010 found that, compared to 1960s pop music, contemporary pop music uses a smaller variety of pitch progressions, greater average volume,[40] less diverse instrumentation and recording techniques, and less timbral variety.[41] Scientific American's John Matson reported that this ""seems to support the popular anecdotal observation that pop music of yore was ""better"", or at least more varied, than today's top-40 stuff"". However, he also noted that the study may not have been entirely representative of pop in each generation.[41]
 In the 1960s,  the majority of mainstream pop music fell in two categories: guitar, drum and bass groups or singers backed by a traditional orchestra.[42] Since early in the decade, it was common for pop producers, songwriters, and engineers to freely experiment with musical form, orchestration, unnatural reverb, and other sound effects. Some of the best known examples are Phil Spector's Wall of Sound and Joe Meek's use of homemade electronic sound effects for acts like the Tornados.[43] At the same time, pop music on radio and in both American and British film moved away from refined Tin Pan Alley to more eccentric songwriting and incorporated reverb-drenched electric guitar, symphonic strings, and horns played by groups of properly arranged and rehearsed studio musicians.[44]  A 2019 study held by New York University in which 643 participants had to rank how familiar a pop song is to them, songs from the 1960s turned out to be the most memorable, significantly more than songs from recent years 2000 to 2015.[45]
 Before the progressive pop of the late 1960s, performers were typically unable to decide on the artistic content of their music.[46] Assisted by the mid-1960s economic boom, record labels began investing in artists, giving them the freedom to experiment, and offering them limited control over their content and marketing.[47] This situation declined after the late 1970s and would not reemerge until the rise of Internet stars.[47] Indie pop, which developed in the late 1970s, marked another departure from the glamour of contemporary pop music, with guitar bands formed on the then-novel premise that one could record and release their own music without having to procure a record contract from a major label.[48]
 The 1980s are commonly remembered for an increase in the use of digital recording, associated with the usage of synthesizers, with synth-pop music and other electronic genres featuring non-traditional instruments increasing in popularity.[49] By 2014, pop music worldwide had been permeated by electronic dance music.[50] In 2018, researchers at the University of California, Irvine, concluded that pop music has become 'sadder' since the 1980s. The elements of happiness and brightness have eventually been replaced with electronic beats making pop music more 'sad yet danceable'.[51]
 Pop music has been dominated by the American and (from the mid-1960s) British music industries, whose influence has made pop music something of an international monoculture, but most regions and countries have their own form of pop music, sometimes producing local versions of wider trends, and lending them local characteristics.[53] Some of these trends (for example Europop) have had a significant impact on the development of the genre.[54]
 The story of pop music is largely the story of the intertwining pop culture of the United States and the United Kingdom in the postwar era.
  — Bob Stanley[50] According to Grove Music Online, ""Western-derived pop styles, whether coexisting with or marginalizing distinctively local genres, have spread throughout the world and have come to constitute stylistic common denominators in global commercial music cultures"".[57] Some non-Western countries, such as Japan, have developed a thriving pop music industry, most of which is devoted to Western-style pop. Japan has for several years produced a greater quantity of music than everywhere except the US.[clarification needed][57] The spread of Western-style pop music has been interpreted variously as representing processes of Americanization, homogenization, modernization, creative appropriation, cultural imperialism, or a more general process of globalization.[57]
 One of the pop music styles that developed alongside other music styles is Latin pop, which rose in popularity in the US during the 1950s with early rock and roll success Ritchie Valens.[58] Later, Los Lobos and Chicano rock gained in popularity during the 1970s and 1980s, and musician Selena saw large-scale popularity in the 1980s and 1990s, along with crossover appeal with fans of Tejano musicians Lydia Mendoza and Little Joe.[citation needed] With later Hispanic and Latino Americans seeing success within pop music charts, 1990s pop successes stayed popular in both their original genres and in broader pop music.[59] Latin pop hit singles, such as ""Macarena"" by Los del Río and ""Despacito"" by Luis Fonsi, have seen record-breaking success on worldwide pop music charts.[60]
 Notable pop artists of the late 20th century that became global superstars include Whitney Houston, Michael Jackson, Madonna, George Michael, and Prince.
 At the beginning of the 2000s, the trends that dominated during the late 1990s still continued, but the music industry started to change as people began to download music from the internet. People were able to discover genres and artists that were outside of the mainstream and propel them to fame, but at the same time smaller artists had a harder time making a living because their music was being pirated.[62] Popular artists were Avril Lavigne, Justin Timberlake, NSYNC, Christina Aguilera, Destiny's Child, and Britney Spears. Pop music often came from many different genres, with each genre in turn influencing the next one, blurring the lines between them and making them less distinct. This change was epitomized in Spears' highly influential 2007 album Blackout, which under the influence of producer Danja, mixed the sounds of EDM, avant-funk, R&B, dance music, and hip hop.[63]
 By 2010, pop music impacted by dance music came to be dominant on the charts. Instead of radio setting the trends, it was now the club. At the beginning of the 2010s, Will.i.am stated, ""The new bubble is all the collective clubs around the world. Radio is just doing its best to keep up.""[64] Songs that talked of escapism through partying became the most popular, influenced by the impulse to forget the economic troubles that had taken over the world after the 2008 crash.[65] Throughout the 2010s, a lot of pop music also began to take cues from Alternative pop. Popularized by artists such as Lana Del Rey[66] and Lorde[67] in the early 2010s and later inspiring other highly influential artists including Billie Eilish and Taylor Swift,[68] it gave space to a more sad and moody tone within pop music.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['pop versus art', 'Michael Jackson and Madonna', 'the birth of the modern pop music industry', 'country, blues, and hillbilly music', 'appeal to all'], 'answer_start': [], 'answer_end': []}"
"
 African
 Asian
 Middle Eastern
 European
 North American
 Oceanic
 South American
 Hip-hop or hip hop music, also known as rap, and formerly as disco rap,[7][8] is a genre of popular music that originated in the early 1970s from African Americans and Afro-Caribbean immigrants in the Bronx,[9][10][11][12] a borough of New York City.[13][14][15] Hip-hop music originated as an anti-drug and anti-violence genre[16] consisting of stylized rhythmic music (usually built around drum beats) that often accompanies rapping, a rhythmic delivery of poetic speech.[17] In the early 1990s, a professor of African American studies at Temple University said, ""hip hop is something that blacks can unequivocally claim as their own.""[18] By the 21st century, the field of rappers had diversified by both race and gender. The music developed as part of the broader hip hop culture, a subculture defined by four key stylistic elements: MCing/rapping, DJing/scratching with turntables, breakdancing, and graffiti art.[19][20][21] While often used to refer solely to rapping and rap music, ""hip hop"" more properly denotes the practice of the entire subculture.[22][23] The term hip hop music is sometimes used synonymously with the term rap music,[17][24] though rapping is not a required component of hip hop music; the genre may also incorporate other elements of the culture, including DJing, turntablism, scratching, beatboxing, and instrumental tracks.[25][26]
 Hip hop as both a musical genre and a culture was formed during the 1970s when block parties became increasingly popular in New York City, particularly among African American youth residing in the Bronx.[27] At block parties, DJs played percussive breaks of popular songs using two turntables and a DJ mixer to be able to play breaks from two copies of the same record, alternating from one to the other and extending the ""break"".[28] Hip hop's early evolution occurred as sampling technology and drum machines became widely available and affordable. Turntablist techniques such as scratching and beatmatching developed along with the breaks. Rapping developed as a vocal style in which the artist speaks or chants along rhythmically with an instrumental or synthesized beat.
 Hip hop music was not officially recorded to play on radio or television until 1979, largely due to poverty during the genre's birth and lack of acceptance outside ghetto neighborhoods.[29] Old-school hip hop was the first mainstream wave of the genre, marked by its disco influence and party-oriented lyrics. The 1980s marked the diversification of hip hop as the genre developed more complex styles and spread around the world. New-school hip hop was the genre's second wave, marked by its electro sound, and led into golden age hip hop, an innovative period between the mid-1980s and mid-1990s that also developed hip hop's own album era. The gangsta rap subgenre, focused on the violent lifestyles and impoverished conditions of inner-city African American youth, gained popularity at this time. West Coast hip hop was dominated by G-funk in the early-mid 1990s, while East Coast hip hop was dominated by jazz rap, alternative hip hop, and hardcore hip hop. Hip hop continued to diversify at this time with other regional styles emerging, such as Southern rap and Atlanta hip hop. Hip hop became a best-selling genre in the mid-1990s and the top-selling music genre by 1999.
 The popularity of hip hop music continued through the late 1990s to early-2000s ""bling era"" with hip hop influences increasingly finding their way into other genres of popular music, such as neo soul, nu metal, and R&B. The United States also saw the success of regional styles such as crunk, a Southern genre that emphasized the beats and music more than the lyrics, and alternative hip hop began to secure a place in the mainstream, due in part to the crossover success of its artists. During the late 2000s and early 2010s ""blog era"", rappers were able to build up a following through online methods of music distribution, such as social media and blogs, and mainstream hip hop took on a more melodic, sensitive direction following the commercial decline of gangsta rap. The trap and mumble rap subgenres have become the most popular form of hip hop during the mid-late 2010s and early 2020s. In 2017, rock music was usurped by hip hop as the most popular genre in the United States. In recent years, hip hop's influence has transcended musical boundaries, impacting fashion, language, and cultural trends worldwide.[30][31][32]
 Amidst its evolution, hip hop has also been a vehicle for social commentary and political expression, reflecting the struggles and aspirations of marginalized communities. From its roots in the Bronx to its global reach today, hip hop has served as a voice for the disenfranchised, shedding light on issues such as racial inequality, poverty, and police brutality.[33] Artists such as Public Enemy, Tupac Shakur, and Kendrick Lamar have used their platforms to address systemic injustices, fostering dialogue and inspiring activism. Hip hop's ability to confront societal issues while simultaneously providing a form of empowerment and self-expression has solidified its significance beyond mere entertainment, making it a significant cultural force worldwide.[34]
 The words ""hip"" and ""hop"" in combination have a long history. In the 1950s, older folks referred to teen house parties as ""hippity hops"".[35] The creation of the term hip hop is often credited to Keef Cowboy, rapper with Grandmaster Flash and the Furious Five.[36] However, Lovebug Starski, Keef Cowboy, and DJ Hollywood used the term when the music was still known as disco rap.[37] It is believed that Cowboy created the term while teasing a friend who had just joined the U.S. Army, by scat singing the words ""hip/hop/hip/hop"" in a way that mimicked the rhythmic cadence of soldiers marching.[36] Cowboy later worked the ""hip hop"" cadence into a part of his stage performance. For example, he would say something along the lines of  ""I said a hip-hop, a hibbit, hibby-dibby, hip-hip-hop and you don't stop"",[35] which was quickly used by other artists such as The Sugarhill Gang in ""Rapper's Delight"".[36] Universal Zulu Nation founder Afrika Bambaataa, also known as ""the Godfather"", is credited with first using the term to describe the subculture in which the music belonged; although it is also suggested that it was a derogatory term to describe the type of music.[38] The term was first used in print to refer to the music by reporter Robert Flipping Jr. in a February 1979 article in the New Pittsburgh Courier,[39][40] and to refer to the culture in a January 1982 interview of Afrika Bambaataa by Michael Holman in the East Village Eye.[41] The term gained further currency in September of that year in another Bambaataa interview in The Village Voice,[42] by Steven Hager, later author of a 1984 history of hip hop.[43]
 There are disagreements about whether or not the terms ""hip hop"" and ""rap"" can be used interchangeably, even amongst its most knowledgeable proponents.[8] The most common view is that hip-hop is a cultural movement that emerged in the South Bronx in New York City during the 1970s, with MCing (or rapping) being one of the primary four elements.[8] Hip hop's other three essential elements are graffiti art (or aerosol art), break dancing, and DJing. Rap music has become by far the most celebrated expression of hip hop culture, due to being the easiest to market to a mass audience.[8]
 Musical genres from which hip hop developed include funk, blues, jazz and rhythm and blues recordings from the 1960s, 1950s, and earlier, including several records by Bo Diddley[citation needed] and gospel group The Jubalaires, whose 1946 song ""Noah"" is often named as the first recorded instance of rap.[44][45]
Muhammad Ali's 1963 spoken-word album I Am the Greatest is regarded by some writers as an early example of hip hop.[46][47][better source needed] Pigmeat Markham's 1968 single ""Here Comes the Judge"" is one of several songs said to be the earliest hip hop record.[48] Leading up to hip hop, there were spoken-word artists such as the Last Poets who released their debut album in 1970, and Gil Scott-Heron, who gained a wide audience with his 1971 track ""The Revolution Will Not Be Televised"". These artists combined spoken word and music to create a kind of ""proto-rap"" vibe.[49]
 Hip hop as music and culture formed during the 1970s in New York City from the multicultural exchange between African Americans and children of immigrants from countries in the Caribbean, most notably Jamaica.[50] Hip hop music in its infancy has been described as an outlet and a voice for the disenfranchised youth of marginalized backgrounds and low-income areas, as the hip hop culture reflected the social, economic and political realities of their lives.[51][52] Many of the people who helped establish hip hop culture, including DJ Kool Herc, DJ Disco Wiz, Grandmaster Flash, and Afrika Bambaataa were of Latin American or Caribbean origin.
 It is hard to pinpoint the exact musical influences that most affected the sound and culture of early hip hop because of the multicultural nature of New York—hip hop's early pioneers were influenced by a mix of cultures, due to the city's diversity.[53] The city experienced a heavy Jamaican hip hop influence during the 1990s. This influence was brought on by cultural shifts particularly because of the heightened immigration of Jamaicans to New York and the American-born Jamaican youth who were coming of age during the 1990s.
 In the 1970s, block parties became increasingly popular in New York, particularly among African American, Caribbean and Hispanic youth residing in the Bronx. Block parties incorporated DJs, who played popular genres of music, especially funk and soul music. Due to the positive reception, DJs began isolating the percussive breaks of popular songs. This technique was common in Jamaican dub music,[55] and was largely introduced into New York by immigrants from the Caribbean, including DJ Kool Herc, one of the pioneers of hip hop.[56][57] Herc has repeatedly denied any direct connections between Jamaican musical traditions and early hip hop, stating that his own biggest influence was James Brown, from whom he says rap originated.[58] Even before moving to the U.S., Herc says his biggest influences came from American music:
 I was listening to American music in Jamaica and my favorite artist was James Brown. That's who inspired me. A lot of the records I played were by James Brown.[59] Herc also says that he was not influenced by Jamaican sound system parties, as he was too young to experience them when he was in Jamaica.[60]
 Later in a 2020 interview, Herc stated that he wanted Jamaica to reclaim Hip Hop “because we are the ones who bought the style and the technique to America, which [later] became hip-hop.”[61]
 Because the percussive breaks in funk, soul and disco records were generally short, Herc and other DJs began using two turntables to extend the breaks. On August 11, 1973, DJ Kool Herc was the DJ at his sister's back-to-school party. He extended the beat of a record by using two record players, isolating the percussion ""breaks"" by using a mixer to switch between the two records. Herc's experiments with making music with record players became what we now know as breaking or ""scratching"".[62]
 A second key musical element in hip hop music is emceeing (also called MCing or rapping). Emceeing is the rhythmic spoken delivery of rhymes and wordplay, delivered at first without accompaniment and later done over a beats. This spoken style was influenced by the African American style of ""capping"", a performance where men tried to outdo each other in originality of their language and tried to gain the favor of the listeners.[63] The basic elements of hip hop—boasting raps, rival ""posses"" (groups), uptown ""throw-downs"", and political and social commentary—were all long present in African American music. MCing and rapping performers moved back and forth between the predominance of songs packed with a mix of boasting, 'slackness' and sexual innuendo and a more topical, political, socially conscious style. The role of the MC originally was as a Master of Ceremonies for a DJ dance event. The MC would introduce the DJ and try to pump up the audience. The MC spoke between the DJ's songs, urging everyone to get up and dance. MCs would also tell jokes and use their energetic language and enthusiasm to rev up the crowd. Eventually, this introducing role developed into longer sessions of spoken, rhythmic wordplay, and rhyming, which became rapping.
 By 1979 hip hop music had become a mainstream genre. Herc also developed upon break-beat deejaying,[64] where the breaks of funk songs—the part most suited to dance, usually percussion-based—were isolated and repeated for the purpose of all-night dance parties. This form of music playback, using hard funk and rock, formed the basis of hip hop music. Campbell's announcements and exhortations to dancers would lead to the syncopated, rhymed spoken accompaniment now known as rapping. He dubbed his dancers ""break-boys"" and ""break-girls"", or simply ""b-boys"" and ""b-girls"". According to Herc, ""breaking"" was also street slang for ""getting excited"" and ""acting energetically"".[65]
 DJs such as Grand Wizzard Theodore, Grandmaster Flash, and Jazzy Jay refined and developed the use of breakbeats, including cutting and scratching.[67] As turntable manipulation continued to evolve a new technique that came from it was needle dropping. Needle dropping was created by Grandmaster Flash, it is prolonged short drum breaks by playing two copies of a record simultaneously and moving the needle on one turntable back to the start of the break while the other played.[68] The approach used by Herc was soon widely copied, and by the late 1970s, DJs were releasing 12-inch records where they would rap to the beat. Popular tunes included Kurtis Blow's ""The Breaks"" and the Sugarhill Gang's ""Rapper's Delight"".[69] Herc and other DJs would connect their equipment to power lines and perform at venues such as public basketball courts and at 1520 Sedgwick Avenue, Bronx, New York, now officially a historic building.[70] The equipment consisted of numerous speakers, turntables, and one or more microphones.[71] By using this technique, DJs could create a variety of music, but according to Rap Attack by David Toop ""At its worst the technique could turn the night into one endless and inevitably boring song"".[72] KC the Prince of Soul, a rapper-lyricist with Pete DJ Jones, is often credited with being the first rap lyricist to call himself an ""MC"".[73]
 Street gangs were prevalent in the poverty of the South Bronx, and much of the graffiti, rapping, and b-boying at these parties were all artistic variations on the competition and one-upmanship of street gangs. Sensing that gang members' often violent urges could be turned into creative ones, Afrika Bambaataa founded the Zulu Nation, a loose confederation of street-dance crews, graffiti artists, and rap musicians. By the late 1970s, the culture had gained media attention, with Billboard magazine printing an article titled ""B Beats Bombarding Bronx"", commenting on the local phenomenon and mentioning influential figures such as Kool Herc.[74] The New York City blackout of 1977 saw widespread looting, arson, and other citywide disorders especially in the Bronx[75] where a number of looters stole DJ equipment from electronics stores. As a result, the hip hop genre, barely known outside of the Bronx at the time, grew at an astounding rate from 1977 onward.[76]
 DJ Kool Herc's house parties gained popularity and later moved to outdoor venues to accommodate more people. Hosted in parks, these outdoor parties became a means of expression and an outlet for teenagers, where ""instead of getting into trouble on the streets, teens now had a place to expend their pent-up energy.""[77] Tony Tone, a member of the Cold Crush Brothers, stated that ""hip hop saved a lot of lives"".[77] For inner-city youth, participating in hip hop culture became a way of dealing with the hardships of life as minorities within America, and an outlet to deal with the risk of violence and the rise of gang culture. MC Kid Lucky mentions that ""people used to break-dance against each other instead of fighting"".[78][79] Inspired by DJ Kool Herc, Afrika Bambaataa created a street organization called Universal Zulu Nation, centered on hip hop, as a means to draw teenagers out of gang life, drugs and violence.[77]
 The lyrical content of many early rap groups focused on social issues, most notably in the seminal track ""The Message"" by Grandmaster Flash and the Furious Five, which discussed the realities of life in the housing projects.[80] ""Young black Americans coming out of the civil rights movement have used hip hop culture in the 1980s and 1990s to show the limitations of the Hip Hop Movement.""[81] Hip hop gave young African Americans a voice to let their issues be heard; ""Like rock-and-roll, hip hop is vigorously opposed by conservatives because it romanticises violence, law-breaking, and gangs"".[81] It also gave people a chance for financial gain by ""reducing the rest of the world to consumers of its social concerns.""[81]
 In late 1979, Debbie Harry of Blondie took Nile Rodgers of Chic to such an event, as the main backing track used was the break from Chic's ""Good Times"".[69] The new style influenced Harry, and Blondie's later hit single from 1981 ""Rapture"" became the first single containing hip hop elements to hit number one on the U.S. Billboard Hot 100—the song itself is usually considered new wave and fuses heavy pop music elements, but there is an extended rap by Harry near the end.
 Boxer Muhammad Ali, as an influential African American celebrity, was widely covered in the media. Ali influenced several elements of hip hop music. Both in the boxing ring and in media interviews, Ali became known in the 1960s for being ""rhyming trickster"". Ali used a ""funky delivery"" for his comments, which included ""boasts, comical trash talk, [and] the endless quotabl[e]"" lines.[82] According to Rolling Stone, his ""freestyle skills"" (a reference to a type of vocal improvisation in which lyrics are recited with no particular subject or structure) and his ""rhymes, flow, and braggadocio"" would ""one day become typical of old-school MCs"" like Run-DMC and LL Cool J,[83] the latter citing Ali as an influence.[82] Hip hop music in its infancy has been described as an outlet and a ""voice"" for the disenfranchised youth of low-income and marginalized economic areas,[51] as the hip hop culture reflected the social, economic and political realities of their lives.[52]
 Hip hop's early evolution occurred around the time that sampling technology and drum-machines became widely available to the general public at a cost that was affordable to the average consumer—not just professional studios. Drum-machines and samplers were combined in machines that came to be known as MPC's or 'Music Production Centers', early examples of which would include the Linn 9000. The first sampler that was broadly adopted to create this new kind of music was the Mellotron used in combination with the TR-808 drum machine. Mellotrons and Linn's were succeeded by the Akai, in the late 1980s.[84]
 Turntablist techniques – such as rhythmic ""scratching"" (pushing a record back and forth while the needle is in the groove to create new sounds and sound effects, an approach attributed to Grand Wizzard Theodore[85][86]), beat mixing and/or beatmatching, and beat juggling – eventually developed along with the percussion breaks, creating a musical accompaniment or base that could be rapped over in a manner similar to signifying.
 Rapping, also referred to as MCing or emceeing, is a vocal style in which the artist speaks lyrically and rhythmically, in rhyme and verse, generally to an instrumental or synthesized beat. Beats, almost always in 4/4 time signature, can be created by sampling and/or sequencing portions of other songs by a producer. They also incorporate synthesizers, drum machines, and live bands. Rappers may write, memorize, or improvise their lyrics and perform their works a cappella or to a beat. Hip hop music predates the introduction of rapping into hip hop culture, and rap vocals are absent from many hip hop tracks, such as ""Hip Hop, Be Bop (Don't Stop)"" by Man Parrish; ""Chinese Arithmetic"" by Eric B. & Rakim; ""Al-Naafiysh (The Soul)"" and ""We're Rocking the Planet"" by Hashim; and ""Destination Earth"" by Newcleus. However, the majority of the genre has been accompanied by rap vocals, such as the Sci-fi influenced electro hip hop group Warp 9.[87] Female rappers appeared on the scene in the late 1970s and early 80s, including Bronx artist MC Sha-Rock, member of the Funky Four Plus One, credited with being the first female MC[88] and the Sequence, a hip hop trio signed to Sugar Hill Records, the first all female group to release a rap record, Funk You Up.[citation needed]
 The roots of rapping are found in African American music and bear similarities to traditional African music, particularly that of the griots[89] of West African culture.[90] The African American traditions of signifyin', the dozens, and jazz poetry all influence hip hop music, as well as the call and response patterns of African and African American religious ceremonies. Early popular radio disc jockeys of the Black-appeal radio period broke into broadcast announcing by using these techniques under the jive talk of the post WWII swing era in the late 1940s and the 1950s.[91] DJ Nat D. was the M.C. at one of the most pitiless places for any aspiring musician trying to break into show business, Amateur Night at the Palace theatre on Beale Street in Memphis, Tennessee. There he was master of ceremonies from 1935 until 1947 along with his sideman, D.J.Rufus Thomas. It was there he perfected the dozens, signifyin' and the personality jock jive patter that would become his schtick when he became the first black radio announcer on the air south of the Mason–Dixon line.[92] Jive popularized black appeal radio, it was the language of the black youth, the double entendres and slightly obscene wordplay was a godsend to radio, re-invigorating ratings at flagging outlets that were losing audience share and flipping to the new format of R&B with black announcers. The 10% of African Americans who heard his broadcasts found that the music he promoted on radio in 1949 was also in the jukeboxes up north in the cities. They were also finding other D.J's like Chicago's Al Benson on WJJD, Austin's Doctor Hep Cat on KVET and Atlanta's Jockey Jack on WERD speaking the same rhyming, cadence laden rap style.[93] Once the white owned stations realized the new upstarts were grabbing their black market share and that Big Band and swing jazz was no longer 'hip', some white DJ's emulated the southern 'mushmouth' and jive talk, letting their audience think they too were African American, playing the blues and Be-Bop.[94] John R Richbourg had a southern drawl that listeners to Nashville's WLAC[95] nighttime R&B programming were never informed belonged not to a black D.J., as were other white DJ's at the station. Dr. Hep Cat's rhymes were published in a dictionary of jive talk, The Jives of Dr. Hepcat, in 1953. Jockey jack is the infamous Jack the Rapper of Family Affair fame, after his radio convention that was a must attend for every rap artist in the 1980s and 1990s[96] These jive talking rappers of the 1950s black appeal radio format were the source and inspiration of Soul singer James Brown, and musical 'comedy' acts such as Rudy Ray Moore, Pigmeat Markham and Blowfly that are often considered ""godfathers"" of hip hop music.[97] Within New York City, performances of spoken-word poetry and music by artists such as the Last Poets, Gil Scott-Heron[98] and Jalal Mansur Nuriddin had a significant impact on the post-civil rights era culture of the 1960s and '1970s, and thus the social environment in which hip hop music was created.
 AM radio at many stations were limited by the 'broadcast Day' as special licenses were required to transmit at night. Those that had such licenses were heard far out to sea and in the Caribbean, where Jocko Henderson and Jockey Jack were American DJs who were listened to at night from broadcast transmitters located in Miami, Florida. Jocko came to have an outsized influence on Jamaican Emcees during the '50s as the R&B music played on the Miami stations was different from that played on JBC, which re-broadcast BBC and local music styles. In Jamaica, DJs would set up large roadside sound systems in towns and villages, playing music for informal gatherings, mostly folks who wandered down from country hills looking for excitement at the end of the week. There the DJs would allow 'Toasts' by an Emcee, which copied the style of the American DJs listened to on AM transistor radios. It was by this method that Jive talk, rapping and rhyming was transposed to the island and locally the style was transformed by 'Jamaican lyricism', or the local patois.
 Hip hop as music and culture formed during the 1970s in New York City from the multicultural exchange between African American youth from the United States and young immigrants and children of immigrants from countries in the Caribbean.[50] Some were influenced by the vocal style of the earliest African American radio MCs (including Jocko Henderson's Rocket Ship Show of the 1950s, which rhymed and was influenced by scat singing), which could be heard over the radio in Jamaica.
 The first records by Jamaican DJs, including Sir Lord Comic (The Great Wuga Wuga, 1967) came as part of the local dance hall culture, which featured 'specials,' unique mixes or 'versions' pressed on soft discs or acetate discs, and rappers (called DJs) such as King Stitt, Count Machuki, U-Roy, I-Roy, Big Youth and many others. Recordings of talk-over, which is a different style from the dancehall's DJ style, were also made by Jamaican artists such as Prince Buster and Lee ""Scratch"" Perry (Judge Dread) as early as 1967, somehow rooted in the 'talking blues' tradition. The first full-length Jamaican DJ record was a duet on a Rastafarian topic by Kingston ghetto dwellers U-Roy and Peter Tosh named Righteous Ruler (produced by Lee ""Scratch"" Perry in 1969). The first DJ hit record was Fire Corner by Coxsone's Downbeat sound system DJ, King Stitt that same year; 1970 saw a multitude of DJ hit records in the wake of U-Roy's early, massive hits, most famously Wake the Town and many others. As the tradition of remix (which also started in Jamaica where it was called 'version' and 'dub') developed, established young Jamaican DJ/rappers from that period, who had already been working for sound systems for years, were suddenly recorded and had many local hit records, widely contributing to the reggae craze triggered by Bob Marley's impact in the 1970s. The main Jamaican DJs of the early 1970s were King Stitt, Samuel the First, Count Machuki, Johnny Lover (who 'versioned' songs by Bob Marley and the Wailers as early as 1971), Dave Barker, Scotty, Lloyd Young, Charlie Ace and others, as well as soon-to-be reggae stars U-Roy, Dennis Alcapone, I-Roy, Prince Jazzbo, Prince Far I, Big Youth and Dillinger. Dillinger scored the first international rap hit record with Cocaine in my Brain in 1976 (based on the Do It Any Way You Wanna Do rhythm by the People's Choice as re-recorded by Sly and Robbie), where he even used a New York accent, consciously aiming at the new NYC rap market. The Jamaican DJ dance music was deeply rooted in the sound system tradition that made music available to poor people in a very poor country where live music was only played in clubs and hotels patronized by the middle and upper classes. By 1973 Jamaican sound system enthusiast DJ Kool Herc moved to the Bronx, taking with him Jamaica's sound system culture, and teamed up with another Jamaican, Coke La Rock, at the mike. Although other influences, most notably musical sequencer Grandmaster Flowers of Brooklyn and Grandwizard Theodore of the Bronx contributed to the birth of hip hop in New York, and although it was downplayed in most US books about hip hop, the main root of this sound system culture was Jamaican. The roots of rap in Jamaica are explained in detail in Bruno Blum's book, 'Le Rap'.[99]
 DJ Kool Herc and Coke La Rock provided an influence on the vocal style of rapping by delivering simple poetry verses over funk music breaks, after party-goers showed little interest in their previous attempts to integrate reggae-infused toasting into musical sets.[55][100] DJs and MCs would often add call and response chants, often consisting of a basic chorus, to allow the performer to gather his thoughts (e.g. ""one, two, three, y'all, to the beat""). Later, the MCs grew more varied in their vocal and rhythmic delivery, incorporating brief rhymes, often with a sexual or scatological theme, in an effort to differentiate themselves and to entertain the audience. These early raps incorporated the dozens, a product of African American culture. Kool Herc & the Herculoids were the first hip hop group to gain recognition in New York,[100] but the number of MC teams increased over time.
 Often these were collaborations between former gangs, such as Afrikaa Bambaataa's Universal Zulu Nation—now an international organization. Melle Mel, a rapper with the Furious Five is often credited with being the first rap lyricist to call himself an ""MC"".[101] During the early 1970s B-boying arose during block parties, as b-boys and b-girls got in front of the audience to dance in a distinctive and frenetic style. The style was documented for release to a worldwide audience for the first time in documentaries and movies such as Style Wars, Wild Style, and Beat Street. The term ""B-boy"" was coined by DJ Kool Herc to describe the people who would wait for the break section of the song, showing off athleticism, spinning on the stage to 'break-dance' in the distinctive, frenetic style.[102]
 Although there were some early MCs that recorded solo projects of note, such as DJ Hollywood, Kurtis Blow, and Spoonie Gee, the frequency of solo artists did not increase until later with the rise of soloists with stage presence and drama, such as LL Cool J. Most early hip hop was dominated by groups where collaboration between the members was integral to the show.[103] An example would be the early hip hop group Funky Four Plus One, who performed in such a manner on Saturday Night Live in 1981.[104]
 The earliest hip hop music was performed live, at house parties and block party events, and it was not recorded. Prior to 1979, recorded hip hop music consisted mainly of PA system soundboard recordings of live party shows and early hip hop mixtapes by DJs. Puerto Rican DJ Disco Wiz is credited as the first hip hop DJ to create a ""mixed plate,"" or mixed dub recording, when, in 1977, he combined sound bites, special effects and paused beats to technically produce a sound recording.[105] The first hip hop record is widely regarded to be the Sugarhill Gang's ""Rapper's Delight"", from 1979. It was the first hip hop record to gain widespread popularity in the mainstream and was where hip hop music got its name from (from the opening bar).[106] However, much controversy surrounds this assertion as some regard the March 1979 single ""King Tim III (Personality Jock)"" by the Fatback Band, as a rap record.[107] There are various other claimants for the title of first hip hop record.
 By the early 1980s, all the major elements and techniques of the hip hop genre were in place, and by 1982, the electronic (electro) sound had become the trend on the street and in dance clubs. New York City radio station WKTU featured Warp 9's ""Nunk"", in a commercial to promote the station's signature sound of emerging hip hop[108] Though not yet mainstream, hip hop had begun to permeate the music scene outside of New York City; it could be found in cities as diverse as Los Angeles, Atlanta, Chicago, Washington, D.C., Baltimore, Dallas, Kansas City, San Antonio, Miami, Seattle, St. Louis, New Orleans, Houston, and Toronto. Indeed, ""Funk You Up"" (1979), the first hip hop record released by a female group, and the second single released by Sugar Hill Records, was performed by the Sequence, a group from Columbia, South Carolina which featured Angie Stone.[109] Despite the genre's growing popularity, Philadelphia was, for many years, the only city whose contributions could be compared to New York City's. Hip hop music became popular in Philadelphia in the late 1970s. The first released record was titled ""Rhythm Talk"", by Jocko Henderson.
 The New York Times had dubbed Philadelphia the ""Graffiti Capital of the World"" in 1971. Philadelphia native DJ Lady B recorded ""To the Beat Y'All"" in 1979, and became the first female solo hip hop artist to record music.[110] Schoolly D, starting in 1984 and also from Philadelphia, began creating a style that would later be known as gangsta rap.
 Hip hop music was influenced by disco music, as disco also emphasized the key role of the DJ in creating tracks and mixes for dancers, and old school hip hop often used disco tracks as beats. At the same time however, hip hop music was also a backlash against certain subgenres of late 1970s disco. While the early disco was African American and Italian-American-created underground music developed by DJs and producers for the dance club subculture, by the late 1970s, disco airwaves were dominated by mainstream, expensively recorded music industry-produced disco songs. According to Kurtis Blow, the early days of hip hop were characterized by divisions between fans and detractors of disco music. Hip hop had largely emerged as ""a direct response to the watered down, Europeanised, disco music that permeated the airwaves"".[111][112] The earliest hip hop was mainly based on hard funk loops sourced from vintage funk records. By 1979, disco instrumental loops/tracks had become the basis of much hip hop music. This genre was called ""disco rap"". Ironically, the rise of hip hop music also played a role in the eventual decline in disco's popularity.
 The disco sound had a strong influence on early hip hop music. Most of the early rap/hip-hop songs were created by isolating existing disco bass-guitar bass lines and dubbing over them with MC rhymes. the Sugarhill Gang used Chic's ""Good Times"" as the foundation for their 1979 hit ""Rapper's Delight"", generally considered to be the song that first popularized rap music in the United States and around the world. In 1982, Afrika Bambaataa released the single ""Planet Rock"", which incorporated electronica elements from Kraftwerk's ""Trans-Europe Express"" and ""Numbers"" as well as YMO's ""Riot in Lagos"". The Planet Rock sound also spawned a hip-hop electronic dance trend, electro music, which included songs such as Planet Patrol's ""Play at Your Own Risk"" (1982), C Bank's ""One More Shot"" (1982), Cerrone's ""Club Underworld"" (1984), Shannon's ""Let the Music Play"" (1983), Freeez's ""I.O.U."" (1983), Midnight Star's ""Freak-a-Zoid"" (1983), Chaka Khan's ""I Feel For You"" (1984).
 DJ Pete Jones, Eddie Cheeba, DJ Hollywood, and Love Bug Starski were disco-influenced hip hop DJs. Their styles differed from other hip hop musicians who focused on rapid-fire rhymes and more complex rhythmic schemes. Afrika Bambaataa, Paul Winley, Grandmaster Flash, and Bobby Robinson were all members of third s latter group. In Washington, D.C. go-go emerged as a reaction against disco and eventually incorporated characteristics of hip hop during the early 1980s. The DJ-based genre of electronic music behaved similarly, eventually evolving into underground styles known as house music in Chicago and techno in Detroit.
 The 1980s marked the diversification of hip hop as the genre developed more complex styles.[113] New York City became a veritable laboratory for the creation of new hip hop sounds. Early examples of the diversification process can be heard in tracks such as Grandmaster Flash's ""The Adventures of Grandmaster Flash on the Wheels of Steel"" (1981), a single consisting entirely of sampled tracks[114] as well as Afrika Bambaataa's ""Planet Rock"" (1982), and Warp 9's ""Nunk"", (1982)[115] which signified the fusion of hip hop music with electro. In addition, Rammellzee & K-Rob's ""Beat Bop"" (1983) was a 'slow jam' which had a dub influence with its use of reverb and echo as texture and playful sound effects. ""Light Years Away"", by Warp 9 (1983), (produced and written by Lotti Golden and Richard Scher) described as a ""cornerstone of early 80s beatbox afrofuturism,"" by the UK paper, The Guardian,[87] introduced social commentary from a sci-fi perspective. In the 1970s, hip hop music typically used samples from funk and later, from disco. The mid-1980s marked a paradigm shift in the development of hip hop, with the introduction of samples from rock music, as demonstrated in the albums King of Rock and Licensed to Ill. Hip hop prior to this shift is characterized as old-school hip hop.
 In 1980, the Roland Corporation launched the TR-808 Rhythm Composer. It was one of the earliest programmable drum machines, with which users could create their own rhythms rather than having to use preset patterns. Though it was a commercial failure, over the course of the decade the 808 attracted a cult following among underground musicians for its affordability on the used market,[116] ease of use,[117] and idiosyncratic sounds, particularly its deep, ""booming"" bass drum.[118] It became a cornerstone of the emerging electronic, dance, and hip hop genres, popularized by early hits such as Afrika Bambaataa and the Soulsonic Force's ""Planet Rock"".[119] The 808 was eventually used on more hit records than any other drum machine;[120] its popularity with hip hop in particular has made it one of the most influential inventions in popular music, comparable to the Fender Stratocaster's influence on rock.[121][122]
 Over time sampling technology became more advanced. However, earlier producers such as Marley Marl used drum machines to construct their beats from small excerpts of other beats in synchronisation, in his case, triggering three Korg sampling-delay units through a Roland 808. Later, samplers such as the E-mu SP-1200 allowed not only more memory but more flexibility for creative production. This allowed the filtration and layering different hits, and with a possibility of re-sequencing them into a single piece. With the emergence of a new generation of samplers such as the AKAI S900 in the late 1980s, producers did not have to create complex, time-consuming tape loops. Public Enemy's first album was created with the help of large tape loops. The process of looping a break into a breakbeat now became more commonly done with a sampler, now doing the job which so far had been done manually by the DJs using turntables. In 1989, DJ Mark James, under the moniker ""45 King"", released ""The 900 Number"", a breakbeat track created by synchronizing samplers and vinyl records.[103]
 The lyrical content and other instrumental accompaniment of hip hop developed as well. The early lyrical styles in the 1970, which tended to be boasts and clichéd chants, were replaced with metaphorical lyrics exploring a wider range of subjects. As well, the lyrics were performed over more complex, multi-layered instrumental accompaniment. Artists such as Melle Mel, Rakim, Chuck D, KRS-One and Warp 9 revolutionized hip hop by transforming it into a more mature art form, with sophisticated arrangements, often featuring ""gorgeous textures and multiple layers""[123] The influential single ""The Message"" (1982) by Grandmaster Flash and the Furious Five is widely considered to be the pioneering force for conscious rap.
 Independent record labels like Tommy Boy, Prism Records and Profile Records became successful in the early 1980s, releasing records at a furious pace in response to the demand generated by local radio stations and club DJs. Early 1980s electro music and rap were catalysts that sparked the hip hop movement, led by artists such as Cybotron, Hashim, Afrika Bambaataa, Planet Patrol, Newcleus and Warp 9. In the New York City recording scene, artists collaborated with producer/writers such as Arthur Baker, John Robie, Lotti Golden and Richard Scher, exchanging ideas that contributed to the development of hip hop.[124] Some rappers eventually became mainstream pop performers. Kurtis Blow's appearance in a Sprite soda pop commercial[125] marked the first hip hop musician to do a commercial for a major product. The 1981 songs ""Rapture"" by Blondie and ""Christmas Wrapping"" by the new wave band the Waitresses were among the first pop songs to use rap. In 1982, Afrika Bambaataa introduced hip hop to an international audience with ""Planet Rock.""
 Prior to the 1980s, hip hop music was largely confined within the context of the United States. However, during the 1980s, it began its spread and became a part of the music scene in dozens of countries. Greg Wilson was the first DJ to introduce electro hip hop to UK club audiences in the early 1980s, opting for the dub or instrumental versions of Nunk by Warp 9, Extra T's ""ET Boogie"", Hip Hop, Be Bop (Don't Stop) by Man Parrish, Planet Rock and Dirty Talk.[126]
 In the early part of the decade, B-boying became the first aspect of hip hop culture to reach Japan, Australia and South Africa. In South Africa, the breakdance crew Black Noise established the practice before beginning to rap later in the decade. Musician and presenter Sidney became France's first black TV presenter with his show H.I.P. H.O.P.[127] which screened on TF1 during 1984, a first for the genre worldwide. Sidney is considered the father of French hip hop. Radio Nova helped launch other French hip hop stars including Dee Nasty, whose 1984 album Paname City Rappin' along with compilations Rapattitude 1 and 2 contributed to a general awareness of hip hop in France.
 Hip hop has always kept a very close relationship with the Hispanic community in New York. DJ Disco Wiz and the Rock Steady Crew were among early innovators from Puerto Rico, combining English and Spanish in their lyrics. the Mean Machine recorded their first song under the label ""Disco Dreams"" in 1981, while Kid Frost from Los Angeles began his career in 1982. Cypress Hill was formed in 1988 in the suburb of South Gate outside Los Angeles when Senen Reyes (born in Havana) and his younger brother Ulpiano Sergio (Mellow Man Ace) moved from Cuba to South Gate with his family in 1971. They teamed up with DVX from Queens (New York), Lawrence Muggerud (DJ Muggs) and Louis Freese (B-Real), a Mexican/Cuban-American native of Los Angeles. After the departure of ""Ace"" to begin his solo career, the group adopted the name of Cypress Hill named after a street running through a neighborhood nearby in South Los Angeles.
 
Japanese hip hop is said to have begun when Hiroshi Fujiwara returned to Japan and started playing hip hop records in the early 1980s.[128] Japanese hip hop generally tends to be most directly influenced by old school hip hop, taking the era's catchy beats, dance culture, and overall fun and carefree nature and incorporating it into their music. Hip hop became one of the most commercially viable mainstream music genres in Japan, and the line between it and pop music is frequently blurred. The new school of hip hop was the second wave of hip hop music, originating in 1983–84 with the early records of Run-D.M.C. and LL Cool J. As with the hip hop preceding it (which subsequently became known as old-school hip hop), the new school came predominantly from New York City. The new school was initially characterized in form by drum machine-led minimalism, with influences from rock music, a hip hop ""metal music for the 80s–a hard-edge ugly/beauty trance as desperate and stimulating as New York itself.""[129] It was notable for taunts and boasts about rapping, and socio-political commentary, both delivered in an aggressive, self-assertive style. In image as in song its artists projected a tough, cool, street b-boy attitude.
 These elements contrasted sharply with much of the previous funk- and disco-influenced hip hop groups, whose music was often characterized by novelty hits, live bands, synthesizers, and ""party rhymes"" (not all artists prior to 1983–84 had these styles). New-school artists made shorter songs that could more easily gain radio play, and they produced more cohesive LP albums than their old-school counterparts. By 1986, their releases began to establish the hip-hop album as a fixture of mainstream music. Hip hop music became commercially successful, as exemplified by the Beastie Boys' 1986 album Licensed to Ill, which was the first rap album to hit No. 1 on the Billboard charts.[130]
 Hip hop's ""golden age"" (or ""golden era"") is a name given to a period in mainstream hip hop, produced between the mid-1980s and the mid-1990s,[131][132][133] which is characterized by its diversity, quality, innovation and influence.[134][135] There were strong themes of Afrocentrism and political militancy in golden age hip hop lyrics. The music was experimental and the sampling drew on eclectic sources.[136] There was often a strong jazz influence in the music. The artists and groups most often associated with this phase are Public Enemy, Boogie Down Productions, Eric B. & Rakim, De La Soul, A Tribe Called Quest, Gang Starr, Big Daddy Kane and the Jungle Brothers.[137] The Digable Planets made unique contributions as well, earning a Grammy in 1993.
 The golden age is noted for its innovation – a time ""when it seemed that every new single reinvented the genre""[138] according to Rolling Stone. Referring to ""hip-hop in its golden age"",[139] Spin's editor-in-chief Sia Michel says, ""there were so many important, groundbreaking albums coming out right about that time"",[139]
and MTV's Sway Calloway adds: ""The thing that made that era so great is that nothing was contrived. Everything was still being discovered and everything was still innovative and new"".[140] Writer William Jelani Cobb says ""what made the era they inaugurated worthy of the term golden was the sheer number of stylistic innovations that came into existence... in these golden years, a critical mass of mic prodigies were literally creating themselves and their art form at the same time"".[141]
 The golden age spans ""from approximately 1986 to 1997"", according to Carl Stoffers of New York Daily News.[131] In their article ""In Search of the Golden Age Hip-Hop Sound"", music theorists Ben Duinker and Denis Martin of Empirical Musicology Review use ""the 11 years between and including 1986 and 1996 as chronological boundaries"" to define the golden age, beginning with the releases of Run-DMC's Raising Hell and the Beastie Boys' Licensed to Ill, and ending with the deaths of Tupac Shakur and the Notorious B.I.G.[133] The Boombox writer Todd ""Stereo"" Williams also cites the May 1986 release of Raising Hell (which sold more than three million copies) as the start of the period and notes that over the next year other important albums were released to success, including Licensed to Ill, Boogie Down Productions' Criminal Minded (1987), Public Enemy's Yo! Bum Rush the Show (1987), and Eric B. & Rakim's Paid in Full (1987). Williams views this development as the beginning of hip hop's own ""album era"" from the late 1980s to the late 1990s, during which hip hop albums earned an unprecedented critical recognition and ""would be the measuring stick by which most of the genre's greats would be judged"".[142]
 Many black rappers—including Ice-T and Sister Souljah—contend that they are being unfairly singled out because their music reflects deep changes in society not being addressed anywhere else in the public forum. The white politicians, the artists complain, neither understand the music nor desire to hear what's going on in the devastated communities that gave birth to the art form.
 — Chuck Philips, Los Angeles Times, 1992[143] Gangsta rap is a subgenre of hip hop that reflects the violent lifestyles of inner-city American black youths.[144] Gangsta is a non-rhotic pronunciation of the word gangster. The genre was pioneered in the mid-1980s by rappers such as Schoolly D and Ice-T, and was popularized in the later part of the 1980s by groups like N.W.A. In 1985 Schoolly D released ""P.S.K. What Does It Mean?"", which is often regarded as the first gangsta rap song, which was followed by Ice-T's ""6 in the Mornin'"" in 1986. After the national attention and controversy that Ice-T and N.W.A created in the late 1980s and early 1990s, as well as the mainstreaming of G-funk in the mid-1990s, gangsta rap became the most commercially-lucrative subgenre of hip hop. Some gangsta rappers were known for mixing the political and social commentary of political rap with the criminal elements and crime stories found in gangsta rap.[145]
 N.W.A is the group most frequently associated with the founding of gangsta rap. Their lyrics were more violent, openly confrontational, and shocking than those of established rap acts, featuring incessant profanity and, controversially, use of the word ""nigga"". These lyrics were placed over rough, rock guitar-driven beats, contributing to the music's hard-edged feel. The first blockbuster gangsta rap album was N.W.A's Straight Outta Compton, released in 1988. Straight Outta Compton would establish West Coast hip hop as a vital genre, and establish Los Angeles as a legitimate rival to hip hop's long-time capital, New York City. Straight Outta Compton sparked the first major controversy regarding hip hop lyrics when their song ""Fuck tha Police"" earned a letter from FBI Assistant Director, Milt Ahlerich, strongly expressing law enforcement's resentment of the song.[146][147]
 Controversy surrounded Ice-T's album Body Count, in particular over its song ""Cop Killer"". The song was intended to speak from the viewpoint of a criminal getting revenge on racist, brutal cops. Ice-T's rock song infuriated government officials, the National Rifle Association of America and various police advocacy groups.[148][149] Consequently, Time Warner Music refused to release Ice-T's upcoming album Home Invasion because of the controversy surrounding ""Cop Killer"".[150] Ice-T suggested that the furor over the song was an overreaction, telling journalist Chuck Philips ""...they've done movies about nurse killers and teacher killers and student killers. [Actor] Arnold Schwarzenegger blew away dozens of cops as the Terminator. But I don't hear anybody complaining about that."" In the same interview, Ice-T suggested to Philips that the misunderstanding of Cop Killer and the attempts to censor it had racial overtones: ""The Supreme Court says it's OK for a white man to burn a cross in public. But nobody wants a black man to write a record about a cop killer.""[148]
 The subject matter inherent in gangsta rap more generally has caused controversy. The White House administrations of both George H. W. Bush and Bill Clinton criticized the genre.[143] ""The reason why rap is under attack is because it exposes all the contradictions of American culture ...What started out as an underground art form has become a vehicle to expose a lot of critical issues that are not usually discussed in American politics. The problem here is that the White House and wanna-bes like Bill Clinton represent a political system that never intends to deal with inner city urban chaos,"" Sister Souljah told The Times.[143] Due to the influence of Ice-T and N.W.A, gangsta rap is often viewed as a primarily West Coast phenomenon, despite the contributions of East Coast acts like Schoolly D and Boogie Down Productions in shaping the genre.
 
In 1990, Public Enemy's Fear of a Black Planet was a significant success with music critics and consumers.[151] The album played a key role in hip hop's mainstream emergence in 1990, dubbed by Billboard editor Paul Grein as ""the year that rap exploded"".[151] In a 1990 article on its commercial breakthrough, Janice C. Thompson of Time wrote that hip hop ""has grown into the most exciting development in American pop music in more than a decade.""[152] Thompson noted the impact of Public Enemy's 1989 single ""Fight the Power"", rapper Tone Lōc's single Wild Thing being the best-selling single of 1989, and that at the time of her article, nearly a third of the songs on the Billboard Hot 100 were hip hop songs.[152] In a similar 1990 article, Robert Hilburn of the Los Angeles Times put hip hop music's commercial emergence into perspective: It was 10 years ago that the Sugarhill Gang's ""Rapper's Delight"" became the first rap single to enter the national Top 20. Who ever figured then that the music would even be around in 1990, much less produce attractions that would command as much pop attention as Public Enemy and N.W.A? ""Rapper's Delight"" was a novelty record that was considered by much of the pop community simply as a lightweight offshoot of disco—and that image stuck for years. Occasional records—including Grandmaster Flash's ""The Message"" in 1982 and Run-DMC's ""It's Like That"" in 1984—won critical approval, but rap, mostly, was dismissed as a passing fancy—too repetitious, too one dimensional. Yet rap didn't go away, and an explosion of energy and imagination in the late 1980s leaves rap today as arguably the most vital new street-oriented sound in pop since the birth of rock in the 1950s.[153] Rap is the rock 'n' roll of the day. Rock 'n' roll was about attitude, rebellion, a big beat, sex and, sometimes, social comment. If that's what you're looking for now, you're going to find it here.
 — Bill Adler, Time, 1990[152] MC Hammer hit mainstream success with the multi platinum album Please Hammer, Don't Hurt 'Em. The record reached No. 1 and the first single, ""U Can't Touch This"" charted on the top ten of the Billboard Hot 100. MC Hammer became one of the most successful rappers of the early nineties and one of the first household names in the genre. The album raised rap music to a new level of popularity. It was the first hip-hop album certified diamond by the RIAA for sales of over ten million.[154] It remains one of the genre's all-time best-selling albums.[155] To date, the album has sold as many as 18 million units.[156][157][158][159] Released in 1990, ""Ice Ice Baby"" by Vanilla Ice was the first hip hop single to top the Billboard charts in the U.S. It also reached number one in the UK, Australia among others and has been credited for helping diversify hip hop by introducing it to a mainstream audience.[160] In 1992, Dr. Dre released The Chronic. As well as helping to establish West Coast gangsta rap as more commercially viable than East Coast hip hop,[161] this album founded a style called G Funk, which soon came to dominate West Coast hip hop. The style was further developed and popularized by Snoop Dogg's 1993 album Doggystyle. However, hip hop was still met with resistance from black radio, including urban contemporary radio stations. Russell Simmons said in 1990, ""Black radio [stations] hated rap from the start and there's still a lot of resistance to it"".[153]
 Despite the lack of support from some black radio stations, hip hop became a best-selling music genre in the mid-1990s and the top selling music genre by 1999 with 81 million CDs sold.[162][163][164] By the late 1990s hip hop was artistically dominated by the Wu-Tang Clan, Diddy and the Fugees.[161] The Beastie Boys continued their success throughout the decade crossing color lines and gaining respect from many different artists. Record labels based out of Atlanta, St. Louis, and New Orleans also gained fame for their local scenes. The midwest rap scene was known for fast vocal styles from artists such as Bone Thugs-n-Harmony, Tech N9ne, and Twista. By the end of the decade, hip hop was an integral part of popular music, and many American pop songs had hip hop components.
 Hip hop has been described as a ""mainstream subculture"". The main reasons why hip hop culture secured its subcultural authority despite becoming a part of the mass media and mainstream industries can be summarized as follows. First, hip hop artists promoted symbolic and conspicuous consumption in their music from a very early stage. Second, the continuing display of resistance in hip-hop has continuously attracted new generations of rebellious fans. Third, owing to the subcultural ideal of rising from the underground, the hip hop scene has remained committed to its urban roots. Fourth, the concept of battle rap has prevented hip-hop music from excessive cultural dilution. Finally, the solidarity within the African American community has shielded the subculture from erosion through mainstream commercialization.[165]
 The East Coast–West Coast hip hop rivalry was a feud from 1991 to 1997 between artists and fans of the East Coast hip hop and West Coast hip hop scenes in the United States, especially from 1994 to 1997. Focal points of the feud were East Coast-based rapper the Notorious B.I.G. (and his New York-based label, Bad Boy Records) and West Coast-based rapper Tupac Shakur (and his Los Angeles-based label, Death Row Records). This rivalry started before the rappers themselves hit the scene. Because New York is the birthplace of hip-hop, artists from the West Coast felt as if they were not receiving the same media coverage and public attention as the East Coast.[166] As time went on both rappers began to grow in fame and as they both became more known the tensions continued to arise. Eventually both artists were fatally shot following drive-by shootings by unknown assailants in 1997 and 1996, respectively.
 In the early 1990s East Coast hip hop was dominated by the Native Tongues posse, which was loosely composed of De La Soul with producer Prince Paul, A Tribe Called Quest, the Jungle Brothers, as well as their loose affiliates 3rd Bass, Main Source, and the less successful Black Sheep and KMD. Although originally a ""daisy age"" conception stressing the positive aspects of life, darker material (such as De La Soul's thought-provoking ""Millie Pulled a Pistol on Santa"") soon crept in. Artists such as Masta Ace (particularly for SlaughtaHouse), Brand Nubian, Public Enemy, Organized Konfusion, and Tragedy Khadafi had a more overtly-militant pose, both in sound and manner. In 1993, the Wu-Tang Clan's Enter the Wu-Tang (36 Chambers) revitalized the New York hip hop scene by pioneering an East Coast hardcore rap equivalent in intensity to what was being produced on the West Coast.[167] According to Allmusic, the production on two Mobb Deep albums, The Infamous (1995) and Hell on Earth (1996), are ""indebted"" to RZA's early production with the Wu-Tang Clan.[168][169]
 The success of albums such as Nas's Illmatic and Notorious B.I.G.'s Ready to Die in 1994 cemented the status of the East Coast during a time of West Coast dominance. In a March 2002 issue of The Source Magazine, Nas referred to 1994 as ""a renaissance of New York [City] Hip-Hop.""[170] The productions of RZA, particularly for the Wu-Tang Clan, became influential with artists such as Mobb Deep due to the combination of somewhat detached instrumental loops, highly compressed and processed drums, and gangsta lyrical content. Wu-Tang solo albums such as Raekwon the Chef's Only Built 4 Cuban Linx, Ghostface Killah's Ironman, and GZA's Liquid Swords are now viewed as classics along with Wu-Tang ""core"" material. The clan's base extended into further groups called ""Wu-affiliates"". Producers such as DJ Premier (primarily for Gang Starr but also for other affiliated artists, such as Jeru the Damaja), Pete Rock (with CL Smooth, and supplying beats for many others), Buckwild, Large Professor, Diamond D, and Q-Tip supplied beats for numerous MCs at the time, regardless of location. Albums such as Nas's Illmatic, O.C.'s Word...Life (1994), and Jay-Z's Reasonable Doubt (1996) are made up of beats from this pool of producers.
 The rivalry between the East Coast and the West Coast rappers eventually turned personal.[171] Later in the decade the business acumen of the Bad Boy Records tested itself against Jay-Z and his Roc-A-Fella Records and, on the West Coast, Death Row Records. The mid to late 1990s saw a generation of rappers such as the members of D.I.T.C. such as the late Big L and Big Pun. On the East Coast, although the ""big business"" end of the market dominated matters commercially the late 1990s to early 2000s saw a number of relatively successful East Coast indie labels such as Rawkus Records (with whom Mos Def and Talib Kweli garnered success) and later Def Jux. The history of the two labels is intertwined, the latter having been started by EL-P of Company Flow in reaction to the former, and offered an outlet for more underground artists such as Mike Ladd, Aesop Rock, Mr Lif, RJD2, Cage and Cannibal Ox. Other acts such as the Hispanic Arsonists and slam poet turned MC Saul Williams met with differing degrees of success.
 After N.W.A. broke up, former member Dr. Dre released The Chronic in 1992, which peaked at No. 1 on the R&B/hip hop chart,[172] No. 3 on the pop chart, and spawned a No. 2 pop single with ""Nuthin' but a 'G' Thang"". The Chronic took West Coast rap in a new direction,[173] influenced strongly by P funk artists, melding smooth and easy funk beats with slowly-drawled lyrics. This came to be known as G-funk and dominated mainstream hip hop in the early-mid 1990s through a roster of artists on Suge Knight's Death Row Records, including Tupac Shakur, whose double disc album All Eyez on Me was a big hit with hit songs ""Ambitionz az a Ridah"" and ""2 of Amerikaz Most Wanted"";[citation needed] and Snoop Doggy Dogg, whose Doggystyle included the top ten hits ""What's My Name?"" and ""Gin and Juice"".[174] As the Los Angeles-based Death Row built an empire around Dre, Snoop, and Tupac, it also entered into a rivalry with New York City's Bad Boy Records, led by Puff Daddy and the Notorious B.I.G.
 Detached from this scene were other artists such as Freestyle Fellowship and the Pharcyde, as well as more underground artists such as the Solesides collective (DJ Shadow and Blackalicious amongst others), Jurassic 5, Ugly Duckling, People Under the Stairs, Tha Alkaholiks, and earlier Souls of Mischief, who represented a return to hip hop's roots of sampling and well-planned rhyme schemes.
 In the 1990s, hip hop began to diversify with other regional styles emerging on the national scene. Southern rap became popular in the early 1990s.[175] The first Southern rappers to gain national attention were the Geto Boys out of Houston, Texas.[176] Southern rap's roots can be traced to the success of Geto Boy's Grip It! On That Other Level in 1989, the Rick Rubin produced The Geto Boys in 1990, and We Can't Be Stopped in 1991.[177] The Houston area also produced other artists that pioneered the early southern rap sound such as UGK and the solo career of Scarface.
 Atlanta hip hop artists were key in further expanding rap music and bringing southern hip hop into the mainstream. Releases such as Arrested Development's 3 Years, 5 Months and 2 Days in the Life Of... in 1992, Goodie Mob's Soul Food in 1995 and OutKast's ATLiens in 1996 were all critically acclaimed. Other distinctive regional sounds from St. Louis, Chicago, Washington D.C., Detroit and others began to gain popularity.
 What once was rap now is hip hop, an endlessly various mass phenomenon that continues to polarize older rock and rollers, although it's finally convinced some gatekeeping generalists that it may be of enduring artistic value—a discovery to which they were beaten by millions of young consumers black and white.
 — Christgau's Consumer Guide: Albums of the '90s (2000)[178] During the golden age, elements of hip hop continued to be assimilated into other genres of popular music. The first waves of rap rock, rapcore, and rap metal — respective fusions of hip hop and rock, hardcore punk, and heavy metal[179] — became popular among mainstream audiences at this time; Run-DMC, the Beastie Boys, and Rage Against the Machine were among the most well-known bands in these fields. In Hawaii, bands such as Sudden Rush combined hip hop elements with the local language and political issues to form a style called na mele paleoleo.[180]
 Digable Planets' 1993 release Reachin' (A New Refutation of Time and Space) was an influential jazz rap record sampling the likes of Don Cherry, Sonny Rollins, Art Blakey, Herbie Mann, Herbie Hancock, Grant Green, and Rahsaan Roland Kirk. It spawned the hit single ""Rebirth of Slick (Cool Like Dat)"" which reached No. 16 on the Billboard Hot 100.[181]
 During the late 1990s, in the wake of the deaths of Tupac Shakur and the Notorious B.I.G., a new commercial sound emerged in the hip hop scene, sometimes referred to as the ""bling era""[182] (derived from Lil Wayne's ""Bling Bling""),[183] ""jiggy era""[184][185] (derived from Will Smith's ""Gettin' Jiggy wit It""), or ""shiny suit era"" (derived by metallic suits worn by some rappers in music videos at the time, such as in ""Mo Money Mo Problems"" by the Notorious B.I.G., Puff Daddy, and Mase).[186] Before the late 1990s, gangsta rap, while a huge-selling genre, had been regarded as well outside of the pop mainstream, committed to representing the experience of the inner-city and not ""selling out"" to the pop charts. However, the rise of Sean ""Puff Daddy"" Combs's Bad Boy Records, propelled by the massive crossover success of Combs's 1997 ensemble album No Way Out, signaled a major stylistic change in gangsta rap (and mainstream hip hop in general), as it would become even more commercially successful and popularly accepted. Silky R&B-styled hooks and production, more materialist subject matter, and samples of hit soul and pop songs from the 1970s and 1980s were the staples of this sound, which was showcased by producers such as Combs, Timbaland, the Trackmasters, the Neptunes, and Scott Storch. Also achieving similar levels of success at this time were Master P and his No Limit label in New Orleans; Master P built up a roster of artists (the No Limit posse) based out of New Orleans, and incorporated G funk and Miami bass influences in his music. The New Orleans upstart Cash Money label was also gaining popularity during this time,[187] with emerging artists such as Birdman, Lil Wayne, B.G, and Juvenile.
 Many of the rappers who achieved mainstream success at this time, such as Nelly, Puff Daddy, Jay-Z, the later career of Fat Joe and his Terror Squad, Mase, Ja Rule, Fabolous, and Cam'ron, had a pop-oriented style, while others such as Big Pun, Fat Joe (in his earlier career), DMX, Eminem, 50 Cent and his G-Unit, and the Game enjoyed commercial success at this time with a grittier style. Although white rappers like the Beastie Boys, House of Pain, and 3rd Bass previously had some popular success or critical acceptance from the hip hop community, Eminem's success, beginning in 1999 with the platinum The Slim Shady LP,[188] surprised many. Hip hop influences also found their way increasingly into mainstream pop during this period, particularly in genres such as R&B (e.g. R. Kelly, Akon, TLC, Destiny's Child, Beyoncé, Ashanti, Aaliyah, Usher), neo soul (e.g. Lauryn Hill, Erykah Badu, Jill Scott), and nu metal (e.g. Korn, Limp Bizkit).
 Dr. Dre remained an important figure in this era, making his comeback in 1999 with the album 2001. In 2000, he produced The Marshall Mathers LP by Eminem, and also produced 50 Cent's 2003 album Get Rich or Die Tryin', which debuted at number one on the U.S. Billboard 200 charts.[189] Jay-Z represented the cultural triumph of hip hop in this era. As his career progressed, he went from performing artist to entrepreneur, label president, head of a clothing line, club owner, and market consultant—along the way breaking Elvis Presley's record for most number one albums on the Billboard magazine charts by a solo artist.
 Alternative hip hop, which was introduced in the 1980s and then declined, resurged in the early-mid 2000s with the rejuvenated interest in indie music by the general public. The genre began to attain a place in the mainstream, due in part to the crossover success of artists such as OutKast, Kanye West, and Gnarls Barkley.[190] OutKast's 2003 album Speakerboxxx/The Love Below received high acclaim from music critics, and appealed to a wide range of listeners, being that it spanned numerous musical genres – including rap, rock, R&B, punk, jazz, indie, country, pop, electronica, and gospel. The album also spawned two number-one hit singles, and has been certified diamond by selling 11 times platinum by the RIAA for shipping more than 11 million units,[191] becoming one of the best selling hip-hop albums of all time. It also won a Grammy Award for Album of the Year at the 46th Annual Grammy Awards, being only the second rap album to do so. Previously, alternative hip hop acts had attained much critical acclaim, but received relatively little exposure through radio and other media outlets; during this time, alternative hip hop artists such as MF Doom,[192] the Roots, Dilated Peoples, Gnarls Barkley, Mos Def, and Aesop Rock[193][194] began to achieve significant recognition.
 Glitch hop and wonky music evolved following the rise of trip hop, dubstep and intelligent dance music (IDM). Both glitch hop and wonky music frequently reflect the experimental nature of IDM and the heavy bass featured in dubstep songs. While trip hop has been described as being a distinct British upper-middle class take on hip-hop, glitch-hop and wonky music have much more stylistic diversity. Both genres are melting pots of influence. Glitch hop contains echoes of 1980s pop music, Indian ragas, eclectic jazz and West Coast rap. Los Angeles, London, Glasgow and a number of other cities have become hot spots for these scenes, and underground scenes have developed across the world in smaller communities. Both genres often pay homage to older and more well established electronic music artists such as Radiohead, Aphex Twin and Boards of Canada as well as independent hip hop producers like J Dilla and Madlib.
 Glitch hop is a fusion genre of hip hop and glitch music that originated in the early to mid-2000s in the United States and Europe. Musically, it is based on irregular, chaotic breakbeats, glitchy basslines and other typical sound effects used in glitch music, like skips. Glitch hop artists include Prefuse 73, Dabrye and Flying Lotus.[195] Wonky is a subgenre of hip hop that originated around 2008, but most notably in the United States and United Kingdom, and among international artists of the Hyperdub music label, under the influence of glitch hop and dubstep. Wonky music is of the same glitchy style as glitch hop, but it was specifically noted for its melodies, rich with ""mid-range unstable synths"". Scotland has become one of the most prominent wonky scenes, with artists like Hudson Mohawke and Rustie.
 Glitch hop and wonky are popular among a relatively smaller audience interested in alternative hip hop and electronic music (especially dubstep); neither glitch hop nor wonky have achieved mainstream popularity. However, artists like Flying Lotus, the Glitch Mob and Hudson Mohawke have seen success in other avenues. Flying Lotus's music has earned multiple positive reviews on the independent music review site Pitchfork.com as well as a prominent (yet uncredited) spot during Adult Swim commercial breaks.[196][197] Hudson Mohawke is one of few glitch hop artists to play at major music festivals such as Sasquatch! Music Festival.
 Crunk is a regional hip hop genre that originated in Tennessee in the southern United States in the 1990s, influenced by Miami bass.[198] One of the pioneers of crunk, Lil Jon, said that it was a fusion of hip hop, electro, and electronic dance music. The style was pioneered and commercialized by artists from Memphis, Tennessee and Atlanta, Georgia, gaining considerable popularity in the mid-2000s via Lil Jon and the Ying Yang Twins.[199] Looped, stripped-down drum machine rhythms are usually used. The Roland TR-808 and 909 are among the most popular. The drum machine loops are usually accompanied by simple, repeated synthesizer melodies and heavy bass ""stabs"". The tempo of the music is somewhat slower than hip-hop, around the speed of reggaeton. The focal point of crunk is more often the beats and instrumental music rather than the lyrics. Crunk rappers, however, often shout and scream their lyrics, creating an aggressive, almost heavy, style of hip-hop. While other subgenres of hip-hop address sociopolitical or personal concerns, crunk is almost exclusively ""party music"", favoring call and response hip-hop slogans in lieu of more substantive approaches.[200] Crunk helped southern hip hop gain mainstream prominence during this period, as the classic East and West Coast styles of the 1990s gradually lost dominance.[201]
 Snap rap (also known as ringtone rap) is a subgenre of crunk that emerged from Atlanta, Georgia in the late 1990s.[202] The genre gained mainstream popularity in the mid-late 2000s, and artists from other Southern states such as Tennessee also began to emerge performing in this style. Tracks commonly consist of a Roland TR-808 bass drum, hi-hat, bass, finger snapping, a main groove, and a simplistic vocal hook. Hit snap songs include ""Lean wit It, Rock wit It"" by Dem Franchize Boyz, ""Laffy Taffy"" by D4L, ""It's Goin' Down"" by Yung Joc, and ""Crank That (Soulja Boy)"" by Soulja Boy Tell 'Em. In retrospect, Soulja Boy has been credited with setting trends in hip hop, such as self-publishing his songs through the Internet (which helped them go viral) and paving the way for a new wave of younger artists.[203][204]
 Starting in 2005, sales of hip hop music in the United States began to severely wane, leading Time magazine to question if mainstream hip-hop was ""dying."" Billboard magazine found that, since 2000, rap sales dropped 44%, and declined to 10% of all music sales, which, while still a commanding figure when compared to other genres, is a significant drop from the 13% of all music sales where rap music regularly placed.[205][206] According to Courtland Milloy of The Washington Post, for the first time on five years, no rap albums were among the top 10 sellers in 2006.[207] NPR culture critic Elizabeth Blair noted that, ""some industry experts say young people are fed up with the violence, degrading imagery and lyrics."" However, the 2005 report Generation M: Media in the Lives of 8–18 Year-Olds found that hip hop music is by far the most popular music genre for children and teenagers with 65 percent of 8- to-18-year-olds listening to it on a daily basis.[208]
 Other journalists say the music is just as popular as it ever was, but that fans have found other means to consume the music,[209] such as illegally downloading music through P2P networks, instead of purchasing albums and singles from legitimate stores. For example, Flo Rida is known for his low album sales regardless of his singles being mainstream and having digital success. His second album R.O.O.T.S. sold only 200,000+ total units in the U.S., which could not line up to the sales of the album's lead single ""Right Round"". This also happened to him in 2008.[210] Some put the blame on hip hop becoming less lyrical over time, such as Soulja Boy's 2007 debut album souljaboytellem.com which was met with negative reviews.[211] Lack of sampling, a key element of early hip hop, has also been noted for the decrease in quality of modern albums. For example, there are only four samples used in 2008's Paper Trail by T.I., while there are 35 samples in 1998's Moment of Truth by Gang Starr. The decrease in sampling is in part due to it being too expensive for producers.[212]
 In Byron Hurt's documentary Hip Hop: Beyond Beats and Rhymes, he claims that hip hop had changed from ""clever rhymes and dance beats"" to ""advocating personal, social and criminal corruption.""[213] Despite the fall in record sales throughout the music industry,[214] hip-hop had remained a popular genre, with hip-hop artists still regularly topping the Billboard 200 Charts. In the first half of 2009 alone artists such as Eminem,[215] Rick Ross,[216] the Black Eyed Peas,[217] and Fabolous[218] all had albums that reached the No. 1 position on the Billboard 200 charts. Eminem's album Relapse was one of the fastest selling albums of 2009.[219]
 By the late 2000s, alternative hip hop had secured its place within the mainstream, due in part to the declining commercial viability of gangsta rap. Industry observers view the sales race between Kanye West's Graduation and 50 Cent's Curtis as a turning point for hip hop. West emerged the victor, selling nearly a million copies in the first week alone, proving that innovative rap music could be just as commercially viable as gangsta rap, if not more so.[220] Although he designed it as a melancholic pop album rather than a rap album, Kanye's following 808s & Heartbreak would have a significant effect on hip hop music. While his decision to sing about love, loneliness, and heartache for the entirety of the album was at first heavily criticized by music audiences and the album was predicted to be a flop, its subsequent critical acclaim and commercial success encouraged other mainstream rappers to take greater creative risks with their music.[221][222] During the release of The Blueprint 3, New York rap mogul Jay-Z revealed that next studio album would be an experimental effort, stating, ""... it's not gonna be a #1 album. That's where I'm at right now. I wanna make the most experimental album I ever made.""[223] Jay-Z elaborated that like Kanye, he was unsatisfied with contemporary hip hop, was being inspired by indie-rockers like Grizzly Bear, and asserted his belief that the indie rock movement would play an important role in the continued evolution of hip-hop.[224]
 The alternative hip hop movement was not limited only to the United States, as rappers such as Somali-Canadian poet K'naan, Japanese rapper Shing02, and Sri Lankan British artist M.I.A. achieved considerable worldwide recognition. In 2009, Time magazine placed M.I.A in the Time 100 list of ""World's Most Influential people"" for having ""global influence across many genres.""[225][226] Global-themed movements have also sprung out of the international hip-hop scene with microgenres like ""Islamic Eco-Rap"" addressing issues of worldwide importance through traditionally disenfranchised voices.[227][228]
 Due in part to the increasing use of music distribution through social media and blogging, many alternative and non-alternative rappers found acceptance by far-reaching audiences, hence why this era of hip hop is sometimes termed the ""blog era"".[229][230] Several artists, such as Kid Cudi and Drake, managed to attain chart-topping hit songs, ""Day 'n' Nite"" and ""Best I Ever Had"" respectively, by releasing their music on free online mixtapes without the help of a major record label. Emerging artists at the time such as Wale, Kendrick Lamar,[231] J. Cole, Lupe Fiasco, the Cool Kids, Jay Electronica, and B.o.B were noted by critics as expressing eclectic sounds, sensitive life experiences, and vulnerable emotions that were rarely seen in the prior bling era.[232][233]
 Also at this time, the Auto-Tune vocal effect was bolstered in popularity by rapper T-Pain, who elaborated on the effect and made active use of Auto-Tune in his songs.[234] He cites new jack swing producer Teddy Riley and funk artist Roger Troutman's use of the talk box as inspirations for his own use of Auto-Tune.[235] T-Pain became so associated with Auto-Tune that he had an iPhone app named after him that simulated the effect, called ""I Am T-Pain"".[236] Eventually dubbed the ""T-Pain effect"",[237] the use of Auto-Tune became a popular fixture of late 2000s and early 2010s hip hop, examples being Snoop Dogg's ""Sexual Eruption"",[238] Lil Wayne's ""Lollipop"",[239] Kanye West's album 808s & Heartbreak,[240] and the Black Eyed Peas' number-one hit ""Boom Boom Pow"".[237]
 Trap music is a subgenre of Southern rap that originated in the early 1990s. It grew in the 2000s to become a mainstream sensation,[241] eventually reaching ubiquity in the mid-late 2010s and frequently having songs top the Billboard hip hop charts.[242][243][244] It is typified by double or triple-time sub-divided hi-hats,[245] heavy kick drums from the Roland TR-808 drum machine, layered synthesizers and an overall dark, ominous or bleak atmosphere.[246] The strong influence of the sound led to other artists within the genre to move towards the trap sound, with a notable example being Jay-Z and Kanye West on their joint song, ""H•A•M"". Non-rappers have also experimented with trap, such as ""7/11"" by Beyoncé and ""Dark Horse"" by Katy Perry (featuring rapper Juicy J).
 Major artists to arise from the genre in the 2010s include Lil Nas X, Waka Flocka Flame, Future, Chief Keef, Migos, Young Thug, Travis Scott, Kodak Black, 21 Savage, Yung Lean, Lil Uzi Vert, XXXTentacion, Ski Mask the Slump God, Juice Wrld, Trippie Redd, Lil Pump, Smokepurpp, Rae Sremmurd, Tekashi 6ix9ine, NBA YoungBoy, Lil Baby, Fetty Wap, among others. Female rappers Nicki Minaj, Cardi B, Saweetie, Doja Cat, Iggy Azalea, City Girls and Megan Thee Stallion also entered the mainstream.[249] Trap artists that originated in the 2000s were able to recapture mainstream success in the 2010s with the rise of trap, including 2 Chainz, Gucci Mane and Juicy J, becoming more successful in the latter part of their career than when they debuted. Trap producers to reach mainstream success include Metro Boomin, Pi'erre Bourne, London on da Track, and Mike Will Made-It.[citation needed]
 Critics of the trap genre have used the term ""mumble rap"" to describe the heavily auto-tuned, and sometimes hard to understand- delivery of verses from a majority of the artists.[250] Artists longstanding within the genre have had their own comments regarding the rise of mumble rap, such as Rick Rubin stating that Eminem was confused by it,[251] and Snoop Dogg claiming that he can not differentiate between artists.[252] Black Thought, lead rapper from the Roots, stated that the ""game has changed. It's different. The standards are different, the criteria that's taken into consideration in determining validity is different. We're at a point in history where lyricism almost comes last in very many regards.""[253]
 On July 17, 2017, Forbes reported that hip hop/R&B (which Nielsen SoundScan classifies as being the same genre) had usurped rock as the most consumed musical genre, becoming the most popular genre in music for the first time in U.S. history.[254][255][256][257]
 In the 2010s, Atlanta hip hop dominated the mainstream.[258]
 In the late 2010s and early 2020s, Brooklyn drill became popular since Pop Smoke emerged before his death. The 2020s decade began with Roddy Ricch as the first rapper to have a Billboard Hot 100 number-one entry.[259][260]
 The rise of streaming platforms such as Spotify and Apple Music in the mid-late 2010s greatly impacted the entire music business as a whole.[262][263] Despite being a free streaming-only mixtape with no commercial release, Chance the Rapper's Coloring Book won Best Rap Album at the 2017 Grammy Awards, being the first streaming album ever to win a Grammy Award.[264][265] Kanye West has stated that his own album, Yeezus, marked the death of CDs, and thus his subsequent release, The Life of Pablo was only released digitally.[266] The Life of Pablo was also nominated for 2017 Best Rap Album. In 2017, Drake released a free streaming-only project titled More Life, which he called a ""playlist"", insisting that it was neither a mixtape nor an album.[267]
 The online audio distribution platform SoundCloud played a massive role in the creation of various artists' careers in the latter half of the 2010s. Mainstream acts to start on SoundCloud include Post Malone, Lil Uzi Vert, Russ, Bryson Tiller, Lil Xan, Lil Pump, Lil Peep, Lil Skies, Smokepurpp, Ski Mask the Slump God, XXXTentacion, Trippie Redd, Playboi Carti, YBN Nahmir, Tay-K, ZillaKami, Ugly God, Nav, and others. These songs are usually closely related to trap, but have also been labeled separately as SoundCloud rap and sometimes emo rap. They have been characterized as usually having moody, sad undertones, and usually feature lo-fi rough production. The genre has been met with criticism for its perceived low effort in lyrics and production,[268] and the problematic nature of the artists to arise from it, such as Lil Peep's drug abuse that led to his death,[269] the multiple assault charges to XXXTentacion,[270] 6ix9ine pleading guilty to using a child in a sexual performance,[271] and the murder charges on Tay-K.[272] On the contrary, the image of artists such as XXXTentacion have been met with praise due to perceived character improvement since their controversies.[273][274]
 In 2021, the most streamed rappers were Doja Cat and Lil Nas X.[276] Other rappers with high streams in 2021 were Drake, Eminem, Lil Baby, Polo G, Megan Thee Stallion, Cardi B, Moneybagg Yo, Masked Wolf, Pop Smoke, J. Cole, and Lil Durk.[277][278] The most streamed rap album of all time on Spotify is XXXTentacion's second album, ? (2018).[275]
 Hip-hop music has reached the cultural corridors of the globe and has been absorbed and reinvented around the world.[279] Hip hop music expanded beyond the US, often blending local styles with hip hop. Hip hop has globalized into many cultures worldwide, as evident through the emergence of numerous regional scenes. It has emerged globally as a movement based upon the main tenets of hip hop culture. The music and the art continue to embrace, even celebrate, its transnational dimensions while staying true to the local cultures to which it is rooted. Hip-hop's impact differs depending on each culture. Still, the one thing virtually all hip hop artists worldwide have in common is that they acknowledge their debt to those African American people in New York who launched the global movement.[280]
 Hispanics and people from the Caribbean played an integral role in the early development of hip hop in New York, and the style spread to almost every country in that region. Hip hop first developed in the South Bronx, which had a high Hispanic, particularly Puerto Rican, population in the 1970s.[281] Some famous rappers from New York City of Puerto Rican origin are the late Big Pun, Fat Joe, and Angie Martinez. With Hispanic rap groups like Cypress Hill on the American charts, Mexican rap rock groups, such as Control Machete, rose to prominence in their native land.
 In many Latin American countries, as in the U.S., hip hop has been a tool with which marginalized people can articulate their struggle. Hip hop grew steadily more popular in Cuba in the 1980s and 1990s through Cuba's Special Period that came with the fall of the Soviet Union.[282] During this period of economic crisis, which the country's poor and black populations especially hard, hip hop became a way for the country's Afro-descended population to embrace their blackness and articulate a demand for racial equality for black people in Cuba.[282] The idea of blackness and black liberation was not always compatible with the goals of the Cuban government, which was still operating under the idea that a raceless society was the correct realization of the Cuban Revolution. When hip-hop emerged, the Cuban government opposed the vulgar image that rappers portrayed, but later accepted that it might be better to have hip-hop under the influence of the Ministry of Culture as an authentic expression of Cuban Culture.[283] Rappers who explicitly speak about race or racism in Cuba are still under scrutiny by the government.[284] An annual Cuban hip hop concert, beginning in 1995, held at Alamar in Havana helped popularize Cuban hip hop. Famous Cuban rap groups include Krudas Cubensi and Supercrónica Obsesión.
 Black and indigenous people in Latin America and Caribbean islands have been using hip hop for decades to discuss race and class issues in their respective countries. Brazilian hip hop is heavily associated with racial and economic issues in the country, where a lot of Afro-Brazilians live in economically disadvantaged communities, known in Brazil as favelas. São Paulo is where hip hop began in the country, but it soon spread all over Brazil, and today, almost every big Brazilian city, including Rio de Janeiro, Salvador, Curitiba, Porto Alegre, Belo Horizonte, Recife and Brasilia, has a hip hop scene. Some notable artists include Racionais MC's, Thaide, and Marcelo D2. One of Brazil's most popular rappers, MV Bill, has spent his career advocating for black youth in Rio de Janeiro.[284]
 Reggaeton, a Puerto Rican style of music, has a lot of similarities with U.S.-based hip hop. Both were influenced by Jamaican music, and both incorporate rapping and call and response.[285] Dancehall music and hip from the United States are both popular music in Puerto Rico, and reggaeton is the cumulation of different musical traditions founded by Afro-descended people in the Caribbean and the United States.[286] Some of reggaeton's most popular artists include Don Omar, Tego Calderón, and Daddy Yankee.
 In Venezuela, social unrest at the end of the 1980s and beginning of the 1990s coincided with the rise of gangsta rap in the United States and led to the rise of that music in Venezuela as well. Venezuelan rappers in the 1990s generally modeled their music after gangsta rap, embracing and attempting to redefine negative stereotypes about poor and black youth as dangerous and materialistic and incorporating socially conscious critique of Venezuela's criminalization of young, poor, Afro-descended people into their music.[287]
 In Haiti, hip hop developed in the early 1980s. Master Dji and his songs ""Vakans"" and ""Politik Pa m"" are mostly credited with the rise of Haitian hip hop. What later became known as ""Rap Kreyòl"" grew in popularity in the late 1990s with King Posse and Original Rap Stuff. Due to cheaper recording technology and flows of equipment to Haiti, more Rap Kreyòl groups are recording songs, even after the January 12 earthquake. Haitian hip hop has recently become a way for artists of Haitian backgrounds in the Haiti and abroad to express their national identity and political opinions about their country of origin.[288] Rappers have embraced the red and blue of the Flag of Haiti and rapping in Haitian Creole to display their national origin. In the Dominican Republic, a recording by Santi Y Sus Duendes and Lisa M became the first single of merenrap, a fusion of hip hop and merengue.
 In Europe, Africa, and Asia, hip hop began to move from the underground to mainstream audiences. In Europe, hip hop was the domain of both ethnic nationals and immigrants. British hip hop, for example, became a genre of its own and spawned artists such as Wiley, Dizzee Rascal, the Streets and many more. Germany produced the well-known Die Fantastischen Vier as well as several Turkish performers like the controversial Cartel, Kool Savaş, and Azad. In France, hip hop music developed itself from the end of the 80s. It can be divided into three eras:[289] The classical period, which extends from the end of the 1980s to the beginning of the 2000s marked by a majority of black artists like Oxmo Puccino, Mc Solaar, Kery James (with IdealJ), IAM, NTM,[290] the period of democratization from the 2000s,[291] with groups and artists like Lunatic, Diam's, Sinik, Rim'K, Sefyu,[292][293][294] Sniper, Rohff, La Fouine, which are beginning to affect the French population in general and to record the first significant commercial successes. Finally, from the 2010s, French-speaking rap experienced a rather paradoxical period of innovation, the logical start of new experiments that opened up French rap to new musical genres, such as trap, drill or ""folk"" rap. This period is distinguished by the great variety of French hip hop music, where several movements beginning to separate, artists like Booba, Kaaris, JuL, Gims, Freeze Corleone, Ziak or Soolking try to innovate and look for new tracks to explore. In the Netherlands, important nineties rappers include the Osdorp Posse, a crew from Amsterdam, Extince, from Oosterhout, and Postmen. Italy found its own rappers, including Jovanotti and Articolo 31, grow nationally renowned, while the Polish scene began in earnest early in the decade with the rise of PM Cool Lee. In Romania, B.U.G. Mafia came out of Bucharest's Pantelimon neighborhood, and their brand of gangsta rap underlines the parallels between life in Romania's Communist-era apartment blocks and in the housing projects of America's ghettos.
 One of the countries outside the US where hip-hop is most popular is the United Kingdom. Grime, a genre of music derived from UK Garage and drum and bass and influenced by hip hop, emerged in the early 2000s with artists such as Dizzee Rascal becoming successful. Although it is immensely popular, many British politicians criticize the music for what they see as promoting theft and murder, similar to gangsta rap in America. These criticisms have been deemed racist by the mostly Black British grime industry. Despite its controversial nature, grime has had a major effect on British fashion and pop music, with many young working-class youth emulating the clothing worn by grime stars like Dizzee Rascal and Wiley. There are many subgenres of grime, including ""Rhythm and Grime"", a mix of R&B and grime, and grindie, a mix of indie rock and grime popularized by indie rock band Hadouken!
 In Germany and France, gangsta rap has become popular among youths who like the violent and aggressive lyrics. Some German rappers openly or comically flirt with Nazism; for example, Bushido (born Anis Mohamed Youssef Ferchichi) raps ""Salutiert, steht stramm, Ich bin der Leader wie A"" (Salute, stand to attention, I am the leader like 'A') and Fler had a hit with the record Neue Deutsche Welle (New German Wave) complete with the title written in Third Reich style Gothic print and advertised with an Adolf Hitler quote. These references also spawned great controversy in Germany. Meanwhile, in France, artists like Kery James' Idéal J maintained a radical, anti-authoritarian attitude and released songs like Hardcore which attacked the growth of the French far right. In the Netherlands, MC Brainpower went from being an underground battle rapper to mainstream recognition in the Benelux, thus influencing numerous rap artists in the region. In Israel, rapper Subliminal reaches out to Israeli youth with political and religious-themed lyrics, usually with a Zionist message.
 In Asia, mainstream stars rose to prominence in the Philippines, led by Francis Magalona, Rap Asia, MC Lara and Lady Diane. In Japan, where underground rappers had previously found a limited audience, and popular teen idols brought a style called J-rap to the top of the charts in the middle of the 1990s. Of particular importance is the influence on East Asian nations, where hip hop music has become fused with local popular music to form different styles such as K-pop, C-pop and J-pop.
 In South Africa, the hip hop scene overlaps with kwaito, a music genre that emphasizes African culture and social issues. Rappers such as pope troy have harnessed the use of socio-economic issues plaguing the political spheres of south africa and hip-hop as a whole whilst balancing his lingual approach in order to communicate with the masses about the technical aspects that are creating the issues,[297] Prominent South African rappers include Cassper Nyovest and Big Zulu.
 Israel's hip hop grew greatly in popularity at the end of the decade, with several stars both Palestinian (Tamer Nafar) and Israeli (Subliminal). In Portugal hip hop has his own kind of rapping, which is more political and underground scene, they are known for Valete, Dealema and Halloween. Russian hip hop emerged during last years of Soviet Union and cemented later, with groups like Malchishnik and Bad Balance enjoying mainstream popularity in the 1990s, while Ligalize and Kasta were popular in the 2000s. In former Yugoslavia hip hop first appeared during the 1980s mostly with Serbian hip hop with performers such as B-boy, the Master Scratch Band, Badvajzer, and others. During the late 1990s hip hop had a boom, with Rambo Amadeus and later Beogradski sindikat becoming a major performer. Bosnian and Herzegovinian hip hop is nowadays dominated by Edo Maajka. In the region hip hop is often used as a political and social message in song themes such as war, profiteering, corruption, etc. Frenkie, another Bosnian rapper, is associated with Edo Maajka, and has collaborated beyond Bosnian borders.
 In Tanzania in the early 2000s, local hip hop artists became popular by infusing local styles of Afrobeat and arabesque melodies, dancehall and hip-hop beats with Swahili lyrics.
 In the 2010s, hip hop became popular in Canada with Canadians rappers such as Drake, Nav, Belly and Tory Lanez. Drake was the most streamed artist of the decade.[298]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Rastafarian', 'Lotti Golden and Richard Scher', 'worldwide importance', 'theft and murder', 'more materialist'], 'answer_start': [], 'answer_end': []}"
"
 Electronic music broadly is a group of music genres that employ electronic musical instruments, circuitry-based music technology and software, or general-purpose electronics (such as personal computers) in its creation. It includes both music made using electronic and electromechanical means (electroacoustic music). Pure electronic instruments depended entirely on circuitry-based sound generation, for instance using devices such as an electronic oscillator, theremin, or synthesizer. Electromechanical instruments can have mechanical parts such as strings, hammers, and electric elements including magnetic pickups, power amplifiers and loudspeakers. Such electromechanical devices include the telharmonium, Hammond organ, electric piano and electric guitar.[3][4]
 The first electronic musical devices were developed at the end of the 19th century. During the 1920s and 1930s, some electronic instruments were introduced and the first compositions featuring them were written. By the 1940s, magnetic audio tape allowed musicians to tape sounds and then modify them by changing the tape speed or direction, leading to the development of electroacoustic tape music in the 1940s, in Egypt and France. Musique concrète, created in Paris in 1948, was based on editing together recorded fragments of natural and industrial sounds. Music produced solely from electronic generators was first produced in Germany in 1953 by Karlheinz Stockhausen. Electronic music was also created in Japan and the United States beginning in the 1950s and Algorithmic composition with computers was first demonstrated in the same decade.
 During the 1960s, digital computer music was pioneered, innovation in live electronics took place, and Japanese electronic musical instruments began to influence the music industry. In the early 1970s, Moog synthesizers and drum machines helped popularize synthesized electronic music. The 1970s also saw electronic music begin to have a significant influence on popular music, with the adoption of polyphonic synthesizers, electronic drums, drum machines, and turntables, through the emergence of genres such as disco, krautrock, new wave, synth-pop, hip hop, and EDM. In the early 1980s mass-produced digital synthesizers, such as the Yamaha DX7, became popular, and MIDI (Musical Instrument Digital Interface) was developed. In the same decade, with a greater reliance on synthesizers and the adoption of programmable drum machines, electronic popular music came to the fore. During the 1990s, with the proliferation of increasingly affordable music technology, electronic music production became an established part of popular culture.[5] In Berlin starting in 1989, the Love Parade became the largest street party with over 1 million visitors, inspiring other such popular celebrations of electronic music.[6]
 Contemporary electronic music includes many varieties and ranges from experimental art music to popular forms such as electronic dance music. Pop electronic music is most recognizable in its 4/4 form and more connected with the mainstream than preceding forms which were popular in niche markets.[7]
 At the turn of the 20th century, experimentation with emerging electronics led to the first electronic musical instruments.[8] These initial inventions were not sold, but were instead used in demonstrations and public performances. The audiences were presented with reproductions of existing music instead of new compositions for the instruments.[9] While some were considered novelties and produced simple tones, the Telharmonium synthesized the sound of several orchestral instruments with reasonable precision. It achieved viable public interest and made commercial progress into streaming music through telephone networks.[10]
 Critics of musical conventions at the time saw promise in these developments. Ferruccio Busoni encouraged the composition of microtonal music allowed for by electronic instruments. He predicted the use of machines in future music, writing the influential Sketch of a New Esthetic of Music (1907).[11][12] Futurists such as Francesco Balilla Pratella and Luigi Russolo began composing music with acoustic noise to evoke the sound of machinery. They predicted expansions in timbre allowed for by electronics in the influential manifesto The Art of Noises (1913).[13][14]
 Developments of the vacuum tube led to electronic instruments that were smaller, amplified, and more practical for performance.[15] In particular, the theremin, ondes Martenot and trautonium were commercially produced by the early 1930s.[16][17]
 From the late 1920s, the increased practicality of electronic instruments influenced composers such as Joseph Schillinger and Maria Schuppel to adopt them. They were typically used within orchestras, and most composers wrote parts for the theremin that could otherwise be performed with string instruments.[16]
 Avant-garde composers criticized the predominant use of electronic instruments for conventional purposes.[16] The instruments offered expansions in pitch resources[18] that were exploited by advocates of microtonal music such as Charles Ives, Dimitrios Levidis, Olivier Messiaen and Edgard Varèse.[19][20][21] Further, Percy Grainger used the theremin to abandon fixed tonation entirely,[22] while Russian composers such as Gavriil Popov treated it as a source of noise in otherwise-acoustic noise music.[23]
 Developments in early recording technology paralleled that of electronic instruments. The first means of recording and reproducing audio was invented in the late 19th century with the mechanical phonograph.[24] Record players became a common household item, and by the 1920s composers were using them to play short recordings in performances.[25]
 The introduction of electrical recording in 1925 was followed by increased experimentation with record players. Paul Hindemith and Ernst Toch composed several pieces in 1930 by layering recordings of instruments and vocals at adjusted speeds. Influenced by these techniques, John Cage composed Imaginary Landscape No. 1 in 1939 by adjusting the speeds of recorded tones.[26]
 Composers began to experiment with newly developed sound-on-film technology. Recordings could be spliced together to create sound collages, such as those by Tristan Tzara, Kurt Schwitters, Filippo Tommaso Marinetti, Walter Ruttmann and Dziga Vertov. Further, the technology allowed sound to be graphically created and modified. These techniques were used to compose soundtracks for several films in Germany and Russia, in addition to the popular Dr. Jekyll and Mr. Hyde in the United States. Experiments with graphical sound were continued by Norman McLaren from the late 1930s.[27]
 The first practical audio tape recorder was unveiled in 1935.[28] Improvements to the technology were made using the AC biasing technique, which significantly improved recording fidelity.[29][30] As early as 1942, test recordings were being made in stereo.[31] Although these developments were initially confined to Germany, recorders and tapes were brought to the United States following the end of World War II.[32] These were the basis for the first commercially produced tape recorder in 1948.[33]
 In 1944, before the use of magnetic tape for compositional purposes, Egyptian composer Halim El-Dabh, while still a student in Cairo, used a cumbersome wire recorder to record sounds of an ancient zaar ceremony. Using facilities at the Middle East Radio studios El-Dabh processed the recorded material using reverberation, echo, voltage controls and re-recording. What resulted is believed to be the earliest tape music composition.[34] The resulting work was entitled The Expression of Zaar and it was presented in 1944 at an art gallery event in Cairo. While his initial experiments in tape-based composition were not widely known outside of Egypt at the time, El-Dabh is also known for his later work in electronic music at the Columbia-Princeton Electronic Music Center in the late 1950s.[35]
 Following his work with Studio d'Essai at Radiodiffusion Française (RDF), during the early 1940s, Pierre Schaeffer is credited with originating the theory and practice of musique concrète. In the late 1940s, experiments in sound-based composition using shellac record players were first conducted by Schaeffer. In 1950, the techniques of musique concrete were expanded when magnetic tape machines were used to explore sound manipulation practices such as speed variation (pitch shift) and tape splicing.[36][37]
 On 5 October 1948, RDF broadcast Schaeffer's Etude aux chemins de fer. This was the first ""movement"" of Cinq études de bruits, and marked the beginning of studio realizations[38] and musique concrète (or acousmatic art). Schaeffer employed a disc cutting lathe, four turntables, a four-channel mixer, filters, an echo chamber, and a mobile recording unit. Not long after this, Pierre Henry began collaborating with Schaeffer, a partnership that would have profound and lasting effects on the direction of electronic music. Another associate of Schaeffer, Edgard Varèse, began work on Déserts, a work for chamber orchestra and tape. The tape parts were created at Pierre Schaeffer's studio and were later revised at Columbia University.
 In 1950, Schaeffer gave the first public (non-broadcast) concert of musique concrète at the École Normale de Musique de Paris. ""Schaeffer used a PA system, several turntables, and mixers. The performance did not go well, as creating live montages with turntables had never been done before.""[39] Later that same year, Pierre Henry collaborated with Schaeffer on Symphonie pour un homme seul (1950) the first major work of musique concrete.[40] In Paris in 1951, in what was to become an important worldwide trend, RTF established the first studio for the production of electronic music. Also in 1951, Schaeffer and Henry produced an opera, Orpheus, for concrete sounds and voices.
 By 1951 the work of Schaeffer, composer-percussionist Pierre Henry, and sound engineer Jacques Poullin had received official recognition and The Groupe de Recherches de Musique Concrète, Club d 'Essai de la Radiodiffusion-Télévision Française was established at RTF in Paris, the ancestor of the ORTF.[41]
 Karlheinz Stockhausen worked briefly in Schaeffer's studio in 1952, and afterward for many years at the WDR Cologne's Studio for Electronic Music.
 1954 saw the advent of what would now be considered authentic electric plus acoustic compositions—acoustic instrumentation augmented/accompanied by recordings of manipulated or electronically generated sound. Three major works were premiered that year: Varèse's Déserts, for chamber ensemble and tape sounds, and two works by Otto Luening and Vladimir Ussachevsky: Rhapsodic Variations for the Louisville Symphony and A Poem in Cycles and Bells, both for orchestra and tape. Because he had been working at Schaeffer's studio, the tape part for Varèse's work contains much more concrete sounds than electronic. ""A group made up of wind instruments, percussion and piano alternate with the mutated sounds of factory noises and ship sirens and motors, coming from two loudspeakers.""[42]
 At the German premiere of Déserts in Hamburg, which was conducted by Bruno Maderna, the tape controls were operated by Karlheinz Stockhausen.[42] The title Déserts suggested to Varèse not only ""all physical deserts (of sand, sea, snow, of outer space, of empty streets), but also the deserts in the mind of man; not only those stripped aspects of nature that suggest bareness, aloofness, timelessness, but also that remote inner space no telescope can reach, where man is alone, a world of mystery and essential loneliness.""[43]
 In Cologne, what would become the most famous electronic music studio in the world, was officially opened at the radio studios of the NWDR in 1953, though it had been in the planning stages as early as 1950 and early compositions were made and broadcast in 1951.[44] The brainchild of Werner Meyer-Eppler, Robert Beyer, and Herbert Eimert (who became its first director), the studio was soon joined by Karlheinz Stockhausen and Gottfried Michael Koenig. In his 1949 thesis Elektronische Klangerzeugung: Elektronische Musik und Synthetische Sprache, Meyer-Eppler conceived the idea to synthesize music entirely from electronically produced signals; in this way, elektronische Musik was sharply differentiated from French musique concrète, which used sounds recorded from acoustical sources.[45][46]
 In 1953, Stockhausen composed his Studie I, followed in 1954 by Elektronische Studie II—the first electronic piece to be published as a score. In 1955, more experimental and electronic studios began to appear. Notable were the creation of the Studio di fonologia musicale di Radio Milano, a studio at the NHK in Tokyo founded by Toshiro Mayuzumi, and the Philips studio at Eindhoven, the Netherlands, which moved to the University of Utrecht as the Institute of Sonology in 1960.
 ""With Stockhausen and Mauricio Kagel in residence, [Cologne] became a year-round hive of charismatic avante-gardism.""[47] on two occasions combining electronically generated sounds with relatively conventional orchestras—in Mixtur (1964) and Hymnen, dritte Region mit Orchester (1967).[48] Stockhausen stated that his listeners had told him his electronic music gave them an experience of ""outer space"", sensations of flying, or being in a ""fantastic dream world"".[49]
 In the United States, electronic music was being created as early as 1939, when John Cage published Imaginary Landscape, No. 1, using two variable-speed turntables, frequency recordings, muted piano, and cymbal, but no electronic means of production. Cage composed five more ""Imaginary Landscapes"" between 1942 and 1952 (one withdrawn), mostly for percussion ensemble, though No. 4 is for twelve radios and No. 5, written in 1952, uses 42 recordings and is to be realized as a magnetic tape. According to Otto Luening, Cage also performed Williams Mix at Donaueschingen in 1954, using eight loudspeakers, three years after his alleged collaboration.[clarification needed] Williams Mix was a success at the Donaueschingen Festival, where it made a ""strong impression"".[50]
 The Music for Magnetic Tape Project was formed by members of the New York School (John Cage, Earle Brown, Christian Wolff, David Tudor, and Morton Feldman),[51] and lasted three years until 1954. Cage wrote of this collaboration: ""In this social darkness, therefore, the work of Earle Brown, Morton Feldman, and Christian Wolff continues to present a brilliant light, for the reason that at the several points of notation, performance, and audition, action is provocative.""[52]
 Cage completed Williams Mix in 1953 while working with the Music for Magnetic Tape Project.[53] The group had no permanent facility, and had to rely on borrowed time in commercial sound studios, including the studio of Bebe and Louis Barron.
 In the same year Columbia University purchased its first tape recorder—a professional Ampex machine—to record concerts. Vladimir Ussachevsky, who was on the music faculty of Columbia University, was placed in charge of the device, and almost immediately began experimenting with it.
 Herbert Russcol writes: ""Soon he was intrigued with the new sonorities he could achieve by recording musical instruments and then superimposing them on one another.""[54] Ussachevsky said later: ""I suddenly realized that the tape recorder could be treated as an instrument of sound transformation.""[54] On Thursday, 8 May 1952, Ussachevsky presented several demonstrations of tape music/effects that he created at his Composers Forum, in the McMillin Theatre at Columbia University. These included Transposition, Reverberation, Experiment, Composition, and Underwater Valse. In an interview, he stated: ""I presented a few examples of my discovery in a public concert in New York together with other compositions I had written for conventional instruments.""[54]
Otto Luening, who had attended this concert, remarked: ""The equipment at his disposal consisted of an Ampex tape recorder . . . and a simple box-like device designed by the brilliant young engineer, Peter Mauzey, to create feedback, a form of mechanical reverberation. Other equipment was borrowed or purchased with personal funds.""[55]
 Just three months later, in August 1952, Ussachevsky traveled to Bennington, Vermont, at Luening's invitation to present his experiments. There, the two collaborated on various pieces. Luening described the event: ""Equipped with earphones and a flute, I began developing my first tape-recorder composition. Both of us were fluent improvisors and the medium fired our imaginations.""[55] They played some early pieces informally at a party, where ""a number of composers almost solemnly congratulated us saying, 'This is it' ('it' meaning the music of the future).""[55]
 Word quickly reached New York City. Oliver Daniel telephoned and invited the pair to ""produce a group of short compositions for the October concert sponsored by the American Composers Alliance and Broadcast Music, Inc., under the direction of Leopold Stokowski at the Museum of Modern Art in New York. After some hesitation, we agreed. . . . Henry Cowell placed his home and studio in Woodstock, New York, at our disposal. With the borrowed equipment in the back of Ussachevsky's car, we left Bennington for Woodstock and stayed two weeks. . . . In late September 1952, the travelling laboratory reached Ussachevsky's living room in New York, where we eventually completed the compositions.""[55]
 Two months later, on 28 October, Vladimir Ussachevsky and Otto Luening presented the first Tape Music concert in the United States. The concert included Luening's Fantasy in Space (1952)—""an impressionistic virtuoso piece""[55] using manipulated recordings of flute—and Low Speed (1952), an ""exotic composition that took the flute far below its natural range.""[55] Both pieces were created at the home of Henry Cowell in Woodstock, New York. After several concerts caused a sensation in New York City, Ussachevsky and Luening were invited onto a live broadcast of NBC's Today Show to do an interview demonstration—the first televised electroacoustic performance. Luening described the event: ""I improvised some [flute] sequences for the tape recorder. Ussachevsky then and there put them through electronic transformations.""[56]
 The score for Forbidden Planet, by Louis and Bebe Barron,[57] was entirely composed using custom-built electronic circuits and tape recorders in 1956 (but no synthesizers in the modern sense of the word).[clarification needed]
 In 1929, Nikolai Obukhov invented the ""sounding cross"" (la croix sonore), comparable to the principle of the theremin.[58] In the 1930s, Nikolai Ananyev invented ""sonar"", and engineer Alexander Gurov — neoviolena, I. Ilsarov — ilston.,[59] A. Rimsky-Korsakov [ru] and A. Ivanov — emiriton [ru].[58] Composer and inventor Arseny Avraamov was engaged in scientific work on sound synthesis and conducted a number of experiments that would later form the basis of Soviet electro-musical instruments.[60]
 In 1956 Vyacheslav Mescherin created the Ensemble of electro-musical instruments [ru], which used theremins, electric harps, electric organs, the first synthesizer in the USSR ""Ekvodin"",[58] and also created the first Soviet reverb machine. The style in which Meshcherin's ensemble played is known as ""Space age pop"".[60] In 1957, engineer Igor Simonov assembled a working model of a noise recorder (electroeoliphone), with the help of which it was possible to extract various timbres and consonances of a noise nature.[58] In 1958, Evgeny Murzin designed ANS synthesizer, one of the world's first polyphonic musical synthesizers.
 Founded by Murzin in 1966, the Moscow Experimental Electronic Music Studio became the base for a new generation of experimenters – Eduard Artemyev, Alexander Nemtin [ru], Sándor Kallós, Sofia Gubaidulina, Alfred Schnittke, and Vladimir Martynov.[58][60] By the end of the 1960s, musical groups playing light electronic music appeared in the USSR. At the state level, this music began to be used to attract foreign tourists to the country and for broadcasting to foreign countries.[61] In the mid-1970s, composer Alexander Zatsepin designed an ""orchestrolla"" – a modification of the mellotron.[62]
 The Baltic Soviet Republics also had their own pioneers: in Estonian SSR — Sven Grunberg, in Lithuanian SSR — Gedrus Kupriavicius, in Latvian SSR — Opus and Zodiac.[60]
 The world's first computer to play music was CSIRAC, which was designed and built by Trevor Pearcey and Maston Beard. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the Colonel Bogey March, of which no known recordings exist, only the accurate reconstruction.[63] However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice. CSIRAC was never recorded, but the music played was accurately reconstructed. The oldest known recordings of computer-generated music were played by the Ferranti Mark 1 computer, a commercial version of the Baby Machine from the University of Manchester in the autumn of 1951.[64] The music program was written by Christopher Strachey.
 The earliest group of electronic musical instruments in Japan, Yamaha Magna Organ was built in 1935.[65] however, after World War II, Japanese composers such as Minao Shibata knew of the development of electronic musical instruments. By the late 1940s, Japanese composers began experimenting with electronic music and institutional sponsorship enabled them to experiment with advanced equipment. Their infusion of Asian music into the emerging genre would eventually support Japan's popularity in the development of music technology several decades later.[66]
 Following the foundation of electronics company Sony in 1946, composers Toru Takemitsu and Minao Shibata independently explored possible uses for electronic technology to produce music.[67] Takemitsu had ideas similar to musique concrète, which he was unaware of, while Shibata foresaw the development of synthesizers and predicted a drastic change in music.[68] Sony began producing popular magnetic tape recorders for government and public use.[66][69]
 The avant-garde collective Jikken Kōbō (Experimental Workshop), founded in 1950, was offered access to emerging audio technology by Sony. The company hired Toru Takemitsu to demonstrate their tape recorders with compositions and performances of electronic tape music.[70] The first electronic tape pieces by the group were ""Toraware no Onna"" (""Imprisoned Woman"") and ""Piece B"", composed in 1951 by Kuniharu Akiyama.[71] Many of the electroacoustic tape pieces they produced were used as incidental music for radio, film, and theatre. They also held concerts employing a slide show synchronized with a recorded soundtrack.[72] Composers outside of the Jikken Kōbō, such as Yasushi Akutagawa, Saburo Tominaga, and Shirō Fukai, were also experimenting with radiophonic tape music between 1952 and 1953.[69]
 Musique concrète was introduced to Japan by Toshiro Mayuzumi, who was influenced by a Pierre Schaeffer concert. From 1952, he composed tape music pieces for a comedy film, a radio broadcast, and a radio drama.[71][73] However, Schaeffer's concept of sound object was not influential among Japanese composers, who were mainly interested in overcoming the restrictions of human performance.[74] This led to several Japanese electroacoustic musicians making use of serialism and twelve-tone techniques,[74] evident in Yoshirō Irino's 1951 dodecaphonic piece ""Concerto da
Camera"",[73] in the organization of electronic sounds in Mayuzumi's ""X, Y, Z for Musique Concrète"", and later in Shibata's electronic music by 1956.[75]
 Modelling the NWDR studio in Cologne, established an NHK electronic music studio  in Tokyo in 1954, which became one of the world's leading electronic music facilities.The NHK electronic music studio  was equipped with technologies such as tone-generating and audio processing equipment, recording and radiophonic equipment, ondes Martenot, Monochord and Melochord, sine-wave oscillators, tape recorders, ring modulators, band-pass filters, and four- and eight-channel mixers. Musicians associated with the studio included Toshiro Mayuzumi, Minao Shibata, Joji Yuasa, Toshi Ichiyanagi, and Toru Takemitsu. The studio's first electronic compositions were completed in 1955, including Mayuzumi's five-minute pieces ""Studie I: Music for Sine Wave by Proportion of Prime Number"", ""Music for Modulated Wave by Proportion of Prime Number"" and ""Invention for Square Wave and Sawtooth Wave"" produced using the studio's various tone-generating capabilities, and Shibata's 20-minute stereo piece ""Musique Concrète for Stereophonic Broadcast"".[76][77]
 The impact of computers continued in 1956. Lejaren Hiller and Leonard Isaacson composed Illiac Suite for string quartet, the first complete work of computer-assisted composition using algorithmic composition. ""... Hiller postulated that a computer could be taught the rules of a particular style and then called on to compose accordingly.""[78] Later developments included the work of Max Mathews at Bell Laboratories, who developed the influential MUSIC I program in 1957, one of the first computer programs to play electronic music. Vocoder technology was also a major development in this early era. In 1956, Stockhausen composed Gesang der Jünglinge, the first major work of the Cologne studio, based on a text from the Book of Daniel. An important technological development of that year was the invention of the Clavivox synthesizer by Raymond Scott with subassembly by Robert Moog.
 In 1957, Kid Baltan (Dick Raaymakers) and Tom Dissevelt released their debut album, Song Of The Second Moon, recorded at the Philips studio in the Netherlands.[79] The public remained interested in the new sounds being created around the world, as can be deduced by the inclusion of Varèse's Poème électronique, which was played over four hundred loudspeakers at the Philips Pavilion of the 1958 Brussels World Fair. That same year, Mauricio Kagel, an Argentine composer, composed Transición II. The work was realized at the WDR studio in Cologne. Two musicians performed on the piano, one in the traditional manner, the other playing on the strings, frame, and case. Two other performers used tape to unite the presentation of live sounds with the future of prerecorded materials from later on and its past of recordings made earlier in the performance.
 In 1958, Columbia-Princeton developed the RCA Mark II Sound Synthesizer, the first programmable synthesizer.[80] Prominent composers such as Vladimir Ussachevsky, Otto Luening, Milton Babbitt, Charles Wuorinen, Halim El-Dabh, Bülent Arel and Mario Davidovsky used the RCA Synthesizer extensively in various compositions.[81] One of the most influential composers associated with the early years of the studio was Egypt's Halim El-Dabh who,[82] after having developed the earliest known electronic tape music in 1944,[34] became more famous for Leiyla and the Poet, a 1959 series of electronic compositions that stood out for its immersion and seamless fusion of electronic and folk music, in contrast to the more mathematical approach used by serial composers of the time such as Babbitt. El-Dabh's Leiyla and the Poet, released as part of the album Columbia-Princeton Electronic Music Center in 1961, would be cited as a strong influence by a number of musicians, ranging from Neil Rolnick, Charles Amirkhanian and Alice Shields to rock musicians Frank Zappa and The West Coast Pop Art Experimental Band.[83]
 Following the emergence of differences within the GRMC (Groupe de Recherche de Musique Concrète) Pierre Henry, Philippe Arthuys, and several of their colleagues, resigned in April 1958. Schaeffer created a new collective, called Groupe de Recherches Musicales (GRM) and set about recruiting new members including Luc Ferrari, Beatriz Ferreyra, François-Bernard Mâche, Iannis Xenakis, Bernard Parmegiani, and Mireille Chamass-Kyrou. Later arrivals included Ivo Malec, Philippe Carson, Romuald Vandelle, Edgardo Canton and François Bayle.[84]
 These were fertile years for electronic music—not just for academia, but for independent artists as synthesizer technology became more accessible. By this time, a strong community of composers and musicians working with new sounds and instruments was established and growing. 1960 witnessed the composition of Luening's Gargoyles for violin and tape as well as the premiere of Stockhausen's Kontakte for electronic sounds, piano, and percussion. This piece existed in two versions—one for 4-channel tape, and the other for tape with human performers. ""In Kontakte, Stockhausen abandoned traditional musical form based on linear development and dramatic climax. This new approach, which he termed 'moment form', resembles the 'cinematic splice' techniques in early twentieth-century film.""[85]
 The theremin had been in use since the 1920s but it attained a degree of popular recognition through its use in science-fiction film soundtrack music in the 1950s (e.g., Bernard Herrmann's classic score for The Day the Earth Stood Still).[86]
 In the UK in this period, the BBC Radiophonic Workshop (established in 1958) came to prominence, thanks in large measure to their work on the BBC science-fiction series Doctor Who. One of the most influential British electronic artists in this period[87] was Workshop staffer Delia Derbyshire, who is now famous for her 1963 electronic realisation of the iconic Doctor Who theme, composed by Ron Grainer.
 During the time of the UNESCO fellowship for studies in electronic music (1958) Josef Tal went on a study tour in the US and Canada.[88] He summarized his conclusions in two articles that he submitted to UNESCO.[89] In 1961, he established the Centre for Electronic Music in Israel at The Hebrew University of Jerusalem. In 1962, Hugh Le Caine arrived in Jerusalem to install his Creative Tape Recorder in the centre.[90] In the 1990s Tal conducted, together with Dr. Shlomo Markel, in cooperation with the Technion – Israel Institute of Technology, and the Volkswagen Foundation a research project ('Talmark') aimed at the development of a novel musical notation system for electronic music.[91]
 Milton Babbitt composed his first electronic work using the synthesizer—his Composition for Synthesizer (1961)—which he created using the RCA synthesizer at the Columbia-Princeton Electronic Music Center.
 For Babbitt, the RCA synthesizer was a dream come true for three reasons. First, the ability to pinpoint and control every musical element precisely. Second, the time needed to realize his elaborate serial structures were brought within practical reach. Third, the question was no longer ""What are the limits of the human performer?"" but rather ""What are the limits of human hearing?""[92] The collaborations also occurred across oceans and continents. In 1961, Ussachevsky invited Varèse to the Columbia-Princeton Studio (CPEMC). Upon arrival, Varese embarked upon a revision of Déserts. He was assisted by Mario Davidovsky and Bülent Arel.[93]
 The intense activity occurring at CPEMC and elsewhere inspired the establishment of the San Francisco Tape Music Center in 1963 by Morton Subotnick, with additional members Pauline Oliveros, Ramon Sender, Anthony Martin, and Terry Riley.[94]
 Later, the Center moved to Mills College, directed by Pauline Oliveros, and has since been renamed Center for Contemporary Music.[95] Pietro Grossi was an Italian pioneer of computer composition and tape music, who first experimented with electronic techniques in the early sixties. Grossi was a cellist and composer, born in Venice in 1917. He founded the S 2F M (Studio de Fonologia Musicale di Firenze) in 1963 to experiment with electronic sound and composition.
 Simultaneously in San Francisco, composer Stan Shaff and equipment designer Doug McEachern, presented the first ""Audium"" concert at San Francisco State College (1962), followed by work at the San Francisco Museum of Modern Art (1963), conceived of as in time, controlled movement of sound in space. Twelve speakers surrounded the audience, four speakers were mounted on a rotating, mobile-like construction above.[96] In an SFMOMA performance the following year (1964), San Francisco Chronicle music critic Alfred Frankenstein commented, ""the possibilities of the space-sound continuum have seldom been so extensively explored"".[96] In 1967, the first Audium, a ""sound-space continuum"" opened, holding weekly performances through 1970. In 1975, enabled by seed money from the National Endowment for the Arts, a new Audium opened, designed floor to ceiling for spatial sound composition and performance.[97] ""In contrast, there are composers who manipulated sound space by locating multiple speakers at various locations in a performance space and then switching or panning the sound between the sources. In this approach, the composition of spatial manipulation is dependent on the location of the speakers and usually exploits the acoustical properties of the enclosure. Examples include Varese's Poeme Electronique (tape music performed in the Philips Pavilion of the 1958 World Fair, Brussels) and Stan Schaff's Audium installation, currently active in San Francisco.""[98][99] Through weekly programs (over 4,500 in 40 years), Shaff ""sculpts"" sound, performing now-digitized spatial works live through 176 speakers.[100]
 Jean-Jacques Perrey experimented with Schaeffer's techniques on tape loops and was among the first to use the recently released Moog synthesizer developed by Robert Moog. With this instrument he composed some works with Gershon Kingsley and solo.[101] A well-known example of the use of Moog's full-sized Moog modular synthesizer is the 1968 Switched-On Bach album by Wendy Carlos, which triggered a craze for synthesizer music.[102] In 1969 David Tudor brought a Moog modular synthesizer and Ampex tape machines to the National Institute of Design in Ahmedabad with the support of the Sarabhai family, forming the foundation of India's first electronic music studio. Here a group of composers Jinraj Joshipura, Gita Sarabhai, SC Sharma, IS Mathur and Atul Desai developed experimental sound compositions between 1969 and 1973.[103]
 Musical melodies were first generated by the computer CSIRAC in Australia in 1950. There were newspaper reports from America and England (early and recently) that computers may have played music earlier, but thorough research has debunked these stories as there is no evidence to support the newspaper reports (some of which were obviously speculative). Research has shown that people speculated about computers playing music, possibly because computers would make noises,[104] but there is no evidence that they actually did it.[105][106]
 The world's first computer to play music was CSIRAC, which was designed and built by Trevor Pearcey and Maston Beard in the 1950s. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the ""Colonel Bogey March""[107] of which no known recordings exist.
However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice which is current computer-music practice.
 The first music to be performed in England was a performance of the British National Anthem that was programmed by Christopher Strachey on the Ferranti Mark I, late in 1951. Later that year, short extracts of three pieces were recorded there by a BBC outside broadcasting unit: the National Anthem, ""Ba, Ba Black Sheep"", and ""In the Mood"" and this is recognised as the earliest recording of a computer to play music. This recording can be heard at this Manchester University site. Researchers at the University of Canterbury, Christchurch declicked and restored this recording in 2016 and the results may be heard on SoundCloud.[108][109][64]
 The late 1950s, 1960s, and 1970s also saw the development of large mainframe computer synthesis. Starting in 1957, Max Mathews of Bell Labs developed the MUSIC programs, culminating in MUSIC V, a direct digital synthesis language.[110] Laurie Spiegel developed the algorithmic musical composition software ""Music Mouse"" (1986) for Macintosh, Amiga, and Atari computers.
 An important new development was the advent of computers to compose music, as opposed to manipulating or creating sounds. Iannis Xenakis began what is called musique stochastique, or stochastic music, which is a composing method that uses mathematical probability systems. Different probability algorithms were used to create a piece under a set of parameters. Xenakis used computers to compose pieces like ST/4 for string quartet and ST/48 for orchestra (both 1962),[111] Morsima-Amorsima, ST/10, and Atrées. He developed the computer system UPIC for translating graphical images into musical results and composed Mycènes Alpha (1978) with it.
 In Europe in 1964, Karlheinz Stockhausen composed Mikrophonie I for tam-tam, hand-held microphones, filters, and potentiometers, and Mixtur for orchestra, four sine-wave generators, and four ring modulators. In 1965 he composed Mikrophonie II for choir, Hammond organ, and ring modulators.[112]
 In 1966–1967, Reed Ghazala discovered and began to teach ""circuit bending""—the application of the creative short circuit, a process of chance short-circuiting, creating experimental electronic instruments, exploring sonic elements mainly of timbre and with less regard to pitch or rhythm, and influenced by John Cage's aleatoric music  [sic] concept.[113]
 Cosey Fanni Tutti's performance art and musical career explored the concept of 'acceptable' music and she went on to explore the use of sound as a means of desire or discomfort.[114][failed verification]
 Wendy Carlos performed selections from her album Switched-On Bach on stage with a synthesizer with the St. Louis Symphony Orchestra; another live performance was with Kurzweil Baroque Ensemble for ""Bach at the Beacon"" in 1997.[115] In June 2018, Suzanne Ciani released LIVE Quadraphonic, a live album documenting her first solo performance on a Buchla synthesizer in 40 years. It was one of the first quadraphonic vinyl releases in over 30 years.[116]
 In the 1950s,[117][118] Japanese electronic musical instruments began influencing the international music industry.[119][120] Ikutaro Kakehashi, who founded Ace Tone in 1960, developed his own version of electronic percussion that had been already popular on the overseas electronic organ.[121] At the 1964 NAMM Show, he revealed it as the R-1 Rhythm Ace, a hand-operated percussion device that played electronic drum sounds manually as the user pushed buttons, in a similar fashion to modern electronic drum pads.[121][122][123]
 In 1963, Korg released the Donca-Matic DA-20, an electro-mechanical drum machine.[124] In 1965, Nippon Columbia patented a fully electronic drum machine.[125] Korg released the Donca-Matic DC-11 electronic drum machine in 1966, which they followed with the Korg Mini Pops, which was developed as an option for the Yamaha Electone electric organ.[124] Korg's Stageman and Mini Pops series were notable for ""natural metallic percussion"" sounds and incorporating controls for drum ""breaks and fill-ins.""[120]
 In 1967, Ace Tone founder Ikutaro Kakehashi patented a preset rhythm-pattern generator using diode matrix circuit[126] similar to the Seeburg's prior U.S. patent 3,358,068 filed in 1964 (See Drum machine#History), which he released as the FR-1 Rhythm Ace drum machine the same year.[121] It offered 16 preset patterns, and four buttons to manually play each instrument sound (cymbal, claves, cowbell and bass drum). The rhythm patterns could also be cascaded together by pushing multiple rhythm buttons simultaneously, and the possible combination of rhythm patterns were more than a hundred.[121] Ace Tone's Rhythm Ace drum machines found their way into popular music from the late 1960s, followed by Korg drum machines in the 1970s.[120] Kakehashi later left Ace Tone and founded Roland Corporation in 1972, with Roland synthesizers and drum machines becoming highly influential for the next several decades.[121] The company would go on to have a big impact on popular music, and do more to shape popular electronic music than any other company.[123]
 Turntablism has origins in the invention of direct-drive turntables. Early belt-drive turntables were unsuitable for turntablism, since they had a slow start-up time, and they were prone to wear-and-tear and breakage, as the belt would break from backspin or scratching.[127] The first direct-drive turntable was invented by Shuichi Obata, an engineer at Matsushita (now Panasonic),[128] based in Osaka, Japan. It eliminated belts, and instead employed a motor to directly drive a platter on which a vinyl record rests.[129] In 1969, Matsushita released it as the SP-10,[129] the first direct-drive turntable on the market,[130] and the first in their influential Technics series of turntables.[129] It was succeeded by the Technics SL-1100 and SL-1200 in the early 1970s, and they were widely adopted by hip hop musicians,[129] with the SL-1200 remaining the most widely used turntable in DJ culture for several decades.[131]
 In Jamaica, a form of popular electronic music emerged in the 1960s, dub music, rooted in sound system culture. Dub music was pioneered by studio engineers, such as Sylvan Morris, King Tubby, Errol Thompson, Lee ""Scratch"" Perry, and Scientist, producing reggae-influenced experimental music with electronic sound technology, in recording studios and at sound system parties.[132] Their experiments included forms of tape-based composition comparable to aspects of musique concrète, an emphasis on repetitive rhythmic structures (often stripped of their harmonic elements) comparable to minimalism, the electronic manipulation of spatiality, the sonic electronic manipulation of pre-recorded musical materials from mass media, deejays toasting over pre-recorded music comparable to live electronic music,[132] remixing music,[133] turntablism,[134] and the mixing and scratching of vinyl.[135]
 Despite the limited electronic equipment available to dub pioneers such as King Tubby and Lee ""Scratch"" Perry, their experiments in remix culture were musically cutting-edge.[133] King Tubby, for example, was a sound system proprietor and electronics technician, whose small front-room studio in the Waterhouse ghetto of western Kingston was a key site of dub music creation.[136]
 In the late 1960s, pop and rock musicians, including the Beach Boys and the Beatles, began to use electronic instruments, like the theremin and Mellotron, to supplement and define their sound. The first bands to utilize the Moog synthesizer would be the Doors on Strange Days[137] as well as the Monkees on Pisces, Aquarius, Capricorn & Jones Ltd. In his book Electronic and Experimental Music, Thom Holmes recognises the Beatles' 1966 recording ""Tomorrow Never Knows"" as the song that ""ushered in a new era in the use of electronic music in rock and pop music"" due to the band's incorporation of tape loops and reversed and speed-manipulated tape sounds.[138]
 Also in the late 1960s, the music duos Silver Apples, Beaver and Krause, and experimental rock bands like White Noise, the United States of America, Fifty Foot Hose, and Gong are regarded as pioneers in the electronic rock and electronica genres for their work in melding psychedelic rock with oscillators and synthesizers.[139][140][141][142][143][144] The 1969 instrumental ""Popcorn"" written by Gershon Kingsley for Music To Moog By became a worldwide success due to the 1972 version made by Hot Butter.[145][146]
 The Moog synthesizer was brought to the mainstream in 1968 by Switched-On Bach, a bestselling album of Bach compositions arranged for Moog synthesizer by American composer Wendy Carlos. The album achieved critical and commercial success, winning the 1970 Grammy Awards for Best Classical Album, Best Classical Performance – Instrumental Soloist or Soloists (With or Without Orchestra), and Best Engineered Classical Recording.
 In 1969, David Borden formed the world's first synthesizer ensemble called the Mother Mallard's Portable Masterpiece Company in Ithaca, New York.[147]
 By the end of the 1960s, the Moog synthesizer took a leading place in the sound of emerging progressive rock with bands including Pink Floyd, Yes, Emerson, Lake & Palmer, and Genesis making them part of their sound. Instrumental prog rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Cluster, Can, Neu!, and Faust to circumvent the language barrier.[148] Their synthesiser-heavy ""krautrock"", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent electronic rock.[149]
 Ambient dub was pioneered by King Tubby and other Jamaican sound artists, using DJ-inspired ambient electronics, complete with drop-outs, echo, equalization and psychedelic electronic effects. It featured layering techniques and incorporated elements of world music, deep basslines and harmonic sounds.[150] Techniques such as a long echo delay were also used.[151] Other notable artists within the genre include Dreadzone, Higher Intelligence Agency, The Orb, Ott, Loop Guru, Woob and Transglobal Underground.[152]
 Dub music influenced electronic musical techniques later adopted by hip hop music when Jamaican immigrant DJ Kool Herc in the early 1970s introduced Jamaica's sound system culture and dub music techniques to America. One such technique that became popular in hip hop culture was playing two copies of the same record on two turntables in alternation, extending the b-dancers' favorite section.[153] The turntable eventually went on to become the most visible electronic musical instrument, and occasionally the most virtuosic, in the 1980s and 1990s.[134]
 Electronic rock was also produced by several Japanese musicians, including Isao Tomita's Electric Samurai: Switched on Rock (1972), which featured Moog synthesizer renditions of contemporary pop and rock songs,[154] and Osamu Kitajima's progressive rock album Benzaiten (1974).[155] The mid-1970s saw the rise of electronic art music musicians such as Jean Michel Jarre, Vangelis, Tomita and Klaus Schulze who were significant influences on the development of new-age music.[150] The hi-tech appeal of these works created for some years the trend of listing the electronic musical equipment employed in the album sleeves, as a distinctive feature. Electronic music began to enter regularly in radio programming and top-sellers charts, as the French band Space with their debut studio album Magic Fly[156] or Jarre with Oxygène.[157] Between 1977 and 1981, Kraftwerk released albums such as Trans-Europe Express, The Man-Machine or Computer World, which influenced subgenres of electronic music.[158]
 In this era, the sound of rock musicians like Mike Oldfield and The Alan Parsons Project (who is credited the first rock song to feature a digital vocoder in 1975, The Raven) used to be arranged and blended with electronic effects and/or music as well, which became much more prominent in the mid-1980s. Jeff Wayne achieved a long-lasting success[159] with his 1978 electronic rock musical version of The War of the Worlds.
 Film scores also benefit from the electronic sound. During the 1970s and 1980s, Wendy Carlos composed the score for A Clockwork Orange, The Shining and Tron.[160] In 1977, Gene Page recorded a disco version of the hit theme by John Williams from Steven Spielberg film Close Encounters of the Third Kind. Page's version peaked on the R&B chart at #30.[citation needed] The score of 1978 film Midnight Express composed by Italian synth-pioneer Giorgio Moroder won the Academy Award for Best Original Score in 1979, as did it again in 1981 the score by Vangelis for Chariots of Fire.[161] After the arrival of punk rock, a form of basic electronic rock emerged, increasingly using new digital technology to replace other instruments. The American duo Suicide, who arose from the punk scene in New York, utilized drum machines and synthesizers in a hybrid between electronics and punk on their eponymous 1977 album.[162]
 Synth-pop pioneering bands which enjoyed success for years included Ultravox with their 1977 track ""Hiroshima Mon Amour"" on Ha!-Ha!-Ha!,[163] Yellow Magic Orchestra with their self-titled album (1978), The Buggles with their prominent 1979 debut single Video Killed the Radio Star,[164] Gary Numan with his solo debut album The Pleasure Principle and single Cars in 1979,[165] Orchestral Manoeuvres in the Dark with their 1979 single Electricity featured on their eponymous debut album, Depeche Mode with their first single Dreaming of Me recorded in 1980 and released in 1981 album Speak & Spell,[166] A Flock of Seagulls with their 1981 single Talking,[167] New Order with Ceremony[168] in 1981, and The Human League with their 1981 hit Don't You Want Me from their third album Dare.[169]
 The definition of MIDI and the development of digital audio made the development of purely electronic sounds much easier,[170] with audio engineers, producers and composers exploring frequently the possibilities of virtually every new model of electronic sound equipment launched by manufacturers. Synth-pop sometimes used synthesizers to replace all other instruments, but it was more common that bands had one or more keyboardists in their line-ups along with guitarists, bassists, and/or drummers. These developments led to the growth of synth-pop, which after it was adopted by the New Romantic movement, allowed synthesizers to dominate the pop and rock music of the early 1980s until the style began to fall from popularity in the mid-to-end of the decade.[169] Along with the aforementioned successful pioneers, key acts included Yazoo, Duran Duran, Spandau Ballet, Culture Club, Talk Talk, Japan, and Eurythmics.
 Synth-pop was taken up across the world, with international hits for acts including Men Without Hats, Trans-X and Lime from Canada, Telex from Belgium, Peter Schilling, Sandra, Modern Talking, Propaganda and Alphaville from Germany, Yello from Switzerland and Azul y Negro from Spain. Also, the synth sound is a key feature of Italo-disco.
 Some synth-pop bands created futuristic visual styles of themselves to reinforce the idea of electronic sounds were linked primarily with technology, as Americans Devo and Spaniards Aviador Dro.
 Keyboard synthesizers became so common that even heavy metal rock bands, a genre often regarded as the opposite in aesthetics, sound and lifestyle from that of electronic pop artists by fans of both sides, achieved worldwide success with themes as 1983 Jump[171] by Van Halen and 1986 The Final Countdown[172] by Europe, which feature synths prominently.
 Elektronmusikstudion [sv] (EMS), formerly known as Electroacoustic Music in Sweden, is the Swedish national centre for electronic music and sound art. The research organisation started in 1964 and is based in Stockholm.
 STEIM is a center for research and development of new musical instruments in the electronic performing arts, located in Amsterdam, Netherlands. STEIM has existed since 1969. It was founded by Misha Mengelberg, Louis Andriessen, Peter Schat, Dick Raaymakers, Jan van Vlijmen [nl], Reinbert de Leeuw, and Konrad Boehmer. This group of Dutch composers had fought for the reformation of Amsterdam's feudal music structures; they insisted on Bruno Maderna's appointment as musical director of the Concertgebouw Orchestra and enforced the first public fundings for experimental and improvised electronic music in the Netherlands.
 IRCAM in Paris became a major center for computer music research and realization and development of the Sogitec 4X computer system,[173] featuring then revolutionary real-time digital signal processing. Pierre Boulez's Répons (1981) for 24 musicians and 6 soloists used the 4X to transform and route soloists to a loudspeaker system.
 Barry Vercoe describes one of his experiences with early computer sounds:
 At IRCAM in Paris in 1982, flutist Larry Beauregard had connected his flute to DiGiugno's 4X audio processor, enabling real-time pitch-following. On a Guggenheim at the time, I extended this concept to real-time score-following with automatic synchronized accompaniment, and over the next two years Larry and I gave numerous demonstrations of the computer as a chamber musician, playing Handel flute sonatas, Boulez's Sonatine for flute and piano and by 1984 my own Synapse II for flute and computer—the first piece ever composed expressly for such a setup. A major challenge was finding the right software constructs to support highly sensitive and responsive accompaniment. All of this was pre-MIDI, but the results were impressive even though heavy doses of tempo rubato would continually surprise my Synthetic Performer. In 1985 we solved the tempo rubato problem by incorporating learning from rehearsals (each time you played this way the machine would get better). We were also now tracking violin, since our brilliant, young flautist had contracted a fatal cancer. Moreover, this version used a new standard called MIDI, and here I was ably assisted by former student Miller Puckette, whose initial concepts for this task he later expanded into a program called MAX.[175] Released in 1970 by Moog Music, the Mini-Moog was among the first widely available, portable, and relatively affordable synthesizers. It became once the most widely used synthesizer at that time in both popular and electronic art music.[176]
Patrick Gleeson, playing live with Herbie Hancock at the beginning of the 1970s, pioneered the use of synthesizers in a touring context, where they were subject to stresses the early machines were not designed for.[177][178]
 In 1974, the WDR studio in Cologne acquired an EMS Synthi 100 synthesizer, which many composers used to produce notable electronic works—including Rolf Gehlhaar's Fünf deutsche Tänze (1975), Karlheinz Stockhausen's Sirius (1975–1976), and John McGuire's Pulse Music III (1978).[179]
 Thanks to miniaturization of electronics in the 1970s, by the start of the 1980s keyboard synthesizers, became lighter and affordable, integrating into a single slim unit all the necessary audio synthesis electronics and the piano-style keyboard itself, in sharp contrast with the bulky machinery and ""cable spaguetty"" employed along with the 1960s and 1970s. First, with analog synthesizers, the trend followed with digital synthesizers and samplers as well (see below).
 In 1975, the Japanese company Yamaha licensed the algorithms for frequency modulation synthesis (FM synthesis) from John Chowning, who had experimented with it at Stanford University since 1971.[180][181] Yamaha's engineers began adapting Chowning's algorithm for use in a digital synthesizer, adding improvements such as the ""key scaling"" method to avoid the introduction of distortion that normally occurred in analog systems during frequency modulation.[182]
 In 1980, Yamaha eventually released the first FM digital synthesizer, the Yamaha GS-1, but at an expensive price.[183] In 1983, Yamaha introduced the first stand-alone digital synthesizer, the DX7, which also used FM synthesis and would become one of the best-selling synthesizers of all time.[180] The DX7 was known for its recognizable bright tonalities that was partly due to an overachieving sampling rate of 57 kHz.[184]
 The Korg Poly-800 is a synthesizer released by Korg in 1983. Its initial list price of $795 made it the first fully programmable synthesizer that sold for less than $1000. It had 8-voice polyphony with one Digitally controlled oscillator (DCO) per voice.
 The Casio CZ-101 was the first and best-selling phase distortion synthesizer in the Casio CZ line. Released in November 1984, it was one of the first (if not the first) fully programmable polyphonic synthesizers that was available for under $500.
 The Roland D-50 is a digital synthesizer produced by Roland and released in April 1987. Its features include subtractive synthesis, on-board effects, a joystick for data manipulation, and an analogue synthesis-styled layout design. The external Roland PG-1000 (1987–1990) programmer could also be attached to the D-50 for more complex manipulation of its sounds.
 A sampler is an electronic or digital musical instrument which uses sound recordings (or ""samples"") of real instrument sounds (e.g., a piano, violin or trumpet), excerpts from recorded songs (e.g., a five-second bass guitar riff from a funk song) or found sounds (e.g., sirens and ocean waves). The samples are loaded or recorded by the user or by a manufacturer. These sounds are then played back using the sampler program itself, a MIDI keyboard, sequencer or another triggering device (e.g., electronic drums) to perform or compose music. Because these samples are usually stored in digital memory, the information can be quickly accessed. A single sample may often be pitch-shifted to different pitches to produce musical scales and chords.
 Before computer memory-based samplers, musicians used tape replay keyboards, which store recordings on analog tape. When a key is pressed the tape head contacts the moving tape and plays a sound. The Mellotron was the most notable model, used by many groups in the late 1960s and the 1970s, but such systems were expensive and heavy due to the multiple tape mechanisms involved, and the range of the instrument was limited to three octaves at the most. To change sounds a new set of tapes had to be installed in the instrument. The emergence of the digital sampler made sampling far more practical.
 The earliest digital sampling was done on the EMS Musys system, developed by Peter Grogono (software), David Cockerell (hardware and interfacing), and Peter Zinovieff (system design and operation) at their London (Putney) Studio c. 1969.
 The first commercially available sampling synthesizer was the Computer Music Melodian by Harry Mendell (1976).
 First released in 1977–1978,[185] the Synclavier I using FM synthesis, re-licensed from Yamaha,[186] and sold mostly to universities, proved to be highly influential among both electronic music composers and music producers, including Mike Thorne, an early adopter from the commercial world, due to its versatility, its cutting-edge technology, and distinctive sounds.
 The first polyphonic digital sampling synthesizer was the Australian-produced Fairlight CMI, first available in 1979. These early sampling synthesizers used wavetable sample-based synthesis.[187]
 In 1980, a group of musicians and music merchants met to standardize an interface that new instruments could use to communicate control instructions with other instruments and computers. This standard was dubbed Musical Instrument Digital Interface (MIDI) and resulted from a collaboration between leading manufacturers, initially Sequential Circuits, Oberheim, Roland—and later, other participants that included Yamaha, Korg, and Kawai.[188] A paper was authored by Dave Smith of Sequential Circuits and proposed to the Audio Engineering Society in 1981. Then, in August 1983, the MIDI Specification 1.0 was finalized.
 MIDI technology allows a single keystroke, control wheel motion, pedal movement, or command from a microcomputer to activate every device in the studio remotely and synchrony, with each device responding according to conditions predetermined by the composer.
 MIDI instruments and software made powerful control of sophisticated instruments easily affordable by many studios and individuals. Acoustic sounds became reintegrated into studios via sampling and sampled-ROM-based instruments.
 Miller Puckette developed graphic signal-processing software for 4X called Max (after Max Mathews) and later ported it to Macintosh (with Dave Zicarelli extending it for Opcode)[189] for real-time MIDI control, bringing algorithmic composition availability to most composers with modest computer programming background.
 The early 1980s saw the rise of bass synthesizers, the most influential being the Roland TB-303, a bass synthesizer and sequencer released in late 1981 that later became a fixture in electronic dance music,[190] particularly acid house.[191] One of the first to use it was Charanjit Singh in 1982, though it would not be popularized until Phuture's ""Acid Tracks"" in 1987.[191] Music sequencers began being used around the mid 20th century, and Tomita's albums in mid-1970s being later examples.[154] In 1978, Yellow Magic Orchestra were using computer-based technology in conjunction with a synthesiser to produce popular music,[192] making their early use of the microprocessor-based Roland MC-8 Microcomposer sequencer.[193][194][failed verification]
 Drum machines, also known as rhythm machines, also began being used around the late-1950s, with a later example being Osamu Kitajima's progressive rock album Benzaiten (1974), which used a rhythm machine along with electronic drums and a synthesizer.[155] In 1977, Ultravox's ""Hiroshima Mon Amour"" was one of the first singles to use the metronome-like percussion of a Roland TR-77 drum machine.[163] In 1980, Roland Corporation released the TR-808, one of the first and most popular programmable drum machines. The first band to use it was Yellow Magic Orchestra in 1980, and it would later gain widespread popularity with the release of Marvin Gaye's ""Sexual Healing"" and Afrika Bambaataa's ""Planet Rock"" in 1982.[195] The TR-808 was a fundamental tool in the later Detroit techno scene of the late 1980s, and was the drum machine of choice for Derrick May and Juan Atkins.[196]
 The characteristic lo-fi sound of chip music was initially the result of early computer's sound chips and sound cards' technical limitations; however, the sound has since become sought after in its own right.
 Common cheap popular sound chips of the first home computers of the 1980s include the SID of the Commodore 64 and General Instrument AY series and clones (like the Yamaha YM2149) used in the ZX Spectrum, Amstrad CPC, MSX compatibles and Atari ST models, among others.
 Synth-pop continued into the late 1980s, with a format that moved closer to dance music, including the work of acts such as British duos Pet Shop Boys, Erasure and The Communards, achieving success along much of the 1990s.
 The trend has continued to the present day with modern nightclubs worldwide regularly playing electronic dance music (EDM). Today, electronic dance music has radio stations,[197] websites,[198] and publications like Mixmag dedicated solely to the genre. Despite the industry's attempt to create a specific EDM brand, the initialism remains in use as an umbrella term for multiple genres, including dance-pop, house, techno, electro, and trance, as well as their respective subgenres.[199][200][201] Moreover, the genre has found commercial and cultural significance in the United States and North America, thanks to the wildly popular big room house/EDM sound that has been incorporated into the U.S. pop music[202] and the rise of large-scale commercial raves such as Electric Daisy Carnival, Tomorrowland and Ultra Music Festival.
 On the other hand, a broad group of electronic-based music styles intended for listening rather than strictly for dancing became known under the ""electronica"" umbrella[203][204] which was also a music scene in the early 1990s in the United Kingdom.[204] According to a 1997 Billboard article, ""the union of the club community and independent labels"" provided the experimental and trend-setting environment in which electronica acts developed and eventually reached the mainstream, citing American labels such as Astralwerks (the Chemical Brothers, Fatboy Slim, the Future Sound of London, Fluke), Moonshine (DJ Keoki), Sims, Daft Punk and City of Angels (the Crystal Method) for popularizing the latest version of electronic music.[citation needed]
 The category ""indie electronic"" (or ""indietronica"")[205] has been used to refer to a wave of groups with roots in independent rock who embraced electronic elements (such as synthesizers, samplers, drum machines, and computer programs) and influences such as early electronic composition, krautrock, synth-pop, and dance music.[206] Recordings are commonly made on laptops using digital audio workstations.[205]
 The first wave of indie electronic artists began in the 1990s with acts such as Stereolab (who used vintage gear) and Disco Inferno (who embraced modern sampling technology), and the genre expanded in the 2000s as home recording and software synthesizers came into common use.[206] Other acts included Broadcast, Lali Puna, Múm, the Postal Service, Skeletons, and School of Seven Bells.[206] Independent labels associated with the style include Warp, Morr Music, Sub Pop, and Ghostly International.[206]
 As computer technology has become more accessible and music software has advanced, interacting with music production technology is now possible using means that bear no relationship to traditional musical performance practices:[207] for instance, laptop performance (laptronica),[208] live coding[209][210] and Algorave. In general, the term Live PA refers to any live performance of electronic music, whether with laptops, synthesizers, or other devices.
 Beginning around the year 2000, some software-based virtual studio environments emerged, with products such as Propellerhead's Reason and Ableton Live finding popular appeal.[211] Such tools provide viable and cost-effective alternatives to typical hardware-based production studios, and thanks to advances in microprocessor technology, it is now possible to create high-quality music using little more than a single laptop computer. Such advances have democratized music creation,[212] leading to a massive increase in the amount of home-produced electronic music available to the general public via the internet. Software-based instruments and effect units (so-called ""plugins"") can be incorporated in a computer-based studio using the VST platform. Some of these instruments are more or less exact replicas of existing hardware (such as the Roland D-50, ARP Odyssey, Yamaha DX7, or Korg M1).[citation needed]
 Circuit bending is the modification of battery-powered toys and synthesizers to create new unintended sound effects. It was pioneered by Reed Ghazala in the 1960s and Reed coined the name ""circuit bending"" in 1992.[213]
 Following the circuit bending culture, musicians also began to build their own modular synthesizers, causing a renewed interest in the early 1960s designs. Eurorack became a popular system.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['home-produced electronic music available to the general public via the internet', 'Devo and Spaniards Aviador Dro', 'commercial and cultural', 'The public remained interested in the new sounds being created around the world', 'fertile years'], 'answer_start': [], 'answer_end': []}"
"""World music"" is an English phrase for styles of music from non-Western countries, including quasi-traditional, intercultural, and traditional music.  World music's broad nature and elasticity as a musical category pose obstacles to a universal definition, but its ethic of interest in the culturally exotic is encapsulated in Roots magazine's description of the genre as ""local music from out there"".[1][2]
 This music that does not follow ""North American or British pop and folk traditions""[3] was given the term ""world music"" by music industries in Europe and North America.[4] The term was popularized in the 1980s as a marketing category for non-Western traditional music.[5][6] It has grown to include subgenres such as ethnic fusion (Clannad, Ry Cooder, Enya, etc.)[7] and worldbeat.[8][9]
 The term ""world music"" has been credited to ethnomusicologist Robert E. Brown, who coined it in the early 1960s at Wesleyan University in Connecticut, where he developed undergraduate through doctoral programs in the discipline. To enhance the learning process (John Hill), he invited more than a dozen visiting performers from Africa and Asia and began a world music concert series.[10][11] The term became current in the 1980s as a marketing/classificatory device in the media and the music industry.[12] There are several conflicting definitions for world music. One is that it consists of ""all the music in the world"", though such a broad definition renders the term virtually meaningless.[13][14]
 Examples of popular forms of world music include the various forms of non-European classical music (e.g. Chinese guzheng music, Indian raga music, Tibetan chants), Eastern European folk music (e.g. the village music of the Balkans, The Mystery of the Bulgarian Voices), Nordic folk music, Latin music, Indonesian music, and the many forms of folk and tribal music of the Middle East, Africa, Asia, Oceania, Central and South America.
 The broad category of world music includes isolated forms of ethnic music from diverse geographical regions. These dissimilar strains of ethnic music are commonly categorized together by virtue of their indigenous roots. Over the 20th century, the invention of sound recording, low-cost international air travel, and common access to global communication among artists and the general public have given rise to a related phenomenon called ""crossover"" music. Musicians from diverse cultures and locations could readily access recorded music from around the world, see and hear visiting musicians from other cultures and visit other countries to play their own music, creating a melting pot of stylistic influences. While communication technology allows greater access to obscure forms of music, the pressures of commercialization also present the risk of increasing musical homogeneity, the blurring of regional identities, and the gradual extinction of traditional local music-making practices.[15]
 Since the music industry established this term, the fuller scope of what an average music consumer defines as ""world"" music in today's market has grown to include various blends of ethnic music tradition, style and interpretation,[9] and derivative world music genres have been coined to represent these hybrids, such as ethnic fusion and worldbeat. Good examples of hybrid, world fusion are the Irish-West African meld of Afro Celt Sound System,[16] the pan-cultural sound of AO Music[17] and the jazz / Finnish folk music of Värttinä,[18] each of which bear tinges of contemporary, Western influence—an increasingly noticeable element in the expansion genres of world music. Worldbeat and ethnic fusion can also blend specific indigenous sounds with more blatant elements of Western pop. Good examples are Paul Simon's album Graceland, on which South African mbaqanga music is heard; Peter Gabriel's work with Pakistani Sufi singer Nusrat Fateh Ali Khan; the Deep Forest project, in which vocal loops from West Africa are blended with Western, contemporary rhythmic textures and harmony structure; and the work of Mango, who combined pop and rock music with world elements.
 Depending on style and context, world music can sometimes share the new-age music genre, a category that often includes ambient music and textural expressions from indigenous roots sources. Good examples are Tibetan bowls, Tuvan throat singing, Gregorian chant or Native American flute music. World music blended with new-age music is a sound loosely classified as the hybrid genre 'ethnic fusion'. Examples of ethnic fusion are Nicholas Gunn's ""Face-to-Face"" from Beyond Grand Canyon, featuring authentic Native American flute combined with synthesizers, and ""Four Worlds"" from The Music of the Grand Canyon, featuring spoken word from Razor Saltboy of the Navajo Indian Nation.
 The subgenre world fusion is often mistakenly assumed to refer exclusively to a blending of Western jazz fusion elements with world music. Although such a hybrid expression falls easily into the world fusion category, the suffix ""fusion"" in the term world fusion should not be assumed to mean jazz fusion. Western jazz combined with strong elements of world music is more accurately termed world fusion jazz,[19] ethnic jazz or non-Western jazz. World fusion and global fusion are nearly synonymous with the genre term worldbeat, and though these are considered subgenres of popular music, they may also imply universal expressions of the more general term world music.[9] In the 1970s and 80s, fusion in the jazz music genre implied a blending of jazz and rock music, which is where the misleading assumption is rooted.[20]
 Millie Small released ""My Boy Lollipop"" in 1964. Small's version was a hit, reaching number 2 both in the UK Singles Chart[21] and in the US Billboard Hot 100. In the 1960s, Miriam Makeba and Hugh Masekela had popular hits in the USA. In 1969 Indian musician Ravi Shankar played sitar at the Woodstock festival.[22]
 In the 1970s, Manu Dibango's funky track ""Soul Makossa""[23] (1972) became a hit, and Osibisa released ""Sunshine Day"" (1976). Fela Kuti created Afrobeat[24] and Femi Kuti, Seun Kuti and Tony Allen followed Fela Kuti's funky music. Salsa musicians such as José Alberto ""El Canario"", Ray Sepúlveda, Johnny Pacheco, Fania All-Stars, Ray Barretto, Rubén Blades, Gilberto Santa Rosa, Roberto Roena, Bobby Valentín, Eddie Palmieri, Héctor Lavoe and Willie Colón developed Latin music.[25]
 The Breton musician Alan Stivell pioneered the connection between traditional folk music, modern rock music and world music with his 1972 album Renaissance of the Celtic Harp.[26] Around the same time, Stivell's contemporary, Welsh singer-songwriter Meic Stevens popularised Welsh folk music.[27] Neo-traditional Welsh language music featuring a fusion of modern instruments and traditional instruments such as the pibgorn and the Welsh harp has been further developed by Bob Delyn a'r Ebillion. Lebanese musical pioneer Lydia Canaan fused Middle-Eastern quarter notes and microtones with anglophone folk, and is listed in the catalog of the Rock and Roll Hall of Fame and Museum's Library and Archives[28][29] as the first rock star of the Middle East.[29][30][31][32][33]
 Although it primarily describes traditional music, the world music category also includes popular music from non-Western urban communities (e.g. South African ""township"" music) and non-European music forms that have been influenced by other so-called third-world musics (e.g. Afro-Cuban music).[34]
 The inspiration of Zimbabwe's Thomas Mapfumo in blending the Mbira (finger Piano) style onto the electric guitar, saw a host of other Zimbabwean musicians refining the genre, none more successfully than The Bhundu Boys. The Bhundu Jit music hit Europe with some force in 1986, taking Andy Kershaw and John Peel fully under its spell.
 For many years, Paris has attracted numerous musicians from former colonies in West and North Africa. This scene is aided by the fact that there are many concerts and institutions that help to promote the music.
 Algerian and Moroccan music have an important presence in the French capital. Hundreds of thousands of Algerian and Moroccan immigrants have settled in Paris, bringing the sounds of Amazigh (Berber), raï, and Gnawa music.
 The West African music community is also very large, integrated by people from Senegal, Mali, Ivory Coast, and Guinea.
 Unlike musical styles from other regions of the globe, the American music industry tends to categorize Latin music as its own genre and defines it as any music sung in Spanish from the Spanish-speaking world.[35]
 The most common name for this form of music is also ""folk music"", but is often called ""contemporary folk music"" or ""folk revival music"" to make the distinction.[36] The transition was somewhat centered in the US and is also called the American folk music revival.[37] Fusion genres such as folk rock and others also evolved within this phenomenon.
 On 29 June 1987, a meeting of interested parties gathered to capitalize on the marketing of non-Western folk music. Paul Simon had released the world music-influenced album Graceland in 1986.[38] The concept behind the album had been to express his own sensibilities using the sounds he had fallen in love with while listening to artists from Southern Africa, including Ladysmith Black Mambazo and Savuka. This project and the work of Peter Gabriel and Johnny Clegg among others had, to some degree, introduced non-Western music to a wider audience. They saw this as an opportunity.
 In an unprecedented move, all of the world music labels coordinated together and developed a compilation cassette for the cover of the music magazine NME. The overall running time was 90 minutes, each package containing a mini-catalog showing the other releases on offer.
 By the time of a second meeting it became clear that a successful campaign required its own dedicated press officer. The press officer would be able to juggle various deadlines and sell the music as a concept—not just to national stations, but also regional DJs keen to expand their musical variety. DJs were a key resource as it was important to make ""world music"" important to people outside London—most regions after all had a similarly heritage to tap into. A cost-effective way of achieving all this would be a leafleting campaign.
 The next step was to develop a world music chart, gathering together selling information from around fifty shops, so that it would finally be possible to see which were big sellers in the genre—so new listeners could see what was particularly popular. It was agreed that the NME could again be involved in printing the chart and also Music Week and the London listings magazine City Limits. It was also suggested that Andy Kershaw might be persuaded to do a run down of this chart on his show regularly.
 In most wealthy industrialized countries, large amounts of immigration from other regions has been ongoing for many decades. This has introduced non-Western music to Western audiences not only as ""exotic"" imports, but also as local music played by fellow citizens. But the process is ongoing and continues to produce new forms. In the 2010s several musicians from immigrant communities in the West rose to global popularity, such as Haitian-American Wyclef Jean, Somali-Canadian K'naan, Tamil-Briton M.I.A., often blending the music of their heritage with hip-hop or pop. Cuban-born singer-songwriter Addys Mercedes started her international career from Germany mixing traditional elements of Son with pop.[39]
 Once, an established Western artist might collaborate with an established African artist to produce an album or two. Now, new bands and new genres are built from the ground up by young performers. For example, the Punjabi-Irish fusion band Delhi 2 Dublin is from neither India nor Ireland, but Vancouver, British Columbia, Canada. Country for Syria, an Istanbul based music collective, blends American country music with the music of Syrian refugees and local Turkish music.[40] Musicians and composers also work collectively to create original compositions for various combinations of western and non western instruments.
 
The introduction of non-western music into western culture created a fusion that influenced both parties. (Feld 31)[41] With the quick demand for new music came the technicalities of ownership. As Feld states in page 31:[41] ""This complex traffic in sounds money and media is rooted in the nature of revitalization through appropriation."" There are collaborations between African and American popular music artists that raise questions on who is benefiting from said collaborations.(Feld 31)[41] Feld mentions the example of ""That was your mother"". Alton Rubin and his band the Twisters collaborated with Paul Simon on the song that possessed a zydeco feel, signature of Dopsie's band. Even though Paul Simon wrote and sang the lyrics with them, the whole copyright is attributed to Paul and not to the band as well. (Feld 34) [41] Because of crossovers like this one, where there was a disproportional gain when covering non-western music. Feld states that   ""...international music scene, where worldwide media contact, amalgamation of the music industry towards world record sales domination by three enormous companies, and extensive copyright controls by a few Western countries are having a riveting effect on the commodification of musical skill and styles, and on the power of musical ownership."" (Feld 32)[41] Immigration also heavily influences world music, providing a variety of options for the wider public. In the 1970s Punjabi music was greatly popular in the UK because of its growing Punjabi diaspora. (Schreffler 347)[42] Bhangra music was also greatly covered by its diaspora in cities like New York and Chicago. (Schreffler 351)[42] For a more mainstream integration, the Punjabi music scene integrated collaborations with rappers and started gaining more recognition. One of these successful attempts was a remix of the song ""Mundiān ton Bach ke"" called ""Beware of the Boys"" by Panjabi MC featuring Jay Z. (Schreffler 354)[43] Collaborations between outsider artists provided an integration of their music, even with foreign instrumentation, into the popular music scene.
 Immigration, being a great part of music exportation, plays a big role in cultural identity. Immigrant communities use music to feel as if they are home and future generations it plays the role of educating or giving insight into what their culture is about. In Punjabi culture, music became the carrier of culture around the world. (Schreffler 355)[43]
 World music radio programs today often play African hip hop or reggae artists, crossover Bhangra and Latin American jazz groups, etc. Common media for world music include public radio, webcasting, the BBC, NPR, and the Australian Broadcasting Corporation. By default, non-region-specific or multi-cultural world music projects are often listed under the generic category of world music.
 Examples of radio shows that feature world music include The Culture Cafe on WWUH West Hartford, World of Music on Voice of America, Transpacific Sound Paradise on WFMU, The Planet on Australia's ABC Radio National, DJ Edu presenting D.N.A: DestiNation Africa on BBC Radio 1Xtra, Adil Ray on the BBC Asian Network, Andy Kershaw's show on BBC Radio 3 and Charlie Gillett's show[44] on the BBC World Service.
 The BBC Radio 3 Awards for World Music was an award given to world music artists between 2002 and 2008, sponsored by BBC Radio 3. The award was thought up by fRoots magazine's editor Ian Anderson, inspired by the BBC Radio 2 Folk Awards. Award categories included: Africa, Asia/Pacific, Americas, Europe, Mid East and North Africa, Newcomer, Culture Crossing, Club Global, Album of the Year, and Audience Award. Initial lists of nominees in each category were selected annually by a panel of several thousand industry experts. Shortlisted nominees were voted on by a twelve-member jury, which selected the winners in every category except for the Audience Award category. These jury members were appointed and presided over by the BBC.[45] The annual awards ceremony was held at the BBC Proms and winners were given an award called a ""Planet"". In March 2009, the BBC made a decision to axe the BBC Radio 3 Awards for World Music.[46][47]
 In response to the BBC's decision to end its awards program, the British world music magazine Songlines launched the Songlines Music Awards in 2009 ""to recognise outstanding talent in world music"".[48]
 The WOMEX Awards were introduced in 1999 to honor the high points of world music on an international level and to acknowledge musical excellence, social importance, commercial success, political impact and lifetime achievement.[49] Every October at the WOMEX event, the award figurine—an ancient mother goddess statue dating back about 6000 years to the Neolithic age—is presented in an award ceremony to a worthy member of the world music community.
 Many festivals are identified as being ""world music""; here's a small representative selection:
 Australia
 Bangladesh
 Belgium
 Canada
 Croatia
 France
 Germany
 Ghana
 (Free Electronic Dance Music Festival) was established in (2020) at Busua Beach in the Western Region, by Djsky S K Y M U S I C.[54]
 Hungary
 Iceland
 India
 Indonesia
 Iran
 Italy
 North Macedonia
 Malaysia
 Mali
 Morocco
 New Zealand
 Nigeria
 Poland
 Portugal
 Romania
 Serbia
 Spain 
Spain's most important world music festivals are:
 Sweden
 Tanzania
 Turkey
 Uganda
 Ukraine
 United Kingdom
 United States
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['traditional music', 'Peter Gabriel and Johnny Clegg', 'all the music in the world', 'quasi-traditional, intercultural, and traditional music', 'various forms of non-European classical music'], 'answer_start': [], 'answer_end': []}"
"Dance music is music composed specifically to facilitate or accompany dancing. It can be either a whole piece or part of a larger musical arrangement. In terms of performance, the major categories are live dance music and recorded dance music. While there exist attestations of the combination of dance and music in ancient history (for example Ancient Greek vases sometimes show dancers accompanied by musicians), the earliest Western dance music that we can still reproduce with a degree of certainty are old-fashioned dances. In the Baroque period, the major dance styles were noble court dances (see Baroque dance). In the classical music era, the minuet was frequently used as a third movement, although in this context it would not accompany any dancing. The waltz also arose later in the classical era. Both remained part of the romantic music period, which also saw the rise of various other nationalistic dance forms like the barcarolle, mazurka, ecossaise, ballade and polonaise.
 Modern popular dance music initially emerged from late 19th century's Western ballroom and social dance music. During the early 20th century, ballroom dancing gained popularity among the working class who attended public dance halls. Dance music became enormously popular during the 1920s. In the 1930s, known as the Swing era, Swing music was the popular dance music in America. In the 1950s, rock and roll became the popular dance music. The late 1960s saw the rise of soul and R&B music. Dominican and Cuban New Yorkers created the popular salsa dance in the late 1960s which stemmed from the Latin music genre of salsa. The rise of disco in the early 1970s led to dance music becoming popular with the public. By the late 1970s, electronic dance music was developing. This music, made using electronics, is a style of popular music commonly played in nightclubs, radio stations, shows and raves. Many subgenres of electronic dance music  have evolved.
 Dancing to rhythmic music has long been a cherished tradition in both Western and Eastern African civilizations, where dynamic movements synchronized with percussion instruments such as drums, bells, and rattles serve as integral expressions of cultural identity, social cohesion, and spiritual significance. 
 Folk dance music is music accompanying traditional dance and may be contrasted with historical/classical, and popular/commercial dance music. An example of folk dance music in the United States is the old-time music played at square dances and contra dances.
 While there exist attestations of the combination of dance and music in ancient times (for example Ancient Greek vases sometimes show dancers accompanied by musicians), the earliest Western dance music that we can still reproduce with a degree of certainty are the surviving medieval dances such as carols and the Estampie. The earliest of these surviving dances are almost as old as Western staff-based music notation.
 The Renaissance dance music was written for instruments such as the lute, viol, tabor, pipe, and the sackbut.
 In the Baroque period, the major dance styles were noble court dances (see Baroque dance). Examples of dances include the French courante, sarabande, minuet and gigue. Collections of dances were often collected together as dance suites.
 In the classical music era, the minuet was frequently used as a third movement in four-movement non-vocal works such as sonatas, string quartets, and symphonies, although in this context it would not accompany any dancing. The waltz also arose later in the classical era, as the minuet evolved into the scherzo (literally, ""joke""; a faster-paced minuet).
 Both remained part of the romantic music period, which also saw the rise of various other nationalistic dance forms like the barcarolle, mazurka and polonaise. Also in the romantic music era, the growth and development of ballet extended the composition of dance music to a new height. Frequently, dance music was a part of opera.
 Modern popular dance music initially emerged from late 19th century's Western ballroom and social dance music.
 Dance music works often bear the name of the corresponding dance, e.g. waltzes, the tango, the bolero, the can-can, minuets, salsa, various kinds of jigs and the breakdown. Other dance forms include contradance, the merengue (Dominican Republic), and the cha-cha-cha. Often it is difficult to know whether the name of the music came first or the name of the dance.
 Ballads are commonly chosen for slow-dance routines. However ballads have been commonly deemed the opposite of dance music in terms of their tempo.[citation needed] Originally, the ballad was a type of dance as well (hence the name ""ballad"", from the same root as ""ballroom"" and ""ballet""). Ballads are still danced on the Faeroe Islands.
 ""Dansband"" (""Dance band"") is a term in Swedish for bands who play a kind of popular music, ""dansbandsmusik"" (""Dance band music""), to partner dance to. These terms came into use around 1970, and before that, many of the bands were classified as ""pop groups"". This type of music is mostly popular in the Nordic countries.
 Disco is a genre of dance music containing elements of funk, soul, pop, and salsa. It was most popular during the mid to late 1970s, though it has had brief resurgences afterwards. The first notable fully synthesized disco hit was ""I Feel Love"" by Donna Summer.[1] Looping,It inspired the electronic dance music genre.
 By 1981, a new form of dance music was developing. This music, made using electronics, is a style of popular music commonly played in dance music nightclubs, radio stations, shows and raves. During its gradual decline in the late 1970s, disco became influenced by electronic musical instruments such as synthesizers. sampling and segueing as found in disco continued to be used as creative techniques within trance music, techno music and especially house music.
 Electronic dance music experienced a boom in the late 1980s. In the UK, this manifested itself in the dance element of Tony Wilson's Haçienda scene (in Manchester) and London clubs like Delirium, The Trip, and Shoom. The scene rapidly expanded to the Summer Of Love in Ibiza, which became the European capital of house and trance. In 2018, the release of Fisher's ""Losing It,"" a significant tech-house crossover by the Australian EDM producer, marked a notable shift in trends within the dance music landscape.
 Many music genres that made use of electronic instruments developed into contemporary styles mainly due to the MIDI protocol, which enabled computers, synthesizers, sound cards, samplers, and drum machines to interact with each other and achieve the full synchronization of sounds. Electronic dance music is typically composed using synthesizers and computers, and rarely has any physical instruments. Instead, this is replaced by analogue and digital electronic sounds, with a 4/4 beat. Many producers of this kind of music however, such as Darren Tate and MJ Cole, were trained in classical music before they moved into the electronic medium.
 Associated with dance music are usually commercial tracks that may not easily be categorized, such as ""The Power"" by Snap!, ""No Limit"" by 2 Unlimited, ""Gonna Make You Sweat (Everybody Dance Now)"" by C+C Music Factory, and the Beatmasters' ""Rok da House"" but the term ""dance music"" is applied to many forms of electronic music, both commercial and non-commercial.
 Some of the most popular upbeat genres include house, techno, drum & bass, jungle, hardcore, electronica, industrial, breakbeat, trance, psychedelic trance, UK garage and electro. There are also much slower styles, such as downtempo, chillout and nu jazz.
 Many subgenres of electronic dance music have evolved. Subgenres of house include acid house, kwaito, electro house, hard house, funky house,deep house,afro house, tribal house, hip house, tech house and US garage. Subgenres of drum & bass include techstep, hardstep, jump-up, intelligent D&B/atmospheric D&B, liquid funk, sambass, drumfunk, neurofunk and ragga jungle. Subgenres of other styles include progressive breaks, booty bass, Goa trance, hard trance, hardstyle, minimal techno, gabber techno, breakcore, broken beat, trip hop, folktronica and glitch. Speed garage, breakstep, 2-step, bassline, grime, UK funky, future garage and the reggae-inspired dubstep are all subgenres of UK garage.
 During the early 20th century, ballroom dancing gained popularity among the working class who attended public dance halls.
 Dance music became enormously popular during the 1920s. Nightclubs were frequented by large numbers of people at which a form of jazz, which was characterized by fancy orchestras with strings instruments and complex arrangements, became the standard music at clubs. A particularly popular dance was the fox-trot. At the time this music was simply called jazz, although today people refer to it as ""white jazz"" or big band. Marabi evolved in South Africa in the 1920s, rooted in South African folk music, ragtime, jazz and blues. People were able to dance endlessly without having to have been familiar with the songs being played, before.[2][3]
 Genres: Swing music,mbube, Western swing. Duke Ellington, Benny Goodman and Glenn Miller gained swing jazz hits.
 Genres: Rock and roll, kwela
 In 1952, the television showed that  American Bandstand switched to a format where teenagers dance along as records are played. American Bandstand continued to be shown until 1989. Since the late 1950s, disc jockeys (commonly known as DJs) played recorded music at nightclubs.
 Genres: Rock and roll, R&B, funk, mbaqanga
 In 1960, Chubby Checker released his song ""The Twist"" setting off a dance craze.  The late 1960s saw the rise of soul and R&B music which used lavish orchestral arrangements.
 Genres: Disco, funk, R&B, hip hop
 In 1970, the television show Soul Train premiered featuring famous soul artists who would play or lipsync their hits while the audience danced along. In the early '70s, Kool and the Gang, Ohio Players, and B.T. Express were popular funk bands. By the mid-1970s, disco had become one of the main genres featured.  In 1974, Billboard added a Disco Action chart of top hits to its other charts (see List of Billboard number one dance club songs). Donna Summer, the Bee Gees, the Village People and Gloria Gaynor gained pop hits.[4] Disco was characterized by the use of real orchestral instruments, such as strings, which had largely been abandoned during the 1950s because of rock music. In contrast to the 1920s, however, the use of live orchestras in night clubs was extremely rare due to its expense.  The disco craze reached its peak in the late 1970s when the word ""disco"" became synonymous with ""dance music"" and nightclubs were referred to as ""discos"".
 Genres: Funk, hip hop, New jack swing,[5] R&B, bounce, Miami bass, boogie, disco,jaiva, contemporary R&B, new wave, dark wave, Italo disco, Euro disco, post-disco, synth-pop, dance-pop, dance-rock, house, kwaito, acid house, hip house, techno, freestyle, electro, hi-NRG, EBM, cosmic disco, Balearic beat, new beat
 Genres: New jack swing, contemporary R&B, dancehall, hip hop, G-funk, Miami bass, house, Italo dance, Italo house, Eurodance, Europop, hip house, electro, electroclash, progressive house, French house, techno, minimal techno, trance, alternative dance, drum and bass, jungle, big beat, breakbeat, breakbeat hardcore, rave, hardcore, happy hardcore, speed garage, UK garage, soca, reggaeton, psytrance, Goa trance, Afro house
 Genres: Trance,Afro-tech, electropop, dance-pop, snap, crunk, dancehall, reggaeton, dance-punk, nu-disco, electro house, minimal techno, dubstep, grime, bassline, UK funky, contemporary R&B, hip hop, drum and bass, progressive house, hardstyle, funky house
 Genres: Electropop, synthpop,gqom,amapiano, glitchpop, hip house, nu-disco, new wave, new rave, trance, house, hi-NRG, hard NRG, dance-pop, electro-industrial, deep house, drum and bass, dubstep, techstep, liquid funk, electro house, progressive house, breakbeat, hardstyle, dubstyle, drumstep, hip hop, ghetto house, Jersey club, trap, drill, moombahton, moombahcore, dancehall, tropical house, UK garage, Europop, hyperpop
 The Dance/Mix Show Airplay chart tracks the most popular tracks played by radio stations using a ""dance music"" format. Modern dance music is typically a core component of the rhythmic adult contemporary and rhythmic contemporary formats, and an occasional component of the contemporary hit radio format in the case of dance songs which chart.
 Mixshows are radio programmes which feature a sequence of dance music tracks where each track's outro is mixed into the intro of the next.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['live dance music and recorded dance music', 'Darren Tate and MJ Cole', 'the combination of dance and music in ancient history', 'live dance music and recorded dance music', 'specifically to facilitate or accompany dancing'], 'answer_start': [], 'answer_end': []}"
"
 Music theory is the study of the practices and possibilities of music. The Oxford Companion to Music describes three interrelated uses of the term ""music theory"": The first is the ""rudiments"", that are needed to understand music notation (key signatures, time signatures, and rhythmic notation); the second is learning scholars' views on music from antiquity to the present; the third is a sub-topic of musicology that ""seeks to define processes and general principles in music"". The musicological approach to theory differs from music analysis ""in that it takes as its starting-point not the individual work or performance but the fundamental materials from which it is built.""[1]
 Music theory is frequently concerned with describing how musicians and composers make music, including tuning systems and composition methods among other topics. Because of the ever-expanding conception of what constitutes music, a more inclusive definition could be the consideration of any sonic phenomena, including silence. This is not an absolute guideline, however; for example, the study of ""music"" in the Quadrivium liberal arts university curriculum, that was common in medieval Europe, was an abstract system of proportions that was carefully studied at a distance from actual musical practice.[n 1] But this medieval discipline became the basis for tuning systems in later centuries and is generally included in modern scholarship on the history of music theory.[n 2]
 Music theory as a practical discipline encompasses the methods and concepts that composers and other musicians use in creating and performing music. The development, preservation, and transmission of music theory in this sense may be found in oral and written music-making traditions, musical instruments, and other artifacts. For example, ancient instruments from prehistoric sites around the world reveal details about the music they produced and potentially something of the musical theory that might have been used by their makers. In ancient and living cultures around the world, the deep and long roots of music theory are visible in instruments, oral traditions, and current music-making. Many cultures have also considered music theory in more formal ways such as written treatises and music notation. Practical and scholarly traditions overlap, as many practical treatises about music place themselves within a tradition of other treatises, which are cited regularly just as scholarly writing cites earlier research.
 In modern academia, music theory is a subfield of musicology, the wider study of musical cultures and history. Music theory is often concerned with abstract musical aspects such as tuning and tonal systems, scales, consonance and dissonance, and rhythmic relationships. In addition, there is also a body of theory concerning practical aspects, such as the creation or the performance of music, orchestration, ornamentation, improvisation, and electronic sound production.[3] A person who researches or teaches music theory is a music theorist. University study, typically to the MA or PhD level, is required to teach as a tenure-track music theorist in a US or Canadian university. Methods of analysis include mathematics, graphic analysis, and especially analysis enabled by western music notation. Comparative, descriptive, statistical, and other methods are also used. Music theory textbooks, especially in the United States of America, often include elements of musical acoustics, considerations of musical notation, and techniques of tonal composition (harmony and counterpoint), among other topics.
 Several surviving Sumerian and Akkadian clay tablets include musical information of a theoretical nature, mainly lists of intervals and tunings.[4] The scholar Sam Mirelman reports that the earliest of these texts dates from before 1500 BCE, a millennium earlier than surviving evidence from any other culture of comparable musical thought. Further, ""All the Mesopotamian texts [about music] are united by the use of a terminology for music that, according to the approximate dating of the texts, was in use for over 1,000 years.""[5]
 Much of Chinese music history and theory remains unclear.[6]
 Chinese theory starts from numbers, the main musical numbers being twelve, five and eight. Twelve refers to the number of pitches on which the scales can be constructed. The Lüshi chunqiu from about 238 BCE recalls the legend of Ling Lun. On order of the Yellow Emperor, Ling Lun collected twelve bamboo lengths with thick and even nodes. Blowing on one of these like a pipe, he found its sound agreeable and named it huangzhong, the ""Yellow Bell."" He then heard phoenixes singing. The male and female phoenix each sang six tones. Ling Lun cut his bamboo pipes to match the pitches of the phoenixes, producing twelve pitch pipes in two sets: six from the male phoenix and six from the female: these were called the lülü or later the shierlü.[7]
 Apart from technical and structural aspects, ancient Chinese music theory also discusses topics such as the nature and functions of music. The Yueji (""Record of music"", c1st and 2nd centuries BCE), for example, manifests Confucian moral theories of understanding music in its social context. Studied and implemented by Confucian scholar-officials [...], these theories helped form a musical Confucianism that overshadowed but did not erase rival approaches. These include the assertion of Mozi (c. 468 – c. 376 BCE) that music wasted human and material resources, and Laozi's claim that the greatest music had no sounds. [...] Even the music of the qin zither, a genre closely affiliated with Confucian scholar-officials, includes many works with Daoist references, such as Tianfeng huanpei (""Heavenly Breeze and Sounds of Jade Pendants"").[6] The Samaveda and Yajurveda (c. 1200 – 1000 BCE) are among the earliest testimonies of Indian music, but properly speaking, they contain no theory. The Natya Shastra, written between 200 BCE to 200 CE, discusses intervals (Śrutis), scales (Grāmas), consonances and dissonances, classes of melodic structure (Mūrchanās, modes?), melodic types (Jātis), instruments, etc.[8]
 Early preserved Greek writings on music theory include two types of works:[9]
 Several names of theorists are known before these works, including Pythagoras (c. 570 ~ c. 495 BCE), Philolaus (c. 470 ~ (c. 385 BCE), Archytas (428–347 BCE), and others.
 Works of the first type (technical manuals) include
 More philosophical treatises of the second type include
 The pipa instrument carried with it a theory of musical modes that subsequently led to the Sui and Tang theory of 84 musical modes.[6]
 Medieval Arabic music theorists include:[n 3]
 The Latin treatise De institutione musica by the Roman philosopher Boethius (written c. 500, translated as Fundamentals of Music[2]) was a touchstone for other writings on music in medieval Europe. Boethius represented Classical authority on music during the Middle Ages, as the Greek writings on which he based his work were not read or translated by later Europeans until the 15th century.[19] This treatise carefully maintains distance from the actual practice of music, focusing mostly on the mathematical proportions involved in tuning systems and on the moral character of particular modes. Several centuries later, treatises began to appear which dealt with the actual composition of pieces of music in the plainchant tradition.[20] At the end of the ninth century, Hucbald worked towards more precise pitch notation for the neumes used to record plainchant.
 Guido d'Arezzo wrote a letter to Michael of Pomposa in 1028, entitled Epistola de ignoto cantu,[21] in which he introduced the practice of using syllables to describe notes and intervals. This was the source of the hexachordal solmization that was to be used until the end of the Middle Ages. Guido also wrote about emotional qualities of the modes, the phrase structure of plainchant, the temporal meaning of the neumes, etc.; his chapters on polyphony ""come closer to describing and illustrating real music than any previous account"" in the Western tradition.[19]
 During the thirteenth century, a new rhythm system called mensural notation grew out of an earlier, more limited method of notating rhythms in terms of fixed repetitive patterns, the so-called rhythmic modes, which were developed in France around 1200. An early form of mensural notation was first described and codified in the treatise Ars cantus mensurabilis (""The art of measured chant"") by Franco of Cologne (c. 1280). Mensural notation used different note shapes to specify different durations, allowing scribes to capture rhythms which varied instead of repeating the same fixed pattern; it is a proportional notation, in the sense that each note value is equal to two or three times the shorter value, or half or a third of the longer value. This same notation, transformed through various extensions and improvements during the Renaissance, forms the basis for rhythmic notation in European classical music today.
 D'Erlanger divulges that the Arabic music scale is derived from the Greek music scale, and that Arabic music is connected to certain features of Arabic culture, such as astrology.[18]
 Music is composed of aural phenomena; ""music theory"" considers how those phenomena apply in music. Music theory considers melody, rhythm, counterpoint, harmony, form, tonal systems, scales, tuning, intervals, consonance, dissonance, durational proportions, the acoustics of pitch systems, composition, performance, orchestration, ornamentation, improvisation, electronic sound production, etc.[25]
 Pitch is the lowness or highness of a tone, for example the difference between middle C and a higher C. The frequency of the sound waves producing a pitch can be measured precisely, but the perception of pitch is more complex because single notes from natural sources are usually a complex mix of many frequencies. Accordingly, theorists often describe pitch as a subjective sensation rather than an objective measurement of sound.[26]
 Specific frequencies are often assigned letter names. Today most orchestras assign concert A (the A above middle C on the piano) to the frequency of 440 Hz. This assignment is somewhat arbitrary; for example, in 1859 France, the same A was tuned to 435 Hz. Such differences can have a noticeable effect on the timbre of instruments and other phenomena. Thus, in historically informed performance of older music, tuning is often set to match the tuning used in the period when it was written. Additionally, many cultures do not attempt to standardize pitch, often considering that it should be allowed to vary depending on genre, style, mood, etc.
 The difference in pitch between two notes is called an interval. The most basic interval is the unison, which is simply two notes of the same pitch. The octave interval is two pitches that are either double or half the frequency of one another. The unique characteristics of octaves gave rise to the concept of pitch class: pitches of the same letter name that occur in different octaves may be grouped into a single ""class"" by ignoring the difference in octave. For example, a high C and a low C are members of the same pitch class—the class that contains all C's. [27]
 Musical tuning systems, or temperaments, determine the precise size of intervals. Tuning systems vary widely within and between world cultures. In Western culture, there have long been several competing tuning systems, all with different qualities. Internationally, the system known as equal temperament is most commonly used today because it is considered the most satisfactory compromise that allows instruments of fixed tuning (e.g. the piano) to sound acceptably in tune in all keys.
 Notes can be arranged in a variety of scales and modes. Western music theory generally divides the octave into a series of twelve pitches, called a chromatic scale, within which the interval between adjacent tones is called a semitone, or half step. Selecting tones from this set of 12 and arranging them in patterns of semitones and whole tones creates other scales.[28]
 The most commonly encountered scales are the seven-toned major, the harmonic minor, the melodic minor, and the natural minor. Other examples of scales are the octatonic scale and the pentatonic or five-tone scale, which is common in folk music and blues. Non-Western cultures often use scales that do not correspond with an equally divided twelve-tone division of the octave. For example, classical Ottoman, Persian, Indian and Arabic musical systems often make use of multiples of quarter tones (half the size of a semitone, as the name indicates), for instance in 'neutral' seconds (three quarter tones) or 'neutral' thirds (seven quarter tones)—they do not normally use the quarter tone itself as a direct interval.[28]
 In traditional Western notation, the scale used for a composition is usually indicated by a key signature at the beginning to designate the pitches that make up that scale. As the music progresses, the pitches used may change and introduce a different scale. Music can be transposed from one scale to another for various purposes, often to accommodate the range of a vocalist. Such transposition raises or lowers the overall pitch range, but preserves the intervallic relationships of the original scale. For example, transposition from the key of C major to D major raises all pitches of the scale of C major equally by a whole tone. Since the interval relationships remain unchanged, transposition may be unnoticed by a listener, however other qualities may change noticeably because transposition changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music. This often affects the music's overall sound, as well as having technical implications for the performers.[29]
 The interrelationship of the keys most commonly used in Western tonal music is conveniently shown by the circle of fifths. Unique key signatures are also sometimes devised for a particular composition. During the Baroque period, emotional associations with specific keys, known as the doctrine of the affections, were an important topic in music theory, but the unique tonal colorings of keys that gave rise to that doctrine were largely erased with the adoption of equal temperament. However, many musicians continue to feel that certain keys are more appropriate to certain emotions than others. Indian classical music theory continues to strongly associate keys with emotional states, times of day, and other extra-musical concepts and notably, does not employ equal temperament.
 Consonance and dissonance are subjective qualities of the sonority of intervals that vary widely in different cultures and over the ages. Consonance (or concord) is the quality of an interval or chord that seems stable and complete in itself. Dissonance (or discord) is the opposite in that it feels incomplete and ""wants to"" resolve to a consonant interval. Dissonant intervals seem to clash. Consonant intervals seem to sound comfortable together. Commonly, perfect fourths, fifths, and octaves and all major and minor thirds and sixths are considered consonant. All others are dissonant to a greater or lesser degree.[30]
 Context and many other aspects can affect apparent dissonance and consonance. For example, in a Debussy prelude, a major second may sound stable and consonant, while the same interval may sound dissonant in a Bach fugue. In the Common practice era, the perfect fourth is considered dissonant when not supported by a lower third or fifth. Since the early 20th century, Arnold Schoenberg's concept of ""emancipated"" dissonance, in which traditionally dissonant intervals can be treated as ""higher,"" more remote consonances, has become more widely accepted.[30]
 Rhythm is produced by the sequential arrangement of sounds and silences in time. Meter measures music in regular pulse groupings, called measures or bars. The time signature or meter signature specifies how many beats are in a measure, and which value of written note is counted or felt as a single beat.
 Through increased stress, or variations in duration or articulation, particular tones may be accented. There are conventions in most musical traditions for regular and hierarchical accentuation of beats to reinforce a given meter. Syncopated rhythms contradict those conventions by accenting unexpected parts of the beat.[31] Playing simultaneous rhythms in more than one time signature is called polyrhythm.[32]
 In recent years, rhythm and meter have become an important area of research among music scholars. The most highly cited of these recent scholars are Maury Yeston,[33] Fred Lerdahl and Ray Jackendoff,[34] Jonathan Kramer,[35] and Justin London.[36]
 A melody is a group of musical sounds in agreeable succession or arrangement.[38] Because melody is such a prominent aspect in so much music, its construction and other qualities are a primary interest of music theory.
 The basic elements of melody are pitch, duration, rhythm, and tempo. The tones of a melody are usually drawn from pitch systems such as scales or modes. Melody may consist, to increasing degree, of the figure, motive, semi-phrase, antecedent and consequent phrase, and period or sentence. The period may be considered the complete melody, however some examples combine two periods, or use other combinations of constituents to create larger form melodies.[39]
 A chord, in music, is any harmonic set of three or more notes that is heard as if sounding simultaneously.[40]: pp. 67, 359 [41]: p. 63  These need not actually be played together: arpeggios and broken chords may, for many practical and theoretical purposes, constitute chords. Chords and sequences of chords are frequently used in modern Western, West African,[42] and Oceanian[43] music, whereas they are absent from the music of many other parts of the world.[44]: p. 15 
 The most frequently encountered chords are triads, so called because they consist of three distinct notes: further notes may be added to give seventh chords, extended chords, or added tone chords. The most common chords are the major and minor triads and then the augmented and diminished triads. The descriptions major, minor, augmented, and diminished are sometimes referred to collectively as chordal quality. Chords are also commonly classed by their root note—so, for instance, the chord C major may be described as a triad of major quality built on the note C. Chords may also be classified by inversion, the order in which the notes are stacked.
 A series of chords is called a chord progression. Although any chord may in principle be followed by any other chord, certain patterns of chords have been accepted as establishing key in common-practice harmony. To describe this, chords are numbered, using Roman numerals (upward from the key-note),[45] per their diatonic function. Common ways of notating or representing chords[46] in western music other than conventional staff notation include Roman numerals, figured bass (much used in the Baroque era), chord letters (sometimes used in modern musicology), and various systems of chord charts typically found in the lead sheets used in popular music to lay out the sequence of chords so that the musician may play accompaniment chords or improvise a solo.
 In music, harmony is the use of simultaneous pitches (tones, notes), or chords.[44]: p. 15  The study of harmony involves chords and their construction and chord progressions and the principles of connection that govern them.[47] Harmony is often said to refer to the ""vertical"" aspect of music, as distinguished from melodic line, or the ""horizontal"" aspect.[48] Counterpoint, which refers to the interweaving of melodic lines, and polyphony, which refers to the relationship of separate independent voices, is thus sometimes distinguished from harmony.[49]
 In popular and jazz harmony, chords are named by their root plus various terms and characters indicating their qualities. For example, a lead sheet may indicate chords such as C major, D minor, and G dominant seventh. In many types of music, notably Baroque, Romantic, modern, and jazz, chords are often augmented with ""tensions"". A tension is an additional chord member that creates a relatively dissonant interval in relation to the bass. It is part of a chord, but is not one of the chord tones (1 3 5 7). Typically, in the classical common practice period a dissonant chord (chord with tension) ""resolves"" to a consonant chord. Harmonization usually sounds pleasant to the ear when there is a balance between the consonant and dissonant sounds. In simple words, that occurs when there is a balance between ""tense"" and ""relaxed"" moments.[50][unreliable source?]
 Timbre, sometimes called ""color"", or ""tone color,"" is the principal phenomenon that allows us to distinguish one instrument from another when both play at the same pitch and volume, a quality of a voice or instrument often described in terms like bright, dull, shrill, etc. It is of considerable interest in music theory, especially because it is one component of music that has as yet, no standardized nomenclature. It has been called ""... the psychoacoustician's multidimensional waste-basket category for everything that cannot be labeled pitch or loudness,""[51] but can be accurately described and analyzed by Fourier analysis and other methods[52] because it results from the combination of all sound frequencies, attack and release envelopes, and other qualities that a tone comprises.
 Timbre is principally determined by two things: (1) the relative balance of overtones produced by a given instrument due its construction (e.g. shape, material), and (2) the envelope of the sound (including changes in the overtone structure over time). Timbre varies widely between different instruments, voices, and to lesser degree, between instruments of the same type due to variations in their construction, and significantly, the performer's technique. The timbre of most instruments can be changed by employing different techniques while playing. For example, the timbre of a trumpet changes when a mute is inserted into the bell, the player changes their embouchure, or volume.[citation needed]
 A voice can change its timbre by the way the performer manipulates their vocal apparatus, (e.g. the shape of the vocal cavity or mouth). Musical notation frequently specifies alteration in timbre by changes in sounding technique, volume, accent, and other means. These are indicated variously by symbolic and verbal instruction. For example, the word dolce (sweetly) indicates a non-specific, but commonly understood soft and ""sweet"" timbre. Sul tasto instructs a string player to bow near or over the fingerboard to produce a less brilliant sound. Cuivre instructs a brass player to produce a forced and stridently brassy sound. Accent symbols like marcato (^) and dynamic indications (pp) can also indicate changes in timbre.[53]
 In music, ""dynamics"" normally refers to variations of intensity or volume, as may be measured by physicists and audio engineers in decibels or phons. In music notation, however, dynamics are not treated as absolute values, but as relative ones. Because they are usually measured subjectively, there are factors besides amplitude that affect the performance or perception of intensity, such as timbre, vibrato, and articulation.
 The conventional indications of dynamics are abbreviations for Italian words like forte (f) for loud and piano (p) for soft. These two basic notations are modified by indications including mezzo piano (mp) for moderately soft (literally ""half soft"") and mezzo forte (mf) for moderately loud, sforzando or sforzato (sfz) for a surging or ""pushed"" attack, or fortepiano (fp) for a loud attack with a sudden decrease to a soft level. The full span of these markings usually range from a nearly inaudible pianissississimo (pppp) to a loud-as-possible fortissississimo (ffff).
 Greater extremes of pppppp and fffff and nuances such as p+ or più piano are sometimes found. Other systems of indicating volume are also used in both notation and analysis: dB (decibels), numerical scales, colored or different sized notes, words in languages other than Italian, and symbols such as those for progressively increasing volume (crescendo) or decreasing volume (diminuendo or decrescendo), often called ""hairpins"" when indicated with diverging or converging lines as shown in the graphic above.
 Articulation is the way the performer sounds notes. For example, staccato is the shortening of duration compared to the written note value, legato performs the notes in a smoothly joined sequence with no separation. Articulation is often described rather than quantified, therefore there is room to interpret how to execute precisely each articulation.
 For example, staccato is often referred to as ""separated"" or ""detached"" rather than having a defined or numbered amount by which to reduce the notated duration. Violin players use a variety of techniques to perform different qualities of staccato. The manner in which a performer decides to execute a given articulation is usually based on the context of the piece or phrase, but many articulation symbols and verbal instructions depend on the instrument and musical period (e.g. viol, wind; classical, baroque; etc.).
 There is a set of articulations that most instruments and voices perform in common. They are—from long to short: legato (smooth, connected); tenuto (pressed or played to full notated duration); marcato (accented and detached); staccato (""separated"", ""detached""); martelé (heavily accented or ""hammered"").[contradictory] Many of these can be combined to create certain ""in-between"" articulations. For example, portato is the combination of tenuto and staccato. Some instruments have unique methods by which to produce sounds, such as spiccato for bowed strings, where the bow bounces off the string.
 In music, texture is how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall quality of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices. For example, a thick texture contains many ""layers"" of instruments. One of these layers could be a string section, or another brass.
 The thickness also is affected by the number and the richness of the instruments playing the piece. The thickness varies from light to thick. A lightly textured piece will have light, sparse scoring. A thickly or heavily textured piece will be scored for many instruments. A piece's texture may be affected by the number and character of parts playing at once, the timbre of the instruments or voices playing these parts and the harmony, tempo, and rhythms used.[55] The types categorized by number and relationship of parts are analyzed and determined through the labeling of primary textural elements: primary melody, secondary melody, parallel supporting melody, static support, harmonic support, rhythmic support, and harmonic and rhythmic support.[56][incomplete short citation]
 Common types included monophonic texture (a single melodic voice, such as a piece for solo soprano or solo flute), biphonic texture (two melodic voices, such as a duo for bassoon and flute in which the bassoon plays a drone note and the flute plays the melody), polyphonic texture and homophonic texture (chords accompanying a melody).[citation needed]
 The term musical form (or musical architecture) refers to the overall structure or plan of a piece of music, and it describes the layout of a composition as divided into sections.[58] In the tenth edition of The Oxford Companion to Music, Percy Scholes defines musical form as ""a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration.""[59] According to Richard Middleton, musical form is ""the shape or structure of the work."" He describes it through difference: the distance moved from a repeat; the latter being the smallest difference. Difference is quantitative and qualitative: how far, and of what type, different. In many cases, form depends on statement and restatement, unity and variety, and contrast and connection.[60]
 Musical expression is the art of playing or singing music with emotional communication. The elements of music that comprise expression include dynamic indications, such as forte or piano, phrasing, differing qualities of timbre and articulation, color, intensity, energy and excitement. All of these devices can be incorporated by the performer. A performer aims to elicit responses of sympathetic feeling in the audience, and to excite, calm or otherwise sway the audience's physical and emotional responses. Musical expression is sometimes thought to be produced by a combination of other parameters, and sometimes described as a transcendent quality that is more than the sum of measurable quantities such as pitch or duration.
 Expression on instruments can be closely related to the role of the breath in singing, and the voice's natural ability to express feelings, sentiment and deep emotions.[clarification needed] Whether these can somehow be categorized is perhaps the realm of academics, who view expression as an element of musical performance that embodies a consistently recognizable emotion, ideally causing a sympathetic emotional response in its listeners.[61] The emotional content of musical expression is distinct from the emotional content of specific sounds (e.g., a startlingly-loud 'bang') and of learned associations (e.g., a national anthem), but can rarely be completely separated from its context.[citation needed]
 The components of musical expression continue to be the subject of extensive and unresolved dispute.[62][63][64][65][66][67]
 Musical notation is the written or symbolized representation of music. This is most often achieved by the use of commonly understood graphic symbols and written verbal instructions and their abbreviations. There are many systems of music notation from different cultures and different ages. Traditional Western notation evolved during the Middle Ages and remains an area of experimentation and innovation.[68] In the 2000s, computer file formats have become important as well.[69] Spoken language and hand signs are also used to symbolically represent music, primarily in teaching.
 In standard Western music notation, tones are represented graphically by symbols (notes) placed on a staff or staves, the vertical axis corresponding to pitch and the horizontal axis corresponding to time. Note head shapes, stems, flags, ties and dots are used to indicate duration. Additional symbols indicate keys, dynamics, accents, rests, etc. Verbal instructions from the conductor are often used to indicate tempo, technique, and other aspects.
 In Western music, a range of different music notation systems are used. In Western Classical music, conductors use printed scores that show all of the instruments' parts and orchestra members read parts with their musical lines written out. In popular styles of music, much less of the music may be notated. A rock band may go into a recording session with just a handwritten chord chart indicating the song's chord progression using chord names (e.g., C major, D minor, G7, etc.). All of the chord voicings, rhythms and accompaniment figures are improvised by the band members.
 The scholarly study of music theory in the twentieth century has a number of different subfields, each of which takes a different perspective on what are the primary phenomenon of interest and the most useful methods for investigation.
 Musical analysis is the attempt to answer the question how does this music work? The method employed to answer this question, and indeed exactly what is meant by the question, differs from analyst to analyst, and according to the purpose of the analysis. According to Ian Bent, ""analysis, as a pursuit in its own right, came to be established only in the late 19th century; its emergence as an approach and method can be traced back to the 1750s. However, it existed as a scholarly tool, albeit an auxiliary one, from the Middle Ages onwards.""[70][incomplete short citation] Adolf Bernhard Marx was influential in formalising concepts about composition and music understanding towards the second half of the 19th century. The principle of analysis has been variously criticized, especially by composers, such as Edgard Varèse's claim that, ""to explain by means of [analysis] is to decompose, to mutilate the spirit of a work"".[71]
 Schenkerian analysis is a method of musical analysis of tonal music based on the theories of Heinrich Schenker (1868–1935). The goal of a Schenkerian analysis is to interpret the underlying structure of a tonal work and to help reading the score according to that structure. The theory's basic tenets can be viewed as a way of defining tonality in music. A Schenkerian analysis of a passage of music shows hierarchical relationships among its pitches, and draws conclusions about the structure of the passage from this hierarchy. The analysis makes use of a specialized symbolic form of musical notation that Schenker devised to demonstrate various techniques of elaboration. The most fundamental concept of Schenker's theory of tonality may be that of tonal space.[72] The intervals between the notes of the tonic triad form a tonal space that is filled with passing and neighbour notes, producing new triads and new tonal spaces, open for further elaborations until the surface of the work (the score) is reached.
 Although Schenker himself usually presents his analyses in the generative direction, starting from the fundamental structure (Ursatz) to reach the score, the practice of Schenkerian analysis more often is reductive, starting from the score and showing how it can be reduced to its fundamental structure. The graph of the Ursatz is arrhythmic, as is a strict-counterpoint cantus firmus exercise.[73] Even at intermediate levels of the reduction, rhythmic notation (open and closed noteheads, beams and flags) shows not rhythm but the hierarchical relationships between the pitch-events. Schenkerian analysis is subjective. There is no mechanical procedure involved and the analysis reflects the musical intuitions of the analyst.[74] The analysis represents a way of hearing (and reading) a piece of music.
 Transformational theory is a branch of music theory developed by David Lewin in the 1980s, and formally introduced in his 1987 work, Generalized Musical Intervals and Transformations. The theory, which models musical transformations as elements of a mathematical group, can be used to analyze both tonal and atonal music. The goal of transformational theory is to change the focus from musical objects—such as the ""C major chord"" or ""G major chord""—to relations between objects. Thus, instead of saying that a C major chord is followed by G major, a transformational theorist might say that the first chord has been ""transformed"" into the second by the ""Dominant operation."" (Symbolically, one might write ""Dominant(C major) = G major."") While traditional musical set theory focuses on the makeup of musical objects, transformational theory focuses on the intervals or types of musical motion that can occur. According to Lewin's description of this change in emphasis, ""[The transformational] attitude does not ask for some observed measure of extension between reified 'points'; rather it asks: 'If I am at s and wish to get to t, what characteristic gesture should I perform in order to arrive there?'""[75]
 Music psychology or the psychology of music may be regarded as a branch of both psychology and musicology. It aims to explain and understand musical behavior and experience, including the processes through which music is perceived, created, responded to, and incorporated into everyday life.[76][77] Modern music psychology is primarily empirical; its knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. Music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.
 Music psychology can shed light on non-psychological aspects of musicology and musical practice. For example, it contributes to music theory through investigations of the perception and computational modelling of musical structures such as melody, harmony, tonality, rhythm, meter, and form. Research in music history can benefit from systematic study of the history of musical syntax, or from psychological analyses of composers and compositions in relation to perceptual, affective, and social responses to their music. 
 A music genre is a conventional category that identifies some pieces of music as belonging to a shared tradition or set of conventions.[78] It is to be distinguished from musical form and musical style, although in practice these terms are sometimes used interchangeably.[79][failed verification]
 Music can be divided into different genres in many different ways. The artistic nature of music means that these classifications are often subjective and controversial, and some genres may overlap. There are even varying academic definitions of the term genre itself. In his book Form in Tonal Music, Douglass M. Green distinguishes between genre and form. He lists madrigal, motet, canzona, ricercar, and dance as examples of genres from the Renaissance period. To further clarify the meaning of genre, Green writes, ""Beethoven's Op. 61 and Mendelssohn's Op. 64 are identical in genre—both are violin concertos—but different in form. However, Mozart's Rondo for Piano, K. 511, and the Agnus Dei from his Mass, K. 317 are quite different in genre but happen to be similar in form.""[80] Some, like Peter van der Merwe, treat the terms genre and style as the same, saying that genre should be defined as pieces of music that came from the same style or ""basic musical language.""[81]
 Others, such as Allan F. Moore, state that genre and style are two separate terms, and that secondary characteristics such as subject matter can also differentiate between genres.[82] A music genre or subgenre may also be defined by the musical techniques, the style, the cultural context, and the content and spirit of the themes. Geographical origin is sometimes used to identify a music genre, though a single geographical category will often include a wide variety of subgenres. Timothy Laurie argues that ""since the early 1980s, genre has graduated from being a subset of popular music studies to being an almost ubiquitous framework for constituting and evaluating musical research objects"".[83]
 Musical technique is the ability of instrumental and vocal musicians to exert optimal control of their instruments or vocal cords to produce precise musical effects. Improving technique generally entails practicing exercises that improve muscular sensitivity and agility. To improve technique, musicians often practice fundamental patterns of notes such as the natural, minor, major, and chromatic scales, minor and major triads, dominant and diminished sevenths, formula patterns and arpeggios. For example, triads and sevenths teach how to play chords with accuracy and speed. Scales teach how to move quickly and gracefully from one note to another (usually by step). Arpeggios teach how to play broken chords over larger intervals. Many of these components of music are found in compositions, for example, a scale is a very common element of classical and romantic era compositions.[citation needed]
 Heinrich Schenker argued that musical technique's ""most striking and distinctive characteristic"" is repetition.[84] Works known as études (meaning ""study"") are also frequently used for the improvement of technique.
 Music theorists sometimes use mathematics to understand music, and although music has no axiomatic foundation in modern mathematics, mathematics is ""the basis of sound"" and sound itself ""in its musical aspects... exhibits a remarkable array of number properties"", simply because nature itself ""is amazingly mathematical"".[85] The attempt to structure and communicate new ways of composing and hearing music has led to musical applications of set theory, abstract algebra and number theory. Some composers have incorporated the golden ratio and Fibonacci numbers into their work.[86][87] There is a long history of examining the relationships between music and mathematics. Though ancient Chinese, Egyptians and Mesopotamians are known to have studied the mathematical principles of sound,[88] the Pythagoreans (in particular Philolaus and Archytas)[89] of ancient Greece were the first researchers known to have investigated the expression of musical scales in terms of numerical ratios.
 In the modern era, musical set theory uses the language of mathematical set theory in an elementary way to organize musical objects and describe their relationships. To analyze the structure of a piece of (typically atonal) music using musical set theory, one usually starts with a set of tones, which could form motives or chords. By applying simple operations such as transposition and inversion, one can discover deep structures in the music. Operations such as transposition and inversion are called isometries because they preserve the intervals between tones in a set. Expanding on the methods of musical set theory, some theorists have used abstract algebra to analyze music. For example, the pitch classes in an equally tempered octave form an abelian group with 12 elements. It is possible to describe just intonation in terms of a free abelian group.[90]
 In music theory, serialism is a method or technique of composition that uses a series of values to manipulate different musical elements. Serialism began primarily with Arnold Schoenberg's twelve-tone technique, though his contemporaries were also working to establish serialism as one example of post-tonal thinking. Twelve-tone technique orders the twelve notes of the chromatic scale, forming a row or series and providing a unifying basis for a composition's melody, harmony, structural progressions, and variations. Other types of serialism also work with sets, collections of objects, but not necessarily with fixed-order series, and extend the technique to other musical dimensions (often called ""parameters""), such as duration, dynamics, and timbre. The idea of serialism is also applied in various ways in the visual arts, design, and architecture[91]
 ""Integral serialism"" or ""total serialism"" is the use of series for aspects such as duration, dynamics, and register as well as pitch. [92] Other terms, used especially in Europe to distinguish post-World War II serial music from twelve-tone music and its American extensions, are ""general serialism"" and ""multiple serialism"".[93]
 Musical set theory provides concepts for categorizing musical objects and describing their relationships. Many of the notions were first elaborated by Howard Hanson (1960) in connection with tonal music, and then mostly developed in connection with atonal music by theorists such as Allen Forte (1973), drawing on the work in twelve-tone theory of Milton Babbitt. The concepts of set theory are very general and can be applied to tonal and atonal styles in any equally tempered tuning system, and to some extent more generally than that.[citation needed]
 One branch of musical set theory deals with collections (sets and permutations) of pitches and pitch classes (pitch-class set theory), which may be ordered or unordered, and can be related by musical operations such as transposition, inversion, and complementation. The methods of musical set theory are sometimes applied to the analysis of rhythm as well.[citation needed]
 Music semiology (semiotics) is the study of signs as they pertain to music on a variety of levels. Following Roman Jakobson, Kofi Agawu adopts the idea of musical semiosis being introversive or extroversive—that is, musical signs within a text and without.[citation needed] ""Topics"", or various musical conventions (such as horn calls, dance forms, and styles), have been treated suggestively by Agawu, among others.[citation needed] The notion of gesture is beginning to play a large role in musico-semiotic enquiry.[citation needed]
 Writers on music semiology include Kofi Agawu (on topical theory,[citation needed] Heinrich Schenker,[102][103] Robert Hatten (on topic, gesture)[citation needed], Raymond Monelle (on topic, musical meaning)[citation needed], Jean-Jacques Nattiez (on introversive taxonomic analysis and ethnomusicological applications)[citation needed], Anthony Newcomb (on narrativity)[citation needed], and Eero Tarasti[citation needed].
 Roland Barthes, himself a semiotician and skilled amateur pianist, wrote about music in Image-Music-Text,[full citation needed] The Responsibilities of Form,[full citation needed] and Eiffel Tower,[full citation needed] though he did not consider music to be a semiotic system[citation needed].
 Signs, meanings in music, happen essentially through the connotations of sounds, and through the social construction, appropriation and amplification of certain meanings associated with these connotations. The work of Philip Tagg (Ten Little Tunes,[full citation needed] Fernando the Flute,[full citation needed] Music's Meanings[full citation needed]) provides one of the most complete and systematic analysis of the relation between musical structures and connotations in western and especially popular, television and film music. The work of Leonard B. Meyer in Style and Music[full citation needed] theorizes the relationship between ideologies and musical structures and the phenomena of style change, and focuses on romanticism as a case study.
 Music theory in the practical sense has been a part of education at conservatories and music schools for centuries, but the status music theory currently has within academic institutions is relatively recent. In the 1970s, few universities had dedicated music theory programs, many music theorists had been trained as composers or historians, and there was a belief among theorists that the teaching of music theory was inadequate and that the subject was not properly recognised as a scholarly discipline in its own right.[104] A growing number of scholars began promoting the idea that music theory should be taught by theorists, rather than composers, performers or music historians.[104] This led to the founding of the Society for Music Theory in the United States in 1977. In Europe, the French Société d'Analyse musicale was founded in 1985. It called the First European Conference of Music Analysis for 1989, which resulted in the foundation of the Société belge d'Analyse musicale in Belgium and the Gruppo analisi e teoria musicale in Italy the same year, the Society for Music Analysis in the UK in 1991, the Vereniging voor Muziektheorie in the Netherlands in 1999 and the Gesellschaft für Musiktheorie in Germany in 2000.[105] They were later followed by the Russian Society for Music Theory in 2013, the Polish Society for Music Analysis in 2015 and the Sociedad de Análisis y Teoría Musical in Spain in 2020, and others are in construction. These societies coordinate the publication of music theory scholarship and support the professional development of music theory researchers. They formed in 2018 a network of European societies for Theory and/or Analysis of Music, the EuroT&AM
 As part of their initial training, music theorists will typically complete a B.Mus or a B.A. in music (or a related field) and in many cases an M.A. in music theory. Some individuals apply directly from a bachelor's degree to a PhD, and in these cases, they may not receive an M.A. In the 2010s, given the increasingly interdisciplinary nature of university graduate programs, some applicants for music theory PhD programs may have academic training both in music and outside of music (e.g., a student may apply with a B.Mus. and a Masters in Music Composition or Philosophy of Music).
 Most music theorists work as instructors, lecturers or professors in colleges, universities or conservatories. The job market for tenure-track professor positions is very competitive: with an average of around 25 tenure-track positions advertised per year in the past decade, 80–100 PhD graduates are produced each year (according to the Survey of Earned Doctorates) who compete not only with each other for those positions but with job seekers that received PhD's in previous years who are still searching for a tenure-track job. Applicants must hold a completed PhD or the equivalent degree (or expect to receive one within a year of being hired—called an ""ABD"", for ""All But Dissertation"" stage) and (for more senior positions) have a strong record of publishing in peer-reviewed journals. Some PhD-holding music theorists are only able to find insecure positions as sessional lecturers. The job tasks of a music theorist are the same as those of a professor in any other humanities discipline: teaching undergraduate and/or graduate classes in this area of specialization and, in many cases some general courses (such as Music appreciation or Introduction to Music Theory), conducting research in this area of expertise, publishing research articles in peer-reviewed journals, authoring book chapters, books or textbooks, traveling to conferences to present papers and learn about research in the field, and, if the program includes a graduate school, supervising M.A. and PhD students and giving them guidance on the preparation of their theses and dissertations. Some music theory professors may take on senior administrative positions in their institution, such as Dean or Chair of the School of Music.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['musicology', 'Peter van der Merwe', 'The period may be considered the complete melody', 'pitch-events', 'technical'], 'answer_start': [], 'answer_end': []}"
"
The history of film chronicles the development of a visual art form created using film technologies that began in the late 19th century.
 The advent of film as an artistic medium is not clearly defined. There were earlier cinematographic screenings by others, however, the commercial, public screening of ten Lumière brothers' short films in Paris on 28 December 1895, can be regarded as the breakthrough of projected cinematographic motion pictures. The earliest films were in black and white, under a minute long, without recorded sound, and consisted of a single shot from a steady camera. The first decade saw film move from a novelty, to an established mass entertainment industry, with film production companies and studios established throughout the world. Conventions toward a general cinematic language developed, with film editing camera movements and other cinematic techniques contributing specific roles in the narrative of films.
 Popular new media, including television (mainstream since the 1950s), home video (1980s), and the internet (1990s), influenced the distribution and consumption of films. Film production usually responded with content to fit the new media, and technical innovations (including widescreen (1950s), 3D, and 4D film) and more spectacular films to keep theatrical screenings attractive. Systems that were cheaper and more easily handled (including 8mm film, video, and smartphone cameras) allowed for an increasing number of people to create films of varying qualities, for any purpose including home movies and video art. The technical quality was usually lower than professional movies, but improved with digital video and affordable, high-quality digital cameras. Improving over time, digital production methods became more popular during the 1990s, resulting in increasingly realistic visual effects and popular feature-length computer animations.
 Various film genres have emerged during the history of film, and enjoyed variable degrees of success.
 The use of film as an art form traces its origins to several earlier traditions in the arts such as (oral) storytelling, literature, theatre and visual arts. Cantastoria and similar ancient traditions combined storytelling with series of images that were shown or indicated one after the other. Predecessors to film that had already used light and shadows to create art before the advent of modern film technology include shadowgraphy, shadow puppetry, camera obscura, and the magic lantern.
 Shadowgraphy and shadow puppetry represent early examples of the intent to use moving imagery for entertainment and storytelling.[1] Thought to have originated in the Far East, the art form used shadows cast by hands or objects to assist in the creation of narratives. Shadow puppetry enjoyed popularity for centuries around Asia, notably in Java, and eventually spread to Europe during the Age of Enlightenment.[2]
 By the 16th century, entertainers often conjured images of ghostly apparitions, using techniques such as camera obscura and other forms of projection to enhance their performances.[3] Magic lantern shows developed in the latter half of the 17th century seem to have continued this tradition with images of death, monsters and other scary figures.[4] Around 1790, this practice was developed into a type of multimedia ghost show known as phantasmagoria. These popular shows entertained audiences using mechanical slides, rear projection, mobile projectors, superimposition, dissolves, live actors, smoke (on which projections may have been cast), odors, sounds and even electric shocks.[5][6] While many first magic lantern shows were intended to frighten viewers, advances by projectionists allowed for creative and even educational storytelling that could appeal to wider family audiences.[7] Newly pioneered techniques such as the use of dissolving views and the chromatrope allowed for smoother transitions between two projected images and aided in providing stronger narratives.[8]
 In 1833, scientific study of a stroboscopic illusion in spoked wheels by Joseph Plateau, Michael Faraday and Simon Stampfer led to the invention of the Fantascope, also known as the stroboscopic disk or the phenakistiscope, which was popular in several European countries for a while. Plateau thought it could be further developed for use in phantasmagoria and Stampfer imagined a system for longer scenes with strips on rollers, as well as a transparent version (probably intended for projection). Plateau, Charles Wheatstone, Antoine Claudet and others tried to combine the technique with the stereoscope (introduced in 1838) and photography (introduced in 1839) for a more complete illusion of reality, but for decades such experiments were mostly hindered by the need for long exposure times, with motion blur around objects that moved while the reflected light fell on the photo-sensitive chemicals. A few people managed to get decent results from stop motion techniques, but these were only very rarely marketed and no form of animated photography had much cultural impact before the advent of chronophotography.
 Most early photographic sequences, known as chronophotography, were not initially intended to be viewed in motion and were typically presented as a serious, even scientific, method of studying locomotion. The sequences almost exclusively involved humans or animals performing a simple movement in front of the camera.[9] Starting in 1878 with the publication of The Horse in Motion cabinet cards, photographer Eadweard Muybridge began making hundreds of chronophotographic studies of the motion of animals and humans in real-time. He was soon followed by other chronophotographers like Étienne-Jules Marey, Georges Demenÿ, Albert Londe and Ottomar Anschütz. In 1879, Muybridge started lecturing on animal locomotion and used his Zoopraxiscope to project animations of the contours of his recordings, traced onto glass discs.[10]
 In 1887, the German inventor and photographer Ottomar Anschütz started presenting his chronophotographic recordings in motion, using a device he called the Elektrischen Schnellseher (also known as the Electrotachyscope), which displayed short loops on a small milk glass screen. By 1891, he had started mass production of a more economical, coin-operated peep-box viewing device of the same name that was exhibited at international exhibitions and fairs. Some machines were installed for longer periods, including some at The Crystal Palace in London, and in several U.S. stores. Shifting the focus of the medium from technical and scientific interest in motion to entertainment for the masses, he recorded wrestlers, dancers, acrobats, and scenes of everyday life. Nearly 34,000 people paid to see his shows at the Berlin Exhibition Park in summer 1892. Others saw it in London or at the 1893 Chicago World's Fair. Though little evidence remains for most of these recordings, some scenes probably depicted staged comical sequences. Extant records suggest some of his output directly influenced later works by the Edison Company, such as the 1894 film Fred Ott's Sneeze.[11]
 Advances towards motion picture projection technologies were based on the popularity of magic lanterns, chronophotographic demonstrations, and other closely related forms of projected entertainment such as illustrated songs. From October 1892 to March 1900, inventor Émile Reynaud exhibited his Théâtre Optique (""Optical Theatre"") film system at the Musée Grévin in Paris. Reynaud's device, which projected a series of animated stories such as Pauvre Pierrot and Autour d'une cabine, was displayed to over 500,000 visitors over the course of 12,800 shows.[12][13] On 25, 29 and 30 November 1894, Ottomar Anschütz projected moving images from Electrotachyscope discs on a large screen in the darkened Grand Auditorium of a Post Office Building in Berlin. From 22 February to 30 March 1895, a commercial 1.5-hour program of 40 different scenes was screened for audiences of 300 people at the old Reichstag and received circa 4,000 visitors.[14]
 Throughout the late 19th century, several inventors such as Wordsworth Donisthorpe, Louis Le Prince, William Friese-Greene, and the Skladanowsky brothers made pioneering contributions to the development of devices that could capture and display moving images, laying the groundwork for the emergence of cinema as an artistic medium. The scenes in these experiments primarily served to demonstrate the technology itself and were usually filmed with family, friends or passing traffic as the moving subjects. The earliest surviving film, known today as the Roundhay Garden Scene (1888), was captured by Louis Le Prince and briefly depicted members of his family in motion.[15]
 
In June 1889, American inventor Thomas Edison assigned a lab assistant, William Kennedy Dickson, to help develop a device that could produce visuals to accompany the sounds produced from the phonograph. Building upon previous machines by Muybridge, Marey, Anschütz and others, Dickson and his team created the Kinetoscope peep-box viewer, with celluloid loops containing about half a minute of motion picture entertainment.[16] After an early preview on 20 May 1891, Edison introduced the machine in 1893.[17] Many of the movies presented on the Kinetoscope showcased well-known vaudeville acts performing in Edison's Black Maria studio.[18] The Kinetoscope quickly became a global sensation with multiple viewing parlors across major cities by 1895.[17] As the initial novelty of the images wore off, the Edison Company was slow to diversify their repertoire of films and waning public interest caused business to slow by Spring 1895. To remedy declining profits, experiments, such as The Dickson Experimental Sound Film, were conducted in an attempt to achieve the device's original goal of providing visual accompaniment for sound recordings. Limitations in syncing the sound to the visuals, however, prevented widespread application.[19] During that same period, inventors began advancing technologies towards film projection that would eventually overtake Edison's peep-box format.[20]  The Skladanowsky brothers, used their self-made Bioscop to display the first moving picture show to a paying audience on 1 November 1895, in Berlin. But they did not have the quality or financial resources to acquire momentum. Most of these films never passed the experimental stage and their efforts garnered little public attention until after cinema had become successful.
 In the latter half of 1895, brothers Auguste and Louis Lumière filmed a number of short scenes with their invention, the Cinématographe. On 28 December 1895, the brothers gave their first commercial screening in Paris (though evidence exists of demonstrations of the device to small audiences as early as October 1895).[21] The screening consisted of ten films and lasted roughly 20 minutes. The program consisted mainly of actuality films such as Workers Leaving the Lumière Factory as truthful documents of the world, but the show also included the staged comedy L'Arroseur Arrosé.[22] The most advanced demonstration of film projection thus far, the Cinématographe was an instant success, bringing in an average of 2,500 to 3,000 francs daily by the end of January 1896.[17] Following the first screening, the order and selection of films were changed often.[20]
 The Lumière brothers' primary business interests were in selling cameras and film equipment to exhibitors, not the actual production of films. Despite this, filmmakers across the world were inspired by the potential of film as exhibitors brought their shows to new countries. This era of filmmaking, dubbed by film historian Tom Gunning as ""the cinema of attractions"", offered a relatively cheap and simple way of providing entertainment to the masses. Rather than focusing on stories, Gunning argues, filmmakers mainly relied on the ability to delight audiences through the ""illusory power"" of viewing sequences in motion, much as they did in the Kinetoscope era that preceded it.[23] Despite this, early experimentation with fiction filmmaking (both in actuality film and other genres) did occur. Films were mostly screened inside temporary storefront spaces, in tents of traveling exhibitors at fairs, or as ""dumb"" acts in vaudeville programs.[24] During this period, before the process of post-production was clearly defined, exhibitors were allowed to exercise their creative freedom in their presentations. To enhance the viewers' experience, some showings were accompanied by live musicians in an orchestra, a theatre organ, live sound effects and commentary spoken by the showman or projectionist.[25][26]
 Experiments in film editing, special effects, narrative construction, and camera movement during this period by filmmakers in France, England, and the United States became influential in establishing an identity for film going forward. At both the Edison and Lumière studios, loose narratives such as the 1895 Edison film, Washday Troubles, established short relationship dynamics and simple storylines.[27] In 1896, La Fée aux Choux (The Fairy of the Cabbages) was first released. Directed and edited by Alice Guy, the story is arguably the earliest narrative film in history, as well as the first film to be directed by a woman.[28] That same year, the Edison Manufacturing Company released The May Irwin Kiss in May to widespread financial success. The film, which featured the first kiss in cinematic history, led to the earliest known calls for film censorship.[29]
 Another early film producer was Australia's Limelight Department. Commencing in 1898, it was operated by The Salvation Army in Melbourne, Australia. The Limelight Department produced evangelistic material for use by the Salvation Army, including lantern slides as early as 1891, as well as private and government contracts. In its nineteen years of operation, the Limelight Department produced about 300 films of various lengths, making it one of largest film producers of its time. The Limelight Department made a 1904 film by Joseph Perry called Bushranging in North Queensland, which is believed to be the first ever film about bushrangers.
 In its infancy, film was rarely recognized as an art form by presenters or audiences. Regarded by the upper class as a ""vulgar"" and ""lowbrow"" form of cheap entertainment, films largely appealed to the working class and were often too short to hold any strong narrative potential.[30] Initial advertisements promoted the technologies used to screen films rather than the films themselves. As the devices became more familiar to audiences, their potential for capturing and recreating events was exploited primarily in the form of newsreels and actualities.[31] During the creation of these films, cinematographers often drew upon aesthetic values established by past art forms such as framing and the intentional placement of the camera in the composition of their image.[32] In a 1955 article for The Quarterly of Film Radio and television, film producer and historian Kenneth Macgowan asserted that the intentional staging and recreation of events for newsreels ""brought storytelling to the screen"".[33]
 With the advertisement of film technologies over content, actualities initially began as a ""series of views"" that often contained shots of beautiful and lively places or performance acts.[32] Following the success of their 1895 screening, The Lumière brothers established a company and sent cameramen across the world to capture new subjects for presentation. After the cinematographer shot scenes, they often exhibited their recordings locally and then sent them back to the company factory in Lyon to make duplicate prints for sale to whoever wanted them.[34] In the process of filming actualities, especially those of real events, filmmakers discovered and experimented with multiple camera techniques to accommodate for their unpredictable nature.[35] Due to the short length (often only one shot) of many actualities, catalogue records indicate that production companies marketed to exhibitors by promoting multiple actualities with related subject matters that could be purchased to complement each other. Exhibitors who bought the films often presented them in a program and would provide spoken accompaniment to explain the action on screen to audiences.[32]
 The first paying audience for a motion picture gathered at Madison Square Garden to see a staged actuality that purported itself to be a boxing fight filmed by Woodville Latham using a device called the Eidoloscope on May 20, 1895. Commissioned by Latham, the French inventor Eugene Augustin Lauste created the device with additional expertise from William Kennedy Dickson and crafted a mechanism that came to be known as the Latham loop, which allowed for longer continuous runtimes and was less abrasive on the celluloid film.[36]
 In subsequent years, screenings of actualities and newsreels proved to be profitable. In 1897, The Corbett-Fitzsimmons Fight was released. The film was a complete recording of a heavyweight world championship boxing match at Carson City, Nevada. It generated more income in box office than in live gate receipts and was the longest film produced at the time. Audiences had probably been drawn to the Corbett-Fitzsimmons film en masse because James J. Corbett (a.k.a. Gentleman Jim) had become a matinee idol since he had played a fictionalized version of himself in a stage play.[37]
 From 1910 on, regular newsreels were exhibited and soon became a popular way of discovering the news before the advent of television –  the British Antarctic Expedition to the South Pole was filmed for the newsreels as were the suffragette demonstrations that were happening at the same time. F. Percy Smith was an early nature documentary pioneer working for Charles Urban when he pioneered the use of time lapse and micro cinematography in his 1910 documentary on the growth of flowers.[38][39]
 Following the successful exhibition of the Cinématographe, development of a motion picture industry rapidly accelerated in France. Multiple filmmakers experimented with the technology as they worked to attain the same success that the Lumière brothers had with their screening. These filmmakers established new companies such as the Star Film Company, Pathé Frères, and the Gaumont Film Company.
 The most widely cited progenitor of narrative filmmaking is the French filmmaker, Georges Méliès. Méliès was an illusionist who had previously used magic lantern projections to enhance his magic act. In 1895, Méliès attended the demonstration of the Cinematographe and recognized the potential of the device to aid his act. He attempted to buy a device from the Lumière brothers, but they refused.[40] Months later, he bought a camera from Robert W. Paul and began experiments with the device by creating actualities. During this period of experimentation, Méliès discovered and implemented various special effects including the stop trick, the multiple exposure, and the use of dissolves in his films.[16] At the end of 1896, Méliès established the Star Film Company and started producing, directing, and distributing a body of work that would eventually contain over 500 short films.[41] Recognizing the narrative potential afforded by combining his theater background with the newly discovered effects for the camera, Méliès designed an elaborate stage that contained trapdoors and a fly system.[33] The stage construction and editing techniques allowed for the development of more complex stories, such as the 1896 film, Le Manoir du Diable (The House of the Devil), regarded as a first in the horror film genre, and the 1899 film Cendrillon (Cinderella).[42][43] In Méliès' films, he based the placement of the camera on the theatrical construct of proscenium framing, the metaphorical plane or fourth wall that divides the actors and the audience.[44] Throughout his career, Méliès consistently placed the camera in a fixed position and eventually fell out of favor with audiences as other filmmakers experimented with more complex and creative techniques.[45] Méliès is most widely known today for his 1902 film, Le Voyage Dans La Lune (A Trip to the Moon), where he used his expertise in effects and narrative construction to create the first science fiction film.[46]
 In 1900, Charles Pathé began film production under the Pathé-Frères brand, with Ferdinand Zecca hired to lead the creative process.[47] Prior to this focus on production, Pathé had become involved with the industry by exhibiting and selling what were likely counterfeit versions of the Kinetoscope in his phonograph shop. With the creative leadership of Zecca and the capability to mass-produce copies of the films through a partnership with a French toolmaking company, Charles Pathé sought to make Pathé-Frères the leading film producer in the country. Within the next few years, Pathé-Frères became the largest film studio in the world, with satellite offices in major cities and an expanding selection of films available for presentation.[48] The company's films were varied in content, with directors specializing in various genres for fairground presentations throughout the early 1900s.[47]
 The Gaumont Film Company was the main regional rival of Pathé-Frères. Founded in 1895 by Léon Gaumont, the firm initially sold photographic equipment and began film production in 1897, under the direction of Alice Guy, the industry's first female director.[49] Her earlier films share many characteristics and themes with her contemporary competitors, such as the Lumières and Méliès. She explored dance and travel films, often combining the two, such as Le Boléro performed by Miss Saharet (1905) and Tango (1905). Many of Guy's early dance films were popular in music-hall attractions such as the serpentine dance films – also a staple of the Lumières and Thomas Edison film catalogs.[50] In 1906, she made The Life of Christ, a big-budget production for the time, which included 300 extras.
 Both Cecil Hepworth and Robert W. Paul experimented with the use of different camera techniques in their films. Paul's 'Cinematograph Camera No. 1' of 1895 was the first camera to feature reverse-cranking, which allowed the same film footage to be exposed several times, thereby creating multiple exposures. This technique was first used in his 1901 film Scrooge, or, Marley's Ghost. Both filmmakers experimented with the speeds of the camera to generate new effects. Paul shot scenes from On a Runaway Motor Car through Piccadilly Circus (1899) by cranking the camera apparatus very slowly.[51] When the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Hepworth used the opposite effect in The Indian Chief and the Seidlitz Powder (1901). The Chief's movements are sped up by cranking the camera much faster than 16 frames per second, producing what modern audiences would call a ""slow motion"" effect.[52]
 The first films to move from single shots to successive scenes began around the turn of the 20th century. Due to the loss of many early films, a conclusive shift from static singular shots to a series of scenes can be hard to determine. Despite these limitations, Michael Brooke of the British Film Institute attributes real film continuity, involving action moving from one sequence into another, to Robert W. Paul's 1898 film, Come Along, Do!. Only a still from the second shot remains extant today.[53] Released in 1901, the British film Attack on a China Mission was one of the first films to show a continuity of action across multiple scenes.[33] The use of the intertitle to explain actions and dialogue on screen began in the early 1900s. Filmed intertitles were first used in Robert W. Paul's film, Scrooge, or Marley's Ghost.[54] In most countries, intertitles gradually came to be used to provide dialogue and narration for the film, thus dispensing the need for narration provided by exhibitors.
 Development of continuous action across multiple shots was furthered in England by a loosely associated group of film pioneers collectively termed ""the Brighton School"". These filmmakers included George Albert Smith and James Williamson, among others. Smith and Williamson experimented with action continuity and were likely the first to incorporate the use of inserts and close-ups between shots.[33] A basic technique for trick cinematography was the double exposure of the film in the camera. The effect was pioneered by Smith in the 1898 film, Photographing a Ghost. According to Smith's catalogue records, the (now lost) film chronicles a photographer's struggle to capture a ghost on camera. Using the double exposure of the film, Smith overlaid a transparent ghostly figure onto the background in a comical manner to taunt the photographer.[55] Smith's The Corsican Brothers was described in the catalogue of the Warwick Trading Company in 1900: ""By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow.""[56] Smith also initiated the special effects technique of reverse motion. He did this by repeating the action a second time, while filming it with an inverted camera, and then joining the tail of the second negative to that of the first.[57] The first films made using this device were Tipsy, Topsy, Turvy and The Awkward Sign Painter. The earliest surviving example of this technique is Smith's The House That Jack Built, made before September 1900. Cecil Hepworth took this technique further by printing the negatives of the forward motion in reverse frame by frame, producing a print in which the original action was exactly reversed. To do this he built a special printer in which the negative running through a projector was projected into the gate of a camera through a special lens giving a same-size image. This arrangement came to be called a ""projection printer"", and eventually an ""optical printer"".[58]
 In 1898, George Albert Smith experimented with close-ups, filming shots of a man drinking beer and a woman using sniffing tobacco.[33] The following year, Smith made The Kiss in the Tunnel, a sequence consisting of three shots: a train enters a tunnel; a man and a woman exchange a brief kiss in the darkness and then return to their seats; the train exits the tunnel. Smith created the scenario in response to the success of a genre known as a phantom ride. In a phantom ride film, cameras would capture the motion and surroundings from the front of a moving train.[59][60] The separate shots, when edited together, formed a distinct sequence of events and established causality from one shot to the next.[61] Following The Kiss in the Tunnel, Smith more definitively experimented with continuity of action across successive shots and began using inserts in his films, such as Grandma's Reading Glass and Mary Jane's Mishap.[33] In 1900, Smith made As Seen Through a Telescope. The main shot shows a street scene with a young man tying the shoelace and then caressing the foot of his girlfriend, while an old man observes this through a telescope. There is then a cut to close shot of the hands on the girl's foot shown inside a black circular mask, and then a cut back to the continuation of the original scene.[62]
 James Williamson perfected narrative building techniques in his 1900 film, Attack on a China Mission. The film, which film historian John Barnes later described as having ""the most fully developed narrative of any film made in England up to that time"", opens as the first shot shows Chinese Boxer rebels at the gate; it then cuts to the missionary family in the garden, where a fight ensues. The wife signals to British sailors from the balcony, who come and rescue them.[63] The film also used the first ""reverse angle"" cut in film history.[64] The following year, Williamson created The Big Swallow. In the film. a man becomes irritated by the presence of the filmmaker and ""swallows"" the camera and its operator through the use of interpolated close-up shots.[65] He combined these effects, along with superimpositions, use of wipe transitions to denote a scene change, and other techniques to create a film language, or ""film grammar"".[66][67] James Williamson's use of continuous action in his 1901 film, Stop Thief! stimulated a film genre known as the ""chase film.""[68] In the film, a tramp steals a leg of mutton from a butcher's boy in the first shot, is chased by the butcher's boy and assorted dogs in the following shot, and is finally caught by the dogs in the third shot.[68]
 The Execution of Mary Stuart, produced in 1895 by the Edison Company for viewing with the Kinetoscope, showed Mary Queen of Scots being executed in full view of the camera. The effect, known as the stop trick, was achieved by replacing the actor with a dummy for the final shot.[69][70] The technique used in the film is seen as one of the earliest known uses of special effects in film.[71]
 The American filmmaker Edwin S. Porter started making films for the Edison Company in 1901. A former projectionist hired by Thomas Edison to develop his new projection model known as the Vitascope, Porter was inspired in part by the works of Méliès, Smith, and Williamson and drew upon their newly crafted techniques to further the development of continuous narrative through editing.[16] When he began making longer films in 1902, he put a dissolve between every shot, just as Georges Méliès was already doing, and he frequently had the same action repeated across the dissolves.
 In 1902, Porter shot Life of an American Fireman for the Edison Manufacturing Company and distributed the film the following year. In the film, Porter combined stock footage from previous Edison films with newly shot footage and spliced them together to convey a dramatic story of the rescue of a woman and her child by heroic firemen.[16]
 Porter's film, The Great Train Robbery (1903), had a running time of twelve minutes, with twenty separate shots and ten different indoor and outdoor locations. The film is seen as a first in the Western film genre and is significant for the use of shots suggesting simultaneous action occurring at different locations.[16] Porter's use of both staged and real outdoor environments helped to create a sense of space while the placement of the camera in a wider shot established depth and allowed for an extended duration of motion on screen.[72] The Great Train Robbery served as one of the vehicles that would launch the film medium into mass popularity.[40][73] That same year, the Miles Brothers opened the first film exchange in the country, which allowed permanent exhibitors to rent films from the company at a lower cost than the producers that sold their films outright.[74]
 John P. Harris opened the first permanent theater devoted exclusively to the presentation of films, the nickelodeon, in 1905 in Pittsburgh, Pennsylvania. The idea rapidly took off and by 1908, there were around 8,000 nickelodeon theaters across the country.[75] With the arrival of the nickelodeon, audience demand for a larger quantity of story films with a variety of subjects and locations led to a need to hire more creative talent and caused studios to invest in more elaborate stage designs.[74]
 In 1908, Thomas Edison spearheaded the creation of a corporate trust between the major film companies in America known as the Motion Picture Patents Company (MPPC) to limit infringement on his patents. Members of the trust controlled every aspect of the filmmaking process from the creation of film stock, the production of films, and the distribution to cinemas through licensing arrangements. The trust led to increased quality filmmaking spurred by internal competition and placed limits on the amount of foreign films to encourage the growth of the American film industry, but it also discouraged the creation of feature films. By 1915, the MPPC had lost most of its hold on the film industry as the companies moved towards the wider production of feature films.[46]
 With the worldwide film boom, more countries now joined Britain, France, Germany and the United States in serious film production. In Italy, production was spread over several centers, Turin was the first major film production centre, and Milan and Naples gave birth to the first film magazines.[76] In Turin, Ambrosio was the first company in the field in 1905, and remained the largest in the country through this period. Its most substantial rival was Cines in Rome, which started producing in 1906. The great strength of the Italian industry was historical epics, with large casts and massive scenery. As early as 1911, Giovanni Pastrone's two-reel La Caduta di Troia (The Fall of Troy) made a big impression worldwide, and it was followed by even bigger productions like Quo Vadis? (1912), which ran for 90 minutes, and Pastrone's Cabiria of 1914, which ran for two and a half hours.[77]
 Italian companies also had a strong line in slapstick comedy, with actors like André Deed, known locally as ""Cretinetti"", and elsewhere as ""Foolshead"" and ""Gribouille"", achieving worldwide fame with his almost surrealistic gags.
 The most important film-producing country in Northern Europe up until the First World War was Denmark.[77][78] The Nordisk company was set up there in 1906 by Ole Olsen, a fairground showman, and after a brief period imitating the successes of French and British filmmakers, in 1907 he produced 67 films, most directed by Viggo Larsen, with sensational subjects like Den hvide Slavinde (The White Slave), Isbjørnejagt (Polar Bear Hunt) and Løvejagten (The Lion Hunt). By 1910, new smaller Danish companies began joining the business, and besides making more films about the white slave trade, they contributed other new subjects. The most important of these finds was Asta Nielsen in Afgrunden (The Abyss), directed by Urban Gad for Kosmorama, This combined the circus, sex, jealousy and murder, all put over with great conviction, and pushed the other Danish filmmakers further in this direction. By 1912, the Danish film companies were multiplying rapidly.[77]
 The Swedish film industry was smaller and slower to get started than the Danish industry. Here, Charles Magnusson, a newsreel cameraman for the Svenskabiografteatern cinema chain, started fiction film production for them in 1909, directing a number of the films himself. Production increased in 1912, when the company engaged Victor Sjöström and Mauritz Stiller as directors. They started out by imitating the subjects favoured by the Danish film industry, but by 1913 they were producing their own strikingly original work, which sold very well.[77]
 Russia began its film industry in 1908 with Pathé shooting some fiction subjects there, and then the creation of real Russian film companies by Aleksandr Drankov and Aleksandr Khanzhonkov. The Khanzhonkov company quickly became much the largest Russian film company, and remained so until 1918.[77]
 In Germany, Oskar Messter had been involved in film-making from 1896, but did not make a significant number of films per year until 1910. When the worldwide film boom started, he, and the few other people in the German film business, continued to sell prints of their own films outright, which put them at a disadvantage. It was only when Paul Davidson, the owner of a chain of cinemas, brought Asta Nielsen and Urban Gad to Germany from Denmark in 1911, and set up a production company, Projektions-AG ""Union"" (PAGU), that a change-over to renting prints began.[77] Messter replied with a series of longer films starring Henny Porten, but although these did well in the German-speaking world, they were not particularly successful internationally, unlike the Asta Nielsen films. Another of the growing German film producers just before World War I was the German branch of the French Éclair company, Deutsche Éclair. This was expropriated by the German government, and turned into DECLA when the war started. But altogether, German producers only had a minor part of the German market in 1914.[citation needed]
 Overall, from about 1910, American films had the largest share of the market in all European countries except France, and even in France, the American films had just pushed the local production out of first place on the eve of World War I.[citation needed] Pathé Frères expanded and significantly shaped the American film business, creating many ""firsts"" in the film industry, such as adding titles and subtitles to films for the first time, releasing scrolls for the first time, introducing film posters for the first time, producing color pictures for the first time, taking out commercial bills for the first time, contacting exhibitors and studying their needs for the first time. The world's largest film supplier, Pathé, is limited to the U.S. market, which has reached a saturation level, so the U.S. seeks additional profits from foreign markets. Movies are defined as ""pure"" American phenomenon in the United States.[79]
 New film techniques that were introduced in this period include the use of artificial lighting, fire effects and low-key lighting (i.e. lighting in which most of the frame is dark) for enhanced atmosphere during sinister scenes.[77]
 Continuity of action from shot to shot was also refined, such as in Pathé's le Cheval emballé (The Runaway Horse) (1907) where cross-cutting between parallel actions is used. D. W. Griffith also began using cross-cutting in the film The Fatal Hour, made in July 1908. Another development was the use of the point of view shot, first used in 1910 in Vitagraph's Back to Nature. Insert shots were also used for artistic purposes; the Italian film La mala planta (The Evil Plant), directed by Mario Caserini had an insert shot of a snake slithering over the ""Evil Plant"". [citation needed] By 1914 it was widely held in the American film industry that cross-cutting was most generally useful because it made possible the elimination of uninteresting parts of the action that play no part in advancing the drama.[80]
 In 1909, 35mm became the internationally recognized theatrical film gauge.[46]
 As films grew longer, specialist writers were employed to simplify more complex stories derived from novels or plays into a form that could be contained on one reel. Genres began to be used as categories; the main division was into comedy and drama, but these categories were further subdivided.[77]
 Intertitles containing lines of dialogue began to be used consistently from 1908 onwards,[81] such as in Vitagraph's An Auto Heroine; or, The Race for the Vitagraph Cup and How It Was Won. The dialogue was eventually inserted into the middle of the scene and became commonplace by 1912. The introduction of dialogue titles transformed the nature of film narrative. When dialogue titles came to be always cut into a scene just after a character starts speaking, and then left with a cut to the character just before they finish speaking, then one had something that was effectively the equivalent of a present-day sound film.[77]
 The years of the First World War were a complex transitional period for the film industry. The exhibition of films changed from short one-reel programmes to feature films. Exhibition venues became larger and began charging higher prices.[77]
 In the United States, these changes brought destruction to many film companies, the Vitagraph company being an exception. Film production began to shift to Los Angeles during World War I. The Universal Film Manufacturing Company was formed in 1912 as an umbrella company. New entrants included the Jesse Lasky Feature Play Company, and Famous Players, both formed in 1913, and later amalgamated into Famous Players–Lasky. The biggest success of these years was David Wark Griffith's The Birth of a Nation (1915). Griffith followed this up with the even bigger Intolerance (1916), but, due to the high quality of film produced in the US, the market for their films was high.[77]
 In France, film production shut down due to the general military mobilization of the country at the start of the war. Although film production began again in 1915, it was on a reduced scale, and the biggest companies gradually retired from production. Italian film production held up better, although so called ""diva films"", starring anguished female leads were a commercial failure. In Denmark, the Nordisk company increased its production so much in 1915 and 1916 that it could not sell all its films, which led to a very sharp decline in Danish production, and the end of Denmark's importance on the world film scene.[77]
 The German film industry was seriously weakened by the war. The most important of the new film producers at the time was Joe May, who made a series of thrillers and adventure films through the war years, but Ernst Lubitsch also came into prominence with a series of very successful comedies and dramas.[77]
 At this time, studios were blacked out to allow shooting to be unaffected by changing sunlight. This was replaced with floodlights and spotlights. The widespread adoption of irising-in and out to begin and end scenes caught on in this period. This is the revelation of a film shot in a circular mask, which gradually gets larger until it expands beyond the frame. Other shaped slits were used, including vertical and diagonal apertures.[77]
 A new idea taken over from still photography was ""soft focus"". This began in 1915, with some shots being intentionally thrown out of focus for expressive effect, as in Mary Pickford starrer Fanchon the Cricket.[77]
 It was during this period that camera effects intended to convey the subjective feelings of characters in a film really began to be established. These could now be done as Point of View (POV) shots, as in Sidney Drew's The Story of the Glove (1915), where a wobbly hand-held shot of a door and its keyhole represents the POV of a drunken man. The use of anamorphic (in the general sense of distorted shape) images first appears in these years when Abel Gance directed la Folie du Docteur Tube (The Madness of Dr. Tube). In this film the effect of a drug administered to a group of people was suggested by shooting the scenes reflected in a distorting mirror of the fair-ground type.[77]
 Symbolic effects taken over from conventional literary and artistic tradition continued to make some appearances in films during these years. In D. W. Griffith's The Avenging Conscience (1914), the title ""The birth of the evil thought"" precedes a series of three shots of the protagonist looking at a spider, and ants eating an insect. Symbolist art and literature from the turn of the century also had a more general effect on a small number of films made in Italy and Russia. The supine acceptance of death resulting from passion and forbidden longings was a major feature of this art, and states of delirium dwelt on at length were important as well.[77]
 The use of insert shots, i.e. close-ups of objects other than faces, had already been established by the Brighton school, but were infrequently used before 1914. It is really only with Griffith's The Avenging Conscience that a new phase in the use of the Insert Shot starts.[77] As well as the symbolic inserts already mentioned, the film also made extensive use of large numbers of Big Close Up shots of clutching hands and tapping feet as a means of emphasizing those parts of the body as indicators of psychological tension.[81]
 Atmospheric inserts were developed in Europe in the late 1910s. [citation needed] This kind of shot is one in a scene which neither contains any of the characters in the story, nor is a Point of View shot seen by one of them. An early example is when Maurice Tourneur directed The Pride of the Clan (1917), in which there is a series of shots of waves beating on a rocky shore to demonstrate the harsh lives of the fishing folk. Maurice Elvey's Nelson; The Story of England's Immortal Naval Hero (1919) has a symbolic sequence dissolving from a picture of Kaiser Wilhelm II to a peacock, and then to a battleship.[81]
 By 1914, continuity cinema was the established mode of commercial cinema. One of the advanced continuity techniques involved an accurate and smooth transition from one shot to another.[77] Cutting to different angles within a scene also became well-established as a technique for dissecting a scene into shots in American films.[81] If the direction of the shot changes by more than ninety degrees, it is called a reverse-angle cutting.[82] The leading figure in the full development of reverse-angle cutting was Ralph Ince in his films, such as The Right Girl and His Phantom Sweetheart.[81]
 The use of flash-back structures continued to develop in this period, with the usual way of entering and leaving a flash-back being through a dissolve.[citation needed] The Vitagraph Company's The Man That Might Have Been (William J. Humphrey, 1914), is even more complex, with a series of reveries and flash-backs that contrast the protagonist's real passage through life with what might have been, if his son had not died.
 After 1914, cross cutting between parallel actions came to be used –  more so in American films than in European ones. Cross-cutting was used to get new effects of contrast, such as the cross-cut sequence in Cecil B. DeMille's The Whispering Chorus (1918), in which a supposedly dead husband is having a liaison with a Chinese prostitute in an opium den, while simultaneously his unknowing wife is being remarried in church.[81]
 Silent film tinting, too, gained popularity during these periods. Amber tinting meant daytime, or vividly-lit nighttime, blue tints meant dawn or dimly-lit night, red tinting represented fire scenes, green tinting meant a mysterious atmosphere, and brown tints (aka sepia toning) were used usually for full-length films instead of individual scenes. D.W. Griffiths' ground-breaking epic, The Birth of a Nation, the famous 1920 film Dr. Jekyll and Mr. Hyde, and the Robert Wiene epic from the same year, The Cabinet of Dr. Caligari, are some notable examples of tinted silent films. [citation needed]
 The Photo-Drama of Creation, first shown to audiences in 1914, was the first major screenplay to incorporate synchronized sound, moving film, and color slides.[83] Until 1927, most motion pictures were produced without sound. This period is commonly referred to as the silent era of film.[84][85]
 The general trend in the development of cinema, led from the United States, was towards using the newly developed specifically filmic devices for expression of the narrative content of film stories, and combining this with the standard dramatic structures already in use in commercial theatre. [citation needed] D. W. Griffith had the highest standing among American directors in the industry, because of the dramatic excitement he conveyed to the audience through his films. Cecil B. DeMille's The Cheat (1915), brought out the moral dilemmas facing their characters in a more subtle way than Griffith. DeMille was also in closer touch with the reality of contemporary American life. Maurice Tourneur was also highly ranked for the pictorial beauties of his films, together with the subtlety of his handling of fantasy, while at the same time he was capable of getting greater naturalism from his actors at appropriate moments, as in A Girl's Folly (1917).[77]
 Sidney Drew was the leader in developing ""polite comedy"", while slapstick was refined by Fatty Arbuckle and Charles Chaplin, who both started with Mack Sennett's Keystone company. They reduced the usual frenetic pace of Sennett's films to give the audience a chance to appreciate the subtlety and finesse of their movement, and the cleverness of their gags. By 1917 Chaplin was also introducing more dramatic plot into his films, and mixing the comedy with sentiment.[77]
 In Russia, Yevgeni Bauer put a slow intensity of acting combined with Symbolist overtones onto film in a unique way.[77]
 In Sweden, Victor Sjöström made a series of films that combined the realities of people's lives with their surroundings in a striking manner, while Mauritz Stiller developed sophisticated comedy to a new level.[77]
 In Germany, Ernst Lubitsch got his inspiration from the stage work of Max Reinhardt, both in bourgeois comedy and in spectacle, and applied this to his films, culminating in his die Puppe (The Doll), die Austernprinzessin (The Oyster Princess) and Madame DuBarry.[77]
 At the start of the First World War, French and Italian cinema had been the most globally popular. The war came as a devastating interruption to European film industries.
 Throughout the early 20th century, screen artists continued to learn how to work with cameras and create illusions using space and time in their shots. This newly introduced form of creativity made way for a whole new group of people to be introduced to stardom, including David W. Griffith, who made a name for himself with his 1915 film, The Birth of a Nation. In 1920, there were two major changes to the film industry: the introduction of sound and the creation of studio systems. In the 1920s, talent who had been working independently began joining studios and working with other actors and directors. In 1927, The Jazz Singer was released, bringing sound to the motion picture industry.
 The German cinema, marked by those times, saw the era of the German Expressionist film movement. Berlin was its center with the Filmstudio Babelsberg, which is the oldest large-scale film studio in the world.[87] The first Expressionist films made up for a lack of lavish budgets by using set designs with wildly non-realistic, geometrically absurd angles, along with designs painted on walls and floors to represent lights, shadows, and objects. The plots and stories of the Expressionist films often dealt with madness, insanity, betrayal and other ""intellectual"" topics triggered by the experiences of World War I. Films like The Cabinet of Dr. Caligari (1920), Nosferatu (1922) and M (1931), similar to the movement they were part of, had a historic impact on film itself.[88]
 Movies like Metropolis (1927) and Woman in the Moon (1929) partly created the genre of science fiction films[89] and Lotte Reiniger became a pioneer in animation, producing animated feature films like The Adventures of Prince Achmed, the oldest surviving and oldest European made animated movie.
 Many German and German-based directors, actors, writers and others emigrated to the US when the Nazis gained power, giving Hollywood and the American film industry the final edge in its competition with other movie producing countries.[90]
 The American industry, or ""Hollywood"", as it was becoming known after its new geographical center in California, gained the position it has held, more or less, ever since: film factory for the world and exporting its product to most countries on earth.
 By the 1920s, the United States reached what is still its era of greatest-ever output, producing an average of 800 feature films annually,[91] or 82% of the global total (Eyman, 1997). The comedies of Charlie Chaplin and Buster Keaton, the swashbuckling adventures of Douglas Fairbanks and the romances of Clara Bow, to cite just a few examples, made these performers' faces well known on every continent. The Western visual norm that would become classical continuity editing was developed and exported – although its adoption was slower in some non-Western countries without strong realist traditions in art and drama, such as Japan.
 This development was contemporary with the growth of the studio system and its greatest publicity method, the star system, which characterized American film for decades to come and provided models for other film industries. The studios' efficient, top-down control over all stages of their product enabled a new and ever-growing level of lavish production and technical sophistication. At the same time, the system's commercial regimentation and focus on glamorous escapism discouraged daring and ambition beyond a certain degree, a prime example being the brief but still legendary directing career of the iconoclastic Erich von Stroheim in the late teens and the 1920s.
 In 1924, Sam Goldwyn, Louis B. Mayer, and the Metro Pictures Corporation create MGM.[46]
 During late 1927, Warners released The Jazz Singer, which was mostly silent but contained what is generally regarded as the first synchronized dialogue (and singing) in a feature film;[92] but this process was actually accomplished first by Charles Taze Russell in 1914 with the lengthy film The Photo-Drama of Creation. This drama consisted of picture slides and moving pictures synchronized with phonograph records of talks and music. The early sound-on-disc processes such as Vitaphone were soon superseded by sound-on-film methods like Fox Movietone, DeForest Phonofilm, and RCA Photophone. The trend convinced the largely reluctant industrialists that ""talking pictures"", or ""talkies"", were the future. A lot of attempts were made before the success of The Jazz Singer, that can be seen in the List of film sound systems. And in 1926, Warner Bros. Debuts the film Don Juan with synchronized sound effects and music.[46]
 The change was remarkably swift. By the end of 1929, Hollywood was almost all-talkie, with several competing sound systems (soon to be standardized). Total changeover was slightly slower in the rest of the world, principally for economic reasons. Cultural reasons were also a factor in countries like China and Japan, where silents co-existed successfully with sound well into the 1930s, indeed producing what would be some of the most revered classics in those countries, like Wu Yonggang's The Goddess (China, 1934) and Yasujirō Ozu's I Was Born, But... (Japan, 1932). But even in Japan, a figure such as the benshi, the live narrator who was a major part of Japanese silent cinema, found his acting career was ending.
 Sound further tightened the grip of major studios in numerous countries: the vast expense of the transition overwhelmed smaller competitors, while the novelty of sound lured vastly larger audiences for those producers that remained. In the case of the U.S., some historians credit sound with saving the Hollywood studio system in the face of the Great Depression (Parkinson, 1995). Thus began what is now often called ""The Golden Age of Hollywood"", which refers roughly to the period beginning with the introduction of sound until the late 1940s. The American cinema reached its peak of efficiently manufactured glamour and global appeal during this period. The top actors of the era are now thought of as the classic film stars, such as Clark Gable, Katharine Hepburn, Humphrey Bogart, Greta Garbo, and the greatest box office draw of the 1930s, child performer Shirley Temple.
 Creatively, however, the rapid transition was a difficult one, and in some ways, film briefly reverted to the conditions of its earliest days. The late '20s were full of static, stagey talkies as artists in front of and behind the camera struggled with the stringent limitations of the early sound equipment and their own uncertainty as to how to use the new medium. Many stage performers, directors and writers were introduced to cinema as producers sought personnel experienced in dialogue-based storytelling. Many major silent filmmakers and actors were unable to adjust and found their careers severely curtailed or even ended.
 This awkward period was fairly short-lived. 1929 was a watershed year: William Wellman with Chinatown Nights and The Man I Love, Rouben Mamoulian with Applause, Alfred Hitchcock with Blackmail (Britain's first sound feature), were among the directors to bring greater fluidity to talkies and experiment with the expressive use of sound (Eyman, 1997). In this, they both benefited from, and pushed further, technical advances in microphones and cameras, and capabilities for editing and post-synchronizing sound (rather than recording all sound directly at the time of filming).
 Sound films emphasized black history, and benefited different genres to a greater extent than silents did. Most obviously, the musical film was born; the first classic-style Hollywood musical was The Broadway Melody (1929), and the form would find its first major creator in choreographer/director Busby Berkeley (42nd Street, 1933, Dames, 1934). In France, avant-garde director René Clair made surreal use of song and dance in comedies like Under the Roofs of Paris (1930) and Le Million (1931). Universal Pictures began releasing gothic horror films like Dracula and Frankenstein (both 1931). In 1933, RKO Pictures released Merian C. Cooper's classic ""giant monster"" film King Kong. The trend thrived best in India, where the influence of the country's traditional song-and-dance drama made the musical the basic form of most sound films (Cook, 1990); virtually unnoticed by the Western world for decades, this Indian popular cinema would nevertheless become the world's most prolific. (See also Bollywood.)
 At this time, American gangster films like Little Caesar and Wellman's The Public Enemy (both 1931) became popular. Dialogue now took precedence over slapstick in Hollywood comedies: the fast-paced, witty banter of The Front Page (1931) or It Happened One Night (1934), the sexual double entendres of Mae West (She Done Him Wrong, 1933), or the often subversively anarchic nonsense talk of the Marx Brothers (Duck Soup, 1933). Walt Disney, who had previously been in the short cartoon business, stepped into feature films with the first English-speaking animated feature Snow White and the Seven Dwarfs, released by RKO Pictures in 1937. 1939, a major year for American cinema, brought such films as The Wizard of Oz and Gone with The Wind.
 Circa 80 percent of the films of the 1890s to the 1920s had colours.[93] Many made use of monochromatic film tinting dye baths, some had the frames painted in multiple transparent colours by hand, and since 1905 there was a mechanized stencil-process (Pathécolor).
 Kinemacolor, the first commercially successful cinematographic colour process, produced films in two colours (red and cyan) from 1908 to 1914.
 Technicolor's natural three-strip colour process was very successfully introduced in 1932 with Walt Disney's animated Academy Award-winning short ""Flowers and Trees"", directed by Burt Gillett. Technicolor was initially used mainly for musicals like ""The Wizard of Oz"" (1939), in costume films such as ""The Adventures of Robin Hood"", and in animation. Not long after television became prevalent in the early 1950s, colour became more or less standard for theatrical movies.
 The desire for wartime propaganda against the opposition created a renaissance in the film industry in Britain, with realistic war dramas like 49th Parallel (1941), Went the Day Well? (1942), The Way Ahead (1944) and Noël Coward and David Lean's celebrated naval film In Which We Serve in 1942, which won a special Academy Award. These existed alongside more flamboyant films like Michael Powell and Emeric Pressburger's The Life and Death of Colonel Blimp (1943), A Canterbury Tale (1944) and A Matter of Life and Death (1946), as well as Laurence Olivier's 1944 film Henry V, based on the Shakespearean history Henry V. The success of Snow White and the Seven Dwarfs allowed Disney to make more animated features like Pinocchio (1940), Fantasia (1940), Dumbo (1941) and Bambi (1942).
 The onset of US involvement in World War II also brought a proliferation of films as both patriotism and propaganda. American propaganda films included Desperate Journey (1942), Mrs. Miniver (1942), Forever and a Day (1943) and Objective, Burma! (1945). Notable American films from the war years include the anti-Nazi Watch on the Rhine (1943), scripted by Dashiell Hammett; Shadow of a Doubt (1943), Hitchcock's direction of a script by Thornton Wilder; the George M. Cohan biographical film, Yankee Doodle Dandy (1942), starring James Cagney, and the immensely popular Casablanca, with Humphrey Bogart. Bogart would star in 36 films between 1934 and 1942 including John Huston's The Maltese Falcon (1941), one of the first films now considered a classic film noir. In 1941, RKO Pictures released Citizen Kane made by Orson Welles. It is often considered the greatest film of all time. It would set the stage for the modern motion picture, as it revolutionized film story telling.
 The strictures of wartime also brought an interest in more fantastical subjects. These included Britain's Gainsborough melodramas (including The Man in Grey and The Wicked Lady), and films like Here Comes Mr. Jordan, Heaven Can Wait, I Married a Witch and Blithe Spirit. Val Lewton also produced a series of atmospheric and influential small-budget horror films, some of the more famous examples being Cat People, Isle of the Dead and The Body Snatcher. The decade probably also saw the so-called ""women's pictures"", such as Now, Voyager, Random Harvest and Mildred Pierce at the peak of their popularity.
 1946 saw RKO Radio releasing It's a Wonderful Life directed by Italian-born filmmaker Frank Capra. Soldiers returning from the war would provide the inspiration for films like The Best Years of Our Lives, and many of those in the film industry had served in some capacity during the war. Samuel Fuller's experiences in World War II would influence his largely autobiographical films of later decades such as The Big Red One. The Actors Studio was founded in October 1947 by Elia Kazan, Robert Lewis, and Cheryl Crawford, and the same year Oskar Fischinger filmed Motion Painting No. 1.
 In 1943, Ossessione was screened in Italy, marking the beginning of Italian neorealism. Major films of this type during the 1940s included Bicycle Thieves, Rome, Open City, and La Terra Trema. In 1952 Umberto D was released, usually considered the last film of this type.
 In the late 1940s, in Britain, Ealing Studios embarked on their series of celebrated comedies, including Whisky Galore!, Passport to Pimlico, Kind Hearts and Coronets and The Man in the White Suit, and Carol Reed directed his influential thrillers Odd Man Out, The Fallen Idol and The Third Man. David Lean was also rapidly becoming a force in world cinema with Brief Encounter and his Dickens adaptations Great Expectations and Oliver Twist, and Michael Powell and Emeric Pressburger would experience the best of their creative partnership with films like Black Narcissus and The Red Shoes.
 The House Un-American Activities Committee investigated Hollywood in the early 1950s. Protested by the Hollywood Ten before the committee, the hearings resulted in the blacklisting of many actors, writers and directors, including Chayefsky, Charlie Chaplin, and Dalton Trumbo, and many of these fled to Europe, especially the United Kingdom.
 The Cold War era zeitgeist translated into a type of near-paranoia manifested in themes such as invading armies of evil aliens (Invasion of the Body Snatchers, The War of the Worlds) and communist fifth columnists (The Manchurian Candidate).
 During the immediate post-war years the cinematic industry was also threatened by television, and the increasing popularity of the medium meant that some film theatres would bankrupt and close. The demise of the ""studio system"" spurred the self-commentary of films like Sunset Boulevard (1950) and The Bad and the Beautiful (1952).
 In 1950, the Lettrists avante-gardists caused riots at the Cannes Film Festival, when Isidore Isou's Treatise on Slime and Eternity was screened. After their criticism of Charlie Chaplin and split with the movement, the Ultra-Lettrists continued to cause disruptions when they showed their new hypergraphical techniques.
The most notorious film is Guy Debord's Howls for Sade of 1952.
Distressed by the increasing number of closed theatres, studios and companies would find new and innovative ways to bring audiences back. These included attempts to widen their appeal with new screen formats. Cinemascope, which would remain a 20th Century Fox distinction until 1967, was announced with 1953's The Robe. VistaVision, Cinerama, and Todd-AO boasted a ""bigger is better"" approach to marketing films to a dwindling US audience. This resulted in the revival of epic films to take advantage of the new big screen formats. Some of the most successful examples of these Biblical and historical spectaculars include The Ten Commandments (1956), The Vikings (1958), Ben-Hur (1959), Spartacus (1960) and El Cid (1961). Also during this period a number of other significant films were produced in Todd-AO, developed by Mike Todd shortly before his death, including Oklahoma! (1955), Around the World in 80 Days (1956), South Pacific (1958) and Cleopatra (1963) plus many more.
 Gimmicks also proliferated to lure in audiences. The fad for 3-D film would last for only two years, 1952–1954, and helped sell House of Wax and Creature from the Black Lagoon. Producer William Castle would tout films featuring ""Emergo"" ""Percepto"", the first of a series of gimmicks that would remain popular marketing tools for Castle and others throughout the 1960s.
 In 1954, Dorothy Dandridge was nominated as the best actress at the Oscar for her role in the film Carman Jones. She became the first black woman to be nominated for this award.[95]
 In the U.S., a post-WW2 tendency toward questioning the establishment and societal norms and the early activism of the civil rights movement was reflected in Hollywood films such as Blackboard Jungle (1955), On the Waterfront (1954), Paddy Chayefsky's Marty and Reginald Rose's 12 Angry Men (1957). Disney continued making animated films, notably; Cinderella (1950), Peter Pan (1953), Lady and the Tramp (1955), and Sleeping Beauty (1959). He began, however, getting more involved in live action films, producing classics like 20,000 Leagues Under the Sea (1954), and Old Yeller (1957). Television began competing seriously with films projected in theatres, but surprisingly it promoted more filmgoing rather than curtailing it.
 Limelight is probably a unique film in at least one interesting respect. Its two leads, Charlie Chaplin and Claire Bloom, were in the industry in no less than three different centuries. In the 19th century, Chaplin made his theatrical debut at the age of eight, in 1897, in a clog dancing troupe, The Eight Lancaster Lads. In the 21st century, Bloom is still enjoying a full and productive career, having appeared in dozens of films and television series produced up to and including 2022. She received particular acclaim for her role in The King's Speech (2010).
 Following the end of World War II in the 1940s, the following decade, the 1950s, marked a 'golden age' for non-English world cinema,[96][97] especially for Asian cinema.[98][99] Many of the most critically acclaimed Asian films of all time were produced during this decade, including Yasujirō Ozu's Tokyo Story (1953), Satyajit Ray's The Apu Trilogy (1955–1959) and Jalsaghar (1958), Kenji Mizoguchi's Ugetsu (1954) and Sansho the Bailiff (1954), Raj Kapoor's Awaara (1951), Mikio Naruse's Floating Clouds (1955), Guru Dutt's Pyaasa (1957) and Kaagaz Ke Phool (1959), and the Akira Kurosawa films Rashomon (1950), Ikiru (1952), Seven Samurai (1954) and Throne of Blood (1957).[98][99]
 During Japanese cinema's 'Golden Age' of the 1950s, successful films included Rashomon (1950), Seven Samurai (1954) and The Hidden Fortress (1958) by Akira Kurosawa, as well as Yasujirō Ozu's Tokyo Story (1953) and Ishirō Honda's Godzilla (1954).[100] These films have had a profound influence on world cinema. In particular, Kurosawa's Seven Samurai has been remade several times as Western films, such as The Magnificent Seven (1960) and Battle Beyond the Stars (1980), and has also inspired several Bollywood films, such as Sholay (1975) and China Gate (1998). Rashomon was also remade as The Outrage (1964), and inspired films with ""Rashomon effect"" storytelling methods, such as Andha Naal (1954), The Usual Suspects (1995) and Hero (2002). The Hidden Fortress was also an inspiration behind George Lucas' Star Wars (1977). Other famous Japanese filmmakers from this period include Kenji Mizoguchi, Mikio Naruse, Hiroshi Inagaki and Nagisa Oshima.[98] Japanese cinema later became one of the main inspirations behind the New Hollywood movement of the 1960s to 1980s.
 During Indian cinema's 'Golden Age' of the 1950s, it was producing 200 films annually, while Indian independent films gained greater recognition through international film festivals. One of the most famous was The Apu Trilogy (1955–1959) from critically acclaimed Bengali film director Satyajit Ray, whose films had a profound influence on world cinema, with directors such as Akira Kurosawa,[101] Martin Scorsese,[102][103] James Ivory,[104] Abbas Kiarostami, Elia Kazan, François Truffaut,[105] Steven Spielberg,[106][107] Carlos Saura,[108] Jean-Luc Godard,[109] Isao Takahata,[110] Gregory Nava, Ira Sachs, Wes Anderson[111] and Danny Boyle[112] being influenced by his cinematic style. According to Michael Sragow of The Atlantic Monthly, the ""youthful coming-of-age dramas that have flooded art houses since the mid-fifties owe a tremendous debt to the Apu trilogy"".[113] Subrata Mitra's cinematographic technique of bounce lighting also originates from The Apu Trilogy.[114] Other famous Indian filmmakers from this period include Guru Dutt,[98] Ritwik Ghatak,[99] Mrinal Sen, Raj Kapoor, Bimal Roy, K. Asif and Mehboob Khan.[115]
 The cinema of South Korea also experienced a 'Golden Age' in the 1950s, beginning with director Lee Kyu-hwan's tremendously successful remake of Chunhyang-jon (1955).[116] That year also saw the release of Yangsan Province by the renowned director, Kim Ki-young, marking the beginning of his productive career. Both the quality and quantity of filmmaking had increased rapidly by the end of the 1950s. South Korean films, such as Lee Byeong-il's 1956 comedy Sijibganeun nal (The Wedding Day), had begun winning international awards. In contrast to the beginning of the 1950s, when only 5 films were made per year, 111 films were produced in South Korea in 1959.[117]
 The 1950s was also a 'Golden Age' for Philippine cinema, with the emergence of more artistic and mature films, and significant improvement in cinematic techniques among filmmakers. The studio system produced frenetic activity in the local film industry as many films were made annually and several local talents started to earn recognition abroad. The premiere Philippine directors of the era included Gerardo de Leon, Gregorio Fernández, Eddie Romero, Lamberto Avellana, and Cirio Santiago.[118][119]
 During the 1960s, the studio system in Hollywood declined, because many films were now being made on location in other countries, or using studio facilities abroad, such as Pinewood in the UK and Cinecittà in Rome. ""Hollywood"" films were still largely aimed at family audiences, and it was often the more old-fashioned films that produced the studios' biggest successes. Productions like Mary Poppins (1964), My Fair Lady (1964) and The Sound of Music (1965) were among the biggest money-makers of the decade. The growth in independent producers and production companies, and the increase in the power of individual actors also contributed to the decline of traditional Hollywood studio production.
 There was also an increasing awareness of foreign language cinema in America during this period. During the late 1950s and 1960s, the French New Wave directors such as François Truffaut and Jean-Luc Godard produced films such as Les quatre cents coups, Breathless and Jules et Jim which broke the rules of Hollywood cinema's narrative structure. As well, audiences were becoming aware of Italian films like Federico Fellini's La Dolce Vita (1960), 8½ (1963) and the stark dramas of Sweden's Ingmar Bergman.
 In Britain, the ""Free Cinema"" of Lindsay Anderson, Tony Richardson and others lead to a group of realistic and innovative dramas including Saturday Night and Sunday Morning, A Kind of Loving and This Sporting Life. Other British films such as Repulsion, Darling, Alfie, Blowup and Georgy Girl (all in 1965–1966) helped to reduce prohibitions of sex and nudity on screen, while the casual sex and violence of the James Bond films, beginning with Dr. No in 1962 would render the series popular worldwide.
 During the 1960s, Ousmane Sembène produced several French- and Wolof-language films and became the ""father"" of African Cinema. In Latin America, the dominance of the ""Hollywood"" model was challenged by many film makers. Fernando Solanas and Octavio Getino called for a politically engaged Third Cinema in contrast to Hollywood and the European auteur cinema.
 In Egypt, the golden age of Egyptian cinema continued in the 1960s at the hands of many directors, and Egyptian cinema greatly appreciated women at that time, such as Soad Hosny. The Zulfikar brothers; Ezz El-Dine Zulfikar, Salah Zulfikar and Mahmoud Zulfikar were on a date with many productions,[120] including Ezz El Dine Zulfikar's The River of Love (1960),[121] Mahmoud Zulfikar's Soft Hands (1964), and Dearer Than My Life (1965) starring Salah Zulfikar and Salah Zulfikar Films production; My Wife, the Director General (1966)[122] as well as Youssef Chahine's Saladin (1963).[123][124]
 Further, the nuclear paranoia of the age, and the threat of an apocalyptic nuclear exchange (like the 1962 close-call with the USSR during the Cuban Missile Crisis) prompted a reaction within the film community as well. Films like Stanley Kubrick's Dr. Strangelove and Fail Safe with Henry Fonda were produced in a Hollywood that was once known for its overt patriotism and wartime propaganda.
 In documentary film the sixties saw the blossoming of Direct Cinema, an observational style of film making as well as the advent of more overtly partisan films like In the Year of the Pig about the Vietnam War by Emile de Antonio. By the late 1960s however, Hollywood filmmakers were beginning to create more innovative and ground-breaking films that reflected the social revolution taken over much of the western world such as Bonnie and Clyde (1967), The Graduate (1967), 2001: A Space Odyssey (1968), Rosemary's Baby (1968), Midnight Cowboy (1969), Easy Rider (1969) and The Wild Bunch (1969). Bonnie and Clyde is often considered the beginning of the so-called New Hollywood.
 In Japanese cinema, Academy Award-winning director Akira Kurosawa produced Yojimbo (1961), which like his previous films also had a profound influence around the world. The influence of this film is most apparent in Sergio Leone's A Fistful of Dollars (1964) and Walter Hill's Last Man Standing (1996). Yojimbo was also the origin of the ""Man with No Name"" trend.
 The New Hollywood was the period following the decline of the studio system during the 1950s and 1960s and the end of the production code, (which was replaced in 1968 by the MPAA film rating system). During the 1970s, filmmakers increasingly depicted explicit sexual content and showed gunfight and battle scenes that included graphic images of bloody deaths –  a notable example of this is Wes Craven's The Last House on the Left (1972).
 Post-classical cinema is the changing methods of storytelling of the New Hollywood producers. The new methods of drama and characterization played upon audience expectations acquired during the classical/Golden Age period: story chronology may be scrambled, storylines may feature unsettling ""twist endings"", main characters may behave in a morally ambiguous fashion, and the lines between the antagonist and protagonist may be blurred. The beginnings of post-classical storytelling may be seen in 1940s and 1950s film noir films, in films such as Rebel Without a Cause (1955), and in Hitchcock's Psycho. 1971 marked the release of controversial films like Straw Dogs, A Clockwork Orange, The French Connection and Dirty Harry. This sparked heated controversy over the perceived escalation of violence in cinema.
 During the 1970s, a new group of American filmmakers emerged, such as Martin Scorsese, Francis Ford Coppola, George Lucas, Woody Allen, Terrence Malick, and Robert Altman. This coincided with the increasing popularity of the auteur theory in film literature and the media, which posited that a film director's films express their personal vision and creative insights. The development of the auteur style of filmmaking helped to give these directors far greater control over their projects than would have been possible in earlier eras. This led to some great critical and commercial successes, like Scorsese's Taxi Driver, Coppola's The Godfather films, William Friedkin's The Exorcist, Altman's Nashville, Allen's Annie Hall and Manhattan, Malick's Badlands and Days of Heaven, and Polish immigrant Roman Polanski's Chinatown. It also, however, resulted in some failures, including Peter Bogdanovich's At Long Last Love and Michael Cimino's hugely expensive Western epic Heaven's Gate, which helped to bring about the demise of its backer, United Artists.
 The financial disaster of Heaven's Gate marked the end of the visionary ""auteur"" directors of the ""New Hollywood"", who had unrestrained creative and financial freedom to develop films. The phenomenal success in the 1970s of Spielberg's Jaws originated the concept of the modern ""blockbuster"". However, the enormous success of George Lucas' 1977 film Star Wars led to much more than just the popularization of blockbuster filmmaking. The film's revolutionary use of special effects, sound editing and music had led it to become widely regarded as one of the single most important films in the medium's history, as well as the most influential film of the 1970s. Hollywood studios increasingly focused on producing a smaller number of very large budget films with massive marketing and promotional campaigns. This trend had already been foreshadowed by the commercial success of disaster films such as The Poseidon Adventure and The Towering Inferno.
 During the mid-1970s, more pornographic theatres, euphemistically called ""adult cinemas"", were established, and the legal production of hardcore pornographic films began. Porn films such as Deep Throat and its star Linda Lovelace became something of a popular culture phenomenon and resulted in a spate of similar sex films. The porn cinemas finally died out during the 1980s, when the popularization of the home VCR and pornography videotapes allowed audiences to watch sex films at home. In the early 1970s, English-language audiences became more aware of the new West German cinema, with Werner Herzog, Rainer Werner Fassbinder and Wim Wenders among its leading exponents.
 In world cinema, the 1970s saw a dramatic increase in the popularity of martial arts films, largely due to its reinvention by Bruce Lee, who departed from the artistic style of traditional Chinese martial arts films and added a much greater sense of realism to them with his Jeet Kune Do style. This began with The Big Boss (1971), which was a major success across Asia. However, he did not gain fame in the Western world until shortly after his death in 1973, when Enter the Dragon was released. The film went on to become the most successful martial arts film in cinematic history, popularized the martial arts film genre across the world, and cemented Bruce Lee's status as a cultural icon. Hong Kong action cinema, however, was in decline due to a wave of ""Bruceploitation"" films. This trend eventually came to an end in 1978 with the martial arts comedy films, Snake in the Eagle's Shadow and Drunken Master, directed by Yuen Woo-ping and starring Jackie Chan, laying the foundations for the rise of Hong Kong action cinema in the 1980s.
 While the musical film genre had declined in Hollywood by this time, musical films were quickly gaining popularity in the cinema of India, where the term ""Bollywood"" was coined for the growing Hindi film industry in Bombay (now Mumbai) that ended up dominating South Asian cinema, overtaking the more critically acclaimed Bengali film industry in popularity. Hindi filmmakers combined the Hollywood musical formula with the conventions of ancient Indian theatre to create a new film genre called ""Masala"", which dominated Indian cinema throughout the late 20th century.[125] These ""Masala"" films portrayed action, comedy, drama, romance and melodrama all at once, with ""filmi"" song and dance routines thrown in. This trend began with films directed by Manmohan Desai and starring Amitabh Bachchan, who remains one of the most popular film stars in South Asia. The most popular Indian film of all time was Sholay (1975), a ""Masala"" film inspired by a real-life dacoit as well as Kurosawa's Seven Samurai and the Spaghetti Westerns.
 The end of the decade saw the first major international marketing of Australian cinema, as Peter Weir's films Picnic at Hanging Rock and The Last Wave and Fred Schepisi's The Chant of Jimmie Blacksmith gained critical acclaim. In 1979, Australian filmmaker George Miller also garnered international attention for his violent, low-budget action film Mad Max.
 During the 1980s, audiences began increasingly watching films on their home VCRs. In the early part of that decade, the film studios tried legal action to ban home ownership of VCRs as a violation of copyright, which proved unsuccessful. Eventually, the sale and rental of films on home video became a significant ""second venue"" for exhibition of films, and an additional source of revenue for the film industries. Direct-to-video (niche) markets usually offered lower quality, cheap productions that were not deemed very suitable for the general audiences of television and theatrical releases.
 The Lucas–Spielberg combine would dominate ""Hollywood"" cinema for much of the 1980s, and lead to much imitation. Two follow-ups to Star Wars, three to Jaws, and three Indiana Jones films helped to make sequels of successful films more of an expectation than ever before. Lucas also launched THX Ltd, a division of Lucasfilm in 1982,[126] while Spielberg enjoyed one of the decade's greatest successes in E.T. the Extra-Terrestrial the same year. 1982 also saw the release of Disney's Tron which was one of the first films from a major studio to use computer graphics extensively. American independent cinema struggled more during the decade, although Martin Scorsese's Raging Bull (1980), After Hours (1985), and The King of Comedy (1983) helped to establish him as one of the most critically acclaimed American film makers of the era. Also during 1983 Scarface was released, which was very profitable and resulted in even greater fame for its leading actor Al Pacino. Tim Burton's 1989 version of Bob Kane's creation, Batman, saw Jack Nicholson's portrayal of the demented Joker, which earned him $60-$90m after including his percentage of the gross.[127]
 British cinema was given a boost during the early 1980s by the arrival of David Puttnam's company Goldcrest Films. The films Chariots of Fire, Gandhi, The Killing Fields and A Room with a View appealed to a ""middlebrow"" audience which was increasingly being ignored by the major Hollywood studios. While the films of the 1970s had helped to define modern blockbuster motion pictures, the way ""Hollywood"" released its films would now change. Films, for the most part, would premiere in a wider number of theatres, although, to this day, some films still premiere using the route of the limited/roadshow release system. Against some expectations, the rise of the multiplex cinema did not allow less mainstream films to be shown, but simply allowed the major blockbusters to be given an even greater number of screenings. However, films that had been overlooked in cinemas were increasingly being given a second chance on home video.
 During the 1980s, Japanese cinema experienced a revival, largely due to the success of anime films. At the beginning of the 1980s, Space Battleship Yamato (1973) and Mobile Suit Gundam (1979), both of which were unsuccessful as television series, were remade as films and became hugely successful in Japan. In particular, Mobile Suit Gundam sparked the Gundam franchise of Real Robot mecha anime. The success of Macross: Do You Remember Love? also sparked a Macross franchise of mecha anime. This was also the decade when Studio Ghibli was founded. The studio produced Hayao Miyazaki's first fantasy films, Nausicaä of the Valley of the Wind (1984) and Castle in the Sky (1986), as well as Isao Takahata's Grave of the Fireflies (1988), all of which were very successful in Japan and received worldwide critical acclaim. Original video animation (OVA) films also began during this decade; the most influential of these early OVA films was Noboru Ishiguro's cyberpunk film Megazone 23 (1985). The most famous anime film of this decade was Katsuhiro Otomo's cyberpunk film Akira (1988), which although initially unsuccessful at Japanese theaters, went on to become an international success.
 Hong Kong action cinema, which was in a state of decline due to endless Bruceploitation films after the death of Bruce Lee, also experienced a revival in the 1980s, largely due to the reinvention of the action film genre by Jackie Chan. He had previously combined the comedy film and martial arts film genres successfully in the 1978 films Snake in the Eagle's Shadow and Drunken Master. The next step he took was in combining this comedy martial arts genre with a new emphasis on elaborate and highly dangerous stunts, reminiscent of the silent film era. The first film in this new style of action cinema was Project A (1983), which saw the formation of the Jackie Chan Stunt Team as well as the ""Three Brothers"" (Chan, Sammo Hung and Yuen Biao). The film added elaborate, dangerous stunts to the fights and slapstick humor, and became a huge success throughout the Far East. As a result, Chan continued this trend with martial arts action films containing even more elaborate and dangerous stunts, including Wheels on Meals (1984), Police Story (1985), Armour of God (1986), Project A Part II (1987), Police Story 2 (1988), and Dragons Forever (1988). Other new trends which began in the 1980s were the ""girls with guns"" subgenre, for which Michelle Yeoh gained fame; and especially the ""heroic bloodshed"" genre, revolving around Triads, largely pioneered by John Woo and for which Chow Yun-fat became famous. These Hong Kong action trends were later adopted by many Hollywood action films in the 1990s and 2000s.
 The early 1990s saw the development of a commercially successful independent cinema in the United States. Although cinema was increasingly dominated by special-effects films such as Terminator 2: Judgment Day (1991), Jurassic Park (1993) and Titanic (1997), the latter of which became the highest-grossing film of all time at the time up until Avatar (2009), also directed by James Cameron, independent films like Steven Soderbergh's Sex, Lies, and Videotape (1989) and Quentin Tarantino's Reservoir Dogs (1992) had significant commercial success both at the cinema and on home video.
 Filmmakers associated with the Danish film movement Dogme 95 introduced a manifesto aimed to purify filmmaking. Its first few films gained worldwide critical acclaim, after which the movement slowly faded out.
 Scorsese's Goodfellas was released in 1990. It is considered by many as one of the greatest movies to be made, particularly in the gangster genre. It is said to be the highest point of Scorsese's career.
 Major American studios began to create their own ""independent"" production companies to finance and produce non-mainstream fare. One of the most successful independents of the 1990s, Miramax Films, was bought by Disney the year before the release of Tarantino's runaway hit Pulp Fiction in 1994. The same year marked the beginning of film and video distribution online. Animated films aimed at family audiences also regained their popularity, with Disney's Beauty and the Beast (1991), Aladdin (1992), and The Lion King (1994). During 1995, the first feature-length computer-animated feature, Toy Story, was produced by Pixar Animation Studios and released by Disney. After the success of Toy Story, computer animation would grow to become the dominant technique for feature-length animation, which would allow competing film companies such as DreamWorks, 20th Century Fox and Warner Bros. to effectively compete with Disney with successful films of their own. During the late 1990s, another cinematic transition began, from physical film stock to digital cinema technology. Meanwhile, DVDs became the new standard for consumer video, replacing VHS tapes.
 Since the late 2000s streaming media platforms like YouTube provided means for anyone with access to internet and cameras (a standard feature of smartphones) to publish videos to the world. Also competing with the increasing popularity of video games and other forms of home entertainment, the industry once again started to make theatrical releases more attractive, with new 3D technologies and epic (fantasy and superhero) films becoming a mainstay in cinemas.
 The documentary film also rose as a commercial genre for perhaps the first time, with the success of films such as March of the Penguins and Michael Moore's Bowling for Columbine and Fahrenheit 9/11. A new genre was created with Martin Kunert and Eric Manes' Voices of Iraq, when 150 inexpensive DV cameras were distributed across Iraq, transforming ordinary people into collaborative filmmakers. The success of Gladiator led to a revival of interest in epic cinema, and Moulin Rouge! renewed interest in musical cinema. Home theatre systems became increasingly sophisticated, as did some of the special edition DVDs designed to be shown on them. The Lord of the Rings trilogy was released on DVD in both the theatrical version and in a special extended version intended only for home cinema audiences.
 In 2001, the Harry Potter film series began, and by its end in 2011, it had become the highest-grossing film franchise of all time until the Marvel Cinematic Universe passed it in 2015.
 Due to advances in film projection technology, feature films were now able to be released simultaneously to IMAX cinema, the first was in 2002's Disney animation Treasure Planet; and the first live action was in 2003's The Matrix Revolutions and a re-release of The Matrix Reloaded. Later in the decade, The Dark Knight was the first major feature film to have been at least partially shot in IMAX technology.
 There has been an increasing globalization of cinema during this decade, with foreign-language films gaining popularity in English-speaking markets. Examples of such films include Crouching Tiger, Hidden Dragon (Mandarin), Amélie (French), Lagaan (Hindi), Spirited Away (Japanese), City of God (Brazilian Portuguese), The Passion of the Christ (Aramaic), Apocalypto (Mayan) and Inglourious Basterds (multiple European languages). Italy is the most awarded country at the Academy Award for Best Foreign Language Film, with 14 awards won, 3 Special Awards and 31 nominations.
 In 2003, there was a revival in 3D film popularity the first being James Cameron's Ghosts of the Abyss which was released as the first full-length 3-D IMAX feature filmed with the Reality Camera System. This camera system used the latest HD video cameras, not film, and was built for Cameron by Emmy nominated Director of Photography Vince Pace, to his specifications. The same camera system was used to film Spy Kids 3D: Game Over (2003), Aliens of the Deep IMAX (2005), and The Adventures of Sharkboy and Lavagirl in 3-D (2005).
 After James Cameron's 3D film Avatar became the highest-grossing film of all time, 3D films gained brief popularity with many other films being released in 3D, with the best critical and financial successes being in the field of feature film animation such as Universal Pictures/Illumination Entertainment's Despicable Me and DreamWorks Animation's How To Train Your Dragon, Shrek Forever After and Megamind. Avatar is also note-worthy for pioneering highly sophisticated use of motion capture technology and influencing several other films such as Rise of the Planet of the Apes.[128]
 In 2011, the largest film industries by number of feature films produced were those of India, the United States, China, Nigeria, and Japan.[129] In Hollywood, superhero films greatly increased in popularity and financial success, with films based on Marvel and DC Comics released every year.[130] The superhero genre was the most dominant in American box office receipts.
 The list of top-grossing films was dominated by Disney, with 2019 having the most films in the top 50. The 2019 superhero film Avengers: Endgame was the most successful movie of all-time at the box office. Other top earners included Star Wars: The Force Awakens, Avengers: Infinity War, and Jurassic World. Disney releases were frequently the top-grossing films annually in the latter half of the decade, with titles like Toy Story 3, The Avengers, and Frozen. Disney's success culminated in the acquisition of 21st Century Fox by Disney.
 Major film studios tried to emulate Disney's Marvel Cinematic Universe's success with their own franchises. Warner Bros. created franchises like DC Extended Universe. Disney produced live-action or photorealistic remakes of its classic animated films, such as Aladdin, and The Lion King. Film series based on young adult novels became popular, shifting from fantasy to dystopian sci-fi. Notable series included The Hunger Games.
 The Tale of Princess Kaguya, Summer 1993, Leave No Trace and Minding the Gap achieved 100% ratings on Rotten Tomatoes. Other acclaimed films included Mad Max: Fury Road, The Social Network, and Get Out. Films like Portrait of a Lady on Fire, The Tree of Life, Moonlight, and Parasite were frequently listed in critics' polls for the best films of the 2010s. In 2010, the first woman to win the Best Director Award in Oscar history appeared. Katherine Bigelow's The Hurt Locker won six awards.[131] In 2020, Parasite became the first non-English-language film to win the Academy Award for Best Picture.
 The COVID-19 pandemic resulted in the closure of film theaters around the world in response to lockdowns. Many films slated to release in the early 2020s faced delays in development, production, and distribution, with others released on streaming services with little or no theatrical window. The era witnessed a profound transformation in how films are produced, distributed, and consumed globally. The pandemic led to a rapid acceleration in the shift towards streaming, as a primary means of film distribution. The film industry adapted and produced notable works that reflected changing dynamics of the era.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['musical cinema', 'George Albert Smith and James Williamson', ""one of the single most important films in the medium's history"", 'newsreels and actualities', 'elaborate and highly dangerous stunts'], 'answer_start': [], 'answer_end': []}"
"
 A documentary film or documentary is a non-fictional motion picture intended to ""document reality, primarily for instruction, education or maintaining a historical record"".[1] Bill Nichols has characterized the documentary in terms of ""a filmmaking practice, a cinematic tradition, and mode of audience reception [that remains] a practice without clear boundaries"".[2]
 Early documentary films, originally called ""actuality films"", briefly lasted for one minute or less. Over time, documentaries have evolved to become longer in length and to include more categories. Some examples are educational, observational and docufiction. Documentaries are very informative, and are often used within schools as a resource to teach various principles. Documentary filmmakers have a responsibility to be truthful to their vision of the world without intentionally misrepresenting a topic.
 Social media platforms (such as YouTube) have provided an avenue for the growth of the documentary-film genre. These platforms have increased the distribution area and ease-of-accessibility.
 Polish writer and filmmaker Bolesław Matuszewski was among those who identified the mode of documentary film. He wrote two of the earliest texts on cinema, Une nouvelle source de l'histoire (""A New Source of History"") and La photographie animée (""Animated photography""). Both were published in 1898 in French and were among the earliest written works to consider the historical and documentary value of the film.[3] Matuszewski is also among the first filmmakers to propose the creation of a Film Archive to collect and keep safe visual materials.[4]
 The word ""documentary"" was coined by Scottish documentary filmmaker John Grierson in his review of Robert Flaherty's film Moana (1926), published in the New York Sun on 8 February 1926, written by ""The Moviegoer"" (a pen name for Grierson).[5][6]
 Grierson's principles of documentary were that cinema's potential for observing life could be exploited in a new art form; that the ""original"" actor and ""original"" scene are better guides than their fiction counterparts for interpreting the modern world; and that materials ""thus taken from the raw"" can be more real than the acted article. In this regard, Grierson's definition of documentary as ""creative treatment of actuality""[7] has gained some acceptance; however, this position is at variance with Soviet film-maker Dziga Vertov's credos of provocation to present ""life as it is"" (that is, life filmed surreptitiously), and ""life caught unawares"" (life provoked or surprised by the camera).
 The American film critic Pare Lorentz defines a documentary film as ""a factual film which is dramatic.""[8] Others further state that a documentary stands out from the other types of non-fiction films for providing an opinion, and a specific message, along with the facts it presents.[9] Scholar Betsy McLane asserted that documentaries are for filmmakers to convey their views about historical events, people, and places which they find significant.[10] Therefore, the advantage of documentaries lies in introducing new perspectives which may not be prevalent in traditional media such as written publications and school curricula.[11]
 Documentary practice is the complex process of creating documentary projects. It refers to what people do with media devices, content, form, and production strategies to address the creative, ethical, and conceptual problems and choices that arise as they make documentaries.
 Documentary filmmaking can be used as a form of journalism, advocacy, or personal expression.
 Early film (pre-1900) was dominated by the novelty of showing an event. Single-shot moments were captured on film, such as a train entering a station, a boat docking, or factory workers leaving work. These short films were called ""actuality"" films; the term ""documentary"" was not coined until 1926. Many of the first films, such as those made by Auguste and Louis Lumière, were a minute or less in length, due to technological limitations. Examples can be viewed on YouTube.
 Films showing many people (for example, leaving a factory) were often made for commercial reasons: the people being filmed were eager to see, for payment, the film showing them. One notable film clocked in at over an hour and a half, The Corbett-Fitzsimmons Fight. Using pioneering film-looping technology, Enoch J. Rector presented the entirety of a famous 1897 prize-fight on cinema screens across the United States.
 In May 1896, Bolesław Matuszewski recorded on film a few surgical operations in Warsaw and Saint Petersburg hospitals. In 1898, French surgeon Eugène-Louis Doyen invited Matuszewski and Clément Maurice to record his surgical operations. They started in Paris a series of surgical films sometime before July 1898.[12] Until 1906, the year of his last film, Doyen recorded more than 60 operations. Doyen said that his first films taught him how to correct professional errors he had been unaware of. For scientific purposes, after 1906, Doyen combined 15 of his films into three compilations, two of which survive, the six-film series Extirpation des tumeurs encapsulées (1906), and the four-film Les Opérations sur la cavité crânienne (1911). These and five other of Doyen's films survive.[13]
 Between July 1898 and 1901, the Romanian professor Gheorghe Marinescu made several science films in his neurology clinic in Bucharest:[14] Walking Troubles of Organic Hemiplegy (1898), The Walking Troubles of Organic Paraplegies (1899), A Case of Hysteric Hemiplegy Healed Through Hypnosis (1899), The Walking Troubles of Progressive Locomotion Ataxy (1900), and Illnesses of the Muscles (1901). All these short films have been preserved. The professor called his works ""studies with the help of the cinematograph,"" and published the results, along with several consecutive frames, in issues of La Semaine Médicale magazine from Paris, between 1899 and 1902.[15] In 1924, Auguste Lumière recognized the merits of Marinescu's science films: ""I've seen your scientific reports about the usage of the cinematograph in studies of nervous illnesses, when I was still receiving La Semaine Médicale, but back then I had other concerns, which left me no spare time to begin biological studies. I must say I forgot those works and I am thankful to you that you reminded them to me. Unfortunately, not many scientists have followed your way.""[16][17][18]
 Travelogue films were very popular in the early part of the 20th century. They were often referred to by distributors as ""scenics"". Scenics were among the most popular sort of films at the time.[19] An important early film which moved beyond the concept of the scenic was In the Land of the Head Hunters (1914), which embraced primitivism and exoticism in a staged story presented as truthful re-enactments of the life of Native Americans.
 Contemplation is a separate area.[further explanation needed] Pathé was the best-known global manufacturer of such films in the early 20th century. A vivid example is Moscow Clad in Snow (1909).
 Biographical documentaries appeared during this time, such as the feature Eminescu-Veronica-Creangă (1914) on the relationship between the writers Mihai Eminescu, Veronica Micle and Ion Creangă (all deceased at the time of the production), released by the Bucharest chapter of Pathé.
 Early color motion picture processes such as Kinemacolor (known for the feature With Our King and Queen Through India (1912)) and Prizma Color (known for Everywhere With Prizma (1919) and the five-reel feature Bali the Unknown (1921)) used travelogues to promote the new color processes. In contrast, Technicolor concentrated primarily on getting their process adopted by Hollywood studios for fiction feature films.
 Also during this period, Frank Hurley's feature documentary film, South (1919) about the Imperial Trans-Antarctic Expedition was released. The film documented the failed Antarctic expedition led by Ernest Shackleton in 1914.
 With Robert J. Flaherty's Nanook of the North in 1922, documentary film embraced romanticism. Flaherty filmed a number of heavily staged romantic documentary films during this time period, often showing how his subjects would have lived 100 years earlier and not how they lived right then. For instance, in Nanook of the North, Flaherty did not allow his subjects to shoot a walrus with a nearby shotgun, but had them use a harpoon instead. Some of Flaherty's staging, such as building a roofless igloo for interior shots, was done to accommodate the filming technology of the time.
 Paramount Pictures tried to repeat the success of Flaherty's Nanook and Moana with two romanticized documentaries, Grass (1925) and Chang (1927), both directed by Merian C. Cooper and Ernest Schoedsack.
 The city-symphony sub film genre consisted of avant-garde films during the 1920s and 1930s. These films were particularly influenced by modern art, namely Cubism, Constructivism, and Impressionism.[20] According to art historian and author Scott Macdonald,[21] city-symphony films can be described as, ""An intersection between documentary and avant-garde film: an avant-doc""; however, A.L. Rees suggests regarding them as avant-garde films.[20]
 Early titles produced within this genre include: Manhatta (New York; dir. Paul Strand, 1921); Rien que les heures/Nothing But The Hours (France; dir. Alberto Cavalcanti, 1926); Twenty Four Dollar Island (dir. Robert J. Flaherty, 1927); Moscow (dir. Mikhail Kaufman, 1927); Études sur Paris (dir. André Sauvage, 1928); The Bridge (1928) and Rain (1929), both by Joris Ivens; São Paulo, Sinfonia da Metrópole (dir. Adalberto Kemeny, 1929), Berlin: Symphony of a Metropolis (dir. Walter Ruttmann, 1927); Man with a Movie Camera (dir. Dziga Vertov, 1929); Douro, Faina Fluvial (dir. Manoel de Oliveira, 1931); and Rhapsody in Two Languages (dir. Gordon Sparling, 1934).
 A city-symphony film, as the name suggests, is most often based around a major metropolitan city area and seeks to capture the life, events and activities of the city. It can use abstract cinematography (Walter Ruttman's Berlin) or may use Soviet montage theory (Dziga Vertov's, Man with a Movie Camera).  Most importantly, a city-symphony film is a form of cinepoetry, shot and edited in the style of a ""symphony"".
 The European continental tradition (See: Realism) focused on humans within human-made environments, and included the so-called ""city-symphony"" films such as Walter Ruttmann's, Berlin: Symphony of a Metropolis (of which Grierson noted in an article[22] that Berlin, represented what a documentary should not be); Alberto Cavalcanti's, Rien que les heures; and Dziga Vertov's Man with a Movie Camera. These films tend to feature people as products of their environment, and lean towards the avant-garde.
 Dziga Vertov was central to the Soviet Kino-Pravda (literally, ""cinematic truth"") newsreel series of the 1920s. Vertov believed the camera – with its varied lenses, shot-counter shot editing, time-lapse, ability to slow motion, stop motion and fast-motion – could render reality more accurately than the human eye, and created a film philosophy from it.
 The newsreel tradition is important in documentary film. Newsreels at this time were sometimes staged but were usually re-enactments of events that had already happened, not attempts to steer events as they were in the process of happening. For instance, much of the battle footage from the early 20th century was staged; the cameramen would usually arrive on site after a major battle and re-enact scenes to film them.
 The propagandist tradition consists of films made with the explicit purpose of persuading an audience of a point. One of the most celebrated and controversial propaganda films is Leni Riefenstahl's film Triumph of the Will (1935), which chronicled the 1934 Nazi Party Congress and was commissioned by Adolf Hitler. Leftist filmmakers Joris Ivens and Henri Storck directed Borinage (1931) about the Belgian coal mining region. Luis Buñuel directed a ""surrealist"" documentary Las Hurdes (1933).
 Pare Lorentz's The Plow That Broke the Plains (1936) and The River (1938) and Willard Van Dyke's The City (1939) are notable New Deal productions, each presenting complex combinations of social and ecological awareness, government propaganda, and leftist viewpoints. Frank Capra's Why We Fight (1942–1944) series was a newsreel series in the United States, commissioned by the government to convince the U.S. public that it was time to go to war. Constance Bennett and her husband Henri de la Falaise produced two feature-length documentaries, Legong: Dance of the Virgins (1935) filmed in Bali, and Kilou the Killer Tiger (1936) filmed in Indochina.
 In Canada, the Film Board, set up by John Grierson, was set up for the same propaganda reasons. It also created newsreels that were seen by their national governments as legitimate counter-propaganda to the psychological warfare of Nazi Germany orchestrated by Joseph Goebbels.
 In Britain, a number of different filmmakers came together under John Grierson. They became known as the Documentary Film Movement. Grierson, Alberto Cavalcanti, Harry Watt, Basil Wright, and Humphrey Jennings amongst others succeeded in blending propaganda, information, and education with a more poetic aesthetic approach to documentary. Examples of their work include Drifters (John Grierson), Song of Ceylon (Basil Wright), Fires Were Started, and A Diary for Timothy (Humphrey Jennings). Their work involved poets such as W. H. Auden, composers such as Benjamin Britten, and writers such as J. B. Priestley. Among the best known films of the movement are Night Mail and Coal Face.
 Calling Mr. Smith (1943) is an anti-Nazi color film[23][24][25] created by Stefan Themerson which is both a documentary and an avant-garde film against war. It was one of the first anti-Nazi films in history.[citation needed]
 Cinéma vérité (or the closely related direct cinema) was dependent on some technical advances to exist: light, quiet and reliable cameras, and portable sync sound.
 Cinéma vérité and similar documentary traditions can thus be seen, in a broader perspective, as a reaction against studio-based film production constraints. Shooting on location, with smaller crews, would also happen in the French New Wave, the filmmakers taking advantage of advances in technology allowing smaller, handheld cameras and synchronized sound to film events on location as they unfolded.
 Although the terms are sometimes used interchangeably, there are important differences between cinéma vérité (Jean Rouch) and the North American ""direct cinema"" (or more accurately ""cinéma direct""), pioneered by, among others, Canadians Allan King, Michel Brault, and Pierre Perrault,[29] and Americans Robert Drew, Richard Leacock, Frederick Wiseman and Albert and David Maysles.
 The directors of the movement take different viewpoints on their degree of involvement with their subjects. Kopple and Pennebaker, for instance, choose non-involvement (or at least no overt involvement), and Perrault, Rouch, Koenig, and Kroitor favor direct involvement or even provocation when they deem it necessary.
 The films Chronicle of a Summer (Jean Rouch), Dont Look Back (D. A. Pennebaker), Grey Gardens (Albert and David Maysles), Titicut Follies (Frederick Wiseman), Primary and Crisis: Behind a Presidential Commitment (both produced by Robert Drew), Harlan County, USA (directed by Barbara Kopple), Lonely Boy (Wolf Koenig and Roman Kroitor) are all frequently deemed cinéma vérité films.
 The fundamentals of the style include following a person during a crisis with a moving, often handheld, camera to capture more personal reactions. There are no sit-down interviews, and the shooting ratio (the amount of film shot to the finished product) is very high, often reaching 80 to one. From there, editors find and sculpt the work into a film. The editors of the movement – such as Werner Nold, Charlotte Zwerin, Muffie Meyer, Susan Froemke, and Ellen Hovde – are often overlooked, but their input to the films was so vital that they were often given co-director credits.
 Famous cinéma vérité/direct cinema films include Les Raquetteurs,[30] Showman, Salesman, Near Death, and The Children Were Watching.
 In the 1960s and 1970s, documentary film was often regarded as a political weapon against neocolonialism and capitalism in general, especially in Latin America, but also in a changing society. La Hora de los hornos (The Hour of the Furnaces, from 1968), directed by Octavio Getino and Arnold Vincent Kudales Sr., influenced a whole generation of filmmakers. Among the many political documentaries produced in the early 1970s was ""Chile: A Special Report"", public television's first in-depth expository look at the September 1973 overthrow of the Salvador Allende government in Chile by military leaders under Augusto Pinochet, produced by documentarians Ari Martinez and José Garcia.
 A June 2020 article in The New York Times reviewed the political documentary And She Could Be Next, directed by Grace Lee and Marjan Safinia. The Times described the documentary not only as focusing on women in politics, but more specifically on women of color, their communities, and the significant changes they have wrought upon America.[31]
 Box office analysts have noted that the documentary film genre has become increasingly successful in theatrical release with films such as Fahrenheit 9/11, Super Size Me, Food, Inc., Earth, March of the Penguins, and An Inconvenient Truth among the most prominent examples. Compared to dramatic narrative films, documentaries typically have far lower budgets which makes them attractive to film companies because even a limited theatrical release can be highly profitable.
 The nature of documentary films has expanded in the past 30 years from the cinéma vérité style introduced in the 1960s in which the use of portable camera and sound equipment allowed an intimate relationship between filmmaker and subject. The line blurs between documentary and narrative and some works are very personal, such as Marlon Riggs's Tongues Untied (1989) and Black Is...Black Ain't (1995), which mix expressive, poetic, and rhetorical elements and stresses subjectivities rather than historical materials.[32]
 Historical documentaries, such as the landmark 14-hour Eyes on the Prize: America's Civil Rights Years (1986 – Part 1 and 1989 – Part 2) by Henry Hampton, 4 Little Girls (1997) by Spike Lee, The Civil War by Ken Burns, and UNESCO-awarded independent film on slavery 500 Years Later, express not only a distinctive voice but also a perspective and point of views. Some films such as The Thin Blue Line by Errol Morris incorporate stylized re-enactments, and Michael Moore's Roger & Me place far more interpretive control with the director. The commercial success of these documentaries may derive from this narrative shift in the documentary form, leading some critics to question whether such films can truly be called documentaries; critics sometimes refer to these works as ""mondo films"" or ""docu-ganda.""[33] However, directorial manipulation of documentary subjects has been noted since the work of Flaherty, and may be endemic to the form due to problematic ontological foundations.
 Documentary filmmakers are increasingly using social impact campaigns with their films.[34] Social impact campaigns seek to leverage media projects by converting public awareness of social issues and causes into engagement and action, largely by offering the audience a way to get involved.[35] Examples of such documentaries include Kony 2012, Salam Neighbor, Gasland, Living on One Dollar, and Girl Rising.
 Although documentaries are financially more viable with the increasing popularity of the genre and the advent of the DVD, funding for documentary film production remains elusive. Within the past decade, the largest exhibition opportunities have emerged from within the broadcast market, making filmmakers beholden to the tastes and influences of the broadcasters who have become their largest funding source.[36]
 Modern documentaries have some overlap with television forms, with the development of ""reality television"" that occasionally verges on the documentary but more often veers to the fictional or staged. The ""making-of"" documentary shows how a movie or a computer game was produced. Usually made for promotional purposes, it is closer to an advertisement than a classic documentary.
 Modern lightweight digital video cameras and computer-based editing have greatly aided documentary makers, as has the dramatic drop in equipment prices. The first film to take full advantage of this change was Martin Kunert and Eric Manes' Voices of Iraq, where 150 DV cameras were sent to Iraq during the war and passed out to Iraqis to record themselves.
 Films in the documentary form without words have been made. Listen to Britain, directed by Humphrey Jennings and Stuart McAllister in 1942, is a wordless meditation on wartime Britain. From 1982, the Qatsi trilogy and the similar Baraka could be described as visual tone poems, with music related to the images, but no spoken content. Koyaanisqatsi (part of the Qatsi trilogy) consists primarily of slow motion and time-lapse photography of cities and many natural landscapes across the United States. Baraka tries to capture the great pulse of humanity as it flocks and swarms in daily activity and religious ceremonies.
 Bodysong was made in 2003 and won a British Independent Film Award for ""Best British Documentary.""
 The 2004 film Genesis shows animal and plant life in states of expansion, decay, sex, and death, with some, but little, narration.
 The traditional style for narration is to have a dedicated narrator read a script which is dubbed onto the audio track. The narrator never appears on camera and may not necessarily have knowledge of the subject matter or involvement in the writing of the script.
 This style of narration uses title screens to visually narrate the documentary. The screens are held for about 5–10 seconds to allow adequate time for the viewer to read them. They are similar to the ones shown at the end of movies based on true stories, but they are shown throughout, typically between scenes.
 In this style, there is a host who appears on camera, conducts interviews, and who also does voice-overs.
 The release of The Thin Blue Line (1988) directed by Errol Morris introduced possibilities for emerging forms of the hybrid documentary. Indeed, it was disqualified for an Academy Award because of the stylized recreations. Traditional documentary filmmaking typically removes signs of fictionalization to distinguish itself from fictional film genres. Audiences have recently become more distrustful of the media's traditional fact production, making them more receptive to experimental ways of telling facts. The hybrid documentary implements truth games to challenge traditional fact production. Although it is fact-based, the hybrid documentary is not explicit about what should be understood, creating an open dialogue between subject and audience.[37] Clio Barnard's The Arbor (2010), Joshua Oppenheimer's The Act of Killing (2012), Mads Brügger's The Ambassador, and Alma Har'el's Bombay Beach (2011) are a few notable examples.[37]
 Docufiction is a hybrid genre from two basic ones, fiction film and documentary, practiced since the first documentary films were made.
 Fake-fiction is a genre which deliberately presents real, unscripted events in the form of a fiction film, making them appear as staged. The concept was introduced[38] by Pierre Bismuth to describe his 2016 film Where is Rocky II?
 A DVD documentary is a documentary film of indeterminate length that has been produced with the sole intent of releasing it for direct sale to the public on DVD, which is different from a documentary being made and released first on television or on a cinema screen (a.k.a. theatrical release) and subsequently on DVD for public consumption.
 This form of documentary release is becoming more popular and accepted as costs and difficulty with finding TV or theatrical release slots increases. It is also commonly used for more ""specialist"" documentaries, which might not have general interest to a wider TV audience. Examples are military, cultural arts, transport, sports, animals, etc.
 Compilation films were pioneered in 1927 by Esfir Schub with The Fall of the Romanov Dynasty. More recent examples include Point of Order! (1964), directed by Emile de Antonio about the McCarthy hearings. Similarly, The Last Cigarette combines the testimony of various tobacco company executives before the U.S. Congress with archival propaganda extolling the virtues of smoking.
 Poetic documentaries, which first appeared in the 1920s, were a sort of reaction against both the content and the rapidly crystallizing grammar of the early fiction film. The poetic mode moved away from continuity editing and instead organized images of the material world by means of associations and patterns, both in terms of time and space. Well-rounded characters – ""lifelike people"" – were absent; instead, people appeared in these films as entities, just like any other, that are found in the material world. The films were fragmentary, impressionistic, lyrical. Their disruption of the coherence of time and space – a coherence favored by the fiction films of the day – can also be seen as an element of the modernist counter-model of cinematic narrative. The ""real world"" – Nichols calls it the ""historical world"" – was broken up into fragments and aesthetically reconstituted using film form. Examples of this style include Joris Ivens' Rain (1928), which records a passing summer shower over Amsterdam; László Moholy-Nagy's Play of Light: Black, White, Grey (1930), in which he films one of his own kinetic sculptures, emphasizing not the sculpture itself but the play of light around it; Oskar Fischinger's abstract animated films; Francis Thompson's N.Y., N.Y. (1957), a city symphony film; and Chris Marker's Sans Soleil (1982).
 Expository documentaries speak directly to the viewer, often in the form of an authoritative commentary employing voiceover or titles, proposing a strong argument and point of view. These films are rhetorical, and try to persuade the viewer. (They may use a rich and sonorous male voice.) The (voice-of-God) commentary often sounds ""objective"" and omniscient. Images are often not paramount; they exist to advance the argument. The rhetoric insistently presses upon us to read the images in a certain fashion. Historical documentaries in this mode deliver an unproblematic and ""objective"" account and interpretation of past events.
 Examples: TV shows and films like Biography, America's Most Wanted, many science and nature documentaries, Ken Burns' The Civil War (1990), Robert Hughes' The Shock of the New (1980), John Berger's Ways of Seeing (1972), Frank Capra's wartime Why We Fight series, and Pare Lorentz's The Plow That Broke the Plains (1936).
 Observational documentaries attempt to spontaneously observe their subjects with minimal intervention. Filmmakers who worked in this subgenre often saw the poetic mode as too abstract and the expository mode as too didactic. The first observational docs date back to the 1960s; the technological developments which made them possible include mobile lightweight cameras and portable sound recording equipment for synchronized sound. Often, this mode of film eschewed voice-over commentary, post-synchronized dialogue and music, or re-enactments. The films aimed for immediacy, intimacy, and revelation of individual human character in ordinary life situations.
 Participatory documentaries believe that it is impossible for the act of filmmaking to not influence or alter the events being filmed. What these films do is emulate the approach of the anthropologist: participant-observation. Not only is the filmmaker part of the film, we also get a sense of how situations in the film are affected or altered by their presence. Nichols: ""The filmmaker steps out from behind the cloak of voice-over commentary, steps away from poetic meditation, steps down from a fly-on-the-wall perch, and becomes a social actor (almost) like any other. (Almost like any other because the filmmaker retains the camera, and with it, a certain degree of potential power and control over events.)"" The encounter between filmmaker and subject becomes a critical element of the film. Rouch and Morin named the approach cinéma vérité, translating Dziga Vertov's kinopravda into French; the ""truth"" refers to the truth of the encounter rather than some absolute truth.
 Reflexive documentaries do not see themselves as a transparent window on the world; instead, they draw attention to their own constructedness, and the fact that they are representations. How does the world get represented by documentary films? This question is central to this subgenre of films. They prompt us to ""question the authenticity of documentary in general."" It is the most self-conscious of all the modes, and is highly skeptical of ""realism"". It may use Brechtian alienation strategies to jar us, in order to ""defamiliarize"" what we are seeing and how we are seeing it.
 Performative documentaries stress subjective experience and emotional response to the world. They are strongly personal, unconventional, perhaps poetic and/or experimental, and might include hypothetical enactments of events designed to make us experience what it might be like for us to possess a certain specific perspective on the world that is not our own, e.g. that of black, gay men in Marlon Riggs's Tongues Untied (1989) or Jenny Livingston's Paris Is Burning (1991). This subgenre might also lend itself to certain groups (e.g. women, ethnic minorities, gays and lesbians, etc.) to ""speak about themselves"". Often, a battery of techniques, many borrowed from fiction or avant-garde films, are used. Performative docs often link up personal accounts or experiences with larger political or historical realities.
 Documentaries are shown in schools around the world in order to educate students. Used to introduce various topics to children, they are often used with a school lesson or shown many times to reinforce an idea.
 There are several challenges associated with translation of documentaries. The main two are working conditions and problems with terminology.
 Documentary translators very often have to meet tight deadlines. Normally, the translator has between five and seven days to hand over the translation of a 90-minute programme. Dubbing studios typically give translators a week to translate a documentary, but in order to earn a good salary, translators have to deliver their translations in a much shorter period, usually when the studio decides to deliver the final programme to the client sooner or when the broadcasting channel sets a tight deadline, e.g. on documentaries discussing the latest news.[39]
 Another problem is the lack of postproduction script or the poor quality of the transcription. A correct transcription is essential for a translator to do their work properly, however many times the script is not even given to the translator, which is a major impediment since documentaries are characterised by ""the abundance of terminological units and very specific proper names"".[40] When the script is given to the translator, it is usually poorly transcribed or outright incorrect making the translation unnecessarily difficult and demanding because all of the proper names and specific terminology have to be correct in a documentary programme in order for it to be a reliable source of information, hence the translator has to check every term on their own. Such mistakes in proper names are for instance: ""Jungle Reinhard instead of Django Reinhart, Jorn Asten instead of Jane Austen, and Magnus Axle instead of Aldous Huxley"".[40]
 The process of translation of a documentary programme requires working with very specific, often scientific terminology. Documentary translators are not usually specialists in a given field. Therefore, they are compelled to undertake extensive research whenever asked to make a translation of a specific documentary programme in order to understand it correctly and deliver the final product free of mistakes and inaccuracies. Generally, documentaries contain a large number of specific terms, with which translators have to familiarise themselves on their own, for example:
 The documentary Beetles, Record Breakers makes use of 15 different terms to refer to beetles in less than 30 minutes (longhorn beetle, cellar beetle, stag beetle, burying beetle or gravediggers, sexton beetle, tiger beetle, bloody nose beetle, tortoise beetle, diving beetle, devil's coach horse, weevil, click beetle, malachite beetle, oil beetle, cockchafer), apart from mentioning other animals such as horseshoe bats or meadow brown butterflies.[41]
 This poses a real challenge for the translators because they have to render the meaning, i.e. find an equivalent, of a very specific, scientific term in the target language and frequently the narrator uses a more general name instead of a specific term and the translator has to rely on the image presented in the programme to understand which term is being discussed in order to transpose it in the target language accordingly.[42] Additionally, translators of minorised languages often have to face another problem: some terms may not even exist in the target language. In such cases, they have to create new terminology or consult specialists to find proper solutions. Also, sometimes the official nomenclature differs from the terminology used by actual specialists, which leaves the translator to decide between using the official vocabulary that can be found in the dictionary, or rather opting for spontaneous expressions used by real experts in real life situations.[43]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['women in politics', 'Susan Froemke, and Ellen Hovde', 'primarily for instruction, education or maintaining a historical record', 'real, unscripted events', 'social and ecological awareness, government propaganda, and leftist viewpoints'], 'answer_start': [], 'answer_end': []}"
"
 Animation is a filmmaking technique by which still images are manipulated to create moving images. In traditional animation, images are drawn or painted by hand on transparent celluloid sheets (cels) to be photographed and exhibited on film. Animation has been recognized as an artistic medium, specifically within the entertainment industry. Many animations are computer animations made with computer-generated imagery (CGI). Stop motion animation, in particular claymation, has continued to exist alongside these other forms. 
 Animation is contrasted with live-action film, although the two do not exist in isolation. Many moviemakers have produced films that are a hybrid of the two. As CGI increasingly approximates photographic imagery, filmmakers can easily composite 3D animations into their film rather than using practical effects for showy visual effects (VFX).
 Computer animation can be very detailed 3D animation, while 2D computer animation (which may have the look of traditional animation) can be used for stylistic reasons, low bandwidth, or faster real-time renderings. Other common animation methods apply a stop motion technique to two- and three-dimensional objects like paper cutouts, puppets, or clay figures.
 A cartoon is an animated film, usually a short film, featuring an exaggerated visual style. The style takes inspiration from comic strips, often featuring anthropomorphic animals, superheroes, or the adventures of human protagonists. Especially with animals that form a natural predator/prey relationship (e.g. cats and mice, coyotes and birds), the action often centers on violent pratfalls such as falls, collisions, and explosions that would be lethal in real life.
 The illusion of animation—as in motion pictures in general—has traditionally been attributed to the persistence of vision and later to the phi phenomenon and beta movement, but the exact neurological causes are still uncertain. The illusion of motion caused by a rapid succession of images that minimally differ from each other, with unnoticeable interruptions, is a stroboscopic effect. While animators traditionally used to draw each part of the movements and changes of figures on transparent cels that could be moved over a separate background, computer animation is usually based on programming paths between key frames to maneuver digitally created figures throughout a digitally created environment.
 Analog mechanical animation media that rely on the rapid display of sequential images include the phénakisticope, zoetrope, flip book, praxinoscope, and film. Television and video are popular electronic animation media that originally were analog and now operate digitally. For display on computers, technology such as the animated GIF and Flash animation were developed.
 In addition to short films, feature films, television series, animated GIFs, and other media dedicated to the display of moving images, animation is also prevalent in video games, motion graphics, user interfaces, and visual effects.[1]
 The physical movement of image parts through simple mechanics—for instance, moving images in magic lantern shows—can also be considered animation. The mechanical manipulation of three-dimensional puppets and objects to emulate living beings has a very long history in automata. Electronic automata were popularized by Disney as animatronics.
 The word ""animation"" stems from the Latin ""animātiōn"", stem of ""animātiō"", meaning ""a bestowing of life"".[2] The earlier meaning of the English word is ""liveliness"" and has been in use much longer than the meaning of ""moving image medium"".
 Long before modern animation began, audiences around the world were captivated by the magic of moving characters. For centuries, master artists and craftsmen have brought puppets, automatons, shadow puppets, and fantastical lanterns to life, inspiring the imagination through physically manipulated wonders.[3]
 In 1833, the stroboscopic disc (better known as the phénakisticope) introduced the principle of modern animation, which would also be applied in the zoetrope (introduced in 1866), the flip book (1868), the praxinoscope (1877) and film.
 When cinematography eventually broke through in the 1890s, the wonder of the realistic details in the new medium was seen as its biggest accomplishment. It took years before animation found its way to the cinemas. The successful short The Haunted Hotel (1907) by J. Stuart Blackton popularized stop-motion and reportedly inspired Émile Cohl to create Fantasmagorie (1908), regarded as the oldest known example of a complete traditional (hand-drawn) animation on standard cinematographic film. Other great artistic and very influential short films were created by Ladislas Starevich with his puppet animations since 1910 and by Winsor McCay with detailed hand-drawn animation in films such as Little Nemo (1911) and Gertie the Dinosaur (1914).[4]
 During the 1910s, the production of animated ""cartoons"" became an industry in the US.[5] Successful producer John Randolph Bray and animator Earl Hurd, patented the cel animation process that dominated the animation industry for the rest of the century.[6][7] Felix the Cat, who debuted in 1919, became the first fully realized animal character in the history of American animation.[8]
 In 1928, Steamboat Willie, featuring Mickey Mouse and Minnie Mouse, popularized film with synchronized sound and put Walt Disney's studio at the forefront of the animation industry. Although Disney Animation's actual output relative to total global animation output has always been very small, the studio has overwhelmingly dominated the ""aesthetic norms"" of animation ever since.[9]
 The enormous success of Mickey Mouse is seen as the start of the golden age of American animation that would last until the 1960s. The United States dominated the world market of animation with a plethora of cel-animated theatrical shorts.[10] Several studios would introduce characters that would become very popular and would have long-lasting careers, including Walt Disney Productions' Goofy (1932) and Donald Duck (1934), Fleischer Studios/Paramount Cartoon Studios' Out of the Inkwell' Koko the Clown (1918), Bimbo and Betty Boop (1930), Popeye (1933) and Casper the Friendly Ghost (1945), Warner Bros. Cartoon Studios' Looney Tunes' Porky Pig (1935), Daffy Duck (1937), Elmer Fudd (1937–1940), Bugs Bunny (1938–1940), Tweety (1942), Sylvester the Cat (1945), Wile E. Coyote and the Road Runner (1949), MGM cartoon studio's Tom and Jerry (1940) and Droopy, Universal Cartoon Studios' Woody Woodpecker (1940), Terrytoons/20th Century Fox's Mighty Mouse (1942), and United Artists' Pink Panther (1963).
 In 1917, Italian-Argentine director Quirino Cristiani made the first feature-length film El Apóstol (now lost), which became a critical and commercial success. It was followed by Cristiani's Sin dejar rastros in 1918, but one day after its premiere, the film was confiscated by the government.[12]
 After working on it for three years, Lotte Reiniger released the German feature-length silhouette animation Die Abenteuer des Prinzen Achmed in 1926, the oldest extant animated feature.[citation needed][13]
 In 1937, Walt Disney Studios premiered their first animated feature (using the Rotoscope technique invented by Max Fleischer in 1915) Snow White and the Seven Dwarfs, still one of the highest-grossing traditional animation features as of May 2020[update].[14][15] The Fleischer studios followed this example in 1939 with Gulliver's Travels with some success. Partly due to foreign markets being cut off by the Second World War, Disney's next features Pinocchio, Fantasia (both 1940), Fleischer Studios' second animated feature Mr. Bug Goes to Town (1941–1942) and Disney's feature films Cinderella (1950), Alice in Wonderland (1951) and Lady and the Tramp (1955) failed at the box office. For decades afterward, Disney would be the only American studio to regularly produce animated features, until Ralph Bakshi became the first to also release more than a handful features. Sullivan-Bluth Studios began to regularly produce animated features starting with An American Tail in 1986.[citation needed]
 Although relatively few titles became as successful as Disney's features, other countries developed their own animation industries that produced both short and feature theatrical animations in a wide variety of styles, relatively often including stop motion and cutout animation techniques. Soviet Soyuzmultfilm animation studio, founded in 1936, produced 20 films (including shorts) per year on average and reached 1,582 titles in 2018. China, Czechoslovakia / Czech Republic, Italy, France, and Belgium were other countries that more than occasionally released feature films, while Japan became a true powerhouse of animation production, with its own recognizable and influential anime style of effective limited animation.[citation needed]
 Animation became very popular on television since the 1950s, when television sets started to become common in most developed countries. Cartoons were mainly programmed for children, on convenient time slots, and especially US youth spent many hours watching Saturday-morning cartoons. Many classic cartoons found a new life on the small screen and by the end of the 1950s, the production of new animated cartoons started to shift from theatrical releases to TV series. Hanna-Barbera Productions was especially prolific and had huge hit series, such as The Flintstones (1960–1966) (the first prime time animated series), Scooby-Doo (since 1969) and Belgian co-production The Smurfs (1981–1989). The constraints of American television programming and the demand for an enormous quantity resulted in cheaper and quicker limited animation methods and much more formulaic scripts. Quality dwindled until more daring animation surfaced in the late 1980s and in the early 1990s with hit series, the first cartoon of The Simpsons (1987), which later developed into its own show (in 1989) and SpongeBob SquarePants (since 1999) as part of a ""renaissance"" of American animation.[citation needed]
 While US animated series also spawned successes internationally, many other countries produced their own child-oriented programming, relatively often preferring stop motion and puppetry over cel animation. Japanese anime TV series became very successful internationally since the 1960s, and European producers looking for affordable cel animators relatively often started co-productions with Japanese studios, resulting in hit series such as Barbapapa (The Netherlands/Japan/France 1973–1977), Wickie und die starken Männer/小さなバイキング ビッケ (Vicky the Viking) (Austria/Germany/Japan 1974), Maya the Bee (Japan/Germany 1975) and The Jungle Book (Italy/Japan 1989).[citation needed]
 Computer animation was gradually developed since the 1940s. 3D wireframe animation started popping up in the mainstream in the 1970s, with an early (short) appearance in the sci-fi thriller Futureworld (1976).[16]
 The Rescuers Down Under was the first feature film to be completely created digitally without a camera.[17] It was produced using the Computer Animation Production System (CAPS), developed by Pixar  in collaboration with The Walt Disney Company in the late 1980s, in a style similar to traditional cel animation .[18][19][20]
 The so-called 3D style, more often associated with computer animation, became the dominant technique following the success of Pixar's Toy Story (1995), the first computer-animated feature in this style.[21]
 Most of the cel animation studios switched to producing mostly computer-animated films around the 1990s, as it proved cheaper and more profitable. Not only the very popular 3D animation style was generated with computers, but also most of the films and series with a more traditional hand-crafted appearance, in which the charming characteristics of cel animation could be emulated with software, while new digital tools helped developing new styles and effects.[22][23][24][25][26][27]
 In 2010, the animation market was estimated to be worth circa US$80 billion.[28] By 2020, the value had increased to an estimated US$270 billion.[29] Animated feature-length films returned the highest gross margins (around 52%) of all film genres between 2004 and 2013.[30] Animation as an art and industry continues to thrive as of the early 2020s.[31][32][33]
 The clarity of animation makes it a powerful tool for instruction, while its total malleability also allows exaggeration that can be employed to convey strong emotions and to thwart reality. It has therefore been widely used for other purposes than mere entertainment.[34]
 During World War II, animation was widely exploited for propaganda. Many American studios, including Warner Bros. and Disney, lent their talents and their cartoon characters to convey to the public certain war values. Some countries, including China, Japan and the United Kingdom, produced their first feature-length animation for their war efforts.[citation needed]
 Animation has been very popular in television commercials, both due to its graphic appeal, and the humour it can provide. Some animated characters in commercials have survived for decades, such as Snap, Crackle and Pop in advertisements for Kellogg's cereals.[35] Tex Avery was the producer of the first Raid ""Kills Bugs Dead"" commercials in 1966, which were very successful for the company.[36]
 Apart from their success in movie theaters and television series, many cartoon characters would also prove lucrative when licensed for all kinds of merchandise and for other media.
 Animation has traditionally been very closely related to comic books. While many comic book characters found their way to the screen (which is often the case in Japan, where many manga are adapted into anime), original animated characters also commonly appear in comic books and magazines. Somewhat similarly, characters and plots for video games (an interactive form of animation that became its own medium) have been derived from films and vice versa.[37]
 Some of the original content produced for the screen can be used and marketed in other media. Stories and images can easily be adapted into children's books and other printed media. Songs and music have appeared on records and as streaming media.[citation needed]
 While very many animation companies commercially exploit their creations outside moving image media, The Walt Disney Company is the best known and most extreme example. Since first being licensed for a children's writing tablet in 1929, their Mickey Mouse mascot has been depicted on an enormous amount of products, as have many other Disney characters. This may have influenced some pejorative use of Mickey's name, but licensed Disney products sell well, and the so-called Disneyana has many avid collectors, and even a dedicated Disneyana Fan Club (since 1984).[38]
 Disneyland opened in 1955 and features many attractions that were based on Disney's cartoon characters. Its enormous success spawned several other Disney theme parks and resorts. Disney's earnings from the theme parks have relatively often been higher than those from their movies.
 As with any other form of media, animation has instituted awards for excellence in the field. Many are part of general or regional film award programs, like the China's Golden Rooster Award for Best Animation (since 1981). Awards programs dedicated to animation, with many categories, include ASIFA-Hollywood's Annie Awards, the Emile Awards in Europe and the Anima Mundi awards in Brazil.[39][40][41]
 Apart from Academy Awards for Best Animated Short Film (since 1932) and Best Animated Feature (since 2002), animated movies have been nominated and rewarded in other categories, relatively often for Best Original Song and Best Original Score.
 Beauty and the Beast was the first animated film nominated for Best Picture, in 1991. Up (2009) and Toy Story 3 (2010) also received Best Picture nominations, after the academy expanded the number of nominees from five to ten.[42]
 The creation of non-trivial animation works (i.e., longer than a few seconds) has developed as a form of filmmaking, with certain unique aspects.[43] Traits common to both live-action and animated feature-length films are labor intensity and high production costs.[44]
 The most important difference is that once a film is in the production phase, the marginal cost of one more shot is higher for animated films than live-action films.[45] It is relatively easy for a director to ask for one more take during principal photography of a live-action film, but every take on an animated film must be manually rendered by animators (although the task of rendering slightly different takes has been made less tedious by modern computer animation).[46] It is pointless for a studio to pay the salaries of dozens of animators to spend weeks creating a visually dazzling five-minute scene if that scene fails to effectively advance the plot of the film.[47] Thus, animation studios starting with Disney began the practice in the 1930s of maintaining story departments where storyboard artists develop every single scene through storyboards, then handing the film over to the animators only after the production team is satisfied that all the scenes make sense as a whole.[48] While live-action films are now also storyboarded, they enjoy more latitude to depart from storyboards (i.e., real-time improvisation).[citation needed][49]
 Another problem unique to animation is the requirement to maintain a film's consistency from start to finish, even as films have grown longer and teams have grown larger. Animators, like all artists, necessarily have individual styles, but must subordinate their individuality in a consistent way to whatever style is employed on a particular film.[50] Since the early 1980s, teams of about 500 to 600 people, of whom 50 to 70 are animators, typically have created feature-length animated films. It is relatively easy for two or three artists to match their styles; synchronizing those of dozens of artists is more difficult.[51]
 This problem is usually solved by having a separate group of visual development artists develop an overall look and palette for each film before the animation begins. Character designers on the visual development team draw model sheets to show how each character should look like with different facial expressions, posed in different positions, and viewed from different angles.[52][53] On traditionally animated projects, maquettes were often sculpted to further help the animators see how characters would look from different angles.[54][52]
 Unlike live-action films, animated films were traditionally developed beyond the synopsis stage through the storyboard format; the storyboard artists would then receive credit for writing the film.[55] In the early 1960s, animation studios began hiring professional screenwriters to write screenplays (while also continuing to use story departments) and screenplays had become commonplace for animated films by the late 1980s.[citation needed]
 Traditional animation (also called cel animation or hand-drawn animation) was the process used for most animated films of the 20th century.[56] The individual frames of a traditionally animated film are photographs of drawings, first drawn on paper.[57] To create the illusion of movement, each drawing differs slightly from the one before it. The animators' drawings are traced or photocopied onto transparent acetate sheets called cels,[58] which are filled in with paints in assigned colors or tones on the side opposite the line drawings.[59] The completed character cels are photographed one-by-one against a painted background by a rostrum camera onto motion picture film.[60]
 The traditional cel animation process became obsolete by the beginning of the 21st century. Today, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system.[1][61] Various software programs are used to color the drawings and simulate camera movement and effects.[62] The final animated piece is output to one of several delivery media, including traditional 35 mm film and newer media with digital video.[63][1] The ""look"" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 90 years.[54] Some animation producers have used the term ""tradigital"" (a play on the words ""traditional"" and ""digital"") to describe cel animation that uses significant computer technology.
 Examples of traditionally animated feature films include Pinocchio (United States, 1940),[64] Animal Farm (United Kingdom, 1954), Lucky and Zorba (Italy, 1998), and The Illusionist (British-French, 2010). Traditionally animated films produced with the aid of computer technology include The Lion King (US, 1994), The Prince of Egypt (US, 1998), Akira (Japan, 1988),[65] Spirited Away (Japan, 2001), The Triplets of Belleville (France, 2003), and The Secret of Kells (Irish-French-Belgian, 2009).
 Full animation is the process of producing high-quality traditionally animated films that regularly use detailed drawings and plausible movement,[66] having a smooth animation.[67] Fully animated films can be made in a variety of styles, from more realistically animated works like those produced by the Walt Disney studio (The Little Mermaid, Beauty and the Beast, Aladdin, The Lion King) to the more 'cartoon' styles of the Warner Bros. animation studio. Many of the Disney animated features are examples of full animation, as are non-Disney works, The Secret of NIMH (US, 1982), The Iron Giant (US, 1999), and Nocturna (Spain, 2007). Fully animated films are often animated on ""twos"", sometimes on ""ones"", which means that 12 to 24 drawings are required for a single second of film. [68]
 Limited animation involves the use of less detailed or more stylized drawings and methods of movement usually a choppy or ""skippy"" movement animation.[69] Limited animation uses fewer drawings per second, thereby limiting the fluidity of the animation. This is a more economic technique. Pioneered by the artists at the American studio United Productions of America,[70] limited animation can be used as a method of stylized artistic expression, as in Gerald McBoing-Boing (US, 1951), Yellow Submarine (UK, 1968), and certain anime produced in Japan.[71] Its primary use, however, has been in producing cost-effective animated content for media for television (the work of Hanna-Barbera,[72] Filmation,[73] and other TV animation studios[74]) and later the Internet (web cartoons).
 Rotoscoping is a technique patented by Max Fleischer in 1917 where animators trace live-action movement, frame by frame.[75] The source film can be directly copied from actors' outlines into animated drawings,[76] as in The Lord of the Rings (US, 1978), or used in a stylized and expressive manner, as in Waking Life (US, 2001) and A Scanner Darkly (US, 2006). Some other examples are Fire and Ice (US, 1983), Heavy Metal (1981), and Aku no Hana (Japan, 2013).[citation needed]
 Live-action/animation is a technique combining hand-drawn characters into live action shots or live-action actors into animated shots.[77] One of the earlier uses was in Koko the Clown when Koko was drawn over live-action footage.[78] Walt Disney and Ub Iwerks created a series of Alice Comedies (1923–1927), in which a live-action girl enters an animated world. Other examples include Allegro Non Troppo (Italy, 1976), Who Framed Roger Rabbit (US, 1988), Volere volare (Italy 1991), Space Jam (US, 1996) and Osmosis Jones (US, 2001).[citation needed]
 Stop motion is used to describe animation created by physically manipulating real-world objects and photographing them one frame of film at a time to create the illusion of movement.[79] There are many different types of stop-motion animation, usually named after the materials used to create the animation.[80] Computer software is widely available to create this type of animation; traditional stop-motion animation is usually less expensive but more time-consuming to produce than current computer animation.[80]
 Computer animation encompasses a variety of techniques, the unifying factor being that the animation is created digitally on a computer.[62][102] 2D animation techniques tend to focus on image manipulation while 3D techniques usually build virtual worlds in which characters and objects move and interact.[103] 3D animation can create images that seem real to the viewer.[104]
 2D animation figures are created or edited on the computer using 2D bitmap graphics and 2D vector graphics.[105] This includes automated computerized versions of traditional animation techniques, interpolated morphing,[106] onion skinning[107] and interpolated rotoscoping.
2D animation has many applications, including After Effects Animation, analog computer animation, Flash animation, and PowerPoint animation. Cinemagraphs are still photographs in the form of an animated GIF file of which part is animated.[108]
 Final line advection animation is a technique used in 2D animation,[109] to give artists and animators more influence and control over the final product as everything is done within the same department.[110] Speaking about using this approach in Paperman, John Kahrs said that ""Our animators can change things, actually erase away the CG underlayer if they want, and change the profile of the arm.""[111]
 3D animation is digitally modeled and manipulated by an animator. The 3D model maker usually starts by creating a 3D polygon mesh for the animator to manipulate.[112] A mesh typically includes many vertices that are connected by edges and faces, which give the visual appearance of form to a 3D object or 3D environment.[112] Sometimes, the mesh is given an internal digital skeletal structure called an armature that can be used to control the mesh by weighting the vertices.[113][114] This process is called rigging and can be used in conjunction with key frames to create movement.[115]
 Other techniques can be applied, mathematical functions (e.g., gravity, particle simulations), simulated fur or hair, and effects, fire and water simulations.[116] These techniques fall under the category of 3D dynamics.[117]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['video games', 'cats and mice, coyotes and birds', 'the exact neurological causes', 'falls, collisions, and explosions', 'violent pratfalls'], 'answer_start': [], 'answer_end': []}"
"
 A television show, TV program, or simply a TV show, is the general reference to any content produced for viewing on a television set that is traditionally broadcast via over-the-air, satellite, or cable. This includes content made by television broadcasters and content made for broadcasting by film production companies. It excludes breaking news, advertisements, or trailers that are typically placed between shows. Television shows are most often scheduled for broadcast well ahead of time and appear on electronic guides or other TV listings, but streaming services often make them available for viewing anytime. The content in a television show is produced by one of two production methodologies: live taped shows such as variety and news magazine shows shot on a television studio stage or sporting events (all considered linear productions.) The other production model includes animation and a variety of film productions ranging from movies to series. Shows not produced on a television studio stage are usually contracted or licensed to be made by appropriate production companies.
 Television shows can be viewed live (in a linear/real time fashion), be recorded on home video, a digital video recorder for later viewing, be viewed on demand via a set-top box, or streamed over the internet. A television show is also called a television program (British English: programme), especially if it lacks a narrative structure. In the United States and Canada, a television series is usually released in episodes that follow a narrative and are usually divided into seasons. In the UK, a television series is a yearly or semiannual set of new episodes. In effect, a ""series"" in the United Kingdom, Ireland and Australia is the same as a ""season"" in the United States and Canada. A small or one-off collection of episodes may also be called a limited series, TV special or miniseries. A television film or telefilm is a feature film created for broadcasting on television.
 The first television shows were experimental, sporadic broadcasts viewable only within a very short range from the broadcast tower starting in the 1930s. Televised events such as the 1936 Summer Olympics in Germany, the 1937 coronation of King George VI in the United Kingdom, and David Sarnoff's famous introduction at the 1939 New York World's Fair in the United States spurred a growth in the medium, but World War II put a halt to development until after the war. The 1947 World Series inspired many Americans to buy their first television set and then in 1948, the popular radio show Texaco Star Theater made the move and became the first weekly televised variety show, earning host Milton Berle the name ""Mr. Television"", and demonstrating that the medium was a stable, modern form of entertainment which could attract advertisers. The first national live television broadcast in the US took place on September 4, 1951, when President Harry Truman's speech at the Japanese Peace Treaty Conference in San Francisco was transmitted over AT&T's transcontinental cable and microwave radio relay system to broadcast stations in local markets.[1][2][3]
 The first national color broadcast (the 1954 Tournament of Roses Parade) in the US occurred on January 1, 1954. During the following ten years most network broadcasts, and nearly all local programming, continued to be in black-and-white. The color transition was announced for the fall of 1965, during which over half of all network prime-time programming would be broadcast in color. The first all-color prime-time season came just one year later. In 1972, the last holdout among daytime network shows converted to color, resulting in the first completely all-color network season.
 Television shows are more varied than most other forms of media due to the wide variety of formats and genres that can be presented. A show may be fictional (as in comedies and dramas), or non-fictional (as in documentary, news, and reality television). It may be topical (as in the case of a local newscast and some made-for-television films), or historical (as in the case of many documentaries and fictional series). They could be primarily instructional or educational, or entertaining as is the case in situation comedy and game shows.[citation needed]
 A drama program usually features a set of actors playing characters in a historical or contemporary setting. The program follows their lives and adventures. Before the 1980s, shows (except for soap opera-type serials) typically remained static without story arcs, and the main characters and premise changed little.[citation needed] If some change happened to the characters' lives during the episode, it was usually undone by the end. Due to this, the episodes could be broadcast in any order.[citation needed] Since the 1980s, many series feature progressive change in the plot, the characters, or both. For instance, Hill Street Blues and St. Elsewhere were two of the first US prime time drama television series to have this kind of dramatic structure,[4][better source needed] while the later series Babylon 5 further exemplifies such structure in that it had a predetermined story running over its intended five-season run.[citation needed]
 In 2012, it was reported that television was growing into a larger component of major media companies' revenues than film.[5] Some also noted the increase in quality of some television programs. In 2012, Academy Award-winning film director Steven Soderbergh, commenting on ambiguity and complexity of character and narrative, stated: ""I think those qualities are now being seen on television and that people who want to see stories that have those kinds of qualities are watching television.""[6]
 When a person or company decides to create new content for television broadcast, they develop the show's elements, consisting of the concept, the characters, the crew, and cast. Then they often ""pitch"" it to the various networks in an attempt to find one interested enough to order a prototype first episode of the series, known as a pilot.[7] Eric Coleman, an animation executive at Disney, told an interviewer, ""One misconception is that it's very difficult to get in and pitch your show, when the truth is that development executives at networks want very much to hear ideas. They want very much to get the word out on what types of shows they're looking for.""[8]
 To create the pilot, the structure and team of the whole series must be put together. If audiences respond well to the pilot, the network will pick up the show to air it the next season.[citation needed] Sometimes they save it for mid-season, or request rewrites and additional review.[citation needed] Other times, they pass entirely, forcing the show's creator to ""shop it around"" to other networks. Many shows never make it past the pilot stage.[9]
 The method of ""team writing"" is employed on some longer dramatic series (usually running up to a maximum of around 13 episodes). The idea for such a program may be generated ""in-house"" by one of the networks; it could originate from an independent production company (sometimes a product of both). For instance, the BBC's long-running soap opera EastEnders is wholly a BBC production, whereas its popular drama Life on Mars was developed by Kudos in association with the broadcaster.
 There are still a significant number of programs (usually sitcoms) that are built by just one or two writers and a small, close-knit production team. These are ""pitched"" in the traditional way, but since the creators handle all the writing requirements, there is a run of six or seven episodes per series once approval has been given. Many of the most popular British comedies have been made this way, including Monty Python's Flying Circus (albeit with an exclusive team of six writer-performers), Fawlty Towers, Blackadder and The Office.
 The production company is often separate from the broadcaster. The executive producer, often the show's creator, is in charge of running the show. They pick the crew and help cast the actors, approve and sometimes write series plots—some even write or direct major episodes—while various other producers help to ensure that the show runs smoothly. Very occasionally, the executive producer will cast themselves in the show. As with filmmaking or other electronic media production, producing of an individual episode can be divided into three parts: pre-production, principal photography, and post-production.
 Pre-production begins when a script is approved. A director is chosen to plan the episode's final look.
 Pre-production tasks include storyboarding; construction of sets, props, and costumes; casting guest stars; budgeting; acquiring resources like lighting, special effects, stunts, etc. Once the show is planned, it must then be scheduled: scenes are often filmed out of sequence, guest actors or even regulars may only be available at certain times. Sometimes the principal photography of different episodes must be done at the same time, complicating the schedule (a guest star might shoot scenes from two episodes on the same afternoon). Complex scenes are translated from storyboard to animatics to further clarify the action. Scripts are adjusted to meet altering requirements.
 Some shows have a small stable of directors, but also usually rely on outside directors. Given the time constraints of broadcasting, a single show might have two or three episodes in pre-production, one or two episodes in principal photography, and a few more in various stages of post-production. The task of directing is complex enough that a single director can usually not work on more than one episode or show at a time, hence the need for multiple directors.
 Principal photography is the actual filming of the episode. Director, actors, and crew gather at a television studio or on location for filming or videoing a scene. A scene is further divided into shots, which should be planned during pre-production. Depending on scheduling, a scene may be shot in non-sequential order of the story. Conversations may be filmed twice from different camera angles, often using stand-ins, so one actor might perform all their lines in one set of shots, and then the other side of the conversation is filmed from the opposite perspective. To complete a production on time, a second unit may be filming a different scene on another set or location at the same time, using a different set of actors, an assistant director, and a second unit crew. A director of photography supervises the lighting of each shot to ensure consistency.
 Live events are usually covered by Outside Broadcast crews using mobile television studios, known as scanners or OB trucks. Although varying greatly depending on the era and subject covered, these trucks were normally crewed by up to 15 skilled operators and production personnel. In the UK for most of the 20th century, the BBC was the preeminent provider of outside broadcast coverage. BBC crews worked on almost every major event, including Royal weddings and funerals, major political and sporting events, and even drama programs.[10]
 Once principal photography is complete, producers coordinate tasks to begin the video editing. Visual and digital video effects are added to the film; this is often outsourced to companies specializing in these areas. Often music is performed with the conductor using the film as a time reference (other musical elements may be previously recorded). An editor cuts the various pieces of film together, adds the musical score and effects, determines scene transitions, and assembles the completed show.
 Most television networks throughout the world are 'commercial', dependent on selling advertising time or acquiring sponsors.[citation needed] Broadcasting executives' main concern over their programming is audience size.[citation needed] In the past, the number of 'free to air' stations was restricted by the availability of channel frequencies, but cable TV (outside the United States, satellite television) technology has allowed an expansion in the number of channels available to viewers (sometimes at premium rates) in a much more competitive environment.[citation needed]
 In the United States, the average broadcast network drama costs $3 million an episode to produce, while cable dramas cost $2 million on average.[11] The pilot episode may be more expensive than a regular episode.[citation needed] In 2004, Lost's two-hour pilot cost $10 to $14 million, in 2008 Fringe's two-hour pilot cost $10 million, and in 2010, Boardwalk Empire was $18 million for the first episode. In 2011, Game of Thrones was $5 to $10 million, Pan Am cost an estimated $10 million, while Terra Nova's two-hour pilot was between $10 and $20 million.[12][13]
 Many scripted network television shows in the United States are financed through deficit financing: a studio finances the production cost of a show and a network pays a license fee to the studio for the right to air the show. This license fee does not cover the show's production costs, leading to the deficit. Although the studio does not make its money back in the original airing of the show, it retains ownership of the show. This allows the studio to make its money back and earn a profit through syndication and sales of DVDs and Blu-rays. This system places most of the financial risk on the studios; however, a hit show in the syndication and home video markets can more than make up for the misses. Although deficit financing places minimal financial risk on the networks, they lose out on the future profits of big hits since they are only licensing the shows.[14]
 Costs are recouped mainly by advertising revenues for broadcast networks and some cable channels, while other cable channels depend on subscriptions. In general, advertisers, and consequently networks that depend on advertising, are more interested in the number of viewers within the 18–49 age range than in the total number of viewers.[15][16] Advertisers are willing to pay more to advertise on shows successful with young adults because they watch less television and are harder to reach.[17] According to Advertising Age, during the 2007–08 season, Grey's Anatomy was able to charge $419,000 per commercial, compared to only $248,000 for a commercial during CSI, despite CSI having almost five million more viewers on average.[18] Due to its strength with younger viewers, Friends was able to charge almost three times as much for a commercial as Murder, She Wrote, even though the two series had similar total viewer numbers at that time.[15] Glee and The Office drew fewer total viewers than NCIS during the 2009–10 season, but earned an average of $272,694 and $213,617 respectively, compared to $150,708 for NCIS.[19]
 After production, the show is handed over to the television network, which sends it out to its affiliate stations, which broadcast it in the specified broadcast programming time slot. If the Nielsen ratings are good, the show is kept alive as long as possible. If not, the show is usually canceled. The show's creators are then left to shop around remaining episodes, and the possibility of future episodes, to other networks. On especially successful series, the producers sometimes call a halt to a series on their own like Seinfeld, The Cosby Show, Corner Gas, and M*A*S*H and end it with a concluding episode, which sometimes is a big series finale.
 On rare occasions, a series that has not attracted particularly high ratings and has been canceled can be given a reprieve if home video viewership has been particularly strong. This has happened in the cases of Family Guy in the US and Peep Show in the UK.
 In the United States, if the show is popular or lucrative, and a minimum number of episodes (usually 100) have been made, it can go into broadcast syndication, where rights to broadcast the program are then resold for cash or put into a barter exchange (offered to an outlet for free in exchange for airing additional commercials elsewhere in the station's broadcast day).
 The terminology used to define a set of episodes produced by a television series varies from country to country.
 In North American television, a series is a connected set of television program episodes that run under the same title, possibly spanning many seasons. During the 1950s, it was common for television seasons to consist of more than 30 episodes—however, the average length has been declining since.[20]
 Until the 1980s, most new programs for the US broadcast networks debuted in the ""fall season"", which ran from September through March and nominally contained from 24 to 26 episodes. These episodes were rebroadcast during the spring (or summer) season, from April through August. Because of cable television and the Nielsen sweeps, the ""fall"" season now normally extends to May. Thus, a ""full season"" on a broadcast network now usually runs from September through May for at least 22 episodes.[21]
 
A full season is sometimes split into two separate units with a hiatus around the end of the calendar year, such as the first season of Jericho on CBS. When this split occurs, the last half of the episodes sometimes are referred to with the letter B as in ""The last nine episodes (of The Sopranos) will be part of what is being called either 'Season 6, Part 2' or 'Season 6B'"",[22] or in ""Futurama is splitting its seasons similar to how South Park does, doing half a season at a time, so this is season 6B for them.""[23] Since the 1990s, these shorter seasons also have been referred to as ""split"" or ""half"" seasons, which is done to increase profits, as seen with shows such as The Witcher.[24]
 Since at least the 2000s, new broadcast television series are often ordered (funded) for just the first 10 to 13 episodes, to gauge audience interest. If a series is popular, the network places a ""back nine order"" and the season is completed to the regular 20 to 26 episodes. An established series that is already popular, however, will typically receive an immediate full-season order at the outset of the season. A midseason replacement is a less-expensive short-run show of generally 10 to 13 episodes designed to take the place of an original series that failed to garner an audience and has not been picked up. A ""series finale"" is the last show of the series before the show is no longer produced. (In the UK, it means the end of a season, what is known in the United States as a ""season finale"".)  Streaming services time finales to the next quarter to induce consumers to renew at least one more quarter.[25]
 A standard television season in the United States runs predominantly during autumn.[26] During the summer months of June through roughly mid-September, network schedules typically feature reruns of their flagship programs, first-run series with lower rating expectations, and other specials. First-run scripted series are typically shorter and of a lower profile than those aired during the main season and can also include limited series events. Reality and game shows have also been a fixture of the schedule.[26]
 In Canada, the commercial networks air most US programming in tandem with the US television season, but their original Canadian shows follow a model closer to British than US television production. Due to the smaller production budgets available in Canada, a Canadian show's season normally runs to a maximum of 13 episodes rather than 20 or more, although an exceptionally popular series such as Corner Gas or Murdoch Mysteries might receive 20-episode orders in later seasons. Canadian shows do not normally receive ""back nine"" extensions within the same season, however; even a popular series simply ends for the year when the original production order has finished airing, and an expanded order of more than 13 episodes is applied to the next season's renewal order rather than an extension of the current season. Only the public CBC Television normally schedules Canadian-produced programming throughout the year; the commercial networks typically now avoid scheduling Canadian productions to air in the fall, as such shows commonly get lost amid the publicity onslaught of the US fall season. Instead, Canadian-produced shows on the commercial networks typically air either in the winter as mid-season replacements for canceled US shows or in the summer (which may also improve their chances of being picked up by a US network for a summer run).[27]
 While network orders for 13- or 22-episode seasons are still pervasive in the television industry, several shows have deviated from this traditional trend. Written to be closed-ended and of shorter length than other shows, they are marketed with a variety of terms.
 
 In the United Kingdom and other countries, these sets of episodes are referred to as a ""series"". In Australia, the broadcasting may be different from North American usage. The terms series and season are both used and are the same. For example, Battlestar Galactica has an original series as well as a remake, both are considered a different series each with their own number of individual seasons.
 Australian television does not follow ""seasons"" in the way that US television does; for example, there is no ""fall season"" or ""fall schedule"". For many years, popular night-time dramas in Australia would run for much of the year, and would only go into recess during the summer period (December to February, as Australia is in the Southern Hemisphere), when ratings are not taken. Therefore, popular dramas would usually run from February through November each year. This schedule was used in the 1970s for popular dramas including Number 96. Many drama series, such as McLeod's Daughters, have received between 22 and 32 episodes per season. 
 Typically, soap operas, which have always run in season format in Australia, such as Home and Away, would usually begin a new season in late January, while the season finale would air in late November, as the show is off air for two months, or sometimes longer, depending on the schedule. In recent years,[when?] a new season would begin in early February, and the season finale would broadcast in early December. Since Home and Away's inception, it normally receives 230 episodes per season. Some seasons have seen between 205 and 235 episodes commissioned. During the Olympics, Home and Away would often go on hiatus, which was referred to as an ""Olympic cliffhanger"". Therefore, the number of episodes would decrease. Australian situation comedy series' seasons are approximately 13 episodes long and premiere any time between February and November.
 British shows have tended toward shorter series in recent years. For example, the first series of long-running science fiction show Doctor Who in 1963 featured forty-two 25‑minute episodes, and continued with a similar number each year until it was reduced to twenty-five for 1970 to accommodate changes in production and significantly reducing the actors' workload) and continued to 1984. For 1985 fewer but longer episodes were shown, but even after a return to shorter episodes in 1986, lack of support within the BBC meant fewer episodes were commissioned leading to only fourteen 25‑minute episodes up to those in 1989 after which it was cancelled. The revival of Doctor Who from 2005 has comprised thirteen 45‑minute installments. 
 There are some series in the UK that have a larger number of episodes, for example Waterloo Road started with 8 to 12 episodes, but from series three onward it increased to twenty episodes and series seven will contain 30 episodes. Recently, US non-cable networks have also begun to experiment with shorter series for some programs, particularly reality shows, such as Survivor. They often air two series per year, resulting in roughly the same number of episodes per year as a drama.
 This is a reduction from the 1950s, in which many US shows (e.g. Gunsmoke) had between 29 and 39 episodes per season. Actual storytelling time within a commercial television hour has also gradually reduced over the years, from 50 minutes out of every 60 to the current 44 (and even less on some networks), beginning in the early 21st century.
 The usage of ""season"" and ""series"" differ for DVD and Blu-ray releases in both Australia and the UK. In Australia, many locally produced shows are termed differently on home video releases. For example, a set of the television drama series Packed to the Rafters or Wentworth is referred to as ""season"" (""The Complete First Season"", etc.), whereas drama series such as Tangle are known as a ""series"" (""Series 1"", etc.). British-produced shows such as Mrs. Brown's Boys are referred to as ""season"" in Australia for the DVD and Blu-ray releases.
 In the UK and Ireland, most programs are referred to as 'series' while 'season' is starting to be used for some US and international releases.
 The 1980s and 1990s was the golden age of television miniseries attracting millions of Egyptians. For example, The Family of Mr Shalash miniseries, starring Salah Zulfikar and Laila Taher, was the highest rated at the time.[30]
 In the United States, dramas produced for hour-long time slots typically are 37–42 minutes in length (excluding advertisements), while sitcoms produced for 30-minute time slots typically are 18–21 minutes long. There are exceptions: subscription-based TV channels, such as HBO, Starz, Cinemax, and Showtime, have episodes that are 45–48 minutes long, similar to the UK. Audience opinions of length have varied due to factors such as content overload.[31]
 In Britain, dramas typically run from 46–48 minutes on commercial channels, and 57–59 minutes on the BBC. Half-hour programs are around 22 minutes on commercial channels and around 28 minutes on the BBC. The longer duration on the BBC is due to the lack of advertising breaks.
 In France, most television shows (whether dramas, game shows or documentaries) have a duration of 52 minutes. This is the same on nearly all French networks (TF1, France 2, France 5, M6, Canal+, etc.).[32]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['any content produced for viewing on a television set', 'HBO, Starz, Cinemax, and Showtime', 'any content produced for viewing on a television set', 'Royal weddings and funerals', 'lower rating expectations'], 'answer_start': [], 'answer_end': []}"
"
 Stagecraft is a technical aspect of theatrical, film, and video production. It includes constructing and rigging scenery; hanging and focusing of lighting; design and procurement of costumes; make-up; stage management; audio engineering; and procurement of props. Stagecraft is distinct from the wider umbrella term of scenography. Considered a technical rather than an artistic field, it is primarily the practical implementation of a scenic designer's artistic vision.
 In its most basic form, stagecraft may be executed by a single person (often the stage manager of a smaller production) who arranges all scenery, costumes, lighting, and sound, and organizes the cast. Regional theaters and larger community theaters will generally have a technical director and a complement of designers, each of whom has a direct hand in their respective designs.
Within significantly larger productions, for example a modern Broadway show, effectively bringing a show to opening night requires the work of skilled carpenters, painters, electricians, stagehands, stitchers, wigmakers, and the like. Modern stagecraft is highly technical and specialized: it comprises many sub-disciplines and a vast trove of history and tradition.
 Greek theatre made extensive use of stagecraft, and Greek vocabulary and practice continue to influence contemporary Western stagecraft. The defining element of a Greek theatre's stage was the skene, a structure at the back of the stage, often featuring three doors. The usual setting for a classical Greek tragedy was a palace, and skenes were decorated to support that setting.[1]
 On the audience-side of the skene, what are now known as flats could be hung. Flats evolved from one-sided to two-sided painted flats, which would be mounted and centered on a rotating pin. Rope would run around each consecutive pin, allowing the flats to be turned for scene changes. The double-sided-flat eventually evolved into the periaktos.
 Greek stagecraft was essential to the storytelling of its works. An ekkyklema, similar to a contemporary wagon, was used to present the death of a character by rolling out their dead body, instead of showing their death onstage.[2] The mechane, a crane for lifting actors over the skene, supported the conclusions of plays, whose storylines were often suddenly resolved by the introduction of a god. The mechane is the literal source for the contemporary phrase deus ex machina.[3] Performances were lit by sunlight, often taking advantage of the particular time of day to support the story.[4]
 Plays of Medieval times were held in different places such as the streets of towns and cities, performed by traveling, secular troupes. Some were also held in monasteries, performed by church-controlled groups, often portraying religious scenes. The playing place could represent many different things such as indoors or outdoors (as in the Cornish plen-an-gwary amphitheatres). They were played in certain places so the props could be used for the play. Songs and spectacles were often used in plays to enhance participation.[5][page needed]
 More modern stagecraft was developed in England between 1576 and 1642. There were three different types of theaters in London – public, private and court. The size and shape varied but many were suggested to be round theaters. Public playhouses such as the Globe Theatre used rigging housed in a room on the roof to lower and raise scenery or actors, and used the raised stage by developing the practice of using trap-doors in theatrical productions. Most of the theaters had circular-design, with an open area above the pit to allow sunlight to enter and light the stage.[6]
 Proscenium stages, or picture-box stages, were constructed in France around the time of the English Restoration, and maintain the place of the most popular form of stage in use to-date, and originally combined elements of the skene in design, essentially building a skene on-stage. Lighting of the period would have consisted of candles, used as foot-lights, and hanging from chandeliers above the stage.
 Stagecraft during the Victorian era in England developed rapidly with the emergence of the West End. Prompted by and influx of urbanites in the greater London area, Parliament was forced to do away with previous licensing laws and allowed all theaters to perform straight plays in 1843. Electric lighting and hydraulics were introduced to draw large audiences to see on-stage storms, explosions, and miraculous transformations. Technologies developed during the latter part of the 19th-century paved the way for the development of special effects to be used in film.[7]
 Lighting continued to develop. In England, a form lamp using a blowpipe to heat lime to incandescence was developed, for navigation purposes – it was soon adapted to theatrical performances and the limelight became a widespread form of artificial light for theaters. To control the focus of the light, a Fresnel lens was used.
 After candles, came gas lighting, using pipes with small openings which were lit before every performance, and could be dimmed by controlling the flow of gas, so long as the flame never went out. With the turn of the 20th century, many theater companies making the transition from gas to electricity would install the new system right next to the old one, resulting in many explosions and fires due to the electricity igniting the gas lines.
 Modern theatrical lighting is electrically-based. Many lamps and lighting instruments are in use today, and the field is rapidly becoming one of the most diverse and complex in the industry.[8]
 A wide range of disciplines are included in stagecraft, all of which are crucial to creating compelling and immersive theatrical productions. Stagecraft is the foundation of any theatrical play, impacting the audience's experience, from the skilled application of lighting to the elaborate creation of scenery and costumes. Stagecraft promotes flawless performance execution and amplifies the emotional impact of storytelling by coordinating the interaction of visual and aural elements. Stagecraft is essential to producing captivating and unforgettable theatrical moments, whether it is through the creative application of makeup and wigs, the inventive use of mechanical scenery, or the well-planned fusion of sound and visual effects. Stagecraft brings dramatic dreams to life by a fusion of artistic innovation and technical mastery, making a profound impact on audiences all over the world.
 Stagecraft comprises many disciplines, typically divided into a number of main disciplines:
 The theater designer, responsible for orchestrating the visual and auditory aspects of a stage production, is traditionally known as the set designer. This convention has emerged for practical reasons, ensuring efficiency in the design process.[9] Upon entering a theater, viewers are immediately drawn to the set design, shaping their initial impression of the show. In this moment, designers establish the show's concept, define its style, evoke mood and atmosphere, and establish the tone by revealing the date and setting. Stage designers intentionally select elements to craft the audience's perception of the depicted worlds in a play, rather than simply replicating settings realistically. For much of theatrical history, there was no dedicated scenic designer role. Instead, theater practitioners relied on stock scenery or utilized the space itself to establish settings.
 While playwrights, producers, directors, designers, and performers all have specific roles to play, the stage manager is the one who manages the intricate collaboration between all of these important participants. Serving as the production's center point, the stage manager makes sure that every aspect, from staff to materials, works in unison to realize the theatrical concept.[10] Their responsibilities are broad and include managing backstage operations, organizing technical cues, and arranging rehearsals. In addition, the stage manager plays a pivotal role in mediating issues and fostering constructive communication between the actors and the creative team during the course of the production. As a result, even if the actors and directors receive most of the attention, the stage manager's hard work and knowledge behind the scenes preserves the integrity and professionalism of live theater production.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['theater practitioners relied on stock scenery', 'playwrights, producers, directors, designers, and performers', 'there was no dedicated scenic designer role', 'managing backstage operations, organizing technical cues, and arranging rehearsals', 'managing backstage operations, organizing technical cues, and arranging rehearsals'], 'answer_start': [], 'answer_end': []}"
"
 Opera is a form of theatre in which music is a fundamental component and dramatic roles are taken by singers. Such a ""work"" (the literal translation of the Italian word ""opera"") is typically a collaboration between a composer and a librettist[1] and incorporates a number of the performing arts, such as acting, scenery, costume, and sometimes dance or ballet. The performance is typically given in an opera house, accompanied by an orchestra or smaller musical ensemble, which since the early 19th century has been led by a conductor. Although musical theatre is closely related to opera, the two are considered to be distinct from one another.[2]
 Opera is a key part of Western classical music, and Italian in particular, tradition.[3] Originally understood as an entirely sung piece, in contrast to a play with songs, opera has come to include numerous genres, including some that include spoken dialogue such as Singspiel and Opéra comique. In traditional number opera, singers employ two styles of singing: recitative, a speech-inflected style,[4] and self-contained arias. The 19th century saw the rise of the continuous music drama.
 Opera originated in Italy at the end of the 16th century (with Jacopo Peri's mostly lost Dafne, produced in Florence in 1598) especially from works by Claudio Monteverdi, notably L'Orfeo, and soon spread through the rest of Europe: Heinrich Schütz in Germany, Jean-Baptiste Lully in France, and Henry Purcell in England all helped to establish their national traditions in the 17th century. In the 18th century, Italian opera continued to dominate most of Europe (except France), attracting foreign composers such as George Frideric Handel. Opera seria was the most prestigious form of Italian opera, until Christoph Willibald Gluck reacted against its artificiality with his ""reform"" operas in the 1760s. The most renowned figure of late 18th-century opera is Wolfgang Amadeus Mozart, who began with opera seria but is most famous for his Italian comic operas, especially The Marriage of Figaro (Le nozze di Figaro), Don Giovanni, and Così fan tutte, as well as Die Entführung aus dem Serail (The Abduction from the Seraglio), and The Magic Flute (Die Zauberflöte), landmarks in the German tradition.
 The first third of the 19th century saw the high point of the bel canto style, with Gioachino Rossini, Gaetano Donizetti and Vincenzo Bellini all creating signature works of that style. It also saw the advent of grand opera typified by the works of Daniel Auber and Giacomo Meyerbeer as well as Carl Maria von Weber's introduction of German Romantische Oper (German Romantic Opera). The mid-to-late 19th century was a golden age of opera, led and dominated by Giuseppe Verdi in Italy and Richard Wagner in Germany. The popularity of opera continued through the verismo era in Italy and contemporary French opera through to Giacomo Puccini and Richard Strauss in the early 20th century. During the 19th century, parallel operatic traditions emerged in central and eastern Europe, particularly in Russia and Bohemia. The 20th century saw many experiments with modern styles, such as atonality and serialism (Arnold Schoenberg and Alban Berg), neoclassicism (Igor Stravinsky), and minimalism (Philip Glass and John Adams). With the rise of recording technology, singers such as Enrico Caruso and Maria Callas became known to much wider audiences that went beyond the circle of opera fans. Since the invention of radio and television, operas were also performed on (and written for) these media. Beginning in 2006, a number of major opera houses began to present live high-definition video transmissions of their performances in cinemas all over the world. Since 2009, complete performances can be downloaded and are live streamed.
 The words of an opera are known as the libretto (meaning ""small book""). Some composers, notably Wagner, have written their own libretti; others have worked in close collaboration with their librettists, e.g. Mozart with Lorenzo Da Ponte. Traditional opera, often referred to as ""number opera"", consists of two modes of singing: recitative, the plot-driving passages sung in a style designed to imitate and emphasize the inflections of speech,[4] and aria (an ""air"" or formal song) in which the characters express their emotions in a more structured melodic style. Vocal duets, trios and other ensembles often occur, and choruses are used to comment on the action. In some forms of opera, such as singspiel, opéra comique, operetta, and semi-opera, the recitative is mostly replaced by spoken dialogue. Melodic or semi-melodic passages occurring in the midst of, or instead of, recitative, are also referred to as arioso. The terminology of the various kinds of operatic voices is described in detail below.[5]
 During both the Baroque and Classical periods, recitative could appear in two basic forms, each of which was accompanied by a different instrumental ensemble: secco (dry) recitative, sung with a free rhythm dictated by the accent of the words, accompanied only by basso continuo, which was usually a harpsichord and a cello; or accompagnato (also known as strumentato) in which the orchestra provided accompaniment. Over the 18th century, arias were increasingly accompanied by the orchestra. By the 19th century, accompagnato had gained the upper hand, the orchestra played a much bigger role, and Wagner revolutionized opera by abolishing almost all distinction between aria and recitative in his quest for what Wagner termed ""endless melody"". Subsequent composers have tended to follow Wagner's example, though some, such as Stravinsky in his The Rake's Progress have bucked the trend. The changing role of the orchestra in opera is described in more detail below.
 The Italian word opera means ""work"", both in the sense of the labour done and the result produced. The Italian word derives from the Latin word opera, a singular noun meaning ""work"" and also the plural of the noun opus. According to the Oxford English Dictionary, the Italian word was first used in the sense ""composition in which poetry, dance, and music are combined"" in 1639; the first recorded English usage in this sense dates to 1648.[6]
 Dafne by Jacopo Peri was the earliest composition considered opera, as understood today. It was written around 1597, largely under the inspiration of an elite circle of literate Florentine humanists who gathered as the ""Camerata de' Bardi"". Significantly, Dafne was an attempt to revive the classical Greek drama, part of the wider revival of antiquity characteristic of the Renaissance. The members of the Camerata considered that the ""chorus"" parts of Greek dramas were originally sung, and possibly even the entire text of all roles; opera was thus conceived as a way of ""restoring"" this situation. Dafne, however, is lost. A later work by Peri, Euridice, dating from 1600, is the first opera score to have survived until the present day. However, the honour of being the first opera still to be regularly performed goes to Claudio Monteverdi's L'Orfeo, composed for the court of Mantua in 1607.[7] The Mantua court of the Gonzagas, employers of Monteverdi, played a significant role in the origin of opera employing not only court singers of the concerto delle donne (till 1598), but also one of the first actual ""opera singers"", Madama Europa.[8]
 Opera did not remain confined to court audiences for long. In 1637, the idea of a ""season"" (often during the carnival) of publicly attended operas supported by ticket sales emerged in Venice. Monteverdi had moved to the city from Mantua and composed his last operas, Il ritorno d'Ulisse in patria and L'incoronazione di Poppea, for the Venetian theatre in the 1640s. His most important follower Francesco Cavalli helped spread opera throughout Italy. In these early Baroque operas, broad comedy was blended with tragic elements in a mix that jarred some educated sensibilities, sparking the first of opera's many reform movements, sponsored by the Arcadian Academy, which came to be associated with the poet Metastasio, whose libretti helped crystallize the genre of opera seria, which became the leading form of Italian opera until the end of the 18th century. Once the Metastasian ideal had been firmly established, comedy in Baroque-era opera was reserved for what came to be called opera buffa. Before such elements were forced out of opera seria, many libretti had featured a separately unfolding comic plot as sort of an ""opera-within-an-opera"". One reason for this was an attempt to attract members of the growing merchant class, newly wealthy, but still not as cultured as the nobility, to the public opera houses. These separate plots were almost immediately resurrected in a separately developing tradition that partly derived from the commedia dell'arte, a long-flourishing improvisatory stage tradition of Italy. Just as intermedi had once been performed in between the acts of stage plays, operas in the new comic genre of intermezzi, which developed largely in Naples in the 1710s and 1720s, were initially staged during the intermissions of opera seria. They became so popular, however, that they were soon being offered as separate productions.
 Opera seria was elevated in tone and highly stylised in form, usually consisting of secco recitative interspersed with long da capo arias. These afforded great opportunity for virtuosic singing and during the golden age of opera seria the singer really became the star. The role of the hero was usually written for the high-pitched male castrato voice, which was produced by castration of the singer before puberty, which prevented a boy's larynx from being transformed at puberty. Castrati such as Farinelli and Senesino, as well as female sopranos such as Faustina Bordoni, became in great demand throughout Europe as opera seria ruled the stage in every country except France. Farinelli was one of the most famous singers of the 18th century. Italian opera set the Baroque standard. Italian libretti were the norm, even when a German composer like Handel found himself composing the likes of Rinaldo and Giulio Cesare for London audiences. Italian libretti remained dominant in the classical period as well, for example in the operas of Mozart, who wrote in Vienna near the century's close. Leading Italian-born composers of opera seria include Alessandro Scarlatti, Antonio Vivaldi and Nicola Porpora.[9]
 Opera seria had its weaknesses and critics. The taste for embellishment on behalf of the superbly trained singers, and the use of spectacle as a replacement for dramatic purity and unity drew attacks. Francesco Algarotti's Essay on the Opera (1755) proved to be an inspiration for Christoph Willibald Gluck's reforms. He advocated that opera seria had to return to basics and that all the various elements—music (both instrumental and vocal), ballet, and staging—must be subservient to the overriding drama. In 1765 Melchior Grimm published ""Poème lyrique"", an influential article for the Encyclopédie on lyric and opera librettos.[10][11][12][13][14] Several composers of the period, including Niccolò Jommelli and Tommaso Traetta, attempted to put these ideals into practice. The first to succeed however, was Gluck. Gluck strove to achieve a ""beautiful simplicity"". This is evident in his first reform opera, Orfeo ed Euridice, where his non-virtuosic vocal melodies are supported by simple harmonies and a richer orchestra presence throughout.
 Gluck's reforms have had resonance throughout operatic history. Weber, Mozart, and Wagner, in particular, were influenced by his ideals. Mozart, in many ways Gluck's successor, combined a superb sense of drama, harmony, melody, and counterpoint to write a series of comic operas with libretti by Lorenzo Da Ponte, notably Le nozze di Figaro, Don Giovanni, and Così fan tutte, which remain among the most-loved, popular and well-known operas. But Mozart's contribution to opera seria was more mixed; by his time it was dying away, and in spite of such fine works as Idomeneo and La clemenza di Tito, he would not succeed in bringing the art form back to life again.[15]
 The bel canto opera movement flourished in the early 19th century and is exemplified by the operas of Rossini, Bellini, Donizetti, Pacini, Mercadante and many others. Literally ""beautiful singing"", bel canto opera derives from the Italian stylistic singing school of the same name. Bel canto lines are typically florid and intricate, requiring supreme agility and pitch control. Examples of famous operas in the bel canto style include Rossini's Il barbiere di Siviglia and La Cenerentola, as well as Bellini's Norma, La sonnambula and I puritani and Donizetti's Lucia di Lammermoor, L'elisir d'amore and Don Pasquale.
 Following the bel canto era, a more direct, forceful style was rapidly popularized by Giuseppe Verdi, beginning with his biblical opera Nabucco. This opera, and the ones that would follow in Verdi's career, revolutionized Italian opera, changing it from merely a display of vocal fireworks, with Rossini's and Donizetti's works, to dramatic story-telling. Verdi's operas resonated with the growing spirit of Italian nationalism in the post-Napoleonic era, and he quickly became an icon of the patriotic movement for a unified Italy. In the early 1850s, Verdi produced his three most popular operas: Rigoletto, Il trovatore and La traviata. The first of these, Rigoletto, proved the most daring and revolutionary. In it, Verdi blurs the distinction between the aria and recitative as it never before was, leading the opera to be ""an unending string of duets"". La traviata was also novel. It tells the story of courtesan, and it includes elements of verismo or ""realistic"" opera,[16] because rather than featuring great kings and figures from literature, it focuses on the tragedies of ordinary life and society. After these, he continued to develop his style, composing perhaps the greatest French grand opera, Don Carlos, and ending his career with two Shakespeare-inspired works, Otello and Falstaff, which reveal how far Italian opera had grown in sophistication since the early 19th century. These final two works showed Verdi at his most masterfully orchestrated, and are both incredibly influential, and modern. In Falstaff, Verdi sets the pre-eminent standard for the form and style that would dominate opera throughout the twentieth century. Rather than long, suspended melodies, Falstaff contains many little motifs and mottos, that, rather than being expanded upon, are introduced and subsequently dropped, only to be brought up again later. These motifs never are expanded upon, and just as the audience expects a character to launch into a long melody, a new character speaks, introducing a new phrase. This fashion of opera directed opera from Verdi, onward, exercising tremendous influence on his successors Giacomo Puccini, Richard Strauss, and Benjamin Britten.[17]
 After Verdi, the sentimental ""realistic"" melodrama of verismo appeared in Italy. This was a style introduced by Pietro Mascagni's Cavalleria rusticana and Ruggero Leoncavallo's Pagliacci that came to dominate the world's opera stages with such popular works as Giacomo Puccini's La bohème, Tosca, and Madama Butterfly. Later Italian composers, such as Berio and Nono, have experimented with modernism.[18]
 The first German opera was Dafne, composed by Heinrich Schütz in 1627, but the music score has not survived. Italian opera held a great sway over German-speaking countries until the late 18th century. Nevertheless, native forms would develop in spite of this influence. In 1644, Sigmund Staden produced the first Singspiel, Seelewig, a popular form of German-language opera in which singing alternates with spoken dialogue. In the late 17th century and early 18th century, the Theater am Gänsemarkt in Hamburg presented German operas by Keiser, Telemann and Handel. Yet most of the major German composers of the time, including Handel himself, as well as Graun, Hasse and later Gluck, chose to write most of their operas in foreign languages, especially Italian. In contrast to Italian opera, which was generally composed for the aristocratic class, German opera was generally composed for the masses and tended to feature simple folk-like melodies, and it was not until the arrival of Mozart that German opera was able to match its Italian counterpart in musical sophistication.[19] The theatre company of Abel Seyler pioneered serious German-language opera in the 1770s, marking a break with the previous simpler musical entertainment.[20][21]
 Mozart's Singspiele, Die Entführung aus dem Serail (1782) and Die Zauberflöte (1791) were an important breakthrough in achieving international recognition for German opera. The tradition was developed in the 19th century by Beethoven with his Fidelio (1805), inspired by the climate of the French Revolution. Carl Maria von Weber established German Romantic opera in opposition to the dominance of Italian bel canto. His Der Freischütz (1821) shows his genius for creating a supernatural atmosphere. Other opera composers of the time include Marschner, Schubert and Lortzing, but the most significant figure was undoubtedly Wagner.
 Wagner was one of the most revolutionary and controversial composers in musical history. Starting under the influence of Weber and Meyerbeer, he gradually evolved a new concept of opera as a Gesamtkunstwerk (a ""complete work of art""), a fusion of music, poetry and painting. He greatly increased the role and power of the orchestra, creating scores with a complex web of leitmotifs, recurring themes often associated with the characters and concepts of the drama, of which prototypes can be heard in his earlier operas such as Der fliegende Holländer, Tannhäuser and Lohengrin; and he was prepared to violate accepted musical conventions, such as tonality, in his quest for greater expressivity. In his mature music dramas, Tristan und Isolde, Die Meistersinger von Nürnberg, Der Ring des Nibelungen and Parsifal, he abolished the distinction between aria and recitative in favour of a seamless flow of ""endless melody"". Wagner also brought a new philosophical dimension to opera in his works, which were usually based on stories from Germanic or Arthurian legend. Finally, Wagner built his own opera house at Bayreuth with part of the patronage from Ludwig II of Bavaria, exclusively dedicated to performing his own works in the style he wanted.
 Opera would never be the same after Wagner and for many composers his legacy proved a heavy burden. On the other hand, Richard Strauss accepted Wagnerian ideas but took them in wholly new directions, along with incorporating the new form introduced by Verdi. He first won fame with the scandalous Salome and the dark tragedy Elektra, in which tonality was pushed to the limits. Then Strauss changed tack in his greatest success, Der Rosenkavalier, where Mozart and Viennese waltzes became as important an influence as Wagner. Strauss continued to produce a highly varied body of operatic works, often with libretti by the poet Hugo von Hofmannsthal. Other composers who made individual contributions to German opera in the early 20th century include Alexander von Zemlinsky, Erich Korngold, Franz Schreker, Paul Hindemith, Kurt Weill and the Italian-born Ferruccio Busoni. The operatic innovations of Arnold Schoenberg and his successors are discussed in the section on modernism.[22]
 During the late 19th century, the Austrian composer Johann Strauss II, an admirer of the French-language operettas composed by Jacques Offenbach, composed several German-language operettas, the most famous of which was Die Fledermaus.[23] Nevertheless, rather than copying the style of Offenbach, the operettas of Strauss II had distinctly Viennese flavor to them.
 In rivalry with imported Italian opera productions, a separate French tradition was founded by the Italian-born French composer Jean-Baptiste Lully at the court of King Louis XIV. Despite his foreign birthplace, Lully established an Academy of Music and monopolised French opera from 1672. Starting with Cadmus et Hermione, Lully and his librettist Quinault created tragédie en musique, a form in which dance music and choral writing were particularly prominent. Lully's operas also show a concern for expressive recitative which matched the contours of the French language. In the 18th century, Lully's most important successor was Jean-Philippe Rameau, who composed five tragédies en musique as well as numerous works in other genres such as opéra-ballet, all notable for their rich orchestration and harmonic daring. Despite the popularity of Italian opera seria throughout much of Europe during the Baroque period, Italian opera never gained much of a foothold in France, where its own national operatic tradition was more popular instead.[24] After Rameau's death, the German Gluck was persuaded to produce six operas for the Parisian stage in the 1770s. They show the influence of Rameau, but simplified and with greater focus on the drama. At the same time, by the middle of the 18th century another genre was gaining popularity in France: opéra comique. This was the equivalent of the German singspiel, where arias alternated with spoken dialogue. Notable examples in this style were produced by Monsigny, Philidor and, above all, Grétry. During the Revolutionary and Napoleonic period, composers such as Étienne Méhul, Luigi Cherubini and Gaspare Spontini, who were followers of Gluck, brought a new seriousness to the genre, which had never been wholly ""comic"" in any case. Another phenomenon of this period was the 'propaganda opera' celebrating revolutionary successes, e.g. Gossec's Le triomphe de la République (1793).
 By the 1820s, Gluckian influence in France had given way to a taste for Italian bel canto, especially after the arrival of Rossini in Paris. Rossini's Guillaume Tell helped found the new genre of grand opera, a form whose most famous exponent was another foreigner, Giacomo Meyerbeer. Meyerbeer's works, such as Les Huguenots, emphasised virtuoso singing and extraordinary stage effects. Lighter opéra comique also enjoyed tremendous success in the hands of Boïeldieu, Auber, Hérold and Adam. In this climate, the operas of the French-born composer Hector Berlioz struggled to gain a hearing. Berlioz's epic masterpiece Les Troyens, the culmination of the Gluckian tradition, was not given a full performance for almost a hundred years.
 In the second half of the 19th century, Jacques Offenbach created operetta with witty and cynical works such as Orphée aux enfers, as well as the opera Les Contes d'Hoffmann; Charles Gounod scored a massive success with Faust; and Georges Bizet composed Carmen, which, once audiences learned to accept its blend of Romanticism and realism, became the most popular of all opéra comiques. Jules Massenet, Camille Saint-Saëns and Léo Delibes all composed works which are still part of the standard repertory, examples being Massenet's Manon, Saint-Saëns' Samson et Dalila and Delibes' Lakmé. Their operas formed another genre, the opéra lyrique, combined opéra comique and grand opera. It is less grandiose than grand opera, but without the spoken dialogue of opèra comique. At the same time, the influence of Richard Wagner was felt as a challenge to the French tradition. Many French critics angrily rejected Wagner's music dramas while many French composers closely imitated them with variable success. Perhaps the most interesting response came from Claude Debussy. As in Wagner's works, the orchestra plays a leading role in Debussy's unique opera Pelléas et Mélisande (1902) and there are no real arias, only recitative. But the drama is understated, enigmatic and completely un-Wagnerian.
 Other notable 20th-century names include Ravel, Dukas, Roussel, Honegger and Milhaud. Francis Poulenc is one of the very few post-war composers of any nationality whose operas (which include Dialogues des Carmélites) have gained a foothold in the international repertory. Olivier Messiaen's lengthy sacred drama Saint François d'Assise (1983) has also attracted widespread attention.[25]
 In England, opera's antecedent was the 17th-century jig. This was an afterpiece that came at the end of a play. It was frequently libellous and scandalous and consisted in the main of dialogue set to music arranged from popular tunes. In this respect, jigs anticipate the ballad operas of the 18th century. At the same time, the French masque was gaining a firm hold at the English Court, with even more lavish splendour and highly realistic scenery than had been seen before. Inigo Jones became the quintessential designer of these productions, and this style was to dominate the English stage for three centuries. These masques contained songs and dances. In Ben Jonson's Lovers Made Men (1617), ""the whole masque was sung after the Italian manner, stilo recitativo"".[26] The approach of the English Commonwealth closed theatres and halted any developments that may have led to the establishment of English opera. However, in 1656, the dramatist Sir William Davenant produced The Siege of Rhodes. Since his theatre was not licensed to produce drama, he asked several of the leading composers (Lawes, Cooke, Locke, Coleman and Hudson) to set sections of it to music. This success was followed by The Cruelty of the Spaniards in Peru (1658) and The History of Sir Francis Drake (1659). These pieces were encouraged by Oliver Cromwell because they were critical of Spain. With the English Restoration, foreign (especially French) musicians were welcomed back. In 1673, Thomas Shadwell's Psyche, patterned on the 1671 'comédie-ballet' of the same name produced by Molière and Jean-Baptiste Lully. William Davenant produced The Tempest in the same year, which was the first musical adaption of a Shakespeare play (composed by Locke and Johnson).[26] About 1683, John Blow composed Venus and Adonis, often thought of as the first true English-language opera.
 Blow's immediate successor was the better known Henry Purcell. Despite the success of his masterwork Dido and Aeneas (1689), in which the action is furthered by the use of Italian-style recitative, much of Purcell's best work was not involved in the composing of typical opera, but instead, he usually worked within the constraints of the semi-opera format, where isolated scenes and masques are contained within the structure of a spoken play, such as Shakespeare in Purcell's The Fairy-Queen (1692) and Beaumont and Fletcher in The Prophetess (1690) and Bonduca (1696). The main characters of the play tend not to be involved in the musical scenes, which means that Purcell was rarely able to develop his characters through song. Despite these hindrances, his aim (and that of his collaborator John Dryden) was to establish serious opera in England, but these hopes ended with Purcell's early death at the age of 36.
 Following Purcell, the popularity of opera in England dwindled for several decades. A revived interest in opera occurred in the 1730s which is largely attributed to Thomas Arne, both for his own compositions and for alerting Handel to the commercial possibilities of large-scale works in English. Arne was the first English composer to experiment with Italian-style all-sung comic opera, with his greatest success being Thomas and Sally in 1760. His opera Artaxerxes (1762) was the first attempt to set a full-blown opera seria in English and was a huge success, holding the stage until the 1830s. Although Arne imitated many elements of Italian opera, he was perhaps the only English composer at that time who was able to move beyond the Italian influences and create his own unique and distinctly English voice. His modernized ballad opera, Love in a Village (1762), began a vogue for pastiche opera that lasted well into the 19th century. Charles Burney wrote that Arne introduced ""a light, airy, original, and pleasing melody, wholly different from that of Purcell or Handel, whom all English composers had either pillaged or imitated"".
 Besides Arne, the other dominating force in English opera at this time was George Frideric Handel, whose opera serias filled the London operatic stages for decades and influenced most home-grown composers, like John Frederick Lampe, who wrote using Italian models. This situation continued throughout the 18th and 19th centuries, including in the work of Michael William Balfe, and the operas of the great Italian composers, as well as those of Mozart, Beethoven, and Meyerbeer, continued to dominate the musical stage in England.
 The only exceptions were ballad operas, such as John Gay's The Beggar's Opera (1728), musical burlesques, European operettas, and late Victorian era light operas, notably the Savoy operas of W. S. Gilbert and Arthur Sullivan, all of which types of musical entertainments frequently spoofed operatic conventions. Sullivan wrote only one grand opera, Ivanhoe (following the efforts of a number of young English composers beginning about 1876),[26] but he claimed that even his light operas constituted part of a school of ""English"" opera, intended to supplant the French operettas (usually performed in bad translations) that had dominated the London stage from the mid-19th century into the 1870s. London's Daily Telegraph agreed, describing The Yeomen of the Guard as ""a genuine English opera, forerunner of many others, let us hope, and possibly significant of an advance towards a national lyric stage"".[27] Sullivan produced a few light operas in the 1890s that were of a more serious nature than those in the G&S series, including Haddon Hall and The Beauty Stone, but Ivanhoe (which ran for 155 consecutive performances, using alternating casts—a record until Broadway's La bohème) survives as his only grand opera.
 In the 20th century, English opera began to assert more independence, with works of Ralph Vaughan Williams and in particular Benjamin Britten, who in a series of works that remain in standard repertory today, revealed an excellent flair for the dramatic and superb musicality. More recently Sir Harrison Birtwistle has emerged as one of Britain's most significant contemporary composers from his first opera Punch and Judy to his most recent critical success in The Minotaur. In the first decade of the 21st century, the librettist of an early Birtwistle opera, Michael Nyman, has been focusing on composing operas, including Facing Goya, Man and Boy: Dada, and Love Counts. Today composers such as Thomas Adès continue to export English opera abroad.[28]
 Also in the 20th century, American composers like George Gershwin (Porgy and Bess), Scott Joplin (Treemonisha), Leonard Bernstein (Candide), Gian Carlo Menotti, Douglas Moore, and Carlisle Floyd began to contribute English-language operas infused with touches of popular musical styles. They were followed by composers such as Philip Glass (Einstein on the Beach), Mark Adamo, John Corigliano (The Ghosts of Versailles), Robert Moran, John Adams (Nixon in China), André Previn and Jake Heggie. Many contemporary 21st century opera composers have emerged such as Missy Mazzoli, Kevin Puts, Tom Cipullo, Huang Ruo, David T. Little, Terence Blanchard, Jennifer Higdon, Tobias Picker, Michael Ching, Anthony Davis, and Ricky Ian Gordon.
 Opera was brought to Russia in the 1730s by the Italian operatic troupes and soon it became an important part of entertainment for the Russian Imperial Court and aristocracy. Many foreign composers such as Baldassare Galuppi, Giovanni Paisiello, Giuseppe Sarti, and Domenico Cimarosa (as well as various others) were invited to Russia to compose new operas, mostly in the Italian language. Simultaneously some domestic musicians like Maxim Berezovsky and Dmitry Bortniansky were sent abroad to learn to write operas. The first opera written in Russian was Tsefal i Prokris by the Italian composer Francesco Araja (1755). The development of Russian-language opera was supported by the Russian composers Vasily Pashkevich, Yevstigney Fomin and Alexey Verstovsky.
 However, the real birth of Russian opera came with Mikhail Glinka and his two great operas A Life for the Tsar (1836) and Ruslan and Lyudmila (1842). After him, during the 19th century in Russia, there were written such operatic masterpieces as Rusalka and The Stone Guest by Alexander Dargomyzhsky, Boris Godunov and Khovanshchina by Modest Mussorgsky, Prince Igor by Alexander Borodin, Eugene Onegin and The Queen of Spades by Pyotr Tchaikovsky, and The Snow Maiden and Sadko by Nikolai Rimsky-Korsakov. These developments mirrored the growth of Russian nationalism across the artistic spectrum, as part of the more general Slavophilism movement.
 In the 20th century, the traditions of Russian opera were developed by many composers including Sergei Rachmaninoff in his works The Miserly Knight and Francesca da Rimini, Igor Stravinsky in Le Rossignol, Mavra, Oedipus rex, and The Rake's Progress, Sergei Prokofiev in The Gambler, The Love for Three Oranges, The Fiery Angel, Betrothal in a Monastery, and War and Peace; as well as Dmitri Shostakovich in The Nose and Lady Macbeth of the Mtsensk District, Edison Denisov in L'écume des jours, and Alfred Schnittke in Life with an Idiot and Historia von D. Johann Fausten.[29]
 Czech composers also developed a thriving national opera movement of their own in the 19th century, starting with Bedřich Smetana, who wrote eight operas including the internationally popular The Bartered Bride. Smetana's eight operas created the bedrock of the Czech opera repertory, but of these only The Bartered Bride is performed regularly outside the composer's homeland. After reaching Vienna in 1892 and London in 1895 it rapidly became part of the repertory of every major opera company worldwide.
 Antonín Dvořák's nine operas, except his first, have librettos in Czech and were intended to convey the Czech national spirit, as were some of his choral works. By far the most successful of the operas is Rusalka which contains the well-known aria ""Měsíčku na nebi hlubokém"" (""Song to the Moon""); it is played on contemporary opera stages frequently outside the Czech Republic. This is attributable to their uneven invention and libretti, and perhaps also their staging requirements – The Jacobin, Armida, Vanda and Dimitrij need stages large enough to portray invading armies.
 Leoš Janáček gained international recognition in the 20th century for his innovative works. His later, mature works incorporate his earlier studies of national folk music in a modern, highly original synthesis, first evident in the opera Jenůfa, which was premiered in 1904 in Brno. The success of Jenůfa (often called the ""Moravian national opera"") at Prague in 1916 gave Janáček access to the world's great opera stages. Janáček's later works are his most celebrated. They include operas such as Káťa Kabanová and The Cunning Little Vixen, the Sinfonietta and the Glagolitic Mass.
 Spain also produced its own distinctive form of opera, known as zarzuela, which had two separate flowerings: one from the mid-17th century through the mid-18th century, and another beginning around 1850. During the late 18th century up until the mid-19th century, Italian opera was immensely popular in Spain, supplanting the native form.
 In Russian Eastern Europe, several national operas began to emerge. Ukrainian opera was developed by Semen Hulak-Artemovsky (1813–1873) whose most famous work Zaporozhets za Dunayem (A Cossack Beyond the Danube) is regularly performed around the world. Other Ukrainian opera composers include Mykola Lysenko (Taras Bulba and Natalka Poltavka), Heorhiy Maiboroda, and Yuliy Meitus. At the turn of the century, a distinct national opera movement also began to emerge in Georgia under the leadership Zacharia Paliashvili, who fused local folk songs and stories with 19th-century Romantic classical themes.
 The key figure of Hungarian national opera in the 19th century was Ferenc Erkel, whose works mostly dealt with historical themes. Among his most often performed operas are Hunyadi László and Bánk bán. The most famous modern Hungarian opera is Béla Bartók's Duke Bluebeard's Castle.
 Stanisław Moniuszko's opera Straszny Dwór (in English The Haunted Manor) (1861–64) represents a nineteenth-century peak of Polish national opera.[30] In the 20th century, other operas created by Polish composers included King Roger by Karol Szymanowski and Ubu Rex by Krzysztof Penderecki.
 The first known opera from Turkey (the Ottoman Empire) was Arshak II, which was an Armenian opera composed by an ethnic Armenian composer Tigran Chukhajian in 1868 and partially performed in 1873. It was fully staged in 1945 in Armenia.
 The first years of the Soviet Union saw the emergence of new national operas, such as the Koroğlu (1937) by the Azerbaijani composer Uzeyir Hajibeyov. The first Kyrgyz opera, Ai-Churek, premiered in Moscow at the Bolshoi Theatre on 26 May 1939, during Kyrgyz Art Decade. It was composed by Vladimir Vlasov, Abdylas Maldybaev and Vladimir Fere. The libretto was written by Joomart Bokonbaev, Jusup Turusbekov, and Kybanychbek Malikov. The opera is based on the Kyrgyz heroic epic Manas.[31][32]
 In Iran, opera gained more attention after the introduction of Western classical music in the late 19th century. However, it took until mid 20th century for Iranian composers to start experiencing with the field, especially as the construction of the Roudaki Hall in 1967, made possible staging of a large variety of works for stage. Perhaps, the most famous Iranian opera is Rostam and Sohrab by Loris Tjeknavorian premiered not until the early 2000s.
 Chinese contemporary classical opera, a Chinese language form of Western style opera that is distinct from traditional Chinese opera, has had operas dating back to The White-Haired Girl in 1945.[33][34][35]
 In Latin America, opera started as a result of European colonisation. The first opera ever written in the Americas was La púrpura de la rosa, by Tomás de Torrejón y Velasco, although Partenope, by the Mexican Manuel de Zumaya, was the first opera written from a composer born in Latin America (music now lost). The first Brazilian opera for a libretto in Portuguese was A Noite de São João, by Elias Álvares Lobo. However, Antônio Carlos Gomes is generally regarded as the most outstanding Brazilian composer, having a relative success in Italy with its Brazilian-themed operas with Italian librettos, such as Il Guarany. Opera in Argentina developed in the 20th century after the inauguration of Teatro Colón in Buenos Aires—with the opera Aurora, by Ettore Panizza, being heavily influenced by the Italian tradition, due to immigration. Other important composers from Argentina include Felipe Boero and Alberto Ginastera.
 Perhaps the most obvious stylistic manifestation of modernism in opera is the development of atonality. The move away from traditional tonality in opera had begun with Richard Wagner, and in particular the Tristan chord. Composers such as Richard Strauss, Claude Debussy, Giacomo Puccini,[36] Paul Hindemith, Benjamin Britten and Hans Pfitzner pushed Wagnerian harmony further with a more extreme use of chromaticism and greater use of dissonance. Another aspect of modernist opera is the shift away from long, suspended melodies, to short quick mottos, as first illustrated by Giuseppe Verdi in his Falstaff. Composers such as Strauss, Britten, Shostakovich and Stravinsky adopted and expanded upon this style.
 Operatic modernism truly began in the operas of two Viennese composers, Arnold Schoenberg and his student Alban Berg, both composers and advocates of atonality and its later development (as worked out by Schoenberg), dodecaphony. Schoenberg's early musico-dramatic works, Erwartung (1909, premiered in 1924) and Die glückliche Hand display heavy use of chromatic harmony and dissonance in general. Schoenberg also occasionally used Sprechstimme.
 The two operas of Schoenberg's pupil Alban Berg, Wozzeck (1925) and Lulu (incomplete at his death in 1935) share many of the same characteristics as described above, though Berg combined his highly personal interpretation of Schoenberg's twelve-tone technique with melodic passages of a more traditionally tonal nature (quite Mahlerian in character) which perhaps partially explains why his operas have remained in standard repertory, despite their controversial music and plots. Schoenberg's theories have influenced (either directly or indirectly) significant numbers of opera composers ever since, even if they themselves did not compose using his techniques.
 Composers thus influenced include the Englishman Benjamin Britten, the German Hans Werner Henze, and the Russian Dmitri Shostakovich. (Philip Glass also makes use of atonality, though his style is generally described as minimalist, usually thought of as another 20th-century development.)[37]
 However, operatic modernism's use of atonality also sparked a backlash in the form of neoclassicism. An early leader of this movement was Ferruccio Busoni, who in 1913 wrote the libretto for his neoclassical number opera Arlecchino (first performed in 1917).[38] Also among the vanguard was the Russian Igor Stravinsky. After composing music for the Diaghilev-produced ballets Petrushka (1911) and The Rite of Spring (1913), Stravinsky turned to neoclassicism, a development culminating in his opera-oratorio Oedipus rex (1927). Stravinsky had already turned away from the modernist trends of his early ballets to produce small-scale works that do not fully qualify as opera, yet certainly contain many operatic elements, including Renard (1916: ""a burlesque in song and dance"") and The Soldier's Tale (1918: ""to be read, played, and danced""; in both cases the descriptions and instructions are those of the composer). In the latter, the actors declaim portions of speech to a specified rhythm over instrumental accompaniment, peculiarly similar to the older German genre of Melodrama. Well after his Rimsky-Korsakov-inspired works The Nightingale (1914), and Mavra (1922), Stravinsky continued to ignore serialist technique and eventually wrote a full-fledged 18th-century-style diatonic number opera The Rake's Progress (1951). His resistance to serialism (an attitude he reversed following Schoenberg's death) proved to be an inspiration for many[who?] other composers.[39]
 A common trend throughout the 20th century, in both opera and general orchestral repertoire, is the use of smaller orchestras as a cost-cutting measure; the grand Romantic-era orchestras with huge string sections, multiple harps, extra horns, and exotic percussion instruments were no longer feasible. As government and private patronage of the arts decreased throughout the 20th century, new works were often commissioned and performed with smaller budgets, very often resulting in chamber-sized works, and short, one-act operas. Many of Benjamin Britten's operas are scored for as few as 13 instrumentalists; Mark Adamo's two-act realization of Little Women is scored for 18 instrumentalists.
 Another feature of late 20th-century opera is the emergence of contemporary historical operas, in contrast to the tradition of basing operas on more distant history, the re-telling of contemporary fictional stories or plays, or on myth or legend. The Death of Klinghoffer, Nixon in China, and Doctor Atomic by John Adams, Dead Man Walking by Jake Heggie, Anna Nicole by Mark-Anthony Turnage, and Waiting for Miss Monroe[40] by Robin de Raaff exemplify the dramatisation onstage of events in recent living memory, where characters portrayed in the opera were alive at the time of the premiere performance.
 The Metropolitan Opera in the US (often known as the Met) reported in 2011 that the average age of its audience was 60.[41] Many opera companies attempted to attract a younger audience to halt the larger trend of greying audiences for classical music since the last decades of the 20th century.[42] Efforts resulted in lowering the average age of the Met's audience to 58 in 2018, the average age at Berlin State Opera was reported as 54, and Paris Opera reported an average age of 48.[43] New York Times critic Anthony Tommasini has suggested that ""companies inordinately beholden to standard repertory"" are not reaching younger, more curious audiences.[44]
 Smaller companies in the US have a more fragile existence, and they usually depend on a ""patchwork quilt"" of support from state and local governments, local businesses, and fundraisers. Nevertheless, some smaller companies have found ways of drawing new audiences. In addition to radio and television broadcasts of opera performances, which have had some success in gaining new audiences, broadcasts of live performances to movie theatres have shown the potential to reach new audiences.[45]
 By the late 1930s, some musicals began to be written with a more operatic structure. These works include complex polyphonic ensembles and reflect musical developments of their times. Porgy and Bess (1935), influenced by jazz styles, and Candide (1956), with its sweeping, lyrical passages and farcical parodies of opera, both opened on Broadway but became accepted as part of the opera repertory. Popular musicals such as Show Boat, West Side Story, Brigadoon, Sweeney Todd, Passion, Evita, The Light in the Piazza, The Phantom of the Opera and others tell dramatic stories through complex music and in the 2010s they are sometimes seen in opera houses.[46] The Most Happy Fella (1952) is quasi-operatic and has been revived by the New York City Opera. Other rock-influenced musicals, such as Tommy (1969) and Jesus Christ Superstar (1971), Les Misérables (1980), Rent (1996), Spring Awakening (2006), and Natasha, Pierre & The Great Comet of 1812 (2012) employ various operatic conventions, such as through composition, recitative instead of dialogue, and leitmotifs.
 A subtle type of sound electronic reinforcement called acoustic enhancement is used in some modern concert halls and theatres where operas are performed. Although none of the major opera houses ""...use traditional, Broadway-style sound reinforcement, in which most if not all singers are equipped with radio microphones mixed to a series of unsightly loudspeakers scattered throughout the theatre"", many use a sound reinforcement system for acoustic enhancement and for subtle boosting of offstage voices, child singers, onstage dialogue, and sound effects (e.g., church bells in Tosca or thunder effects in Wagnerian operas).[47]
 Operatic vocal technique evolved, in a time before electronic amplification, to allow singers to produce enough volume to be heard over an orchestra, without the instrumentalists having to substantially compromise their volume.
 Singers and the roles they play are classified by voice type, based on the tessitura, agility, power and timbre of their voices. Male singers can be classified by vocal range as bass, bass-baritone, baritone, baritenor, tenor and countertenor, and female singers as contralto, mezzo-soprano and soprano. (Men sometimes sing in the ""female"" vocal ranges, in which case they are termed sopranist or countertenor. The countertenor is commonly encountered in opera, sometimes singing parts written for castrati—men neutered at a young age specifically to give them a higher singing range.) Singers are then further classified by size—for instance, a soprano can be described as a lyric soprano, coloratura, soubrette, spinto, or dramatic soprano. These terms, although not fully describing a singing voice, associate the singer's voice with the roles most suitable to the singer's vocal characteristics.
 Yet another sub-classification can be made according to acting skills or requirements, for example the basso buffo who often must be a specialist in patter as well as a comic actor. This is carried out in detail in the Fach system of German speaking countries, where historically opera and spoken drama were often put on by the same repertory company.
 A particular singer's voice may change drastically over his or her lifetime, rarely reaching vocal maturity until the third decade, and sometimes not until middle age. Two French voice types, premiere dugazon and deuxieme dugazon, were named after successive stages in the career of Louise-Rosalie Lefebvre (Mme. Dugazon). Other terms originating in the star casting system of the Parisian theatres are baryton-martin and soprano falcon.
 The soprano voice has typically been used as the voice of choice for the female protagonist of the opera since the latter half of the 18th century. Earlier, it was common for that part to be sung by any female voice, or even a castrato. The current emphasis on a wide vocal range was primarily an invention of the Classical period. Before that, the vocal virtuosity, not range, was the priority, with soprano parts rarely extending above a high A (Handel, for example, only wrote one role extending to a high C), though the castrato Farinelli was alleged to possess a top D (his lower range was also extraordinary, extending to tenor C). The mezzo-soprano, a term of comparatively recent origin, also has a large repertoire, ranging from the female lead in Purcell's Dido and Aeneas to such heavyweight roles as Brangäne in Wagner's Tristan und Isolde (these are both roles sometimes sung by sopranos; there is quite a lot of movement between these two voice-types). For the true contralto, the range of parts is more limited, which has given rise to the insider joke that contraltos only sing ""witches, bitches, and britches"" roles. In recent years many of the ""trouser roles"" from the Baroque era, originally written for women, and those originally sung by castrati, have been reassigned to countertenors.
 The tenor voice, from the Classical era onwards, has traditionally been assigned the role of male protagonist. Many of the most challenging tenor roles in the repertory were written during the bel canto era, such as Donizetti's sequence of 9 Cs above middle C during La fille du régiment. With Wagner came an emphasis on vocal heft for his protagonist roles, with this vocal category described as Heldentenor; this heroic voice had its more Italianate counterpart in such roles as Calaf in Puccini's Turandot. Basses have a long history in opera, having been used in opera seria in supporting roles, and sometimes for comic relief (as well as providing a contrast to the preponderance of high voices in this genre). The bass repertoire is wide and varied, stretching from the comedy of Leporello in Don Giovanni to the nobility of Wotan in Wagner's Ring Cycle, to the conflicted King Phillip of Verdi's Don Carlos. In between the bass and the tenor is the baritone, which also varies in weight from say, Guglielmo in Mozart's Così fan tutte to Posa in Verdi's Don Carlos; the actual designation ""baritone"" was not standard until the mid-19th century.
 Early performances of opera were too infrequent for singers to make a living exclusively from the style, but with the birth of commercial opera in the mid-17th century, professional performers began to emerge. The role of the male hero was usually entrusted to a castrato, and by the 18th century, when Italian opera was performed throughout Europe, leading castrati who possessed extraordinary vocal virtuosity, such as Senesino and Farinelli, became international stars. The career of the first major female star (or prima donna), Anna Renzi, dates to the mid-17th century. In the 18th century, a number of Italian sopranos gained international renown and often engaged in fierce rivalry, as was the case with Faustina Bordoni and Francesca Cuzzoni, who started a fistfight with one another during a performance of a Handel opera. The French disliked castrati, preferring their male heroes to be sung by an haute-contre (a high tenor), of which Joseph Legros (1739–1793) was a leading example.[48]
 Though opera patronage has decreased in the last century in favor of other arts and media (such as musicals, cinema, radio, television and recordings), mass media and the advent of recording have supported the popularity of many famous singers including Anna Netrebko, Maria Callas, Enrico Caruso, Amelita Galli-Curci, Kirsten Flagstad, Mario Del Monaco, Renata Tebaldi, Risë Stevens, Alfredo Kraus, Franco Corelli, Montserrat Caballé, Joan Sutherland, Birgit Nilsson, Nellie Melba, Rosa Ponselle, Beniamino Gigli, Jussi Björling, Feodor Chaliapin, Cecilia Bartoli, Elena Obraztsova, Renée Fleming, Galina Vishnevskaya, Marilyn Horne, Bryn Terfel, Dmitri Hvorostovsky and The Three Tenors (Luciano Pavarotti, Plácido Domingo, José Carreras).
 Before the 1700s, Italian operas used a small string orchestra, but it rarely played to accompany the singers. Opera solos during this period were accompanied by the basso continuo group, which consisted of the harpsichord, ""plucked instruments"" such as lute and a bass instrument.[49] The string orchestra typically only played when the singer was not singing, such as during a singer's ""...entrances and exits, between vocal numbers, [or] for [accompanying] dancing"". Another role for the orchestra during this period was playing an orchestral ritornello to mark the end of a singer's solo.[49] During the early 1700s, some composers began to use the string orchestra to mark certain aria or recitatives ""...as special""; by 1720, most arias were accompanied by an orchestra. Opera composers such as Domenico Sarro, Leonardo Vinci, Giambattista Pergolesi, Leonardo Leo, and Johann Adolph Hasse added new instruments to the opera orchestra and gave the instruments new roles. They added wind instruments to the strings and used orchestral instruments to play instrumental solos, as a way to mark certain arias as special.[49]
 The orchestra has also provided an instrumental overture before the singers come onstage since the 1600s. Peri's Euridice opens with a brief instrumental ritornello, and Monteverdi's L'Orfeo (1607) opens with a toccata, in this case a fanfare for muted trumpets. The French overture as found in Jean-Baptiste Lully's operas[50] consist of a slow introduction in a marked ""dotted rhythm"", followed by a lively movement in fugato style. The overture was frequently followed by a series of dance tunes before the curtain rose. This overture style was also used in English opera, most notably in Henry Purcell's Dido and Aeneas. Handel also uses the French overture form in some of his Italian operas such as Giulio Cesare.[51]
 In Italy, a distinct form called ""overture"" arose in the 1680s, and became established particularly through the operas of Alessandro Scarlatti, and spread throughout Europe, supplanting the French form as the standard operatic overture by the mid-18th century.[52] It uses three generally homophonic movements: fast–slow–fast. The opening movement was normally in duple metre and in a major key; the slow movement in earlier examples was short, and could be in a contrasting key; the concluding movement was dance-like, most often with rhythms of the gigue or minuet, and returned to the key of the opening section. As the form evolved, the first movement may incorporate fanfare-like elements and took on the pattern of so-called ""sonatina form"" (sonata form without a development section), and the slow section became more extended and lyrical.[52]
 In Italian opera after about 1800, the ""overture"" became known as the sinfonia.[53] Fisher also notes the term Sinfonia avanti l'opera (literally, the ""symphony before the opera"") was ""an early term for a sinfonia used to begin an opera, that is, as an overture as opposed to one serving to begin a later section of the work"".[53] In 19th-century opera, in some operas, the overture, Vorspiel, Einleitung, Introduction, or whatever else it may be called, was the portion of the music which takes place before the curtain rises; a specific, rigid form was no longer required for the overture.
 The role of the orchestra in accompanying the singers changed over the 19th century, as the Classical style transitioned to the Romantic era. In general, orchestras got bigger, new instruments were added, such as additional percussion instruments (e.g., bass drum, cymbals, snare drum, etc.). The orchestration of orchestra parts also developed over the 19th century. In Wagnerian operas, the forefronting of the orchestra went beyond the overture. In Wagnerian operas such as the Ring Cycle, the orchestra often played the recurrent musical themes or leitmotifs, a role which gave a prominence to the orchestra which ""...elevated its status to that of a prima donna"".[54] Wagner's operas were scored with unprecedented scope and complexity, adding more brass instruments and huge ensemble sizes: indeed, his score to Das Rheingold calls for six harps. In Wagner and the work of subsequent composers, such as Benjamin Britten, the orchestra ""often communicates facts about the story that exceed the levels of awareness of the characters therein. As a result, critics began to regard the orchestra as performing a role analogous to that of a literary narrator.""[55]
 As the role of the orchestra and other instrumental ensembles changed over the history of opera, so did the role of leading the musicians. In the Baroque era, the musicians were usually directed by the harpsichord player, although the French composer Lully is known to have conducted with a long staff. In the 1800s, during the Classical period, the first violinist, also known as the concertmaster, would lead the orchestra while sitting. Over time, some directors began to stand up and use hand and arm gestures to lead the performers. Eventually this role of music director became termed the conductor, and a podium was used to make it easier for all the musicians to see him or her. By the time Wagnerian operas were introduced, the complexity of the works and the huge orchestras used to play them gave the conductor an increasingly important role. Modern opera conductors have a challenging role: they have to direct both the orchestra in the orchestra pit and the singers on stage.
 Since the days of Handel and Mozart, many composers have favored Italian as the language for the libretto of their operas. From the Bel Canto era to Verdi, composers would sometimes supervise versions of their operas in both Italian and French. Because of this, operas such as Lucia di Lammermoor or Don Carlos are today deemed canonical in both their French and Italian versions.[56]
 Until the mid-1950s, it was acceptable to produce operas in translations even if these had not been authorized by the composer or the original librettists. For example, opera houses in Italy routinely staged Wagner in Italian.[57] After World War II, opera scholarship improved, artists refocused on the original versions, and translations fell out of favor. Knowledge of European languages, especially Italian, French, and German, is today an important part of the training for professional singers. ""The biggest chunk of operatic training is in linguistics and musicianship"", explains mezzo-soprano Dolora Zajick. ""[I have to understand] not only what I'm singing, but what everyone else is singing. I sing Italian, Czech, Russian, French, German, English.""[58]
 In the 1980s, supertitles (sometimes called surtitles) began to appear. Although supertitles were first almost universally condemned as a distraction,[59] today many opera houses provide either supertitles, generally projected above the theatre's proscenium arch, or individual seat screens where spectators can choose from more than one language. TV broadcasts typically include subtitles even if intended for an audience who knows well the language (for example, a RAI broadcast of an Italian opera). These subtitles target not only the hard of hearing but the audience generally, since a sung discourse is much harder to understand than a spoken one—even in the ears of native speakers. Subtitles in one or more languages have become standard in opera broadcasts, simulcasts, and DVD editions.
 Today, operas are only rarely performed in translation. Exceptions include the English National Opera, the Opera Theatre of Saint Louis, Opera Theater of Pittsburgh, and Opera South East,[60] which favor English translations.[61] Another exception are opera productions intended for a young audience, such as Humperdinck's Hansel and Gretel[62] and some productions of Mozart's The Magic Flute.[63]
 Outside the US, and especially in Europe, most opera houses receive public subsidies from taxpayers.[64] In Milan, Italy, 60% of La Scala's annual budget of €115 million is from ticket sales and private donations, with the remaining 40% coming from public funds.[65] In 2005, La Scala received 25% of Italy's total state subsidy of €464 million for the performing arts.[66] In the UK, Arts Council England provides funds to Opera North, the Royal Opera House, Welsh National Opera, and English National Opera. Between 2012 and 2015, these four opera companies along with the English National Ballet, Birmingham Royal Ballet and Northern Ballet accounted for 22% of the funds in the Arts Council's national portfolio. During that period, the Council undertook an analysis of its funding for large-scale opera and ballet companies, setting recommendations and targets for the companies to meet prior to the 2015–2018 funding decisions.[67] In February 2015, concerns over English National Opera's business plan led to the Arts Council placing it ""under special funding arrangements"" in what The Independent termed ""the unprecedented step"" of threatening to withdraw public funding if the council's concerns were not met by 2017.[68] European public funding to opera has led to a disparity between the number of year-round opera houses in Europe and the United States. For example, ""Germany has about 80 year-round opera houses [as of 2004], while the U.S., with more than three times the population, does not have any. Even the Met only has a seven-month season.""[69]
 A milestone for opera broadcasting in the U.S. was achieved on 24 December 1951, with the live broadcast of Amahl and the Night Visitors, an opera in one act by Gian Carlo Menotti. It was the first opera specifically composed for television in America.[70] Another milestone occurred in Italy in 1992 when Tosca was broadcast live from its original Roman settings and times of the day: the first act came from the 16th-century Church of Sant'Andrea della Valle at noon on Saturday; the 16th-century Palazzo Farnese was the setting for the second at 8:15 pm; and on Sunday at 6 am, the third act was broadcast from Castel Sant'Angelo. The production was transmitted via satellite to 105 countries.[71]
 Major opera companies have begun presenting their performances in local cinemas throughout the United States and many other countries. The Metropolitan Opera began a series of live high-definition video transmissions to cinemas around the world in 2006.[72] In 2007, Met performances were shown in over 424 theaters in 350 U.S. cities. La bohème went out to 671 screens worldwide. San Francisco Opera began prerecorded video transmissions in March 2008. As of June 2008, approximately 125 theaters in 117 U.S. cities carry the showings. The HD video opera transmissions are presented via the same HD digital cinema projectors used for major Hollywood films.[73] European opera houses and festivals including The Royal Opera in London, La Scala in Milan, the Salzburg Festival, La Fenice in Venice, and the Maggio Musicale in Florence have also transmitted their productions to theaters in cities around the world since 2006, including 90 cities in the U.S.[74][75]
 The emergence of the Internet has also affected the way in which audiences consume opera. In 2009 the British Glyndebourne Festival Opera offered for the first time an online digital video download of its complete 2007 production of Tristan und Isolde. In the 2013 season, the festival streamed all six of its productions online.[76][77] In July 2012, the first online community opera was premiered at the Savonlinna Opera Festival. Titled Free Will, it was created by members of the Internet group Opera By You. Its 400 members from 43 countries wrote the libretto, composed the music, and designed the sets and costumes using the Wreckamovie web platform. Savonlinna Opera Festival provided professional soloists, an 80-member choir, a symphony orchestra, and the stage machinery. It was performed live at the festival and streamed live on the internet.[78]
 Sources
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['national folk music', 'Enrico Caruso and Maria Callas', 'tragedies of ordinary life and society', 'War and Peace', 'uneven invention and libretti, and perhaps also their staging requirements'], 'answer_start': [], 'answer_end': []}"
"
 Ballet (French: [balɛ]) is a type of performance dance that originated during the Italian Renaissance in the fifteenth century and later developed into a concert dance form in France and Russia. It has since become a widespread and highly technical form of dance with its own vocabulary. Ballet has been influential globally and has defined the foundational techniques which are used in many other dance genres and cultures. Various schools around the world have incorporated their own cultures. As a result, ballet has evolved in distinct ways.
 A ballet as a unified work comprises the choreography and music for a ballet production. Ballets are choreographed and performed by trained ballet dancers. Traditional classical ballets are usually performed with classical music accompaniment and use elaborate costumes and staging, whereas modern ballets are often performed in simple costumes and without elaborate sets or scenery.
 Ballet is a French word which had its origin in Italian balletto, a diminutive of ballo (dance) which comes from Latin ballo, ballare, meaning ""to dance"",[1][2] which in turn comes from the Greek ""βαλλίζω"" (ballizo), ""to dance, to jump about"".[2][3] The word came into English usage from the French around 1630.
 Ballet originated in the Italian Renaissance courts of the fifteenth and sixteenth centuries. Under Catherine de' Medici's influence as Queen, it spread to France, where it developed even further.[4] The dancers in these early court ballets were mostly noble amateurs.  Ornamented costumes were meant to impress viewers, but they restricted performers' freedom of movement.[5]
 The ballets were performed in large chambers with viewers on three sides.  The implementation of the proscenium arch from 1618 on distanced performers from audience members, who could then better view and appreciate the technical feats of the professional dancers in the productions.[6][7]
 French court ballet reached its height under the reign of King Louis XIV. Louis founded the Académie Royale de Danse (Royal Dance Academy) in 1661 to establish standards and certify dance instructors.[8] In 1672, Louis XIV made Jean-Baptiste Lully the director of the Académie Royale de Musique (Paris Opera) from which the first professional ballet company, the Paris Opera Ballet, arose.[9] Pierre Beauchamp served as Lully's ballet-master. Together their partnership would drastically influence the development of ballet, as evidenced by the credit given to them for the creation of the five major positions of the feet.  By 1681, the first ""ballerinas"" took the stage following years of training at the Académie.[5]
 Ballet started to decline in France after 1830, but it continued to develop in Denmark, Italy, and Russia. The arrival in Europe of the Ballets Russes led by Sergei Diaghilev on the eve of the First World War revived interest in the ballet and started the modern era.[10]
 In the twentieth century, ballet had a wide influence on other dance genres,[11]  Also in the twentieth century, ballet took a turn dividing it from classical ballet to the introduction of modern dance, leading to modernist movements in several countries.[12]
 Famous dancers of the twentieth century include Anna Pavlova, Galina Ulanova, Rudolf Nureyev, Maya Plisetskaya, Margot Fonteyn, Rosella Hightower, Maria Tall Chief, Erik Bruhn, Mikhail Baryshnikov, Suzanne Farrell, Gelsey Kirkland, Natalia Makarova, Arthur Mitchell, and Jeanne Devereaux.[13] Jeanne Devereaux performed as a prima ballerina for three decades and set a world's record by being able to execute 16 triple fouettes.[14]
 Stylistic variations and subgenres have evolved over time. Early, classical variations are primarily associated with geographic origin. Examples of this are Russian ballet, French ballet, and Italian ballet. Later variations, such as contemporary ballet and neoclassical ballet, incorporate both classical ballet and non-traditional technique and movement. Perhaps the most widely known and performed ballet style is late Romantic ballet (or Ballet blanc).
 Classical ballet is based on traditional ballet technique and vocabulary.[15] Different styles have emerged in different countries, such as French ballet, Italian ballet, English ballet, and Russian ballet. Several of the classical ballet styles are associated with specific training methods, typically named after their creators (see below).  The Royal Academy of Dance method is a ballet technique and training system that was founded by a diverse group of ballet dancers. They merged their respective dance methods (Italian, French, Danish and Russian) to create a new style of ballet that is unique to the organization and is recognized internationally as the English style of ballet.[10] Some examples of classical ballet productions are: Swan Lake, The Sleeping Beauty and The Nutcracker.
 Romantic ballet was an artistic movement of classical ballet and several productions remain in the classical repertoire today. The Romantic era was marked by the emergence of pointe work, the dominance of female dancers, and longer, flowy tutus that attempt to exemplify softness and a delicate aura.[5] This movement occurred during the early to mid-nineteenth century (the Romantic era) and featured themes that emphasized intense emotion as a source of aesthetic experience. The plots of many romantic ballets revolved around spirit women (sylphs, wilis, and ghosts) who enslaved the hearts and senses of mortal men. The 1827 ballet La Sylphide is widely considered to be the first, and the 1870 ballet Coppélia is considered to be the last.[4] Famous ballet dancers of the Romantic era include Marie Taglioni, Fanny Elssler, and Jules Perrot. Jules Perrot is also known for his choreography, especially that of Giselle, often considered to be the most widely celebrated romantic ballet.[5]
 Neoclassical ballet is usually abstract, with no clear plot, costumes or scenery. Music choice can be diverse and will often include music that is also neoclassical (e.g. Stravinsky, Roussel). Tim Scholl, author of From Petipa to Balanchine, considers George Balanchine's Apollo in 1928 to be the first neoclassical ballet. Apollo represented a return to form in response to Sergei Diaghilev's abstract ballets. Balanchine worked with modern dance choreographer Martha Graham, and brought modern dancers into his company such as Paul Taylor, who in 1959 performed in Balanchine's Episodes.[16]
 While Balanchine is widely considered the face of neoclassical ballet, there were others who made significant contributions. Frederick Ashton's Symphonic Variations (1946) is a seminal work for the choreographer. Set to César Franck's score of the same title, it is a pure-dance interpretation of the score.[5]
 Another form, Modern Ballet, also emerged as an offshoot of neoclassicism. Among the innovators in this form were Glen Tetley, Robert Joffrey and Gerald Arpino. While difficult to parse modern ballet from neoclassicism, the work of these choreographers favored a greater athleticism that departed from the delicacy of ballet. The physicality was more daring, with mood, subject matter and music more intense. An example of this would be Joffrey's Astarte (1967), which featured a rock score and sexual overtones in the choreography.[10]
 This ballet style is often performed barefoot. Contemporary ballets may include mime and acting, and are usually set to music (typically orchestral but occasionally vocal). It can be difficult to differentiate this form from neoclassical or modern ballet. Contemporary ballet is also close to contemporary dance because many contemporary ballet concepts come from the ideas and innovations of twentieth-century modern dance, including floor work and turn-in of the legs. The main distinction is that ballet technique is essential to perform a contemporary ballet.
 George Balanchine is considered to have been a pioneer of contemporary ballet. Another early contemporary ballet choreographer, Twyla Tharp, choreographed Push Comes To Shove for the American Ballet Theatre in 1976, and in 1986 created In The Upper Room for her own company. Both of these pieces were considered innovative for their melding of distinctly modern movements with the use of pointe shoes and classically trained dancers.
 Today there are many contemporary ballet companies and choreographers. These include Alonzo King and his company LINES Ballet; Matthew Bourne and his company New Adventures; Complexions Contemporary Ballet; Nacho Duato and his Compañia Nacional de Danza; William Forsythe and The Forsythe Company; and Jiří Kylián of the Nederlands Dans Theater. Traditionally ""classical"" companies, such as the Mariinsky (Kirov) Ballet and the Paris Opera Ballet, also regularly perform contemporary works.
 The term ballet has evolved to include all forms associated with it. Someone training as a ballet dancer will now be expected to perform neoclassical, modern and contemporary work. A ballet dancer is expected to be able to be stately and regal for classical work, free and lyrical in neoclassical work, and unassuming, harsh or pedestrian for modern and contemporary work. In addition, there are several modern varieties of dance that fuse classical ballet technique with contemporary dance, such as Hiplet, that require dancers to be practised in non-Western dance styles.[17]
 There are six widely used, internationally recognized methods to teach or study ballet. These methods are the French School, the Vaganova Method, the Cecchetti Method, the Bournonville method, the Royal Academy of Dance method (English style), and the Balanchine method (American style).[18][19] Many more schools of technique exist in various countries.
 Although preschool-age children are a lucrative source of income for a ballet studio, ballet instruction is generally not appropriate for young children.[20] [failed verification] Initial instruction requires standing still and concentrating on posture, rather than dancing. Because of this, many ballet programs have historically not accepted students until approximately age 8. Creative movement and non-demanding pre-ballet classes are recommended as alternatives for children.[21][22]
 The French method is the basis of all ballet training. When Louis XIV created the Académie Royale de Danse in 1661, he helped to create the codified technique still used today by those in the profession, regardless of what method of training they adhere to. The French school was particularly revitalized under Rudolf Nureyev, in the 1980s. His influence revitalized and renewed appreciation for this style, and has drastically shaped ballet as a whole.[23]  In fact, the French school is now sometimes referred to as Nureyev school. The French method is often characterized by technical precision, fluidity and gracefulness, and elegant, clean lines. For this style, fast footwork is often utilized in order to give the impression that the performers are drifting lightly across the stage.[24] Two important trademarks of this technique are the specific way in which the port de bras and the épaulement are performed, more rounded than when dancing in a Russian style, but not as rounded as the Danish style.[25]
 The Vaganova method is a style of ballet training that emerged from Russian ballet, created by Agrippina Vaganova. After retiring from dance in 1916, Vaganova turned to teaching at the Leningrad Choreographic School in 1921. Her training method is now internationally recognized and her book, The Fundamentals of Classical Dance (1934), is a classic reference. This method is marked by the fusion of the classical French style, specifically elements from the Romantic era, with the athleticism of the Italian method, and the soulful passion of Russian ballet.[24] She developed an extremely precise method of instruction in her book Basic Principles of Russian Classical dance (1948). This includes outlining when to teach technical components to students in their ballet careers, for how long to focus on it, and the right amount of focus at each stage of the student's career. These textbooks continue to be extremely important to the instruction of ballet today.
 The method emphasizes development of strength, flexibility, and endurance for the proper performance of ballet. She espoused the belief that equal importance should be placed on the arms and legs while performing ballet, as this will bring harmony and greater expression to the body as a whole.[26]
 Developed by Enrico Cecchetti (1850–1928), this method is one known internationally for its intense reliance of the understanding of anatomy as it relates to classical ballet. The goal of this method is to instill important characteristics for the performance of ballet into students so that they do not need to rely on imitations of teachers. Important components for this method is the emphasis of balance, elevations, ballon, poise, and strength.
 This method espouses the importance of recognizing that all parts of the body move together to create beautiful, graceful lines, and as such cautions against thinking of ballet in terms of the arms, legs, and neck and torso as separate parts. This method is well known for eight port de bras that are utilized.[24]
 The Bournonville method is a Danish method first devised by August Bournonville. Bournonville was heavily influenced by the early French ballet method due to his training with his father, Antoine Bournonville and other important French ballet masters. This method has many style differences that differentiate it from other ballet methods taught today.[27] A key component is the use of diagonal épaulements, with the upper body turning towards the working foot typically. This method also incorporates very basic use of arms, pirouettes from a low développé position into seconde, and use of fifth position bras en bas for the beginning and end of movements.
 The Bournonville method produces dancers who have beautiful ballon (""the illusion of imponderable lightness""[28]).
 The Royal Academy of Dance method, also referred to as the English style of ballet, was established in 1920 by Genee, Karsavina, Bedells, E Espinosa, and Richardson. The goal of this method is to promote academic training in classical ballet throughout Great Britain. This style also spread to the United States, and is widely utilized still today. There are specific grade levels which a student must move through in order to complete training in this method.[29] The key principle behind this method of instruction is that basic ballet technique must be taught at a slow pace, with difficulty progression often much slower than the rest of the methods. The idea behind this is if a student is to put in a large amount of effort into perfecting the basic steps, the technique learned in these steps allow a student to utilize harder ones at a much easier rate.[24]
 Developed by George Balanchine at the New York City Ballet. His method draws heavily on his own training as a dancer in Russia. The technique is known for extreme speed throughout routines, emphasis on lines, and deep pliés. Perhaps one of the most well known differences of this style is the unorthodox positioning of the body.[24] Dancers of this style often have flexed hands and even feet, and are placed in off-balance positions. Important ballet studios teaching this method are the Miami City Ballet, Ballet Chicago Studio company, and the School of American Ballet in New York.[30]
 Ballet costumes play an important role in the ballet community. They are often the only survival of a production, representing a living imaginary picture of the scene.[31]
 The roots of ballet go back to the Renaissance in France and Italy when court wear was the beginning of ballet costumes. Ballet costumes have been around since the early 15th century. Cotton and silk were mixed with flax, woven into semitransparent gauze[31] to create exquisite ballet costumes.
 During the 17th century, different types of fabrics and designs were used to make costumes more spectacular and eye catching. Court dress still remained for women during this century. Silks, satins and fabrics embroidered with real gold and precious stones increased the level of spectacular decoration associated with ballet costumes.[31] Women's costumes also consisted of heavy garments and knee-long skirts which made it difficult for them to create much movement and gesture.
 During the 18th century, stage costumes were still very similar to court wear but progressed over time, mostly due to the French dancer and ballet-master Jean-Georges Noverre (1727–1810) whose proposals to modernize ballet are contained in his revolutionary Lettres sur la danse et les ballets (1760). Noverre's book altered the emphasis in a production away from the costumes towards the physical movements and emotions of the dancers.
 European ballet was centered in the Paris Opera.[31] During this era, skirts were raised a few inches off the ground. Flowers, flounces, ribbons, and lace emphasized this opulent feminine style, as soft pastel tones in citron, peach, pink, and pistachio dominated the color range.[31]
 During the early 19th century, close-fitting body costumes, floral crowns, corsages, and jewels were used. Ideals of Romanticism were reflected through female movements.[31]
 Costumes became much tighter as corsets started to come into use, to show off the curves on a ballerina. Jewels and bedazzled costumes became much more popular.
 During the 20th century, ballet costumes transitioned back to the influence of Russian ballet. Ballerina skirts became knee-length tutus, later on in order to show off their precise pointe work. Colors used on stage costumes also became much more vibrant. Designers used colors such as red, orange, yellow, etc. to create visual expression when ballet dancers perform on stage.
 Professional dancers are generally not well paid, and earn less money than a typical worker.[32]  As of 2020, American dancers (including ballet and other dance forms) were paid an average of US$19 per hour, with pay somewhat better for teachers than for performers.[32]
 The job outlook is not strong, and the competition to get a job is intense, with the number of applicants vastly exceeding the number of job openings.[32]  Most jobs involve teaching in private dance schools.[32]
 Choreographers are paid better than dancers.[32]  Musicians and singers are paid better per hour than either dancers or choreographers, about US$30 per hour; however, full-time work is unusual for musicians.[33]
 Teenage girl ballet dancers are prone to stress fractures in the first rib.[34] Posterior ankle impingement syndrome (PAIS) most commonly affects people who perform repetitive plantar flexion, such as ballet dancers.[35] Eating disorders are thought to be common, and a 2014 meta-analysis suggests that studies do indicate that ballet dancers are at higher risk than the general population for several kinds of eating disorders.[36] In addition, some researchers have noted that intensive training in ballet results in lower bone mineral density in the arms.[37]
 Most ballet choreography is written so that it can only be performed by a relatively young dancer.[38]  The structure of ballet – in which a (usually) male choreographer or director uses (mostly) women's bodies to express his artistic vision, has been criticized as harming women.[by whom?][39]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Eating disorders', 'male choreographer or director', 'recognizing that all parts of the body move together to create beautiful, graceful lines', 'Teenage girl ballet dancers are prone to stress fractures in the first rib', 'distanced performers from audience members'], 'answer_start': [], 'answer_end': []}"
"
 A circus is a company of performers who put on diverse entertainment shows that may include clowns, acrobats, trained animals, trapeze acts, musicians, dancers, hoopers, tightrope walkers, jugglers, magicians, ventriloquists, and unicyclists as well as other object manipulation and stunt-oriented artists. The term circus also describes the field of performance, training and community which has followed various formats through its 250-year modern history. Although not the inventor of the medium, Newcastle-under-Lyme born Philip Astley is credited as the father of the modern circus.[1]
 In 1768, Astley, a skilled equestrian, began performing exhibitions of trick horse riding in an open field called Ha'Penny Hatch on the south side of the Thames River, England.[2] In 1770, he hired acrobats, tightrope walkers, jugglers and a clown to fill in the pauses between the equestrian demonstrations and thus chanced on the format which was later named a ""circus"". Performances developed significantly over the next fifty years, with large-scale theatrical battle reenactments becoming a significant feature. The format in which a ringmaster introduces a variety of choreographed acts set to music, often termed 'traditional' or 'classical' circus, developed in the latter part of the 19th century and remained the dominant format until the 1970s.
 As styles of performance have developed since the time of Astley, so too have the types of venue where these circuses have performed. The earliest modern circuses were performed in open-air structures with limited covered seating. From the late 18th to late 19th century, custom-made circus buildings (often wooden) were built with various types of seating, a centre ring, and sometimes a stage. The traditional large tents commonly known as ""big tops"" were introduced in the mid-19th century as touring circuses superseded static venues. These tents eventually became the most common venue. Contemporary circus is performed in a variety of venues including tents, theatres, casinos, cruise ships and open-air spaces. Many circus performances are still held in a ring, usually 13 m (43 ft) in diameter. This dimension was adopted by Astley in the late 18th century as the minimum diameter that enabled an acrobatic horse rider to stand upright on a cantering horse to perform their tricks.
 A shift in form has been credited with a revival of the circus tradition since the late 1970s, when a number of groups began to experiment with new circus formats and aesthetics, typically avoiding the use of animals to focus exclusively on human artistry. Circus companies and artistes within this movement, often termed 'new circus' or 'cirque nouveau', have tended to favour a theatrical approach, combining character-driven circus acts with original music in a broad variety of styles to convey complex themes or stories. Since the 1990s, a more avant garde approach to presenting traditional circus techniques or 'disciplines' in ways that align more closely to performance art, dance or visual arts has been given the name 'contemporary circus'. This labelling can cause confusion based upon the other use of the phrase contemporary circus to mean 'circus of today'. For this reason, some commentators have begun using the term 21st Century Circus to encompass all the various styles available in the present day. 21st Century Circus continues to develop new variations on the circus tradition while absorbing new skills, techniques, and stylistic influences from other art forms and technological developments.  For aesthetic or economic reasons, 21st Century Circus productions may often be staged in theatres rather than in large outdoor tents.
 First attested in English 14th century, the word circus derives from Latin circus,[3] which is the romanisation of the Greek κίρκος (kirkos), itself a metathesis of the Homeric Greek κρίκος (krikos), meaning ""circle"" or ""ring"".[4] In the book De Spectaculis early Christian writer Tertullian claimed that the first circus games were staged by the goddess Circe in honour of her father Helios, the Sun God.[5]
 The modern and commonly held idea of a circus is of a Big Top with various acts providing entertainment therein; however, the history of circuses is more complex, with historians disagreeing on its origin, as well as revisions being done about the history due to the changing nature of historical research, and the ongoing circus phenomenon. For many, circus history begins with Englishman Philip Astley, while for others its origins go back much further—to Roman Empire times.
 In Ancient Rome, the circus was a roofless arena[6]: 2  for the exhibition of horse and chariot races, equestrian shows, staged battles, gladiatorial combat, and displays of (and fights with) trained animals. The circuses of Rome were similar to the ancient Greek hippodromes, although circuses served varying purposes and differed in design and construction, and for events that involved re-enactments of naval battles, the circus was flooded with water; however, the Roman circus buildings were not circular but rectangular with semi circular ends. The lower seats were reserved for persons of rank; there were also various state boxes for the giver of the games and his friends. The circus was the only public spectacle at which men and women were not separated.[7]  Some circus historians such as George Speaight have stated ""these performances may have taken place in the great arenas that were called 'circuses' by the Romans, but it is a mistake to equate these places, or the entertainments presented there, with the modern circus"".[8] Others have argued that the lineage of the circus does go back to the Roman circuses and a chronology of circus-related entertainment can be traced to Roman times, continued by the Hippodrome of Constantinople that operated until the 13th century, through medieval and renaissance jesters, minstrels and troubadours to the late 18th century and the time of Astley.[9][10]
 The first circus in the city of Rome was the Circus Maximus, in the valley between the Palatine and Aventine hills.[7] It was constructed during the monarchy and, at first, built completely from wood. After being rebuilt several times, the final version of the Circus Maximus could seat 250,000 people; it was built of stone and measured 400m in length and 90m in width.[11] Next in importance were the Circus Flaminius and the Circus Neronis, from the notoriety which it obtained through the Circensian pleasures of Nero. A fourth circus was constructed by Maxentius;[7] its ruins have helped archaeologists reconstruct the Roman circus.
 For some time after the fall of Rome, large circus buildings fell out of use as centres of mass entertainment. Instead, itinerant performers, animal trainers, and showmen travelled between towns throughout Europe, performing at local fairs such as the Bartholomew Fair in London during the Middle Ages.[6]: 4–6 
 The origin of the modern circus has been attributed to Philip Astley, who was born 1742 in Newcastle-under-Lyme, England. He became a cavalry officer who set up the first modern amphitheatre for the display of horse riding tricks in Lambeth, London, on 4 April 1768.[12][13][14] Astley did not originate trick horse riding, nor was he first to introduce acts such as acrobats and clowns to the English public, but he was the first to create a space where all these acts were brought together to perform a show.[15] Astley rode in a circle rather than a straight line as his rivals did, and thus chanced on the format of performing in a circle.[16] Astley performed stunts in a 42 ft diameter ring, which is the standard size used by circuses ever since.[15] Astley referred to the performance arena as a circle and the building as an amphitheatre; these would later be known as a circus.[17] In 1770, Astley hired acrobats, tightrope walkers, jugglers, and a clown to fill in the pauses between acts.[15]
 Astley was followed by Andrew Ducrow, whose feats of horsemanship had much to do with establishing the traditions of the circus, which were perpetuated by Hengler's and Sanger's celebrated shows in a later generation. In England circuses were often held in purpose-built buildings in large cities, such as the London Hippodrome, which was built as a combination of the circus, the menagerie, and the variety theatre, where wild animals such as lions and elephants from time to time appeared in the ring, and where convulsions of nature such as floods, earthquakes, and volcanic eruptions were produced with an extraordinary wealth of realistic display.[18] Joseph Grimaldi, the first mainstream clown, had his first major role as Little Clown in the pantomime The Triumph of Mirth; or, Harlequin's Wedding in 1781.[19] The Royal Circus was opened in London on 4 November 1782 by Charles Dibdin (who coined the term ""circus""),[20] aided by his partner Charles Hughes, an equestrian performer.[21] In 1782, Astley established the Amphithéâtre Anglais in Paris, the first purpose-built circus in France, followed by 18 other permanent circuses in cities throughout Europe.[22][23] Astley leased his Parisian circus to the Italian Antonio Franconi in 1793.[24] In 1826, the first circus took place under a canvas big top.[25]
 The Englishman John Bill Ricketts brought the first modern circus to the United States. He began his theatrical career with Hughes Royal Circus in London in the 1780s, and travelled from England in 1792 to establish his first circus in Philadelphia. The first circus building in the US opened on 3 April 1793 in Philadelphia, where Ricketts gave America's first complete circus performance.[26][27] George Washington attended a performance there later that season.[28]
 
 In the Americas during the first two decades of the 19th century, the Circus of Pepin and Breschard toured from Montreal to Havana, building circus theatres in many of the cities it visited. Victor Pépin, a native New Yorker,[29] was the first American to operate a major circus in the United States.[30] Later the establishments of Purdy, Welch & Co., and of van Amburgh gave a wider popularity to the circus in the United States.[18] In 1825, Joshuah Purdy Brown was the first circus owner to use a large canvas tent for the circus performance. Circus pioneer Dan Rice was the most famous pre-Civil War circus clown,[31] popularising such expressions as ""The One-Horse Show"" and ""Hey, Rube!"". The American circus was revolutionised by P. T. Barnum and William Cameron Coup, who launched the travelling P. T. Barnum's Museum, Menagerie & Circus, the first freak show, in the 1870s. Coup also introduced the first multiple-ring circuses, and was also the first circus entrepreneur to use circus trains to transport the circus between towns. By the 1830s, sideshows were also being established alongside travelling circuses.[6]: 9 
 In 1838, the equestrian Thomas Taplin Cooke returned to England from the United States, bringing with him a circus tent.[32] At this time, itinerant circuses that could be fitted-up quickly were becoming popular in Britain. William Batty's circus, for example, between 1838 and 1840, travelled from Newcastle to Edinburgh and then to Portsmouth and Southampton. Pablo Fanque, who is noteworthy as Britain's only black circus proprietor and who operated one of the most celebrated travelling circuses in Victorian England, erected temporary structures for his limited engagements or retrofitted existing structures.[33] One such structure in Leeds, which Fanque assumed from a departing circus, collapsed, resulting in minor injuries to many but the death of Fanque's wife.[34][35] Traveling circus companies also rented the land they set up their structures on sometimes causing damage to the local ecosystems.[36] Three important circus innovators were the Italian Giuseppe Chiarini, and Frenchmen Louis Soullier and Jacques Tourniaire, whose early travelling circuses introduced the circus to Latin America, Australia, Southeast Asia, China, South Africa, and Russia. Soullier was the first circus owner to introduce Chinese acrobatics to the European circus when he returned from his travels in 1866, and Tourniaire was the first to introduce the performing art to Ranga, where it became extremely popular.
 After an 1881 merger with James Anthony Bailey and James L. Hutchinson's circus and Barnum's death in 1891, his circus travelled to Europe as the Barnum & Bailey Greatest Show On Earth, where it toured from 1897 to 1902, impressing other circus owners with its large scale, its touring techniques (including the tent and circus train), and its combination of circus acts, a zoological exhibition, and a freak show. This format was adopted by European circuses at the turn of the 20th century.
 The influence of the American circus brought about a considerable change in the character of the modern circus. In arenas too large for speech to be easily audible, the traditional comic dialogue of the clown assumed a less prominent place than formerly, while the vastly increased wealth of stage properties relegated to the background the old-fashioned equestrian feats, which were replaced by more ambitious acrobatic performances, and by exhibitions of skill, strength, and daring, requiring the employment of immense numbers of performers, and often of complicated and expensive machinery.[18]
 From the late 19th century through the first half of the 20th century, travelling circuses were a major form of spectator entertainment in the US and attracted huge attention whenever they arrived in a city. After World War II, the popularity of the circus declined as new forms of entertainment (such as television) arrived and the public's tastes changed. From the 1960s onward, circuses attracted growing criticism from animal rights activists. Many circuses went out of business or were forced to merge with other circus companies. Nonetheless, a good number of travelling circuses are still active in various parts of the world, ranging from small family enterprises to three-ring extravaganzas. Other companies found new ways to draw in the public with innovative new approaches to the circus form itself.
 In 1919, Lenin, head of Soviet Russia, expressed a wish for the circus to become ""the people's art-form"", with facilities and status on par with theatre, opera and ballet. The USSR nationalised Russian circuses. In 1927, the State University of Circus and Variety Arts, better known as the Moscow Circus School, was established; performers were trained using methods developed from the Soviet gymnastics programme. When the Moscow State Circus company began international tours in the 1950s, its levels of originality and artistic skill were widely applauded.
 Circuses from China, drawing on Chinese traditions of acrobatics, like the Chinese State Circus are also popular touring acts.
 New Circus (originally known as cirque nouveau) is a performing arts movement that originated in the 1970s in Australia, Canada, France,[37] the West Coast of the United States, and the United Kingdom. New Circus combines traditional circus skills and theatrical techniques to convey a story or theme. Compared with the traditional circus, this genre of circus tends to focus more attention on the overall aesthetic impact, on character and story development, and on the use of lighting design, original music, and costume design to convey thematic or narrative content. Music used in the production is often composed exclusively for that production, and aesthetic influences are drawn as much from contemporary culture as from circus history. Animal acts rarely appear in new circus, in contrast to traditional circus, where animal acts have often been a significant part of the entertainment.
 Early pioneers of the new circus genre included: Circus Oz, forged in Australia in 1977 from SoapBox Circus (1976) and New Circus (1973);[38] the Pickle Family Circus, founded in San Francisco in 1975; Ra-Ra Zoo in 1984 in London; Nofit State Circus in 1984 from Wales; Cirque du Soleil, founded in Quebec in 1984; Cirque Plume and Archaos from France in 1984 and 1986 respectively. More recent examples include: Cirque Éloize (founded in Quebec in 1993); Sweden's Cirkus Cirkör (1995); Teatro ZinZanni (founded in Seattle in 1998); the West African Circus Baobab (late 1990s);[39] and Montreal's Les 7 doigts de la main (founded in 2002).[40] The genre includes other circus troupes such as the Vermont-based Circus Smirkus (founded in 1987 by Rob Mermin) and Le Cirque Imaginaire (later renamed Le Cirque Invisible, both founded and directed by Victoria Chaplin, daughter of Charlie Chaplin).
 The most conspicuous success story in the new circus genre has been that of Cirque du Soleil, the Canadian circus company whose estimated annual revenue exceeds US$810 million in 2009,[41] and whose cirque nouveau shows have been seen by nearly 90 million spectators in over 200 cities on five continents.[42]
 The genre of contemporary circus is largely considered to have begun in 1995 with 'Le Cri du Caméléon', an ensemble performance from the graduating class of the French circus school Le Centre National des Arts du Cirque (CNAC), directed by Joseph Nadj. In contrast to New Circus, Contemporary Circus (as a genre) tends to avoid linear narrative in favour of more suggestive, interdisciplinary approaches to abstract concepts. This includes a strong trend for developing new apparatus and movement languages based on the capacities, experience and interests of individual performers, rather than finding new ways to present traditional repertoire.
 Beyond the performance aspect of circus, is the Social Circus field, catalysed by Reg Bolton. Social Circus engages communities through circus practice and activity to provide health and well-being benefits.[43]
 A traditional circus performance is often led by a ringmaster who has a role similar to a Master of Ceremonies. The ringmaster presents performers, speaks to the audience, and generally keeps the show moving. The activity of the circus traditionally takes place within a ring; large circuses may have multiple rings, like the six-ringed Moscow State Circus. A circus often travels with its own band, whose instrumentation in the United States has traditionally included brass instruments, drums, glockenspiel, and sometimes the distinctive sound of the calliope. Performers have been traditionally referred to as artistes, although in recent years the term artists has also come into regular use. To some performers from multi-generational circus families, the term artiste is still preferred as it is considered to confer higher status than artist. Conversely, some performers from the circus school training route taken by many of the newer generations prefer the term artist as it is considered to be less pretentious than artiste. The physical and creative skills that circus artist/es perform are known as disciplines, and are often grouped for training purposes into the broad categories of juggling, equilibristics, acrobatics, aerial and clowning. These disciplines can be honed into individual acts, which can be performed independently and marketed to many different prospective circus employers, and also used for devising solo or collaborative work created specifically for a single project. 
 Common acts include a variety of acrobatics, gymnastics (including tumbling and trampoline), aerial acts (such as trapeze, aerial silk, corde lisse), contortion, stilt-walking, and a variety of other routines. Juggling is one of the most common acts in a circus; the combination of juggling and gymnastics that includes acts like plate spinning and the rolling globe come under the category equilibristics, along with more classical balance disciplines such as tightwire, slackline and unicycle. Acts like these are some of the most common and the most traditional. Clowns are common to most circuses and are typically skilled in many circus acts; ""clowns getting into the act"" is a very familiar theme in any circus. Famous circus clowns have included Austin Miles, the Fratellini Family, Rusty Russell, Emmett Kelly, Grock, and Bill Irwin. The title clown refers to the role functions and performance skills, not simply to the image of red nose and exaggerated facepaint that was popularised through 20th Century mass media. While many clowns still perform in this styling, there are also many clowns who adopt a more natural look.
 Daredevil stunt acts, freak shows, and sideshow acts are also parts of some circus acts, these activities may include human cannonball, chapeaugraphy, fire eating, breathing, and dancing, knife throwing, magic shows, sword swallowing, or strongman. Famous sideshow performers include Zip the Pinhead and The Doll Family. A popular sideshow attraction from the early 19th century was the flea circus, where fleas were attached to props and viewed through a Fresnel lens.
 A variety of animals have historically been used in acts. While the types of animals used vary from circus to circus, big cats (namely lions, tigers, and leopards), foxes, wolves, polecats, minks, weasels, camels, llamas, elephants, zebras, horses, donkeys, birds (like parrots and doves), sea lions, bears, monkeys, and domestic animals such as cats and dogs are the most common.
 The earliest involvement of animals in circus was just the display of exotic creatures in a menagerie. Going as far back as the early eighteenth century, exotic animals were transported to North America for display, and menageries were a popular form of entertainment.[45] The first true animals acts in the circus were equestrian acts. Soon elephants and big cats were displayed as well. Isaac A. Van Amburgh entered a cage with several big cats in 1833, and is generally considered to be the first wild animal trainer in American circus history.[30] Mabel Stark was a famous female tiger-tamer.
 Animal rights groups have documented many cases of animal cruelty in the training of performing circus animals.[47][48] The animal rights group People for the Ethical Treatment of Animals (PETA) contends that animals in circuses are frequently beaten into submission and that physical abuse has always been the method for training circus animals. It is also alleged that the animals are kept in cages that are too small and are given very little opportunity to walk around outside of their enclosure, thereby violating their right to freedom.
 According to PETA, although the US Animal Welfare Act does not permit any sort of punishment that puts the animals in discomfort,[49] trainers will still go against this law and use such things as electric rods and bullhooks.[50] According to PETA, during an undercover investigation of Carson & Barnes Circus, video footage was captured showing animal care director Tim Frisco training endangered Asian elephants with electrical shock prods and instructing other trainers to ""beat the elephants with a bullhook as hard as they can and sink the sharp metal hook into the elephant's flesh and twist it until they scream in pain"".[50]
 On behalf of the Ministry of Agriculture, Nature and Food Quality of the Netherlands, Wageningen University conducted an investigation into the welfare of circus animals in 2008.[51] The following issues, among others, were found:
 Based on these findings, the researchers called for more stringent regulation regarding the welfare of circus animals. In 2012, the Dutch government announced a ban on the use of wild circus animals.[52]
 In testimony in U.S. District Court in 2009, Ringling Bros. and Barnum & Bailey Circus CEO Kenneth Feld acknowledged that circus elephants are struck behind the ears, under the chin and on their legs with metal tipped prods, called bullhooks. Feld stated that these practices are necessary to protect circus workers. Feld also acknowledged that an elephant trainer was reprimanded for using an electric shock device, known as a hot shot or electric prod, on an elephant, which Feld also stated was appropriate practice. Feld denied that any of these practices harm elephants.[53] In its January 2010 verdict on the case, brought against Feld Entertainment International by the American Society for the Prevention of Cruelty to Animals et al., the Court ruled that evidence against the circus company was ""not credible with regard to the allegations"".[54] In lieu of a USDA hearing, Feld Entertainment Inc. (parent of Ringling Bros.) agreed to pay an unprecedented $270,000 fine for violations of the Animal Welfare Act that allegedly occurred between June 2007 and August 2011.[55]
 A 14-year litigation against the Ringling Bros. and Barnum & Bailey Circus came to an end in 2014 when The Humane Society of the United States and a number of other animal rights groups paid a $16 million settlement to Feld Entertainment; however, the circus closed in May 2017 after a 146-year run when it experienced a steep decline in ticket sales a year after it discontinued its elephant act and sent its pachyderms to a reserve.[56][57]
 On 1 February 1992 at the Great American Circus in Palm Bay, Florida, an elephant named Janet (1965 – 1 February 1992) went out of control while giving a ride to a mother, her two children, and three other children. The elephant then stampeded through the circus grounds outside before being shot to death by police.[58] Also, during a Circus International performance in Honolulu, Hawaii, on 20 August 1994, an elephant called Tyke (1974 – 20 August 1994) killed her trainer, Allen Campbell, and severely mauled her groomer, Dallas Beckwith, in front of hundreds of spectators. Tyke then bolted from the arena and ran through the streets of Kakaako for more than thirty minutes. Police fired 86 shots at Tyke, who eventually collapsed from the wounds and died.[59]
 In December 2018, New Jersey became the first state in the U.S. to ban circuses, carnivals and fairs from featuring elephants, tigers, and other exotic animals.[60]
 In 1998 in the United Kingdom, a parliamentary working group chaired by MP Roger Gale studied living conditions and treatment of animals in UK circuses. All members of this group agreed that a change in the law was needed to protect circus animals. Gale told the BBC, ""It's undignified and the conditions under which they are kept are woefully inadequate—the cages are too small, the environments they live in are not suitable and many of us believe the time has come for that practice to end."" The group reported concerns about boredom and stress, and noted that an independent study by a member of the Wildlife Conservation Research Unit at Oxford University ""found no evidence that circuses contribute to education or conservation.""; however, in 2007, a different working group under the UK Department for Environment, Food and Rural Affairs, having reviewed information from experts representing both the circus industry and animal welfare, found an absence of ""scientific evidence sufficient to demonstrate that travelling circuses are not compatible with meeting the welfare needs of any type of non-domesticated animal presently being used in the United Kingdom.[61]"" According to that group's report, published in October 2007, ""there appears to be little evidence to demonstrate that the welfare of animals kept in travelling circuses is any better or any worse than that of animals kept in other captive environments.""[62]
 A ban prohibiting the use of wild animals in circuses in England was due to be passed in 2015, but Conservative MP Christopher Chope repeatedly blocked the bill under the reasoning that ""The EU Membership Costs and Benefits bill should have been called by the clerk before the circuses bill, so I raised a point of order"". He explained that the circus bill was ""at the bottom of the list"" for discussion.[63] The Animal Defenders International non-profit group dubbed this ""a huge embarrassment for Britain that 30 other nations have taken action before us on this simple and popular measure"".[64] On 1 May 2019 Environmental Secretary Michael Gove announced a new Bill to ban the use of wild animals in travelling circuses.[65] The Wild Animals in Circuses Act 2019 came into effect on 20 January 2020.[66]
 A bill to ban the use of wild animals in travelling circuses in Wales was introduced in June 2019, and subsequently passed by the Welsh Parliament on 15 July 2020.[67] Over 6,500 responses were made by the people of Wales, to the public consultation on the draft Bill, 97% of which supported the ban.
 The use of wild animals in travelling circuses has been banned in Scotland. The Wild Animals in Travelling Circuses (Scotland) Act 2018 came into force on 28 May 2018.
 There are nationwide bans on using some if not all animals in circuses in Austria, Belgium, Bolivia, Bosnia and Herzegovina, Bulgaria, Colombia, Costa Rica, Croatia, Cyprus, Czech Republic, Denmark, Ecuador, El Salvador, Estonia, Finland, Greece, Hungary, India, Iran, Ireland, Israel, Italy, Malta, Mexico, Netherlands, Norway, Panama, Paraguay, Peru, Poland, Portugal, Singapore, Slovenia, Sweden, Switzerland, and Turkey.[68][69][70] Germany, Spain, United Kingdom, Australia, Argentina, Chile, Brazil, Canada, and the United States have locally restricted or banned the use of animals in entertainment.[69] In response to a growing popular concern about the use of animals in entertainment, animal-free circuses are becoming more common around the world.[71] In 2009, Bolivia passed legislation banning the use of any animals, wild or domestic, in circuses. The law states that circuses ""constitute an act of cruelty."" Circus operators had one year from the bill's passage on 1 July 2009 to comply.[72] In 2018 in Germany, an accident with an elephant during a circus performance, prompted calls to ban animal performances in circuses. PETA called the German politicians to outlaw the keeping of animals for circuses.[73]
 A survey confirmed that on average, wild animals spend around 99 to 91 percent of their time in cages, wagons, or enclosure due to transportation. This causes a huge amount of distress to animals and leads to excessive amounts of drooling.[74]
 City ordinances banning performances by wild animals have been enacted in San Francisco (2015),[75] Los Angeles (2017),[76] and New York City (2017).[77]
 Greece became the first European country to ban any animal from performing in any circus in its territory in February 2012, following a campaign by Animal Defenders International and the Greek Animal Welfare Fund (GAWF).[78]
 On 6 June 2015, the Federation of Veterinarians of Europe adopted a position paper in which it recommends the prohibition of the use of wild animals in travelling circuses.[79][80]
 Despite the contemporary circus' shift toward more theatrical techniques and its emphasis on human rather than animal performance, traditional circus companies still exist alongside the new movement. Numerous circuses continue to maintain animal performers, including UniverSoul Circus and the Big Apple Circus from the United States, Circus Krone from Munich, Circus Royale and Lennon Bros Circus from Australia, Vazquez Hermanos Circus, Circo Atayde Hermanos, and Hermanos Mayaror Circus[81] from Mexico, and Moira Orfei Circus[82] from Italy, to name just a few.
 In some towns, there are circus buildings where regular performances are held. The best known are:
 In other countries, purpose-built circus buildings still exist which are no longer used as circuses, or are used for circus only occasionally among a wider programme of events; for example, the Cirkusbygningen (The Circus Building) in Copenhagen, Denmark, Cirkus in Stockholm, Sweden, or Carré Theatre in Amsterdam, Netherlands.
 The International Circus Festival of Monte-Carlo[84] has been held in Monaco since 1974 and was the first of many international awards for circus performers.
 Erich Kästner's children's books Der kleine Mann [de] 1963 (The Little Man) and Der kleine Mann und die kleine Miss [de] 1967 (The Little Man and the Little Miss) are largely set in a circus where the orphaned young protagonist grows up as a ward of the show's magician.
 The atmosphere of the circus has served as a dramatic setting for many musicians. The most famous circus theme song is called ""Entrance of the Gladiators"", and was composed in 1904 by Julius Fučík. Other circus music includes ""El Caballero"", ""Quality Plus"", ""Sunnyland Waltzes"", ""The Storming of El Caney"", ""Pahjamah"", ""Bull Trombone"", ""Big Time Boogie"", ""Royal Bridesmaid March"", ""The Baby Elephant Walk"", ""Liberty Bell March"", ""Java"", Strauss's ""Radetsky March"", and ""Pageant of Progress"". A poster for Pablo Fanque's Circus Royal, one of the most popular circuses of Victorian England, inspired John Lennon to write Being for the Benefit of Mr. Kite! on The Beatles' album, Sgt. Pepper's Lonely Hearts Club Band. The song title refers to William Kite, a well-known circus performer in the 19th century. Producer George Martin and EMI engineers created the song's fairground atmosphere by assembling a sound collage of collected recordings of calliopes and fairground organs, which they cut into strips of various lengths, threw into a box, and then mixed up and edited together randomly, creating a long loop which was mixed into the final production.[85] Another traditional circus song is the John Philip Sousa march ""Stars and Stripes Forever"", which is played only to alert circus performers of an emergency.
 Plays set in a circus include the 1896 musical The Circus Girl by Lionel Monckton, Polly of the Circus written in 1907 by Margaret Mayo, He Who Gets Slapped written by Russian Leonid Andreyev 1915 and later adapted into one of the first circus films, Katharina Knie written in 1928 by Carl Zuckmayer and adapted for the English stage in 1932 as Caravan by playwright Cecily Hamilton, the revue Big Top written by Herbert Farjeon in 1942, Top of the Ladder written by Tyrone Guthrie in 1950, Stop the World, I Want to Get Off written by Anthony Newley in 1961, and Barnum with music by Cy Coleman and lyrics and book by Mark Bramble, Roustabout: The Great Circus Train Wreck written by Jay Torrence in 2006.
 Following World War I, circus films became popular. In 1924 He Who Gets Slapped was the first film released by MGM; in 1925 Sally of the Sawdust (remade 1930), Variety, and Vaudeville were produced, followed by The Devil's Circus in 1926 and The Circus starring Charlie Chaplin, Circus Rookies, 4 Devils; and Laugh Clown Laugh in 1928. German film Salto Mortale about trapeze artists was released in 1931 and remade in the United States and released as Trapeze starring Burt Lancaster in 1956; in 1932 Freaks was released; Charlie Chan at the Circus, Circus (USSR) and The Three Maxiums were released in 1936 and At the Circus starring the Marx Brothers and You Can't Cheat an Honest Man in 1939. Circus films continued to be popular during the Second World War; films from this era included The Great Profile starring John Barrymore (1940), the animated Disney film Dumbo (1941), Road Show (1941), The Wagons Roll at Night (1941) and Captive Wild Woman (1943).
 Tromba, a film about a tiger trainer, was released in 1948. In 1952 Cecil B. de Mille's Oscar-winning film The Greatest Show on Earth was first shown. Released in 1953 were Man on a Tightrope and Ingmar Bergman's Gycklarnas afton (released as Sawdust and Tinsel in the United States); these were followed by Life Is a Circus; Ring of Fear; 3 Ring Circus (1954) and La Strada (1954), an Oscar-winning film by Federico Fellini about a girl who is sold to a circus strongman. Fellini made a second film set in the circus called The Clowns in 1970. Films about the circus made since 1959 include Disney's Toby Tyler (1960), the B-movie Circus of Horrors (also in 1960); the musical film Billy Rose's Jumbo (1962); A Tiger Walks, a Disney film about a tiger that escapes from the circus; and Circus World (1964), starring John Wayne. In Hanna-Barbera's first animated film Hey There, It's Yogi Bear! (1964), Cindy Bear is held captive in a circus where she is cruelly forced to perform until Yogi and Boo-Boo rescue her. Mera Naam Joker (1970), a Hindi drama film directed by Raj Kapoor which was about a clown who must make his audience laugh at the cost of his own sorrows. In the anime film Jungle Emperor Leo (1997), Leo's son Lune is captured and placed in a circus, which burns down when a tiger knocks down a ring of fire while jumping through it. The Greatest Showman, a musical film loosely based on the life of P. T. Barnum, was released in 2017.
 The TV series Circus Humberto, based on the novel by Eduard Bass, follows the history of the circus family Humberto between 1826 and 1924. The setting of the HBO television series Carnivàle, which ran from 2003 to 2005, is also largely set in a travelling circus. The circus has also inspired many writers. Numerous books, both non-fiction and fiction, have been published about circus life. Notable examples of circus-based fiction include Circus Humberto by Eduard Bass, Cirque du Freak by Darren Shan, and Spangle by Gary Jennings. The novel Water for Elephants by Sara Gruen tells the fictional tale of a circus veterinarian and was made into a movie with the same title, starring Robert Pattinson and Reese Witherspoon. Science fiction writer Barry B. Longyear wrote a trilogy about a circus of the future: City of Baraboo; Elephant Song; and Circus World.
 Circus is the central theme in comic books of Super Commando Dhruva, an Indian comic book superhero. According to this series, Dhruva was born and brought up in a fictional Indian circus called Jupiter Circus. When a rival circus burnt down Jupiter Circus, killing everyone in it, including Dhruva's parents, Dhruva vowed to become a crime fighter. A circus-based television series called Circus was also telecast in India in 1989 on DD National, starring Shahrukh Khan as the lead actor.
 
In the video game Phoenix Wright Ace Attorney Justice For All  the main character Phoenix Wright investigates a murder at a circus and he is the defence attorney of an egotistical magician. This happens in case 3 Turnabout Big Top.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Circus', 'Zip the Pinhead and The Doll Family', 'a circus veterinarian', 're-enactments of naval battles', 'suggestive, interdisciplinary approaches to abstract concepts'], 'answer_start': [], 'answer_end': []}"
"
 Street performance or busking is the act of performing in public places for gratuities. In many countries, the rewards are generally in the form of money but other gratuities such as food, drink or gifts may be given. Street performance is practiced all over the world and dates back to antiquity. People engaging in this practice are called street performers or buskers. Outside of New York, buskers is not a term generally used in American English.[1][2]
 Performances are anything that people find entertaining, including acrobatics, animal tricks, balloon twisting, caricatures, clowning, comedy, contortions, escapology, dance, singing, fire skills, flea circus, fortune-telling, juggling, magic, mime, living statue, musical performance, one man band, puppeteering, snake charming, storytelling or reciting poetry or prose, street art such as sketching and painting, street theatre, sword swallowing, ventriloquism and washboarding. Buskers may be solo performers or small groups.
 The term busking was first noted in the English language around the middle 1860s in Great Britain. The verb to busk, from the word busker, comes from the Spanish root word buscar, with the meaning ""to seek"".[3] The Spanish word buscar in turn evolved from the Indo-European word *bhudh-skō (""to win, conquer"").[4] It was used for many street acts, and was the title of a famous Spanish book about one of them, El Buscón. Today, the word is still used in Spanish but mostly reserved for female street sex workers, or mistresses of married men.[citation needed]
 There have been performances in public places for gratuities in every major culture in the world, dating back to antiquity. For many musicians, street performance was the most common means of employment before the advent of recording and personal electronics.[5] Prior to that, a person had to produce any music or entertainment, save for a few mechanical devices such as the barrel organ, the music box, and the piano roll. Organ grinders were commonly found busking in the 19th century and early 20th century.
 Busking is common among some Romani people. Romantic mention of Romani music, dancers and fortune tellers are found in all forms of song poetry, prose and lore. The Roma brought the word busking to England by way of their travels along the Mediterranean coast to Spain and the Atlantic Ocean and then up north to England and the rest of Europe.[citation needed]
 In medieval France, buskers were known by the terms troubadours and jongleurs. In northern France, they were known as trouveres. In old German, buskers were known as Minnesingers and Spielleute. In obsolete French, it evolved to busquer for ""seek, prowl"" and was generally used to describe prostitutes. In Russia, buskers are called skomorokh, and their first recorded history appears around the 11th century.[citation needed]
 Mariachis, Mexican bands that play a style of music by the same name, frequently busk when they perform while traveling through streets and plazas, as well as in restaurants and bars.[6]
 We like playing for big crowds, and the goal all along has been for people to pay a little to come and see us. But it all started on street corners, and that is still very connected to what we do. It's such a validating musical experience. Busking is a very humble and brave act that takes courage to do well. It's also about the energy of music being alive outside in a city ... You can walk right by it right in front of you. Sure, to some people you're just another guy with his hand out, so sometimes busking can be great social barometer. You're able to gauge who you live with on earth.[7]
 Ketch Secor, Old Crow Medicine Show Around the mid-19th century Japanese Chindonya started to be seen using their skills for advertising, and these street performers are still occasionally seen in Japan. Another Japanese street performance form dating from the Edo period is Nankin Tamasudare, in which the performer creates large figures using a bamboo mat.
 In the 19th century, Italian street musicians (mainly from Liguria, Emilia Romagna, Basilicata) began to roam worldwide in search of fortune. Musicians from Basilicata, especially the so-called Viggianesi, would later become professional instrumentalists in symphonic orchestras, especially in the United States.[8] The street musicians from Basilicata are sometimes cited as an influence on Hector Malot's Sans Famille.[9]
 In the United States, medicine shows proliferated in the 19th century. They were traveling vendors selling elixirs and potions which purportedly improved people's health. They would often employ entertainment acts as a way of drawing in potential clients and relaxing them. The people would often associate this feeling of well-being with the products sold. After these performances, they would ""pass the hat"".[citation needed]
 One-man bands have historically performed as buskers playing a variety of instruments simultaneously. One-man bands proliferated in urban areas in the 19th and early 20th centuries and still perform to this day. A current one-man band plays all their instruments acoustically usually combining a guitar, a harmonica, a drum and a tambourine. They may also include singing. Many still busk but some are booked to play at festivals and other events.[citation needed]
 Folk music has always been an important part of the busking scene. Cafe, restaurant, bar and pub busking is a mainstay of this art form. The delta bluesmen were mostly itinerant musicians emanating from the Mississippi Delta region of the USA around the early 1940s and on. B.B. King is one famous example who came from these roots.[citation needed]
 The counterculture of the hippies of the 1960s occasionally staged ""be-ins"", which resembled some present-day buskers festivals. Bands and performers would gather at public places and perform for free, passing the hat to make money. The San Francisco Bay Area was at the epicenter of this movement – be-ins were staged at Golden Gate Park and San Jose's Bee Stadium and other venues. Some of the bands that performed in this manner were Janis Joplin with Big Brother and the Holding Company, the Grateful Dead, Jefferson Airplane, Quicksilver Messenger Service, Country Joe and the Fish, Moby Grape and Jimi Hendrix.[citation needed]
 Christmas caroling can also be a form of busking, as wassailing included singing for alms, wassail or some other form of refreshment such as figgy pudding. In the Republic of Ireland, the traditional Wren Boys, and in England Morris Dancing can be considered part of the busking tradition.[citation needed]
 In India and Pakistan's Gujarati region, Bhavai is a form of street art where there are plays enacted in the village, the barot or the village singer also is part of the local entertainment scene.[citation needed]
 In the 2000s, some performers have begun ""Cyber Busking"". Artists post work or performances on the Internet for people to download or ""stream"" and if people like it they make a donation using PayPal.[citation needed]
 There are three basic forms of street performance: circle shows, walk-by acts, and stoplight performances.
 ""Circle shows"" are shows that tend to gather a crowd around them. They usually have a distinct beginning and end. Usually these are done in conjunction with street theatre, puppeteering, magicians, comedians, acrobats, jugglers and sometimes musicians. Circle shows can be the most lucrative. Sometimes the crowds attracted can be very large. A good busker will control the crowd so the patrons do not obstruct foot traffic.
 ""Walk-by acts"" are acts where the busker performs a musical, living statue or other act that does not have a distinct beginning or end, and the public usually watches for a brief time. A walk-by act may turn into a circle show if the act is unusual or very popular.
 ""Stoplight performances"" are performances in which performers present their act and get contributions from vehicle occupants on a crosswalk while the traffic lights are red. A variety of disciplines can be used in such a format (juggling, break dancing, even magic tricks). Because of the short period of time available to them, stoplight performers must have a very brief, condensed routine. This form is seen more commonly in Latin America than elsewhere.
 Buskers collect donations and tips from the public in a variety of containers and by different methods depending on the type of busking they are performing. For walk-by acts, their open, empty instrument case or a special can, box, or hat is often used. For circle shows the performer will typically collect money at the end of the show, although some performers will also collect during the show, as some audience members do not stay for the entire performance. 
 Sometimes a performer will employ a bottler, hat man, or pitch man to collect money from the audience and encourage them to contribute, sometimes by cajoling them in a humorous fashion. The term bottler is a British term that originated from the use of the top half of a bottle to collect money. The bottle had a leather flap inserted in the bottleneck and a leather pouch attached. This design allowed coins to be put in the bottle but did not allow them to be removed easily without the coins jingling against the glass. The first use of such contrivances was recorded by the famous Punch and Judy troupe of puppeteers in early Victorian times.[10]
 The place where a performance occurs is called a ""pitch"". A good pitch can be the key to success as a busker. An act that might make money at one place and time may not work at all in another setting. Popular pitches tend to be public places with large volumes of pedestrian traffic, high visibility, low background noise and as few elements of interference as possible. Good locations may include tourist spots, popular parks, entertainment districts including many restaurants, cafés, bars and pubs and theaters, subways and bus stops, outside the entrances to large concerts and sporting events, almost any plaza or town square as well as zócalos in Latin America and piazzas in other regions. Other places include shopping malls, strip malls, and outside supermarkets, although permission is usually required from management for these.
 In her book, Underground Harmonies: Music and Politics in the Subways of New York, Susie J. Tanenbaum examined how the adage ""Music hath charms to soothe the savage beast"" plays out in regards to busking. Her sociological studies showed that in areas where buskers regularly perform, crime rates tended to go down, and that those with higher education attainment tended to have a more positive view of buskers than did those of lesser educational attainment.[11] Some cities encourage busking in particular areas,[12] giving preference to city government-approved buskers and even publishing schedules of performances.[13]
 Many cities in the United States have particular areas known to be popular spots for buskers. Performers are found at many locations like Mallory Square in Key West, in New Orleans, in New York around Central Park, Washington Square, and the subway systems, in San Francisco, in Washington, D.C. around the transit centers, in Los Angeles around Venice Beach, the Santa Monica Third Street Promenade, and the Hollywood area, in Chicago on Maxwell Street, in the Delmar Loop district of St. Louis, and many other locations throughout the US.
Busking is still quite common in Scotland, Ireland (Grafton Street, Dublin), and England with musicians and other street performers of varying talent levels.
 The first recorded instances of laws affecting buskers were in ancient Rome in 462 BC. The Law of the Twelve Tables made it a crime to sing about or make parodies of the government or its officials in public places; the penalty was death.[14][15] Louis the Pious ""excluded histriones and scurrae, which included all entertainers without noble protection, from the privilege of justice"".[16] In 1530 Henry VIII ordered the licensing of minstrels and players, fortune-tellers, pardoners and fencers, as well as beggars who could not work. If they did not obey they could be whipped on two consecutive days.[17]
 In the United States under constitutional law and most European common law, the protection of artistic free speech extends to busking. In the U.S. and many countries, the designated places for free speech behavior are the public parks, streets, sidewalks, thoroughfares and town squares or plazas. Under certain circumstances even private property may be open to buskers, particularly if it is open to the general public and busking does not interfere with its function and management allows it or other forms of free speech behaviors or has a history of doing so.[18]
 While there is no universal code of conduct for buskers, there are common law practices that buskers must conform to. Most jurisdictions have corresponding statutory laws. In the UK busking regulation is not universal with most laws (if there are any) being governed by local councils.[19] Some towns in the British Isles limit the licenses issued to bagpipers because of the volume and difficulty of the instrument.[citation needed] In Great Britain places requiring licenses for buskers may also require auditions of anyone applying for a busking license.[citation needed] Oxford City Council have decided to enact a public spaces protection order. Some venues that do not regulate busking may still ask performers to abide by voluntary rules. Some places require a special permit to use electronically amplified sound and may have limits on the volume of sound produced.[20] It is common law that buskers or others should not impede pedestrian traffic flow, block or otherwise obstruct entrances or exits, or do things that endanger the public. It is common law that any disturbing or noisy behaviors may not be conducted after certain hours in the night. These curfew limitations vary from jurisdiction to jurisdiction. It is common law that ""performing blue"" (i.e. using material that is sexually explicit or any vulgar or obscene remarks or gestures) is generally prohibited unless performing for an adults-only environment such as in a bar or pub.
 In London, busking is prohibited in the entire area of the City of London. The London Underground provides busking permits for up to 39 pitches across 25 central London stations.[21] Most London boroughs do not license busking, but they have optional powers, under the London Local Authorities Act 2000, if there is sufficient reason to do so. Where these powers have not been adopted, councils can rely on other legislation including the Environmental Protection Act 1990 to deal with noise nuisance from buskers and the Highways Act 1980 to deal with obstructions. Camden Council is currently looking into further options to control the problem of nuisance buskers and the playing of amplified music to the detriment of local residents and businesses.[22]
 Buskers may find themselves targeted by thieves due to the very open and public nature of their craft. Buskers may have their earnings, instruments or props stolen. One particular technique that thieves use against buskers is to pretend to make a donation while actually taking money out instead, a practice known as ""dipping"" or ""skimming"". George Burns described his days as a youthful busker this way:[23]
 Sometimes the customers threw something in the hats. Sometimes they took something out of the hats. Sometimes they took the hats. Organisations
 Press
 Other
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the playing of amplified music', 'thieves', 'antiquity', 'playing of amplified music to the detriment of local residents and businesses', 'very open and public nature of their craft'], 'answer_start': [], 'answer_end': []}"
"
 The visual arts are art forms such as painting, drawing, printmaking, sculpture, ceramics, photography, video, filmmaking, comics, design, crafts, and architecture. Many artistic disciplines, such as performing arts, conceptual art, and textile arts, also involve aspects of the visual arts as well as arts of other types. Also included within the visual arts[1] are the applied arts,[2] such as industrial design, graphic design, fashion design, interior design, and decorative art.[3]
 Current usage of the term ""visual arts"" includes fine art as well as applied or decorative arts and crafts, but this was not always the case. Before the Arts and Crafts Movement in Britain and elsewhere at the turn of the 20th century, the term 'artist' had for some centuries often been restricted to a person working in the fine arts (such as painting, sculpture, or printmaking) and not the decorative arts, crafts, or applied visual arts media. The distinction was emphasized by artists of the Arts and Crafts Movement, who valued vernacular art forms as much as high forms.[4] Art schools made a distinction between the fine arts and the crafts, maintaining that a craftsperson could not be considered a practitioner of the arts.
 The increasing tendency to privilege painting, and to a lesser degree sculpture, above other arts has been a feature of Western art as well as East Asian art. In both regions, painting has been seen as relying to the highest degree on the imagination of the artist and being the furthest removed from manual labour – in Chinese painting, the most highly valued styles were those of ""scholar-painting"", at least in theory practiced by gentleman amateurs. The Western hierarchy of genres reflected similar attitudes.
 Training in the visual arts has generally been through variations of the apprentice and workshop systems. In Europe, the Renaissance movement to increase the prestige of the artist led to the academy system for training artists, and today most of the people who are pursuing a career in the arts train in art schools at tertiary levels. Visual arts have now become an elective subject in most education systems.[5][6]
 In East Asia, arts education for nonprofessional artists typically focused on brushwork; calligraphy was numbered among the Six Arts of gentlemen in the Chinese Zhou dynasty, and  calligraphy and Chinese painting were numbered among the four arts of scholar-officials in imperial China.[7][8][9]
 Leading country in the development of the arts in Latin America, in 1875 created the National Society of the Stimulus of the Arts, founded by painters Eduardo Schiaffino, Eduardo Sívori, and other artists. Their guild was rechartered as the National Academy of Fine Arts in 1905 and, in 1923, on the initiative of painter and academic Ernesto de la Cárcova, as a department in the University of Buenos Aires, the Superior Art School of the Nation. Currently, the leading educational organization for the arts in the country is the UNA Universidad Nacional de las Artes.[10]
 Drawing is a means of making an image, illustration or graphic using any of a wide variety of tools and techniques available online and offline. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface using dry media such as graphite pencils, pen and ink, inked brushes, wax color pencils, crayons, charcoals, pastels, and markers. Digital tools, including pens, stylus, that simulate the effects of these are also used. The main techniques used in drawing are: line drawing, hatching, crosshatching, random hatching, shading, scribbling, stippling, and blending. An artist who excels at drawing is referred to as a draftsman or draughtsman.[11]
 Drawing and painting go back tens of thousands of years. Art of the Upper Paleolithic includes figurative art beginning between about 40,000 to 35,000 years ago. Non-figurative cave paintings consisting of hand stencils and simple geometric shapes are even older. Paleolithic cave representations of animals are found in areas such as Lascaux, France and Altamira, Spain in Europe, Maros, Sulawesi in Asia, and Gabarnmung, Australia.
 In ancient Egypt, ink drawings on papyrus, often depicting people, were used as models for painting or sculpture. Drawings on Greek vases, initially geometric, later developed into the human form with black-figure pottery during the 7th century BC.[12]
 With paper becoming common in Europe by the 15th century, drawing was adopted by masters such as Sandro Botticelli, Raphael, Michelangelo, and Leonardo da Vinci, who sometimes treated drawing as an art in its own right rather than a preparatory stage for painting or sculpture.
 Painting taken literally is the practice of applying pigment suspended in a carrier (or medium) and a binding agent (a glue) to a surface (support) such as paper, canvas or a wall. However, when used in an artistic sense it means the use of this activity in combination with drawing, composition, or other aesthetic considerations in order to manifest the expressive and conceptual intention of the practitioner. Painting is also used to express spiritual motifs and ideas; sites of this kind of painting range from artwork depicting mythological figures on pottery to The Sistine Chapel, to the human body itself.[13]
 Like drawing, painting has its documented origins in caves and on rock faces. The finest examples, believed by some to be 32,000 years old, are in the Chauvet and Lascaux caves in southern France. In shades of red, brown, yellow and black, the paintings on the walls and ceilings are of bison, cattle, horses and deer.
 Paintings of human figures can be found in the tombs of ancient Egypt. In the great temple of Ramses II, Nefertari, his queen, is depicted being led by Isis.[14] The Greeks contributed to painting but much of their work has been lost. One of the best remaining representations are the Hellenistic Fayum mummy portraits. Another example is mosaic of the Battle of Issus at Pompeii, which was probably based on a Greek painting. Greek and Roman art contributed to Byzantine art in the 4th century BC, which initiated a tradition in icon painting.[15]
 Apart from the illuminated manuscripts produced by monks during the Middle Ages, the next significant contribution to European art was from Italy's renaissance painters. From Giotto in the 13th century to Leonardo da Vinci and Raphael at the beginning of the 16th century, this was the richest period in Italian art as the chiaroscuro techniques were used to create the illusion of 3-D space.[16]
 Painters in northern Europe too were influenced by the Italian school. Jan van Eyck from Belgium, Pieter Bruegel the Elder from the Netherlands and Hans Holbein the Younger from Germany are among the most successful painters of the times. They used the glazing technique with oils to achieve depth and luminosity.
 The 17th century witnessed the emergence of the great Dutch masters such as the versatile Rembrandt who was especially remembered for his portraits and Bible scenes, and Vermeer who specialized in interior scenes of Dutch life.
 The Baroque started after the Renaissance, from the late 16th century to the late 17th century. Main artists of the Baroque included Caravaggio, who made heavy use of tenebrism. Peter Paul Rubens, a Flemish painter who studied in Italy, worked for local churches in Antwerp and also painted a series for Marie de' Medici. Annibale Carracci took influences from the Sistine Chapel and created the genre of illusionistic ceiling painting. Much of the development that happened in the Baroque was because of the Protestant Reformation and the resulting Counter Reformation. Much of what defines the Baroque is dramatic lighting and overall visuals.[17]
 Impressionism began in France in the 19th century with a loose association of artists including Claude Monet, Pierre-Auguste Renoir and Paul Cézanne who brought a new freely brushed style to painting, often choosing to paint realistic scenes of modern life outside rather than in the studio. This was achieved through a new expression of aesthetic features demonstrated by brush strokes and the impression of reality. They achieved intense color vibration by using pure, unmixed colors and short brush strokes. The movement influenced art as a dynamic, moving through time and adjusting to newfound techniques and perception of art. Attention to detail became less of a priority in achieving, whilst exploring a biased view of landscapes and nature to the artist's eye.[18][19]
 Towards the end of the 19th century, several young painters took impressionism a stage further, using geometric forms and unnatural color to depict emotions while striving for deeper symbolism. Of particular note are Paul Gauguin, who was strongly influenced by Asian, African and Japanese art, Vincent van Gogh, a Dutchman who moved to France where he drew on the strong sunlight of the south, and Toulouse-Lautrec, remembered for his vivid paintings of night life in the Paris district of Montmartre.[20]
 Edvard Munch, a Norwegian artist, developed his symbolistic approach at the end of the 19th century, inspired by the French impressionist Manet. The Scream (1893), his most famous work, is widely interpreted as representing the universal anxiety of modern man. Partly as a result of Munch's influence, the German expressionist movement originated in Germany at the beginning of the 20th century as artists such as Ernst Kirschner and Erich Heckel began to distort reality for an emotional effect.
 In parallel, the style known as cubism developed in France as artists focused on the volume and space of sharp structures within a composition. Pablo Picasso and Georges Braque were the leading proponents of the movement. Objects are broken up, analyzed, and re-assembled in an abstracted form. By the 1920s, the style had developed into surrealism with Dali and Magritte.[21]
 Printmaking is creating, for artistic purposes, an image on a matrix that is then transferred to a two-dimensional (flat) surface by means of ink (or another form of pigmentation). Except in the case of a monotype, the same matrix can be used to produce many examples of the print.
 Historically, the major techniques (also called media) involved are woodcut, line engraving, etching, lithography, and screen printing (serigraphy, silk screening) but there are many others, including modern digital techniques. Normally, the print is printed on paper, but other mediums range from cloth and vellum to more modern materials.
 Prints in the Western tradition produced before about 1830 are known as old master prints. In Europe, from around 1400 AD woodcut, was used for master prints on paper by using printing techniques developed in the Byzantine and Islamic worlds. Michael Wolgemut improved German woodcut from about 1475, and Erhard Reuwich, a Dutchman, was the first to use cross-hatching.  At the end of the century Albrecht Dürer brought the Western woodcut to a stage that has never been surpassed, increasing the status of the single-leaf woodcut.[22]
 In China, the art of printmaking developed some 1,100 years ago as illustrations alongside text cut in woodblocks for printing on paper. Initially images were mainly religious but in the Song dynasty, artists began to cut landscapes. During the Ming (1368–1644) and Qing (1616–1911) dynasties, the technique was perfected for both religious and artistic engravings.[23][24]
 Woodblock printing in Japan (Japanese: 木版画, moku hanga) is a technique best known for its use in the ukiyo-e artistic genre; however, it was also used very widely for printing illustrated books in the same period. Woodblock printing had been used in China for centuries to print books, long before the advent of movable type, but was only widely adopted in Japan during the Edo period (1603–1867).[25][26] Although similar to woodcut in western printmaking in some regards, moku hanga differs greatly in that water-based inks are used (as opposed to western woodcut, which uses oil-based inks), allowing for a wide range of vivid color, glazes and color transparency.
 After the decline of ukiyo-e and introduction of modern printing technologies, woodblock printing continued as a method for printing texts as well as for producing art, both within traditional modes such as ukiyo-e and in a variety of more radical or Western forms that might be construed as modern art.  In the early 20th century, shin-hanga that fused the tradition of ukiyo-e with the techniques of Western paintings became popular, and the works of Hasui Kawase and Hiroshi Yoshida gained international popularity.[27][28] Institutes such as the ""Adachi Institute of Woodblock Prints"" and ""Takezasado"" continue to produce ukiyo-e prints with the same materials and methods as used in the past.[29][30]
 Photography is the process of making pictures by means of the action of light. The light patterns reflected or emitted from objects are recorded onto a sensitive medium or storage chip through a timed exposure. The process is done through mechanical shutters or electronically timed exposure of photons into chemical processing or digitizing devices known as cameras.
 The word comes from the Greek φως phos (""light""), and γραφις graphis (""stylus"", ""paintbrush"") or γραφη graphê, together meaning ""drawing with light"" or ""representation by means of lines"" or ""drawing."" Traditionally, the product of photography has been called a photograph. The term photo is an abbreviation; many people also call them pictures. In digital photography, the term image has begun to replace photograph. (The term image is traditional in geometric optics.)
 Architecture is the process and the product of planning, designing, and constructing buildings or any other structures. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements.
 The earliest surviving written work on the subject of architecture is De architectura, by the Roman architect Vitruvius in the early 1st century AD. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation – firmness, commodity and delight. An equivalent in modern English would be:
 Building first evolved out of the dynamics between needs (shelter, security, worship, etc.) and means (available building materials and attendant skills). As human cultures developed and knowledge began to be formalized through oral traditions and practices, building became a craft, and ""architecture"" is the name given to the most highly formalized and respected versions of that craft.
 Filmmaking is the process of making a motion-picture, from an initial conception and research, through scriptwriting, shooting and recording, animation or other special effects, editing, sound and music work and finally distribution to an audience; it refers broadly to the creation of all types of films, embracing documentary, strains of theatre and literature in film, and poetic or experimental practices, and is often used to refer to video-based processes as well.
 Visual artists are no longer limited to traditional visual arts media. Computers have been used as an ever more common tool in the visual arts since the 1960s. Uses include the capturing or creating of images and forms, the editing of those images (including exploring multiple compositions) and the final rendering or printing (including 3D printing).
Computer art is any in which computers played a role in production or display. Such art can be an image, sound, animation, video, CD-ROM, DVD, video game, website, algorithm, performance or gallery installation.
 Many traditional disciplines now integrate digital technologies, so the lines between traditional works of art and new media works created using computers, have been blurred. For instance, an artist may combine traditional painting with algorithmic art and other digital techniques. As a result, defining computer art by its end product can be difficult. Nevertheless, this type of art is beginning to appear in art museum exhibits, though it has yet to prove its legitimacy as a form unto itself and this technology is widely seen in contemporary art more as a tool, rather than a form as with painting. On the other hand, there are computer-based artworks which belong to a new conceptual and postdigital strand, assuming the same technologies, and their social impact, as an object of inquiry.
 Computer usage has blurred the distinctions between illustrators, photographers, photo editors, 3-D modelers, and handicraft artists. Sophisticated rendering and editing software has led to multi-skilled image developers. Photographers may become digital artists. Illustrators may become animators. Handicraft may be computer-aided or use computer-generated imagery as a template. Computer clip art usage has also made the clear distinction between visual arts and page layout less obvious due to the easy access and editing of clip art in the process of paginating a document, especially to the unskilled observer.
 Plastic arts is a term for art forms that involve physical manipulation of a plastic medium by moulding or modeling such as sculpture or ceramics. The term has also been applied to all the visual (non-literary, non-musical) arts.[31][32]
 Materials that can be carved or shaped, such as stone or wood, concrete or steel, have also been included in the narrower definition, since, with appropriate tools, such materials are also capable of modulation.[citation needed] This use of the term ""plastic"" in the arts should not be confused with Piet Mondrian's use, nor with the movement he termed, in French and English, ""Neoplasticism.""
 Sculpture is three-dimensional artwork created by shaping or combining hard or plastic material, sound, or text and or light, commonly stone (either rock or marble), clay, metal, glass, or wood. Some sculptures are created directly by finding or carving; others are assembled, built together and fired, welded, molded, or cast. Sculptures are often painted.[33] A person who creates sculptures is called a sculptor.
 The earliest undisputed examples of sculpture belong to the Aurignacian culture, which was located in Europe and southwest Asia and active at the beginning of the Upper Paleolithic. As well as producing some of the earliest known cave art, the people of this culture developed finely-crafted stone tools, manufacturing pendants, bracelets, ivory beads, and bone-flutes, as well as three-dimensional figurines.[34][35][36]
 Because sculpture involves the use of materials that can be moulded or modulated, it is considered one of the plastic arts. The majority of public art is sculpture. Many sculptures together in a garden setting may be referred to as a sculpture garden. Sculptors do not always make sculptures by hand. With increasing technology in the 20th century and the popularity of conceptual art over technical mastery, more sculptors turned to art fabricators to produce their artworks. With fabrication, the artist creates a design and pays a fabricator to produce it. This allows sculptors to create larger and more complex sculptures out of materials like cement, metal and plastic, that they would not be able to create by hand. Sculptures can also be made with 3-d printing technology.
 In the United States, the law protecting the copyright over a piece of visual art gives a more restrictive definition of ""visual art"".[37]
 A ""work of visual art"" is —
(1) a painting, drawing, print or sculpture, existing in a single copy, in a limited edition of 200 copies or fewer that are signed and consecutively numbered by the author, or, in the case of a sculpture, in multiple cast, carved, or fabricated sculptures of 200 or fewer that are consecutively numbered by the author and bear the signature or other identifying mark of the author; or
(2) a still photographic image produced for exhibition purposes only, existing in a single copy that is signed by the author, or in a limited edition of 200 copies or fewer that are signed and consecutively numbered by the author.
A work of visual art does not include —
(A)(i) any poster, map, globe, chart, technical drawing, diagram, model, applied art, motion picture or other audiovisual work, book, magazine, newspaper, periodical, data base, electronic information service, electronic publication, or similar publication;
  (ii) any merchandising item or advertising, promotional, descriptive, covering, or packaging material or container;
  (iii) any portion or part of any item described in clause (i) or (ii);
(B) any work made for hire; or
(C) any work not subject to copyright protection under this title.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Architecture', 'Pablo Picasso and Georges Braque', 'Historical civilizations are often identified with their surviving architectural achievements', 'comics, design, crafts, and architecture', 'creation of all types of films'], 'answer_start': [], 'answer_end': []}"
"Other reasons this message may be displayed:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['this message may be displayed:', 'this message may be displayed:', 'this message may be displayed', 'this message may be displayed:', 'reasons'], 'answer_start': [], 'answer_end': []}"
"Other reasons this message may be displayed:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['this message may be displayed:', 'this message may be displayed:', 'this message may be displayed', 'this message may be displayed:', 'reasons'], 'answer_start': [], 'answer_end': []}"
"Digital art refers to any artistic work or practice that uses digital technology as part of the creative or presentation process. It can also refer to computational art that uses and engages with digital media.[2]
 Since the 1960s, various names have been used to describe digital art, including computer art, electronic art, multimedia art,[3] and new media art.[4][5]
 In the early 1960s, John Whitney developed the first computer-generated art using mathematical operations.[6] In 1963, Ivan Sutherland invented the first user interactive computer-graphics interface known as Sketchpad.[7]
Between 1974 and 1977, Salvador Dalí created two big canvases of Gala Contemplating the Mediterranean Sea which at a distance of 20 meters is transformed into the portrait of Abraham Lincoln (Homage to Rothko)[8] and prints of Lincoln in Dalivision based on a portrait of Abraham Lincoln processed on a computer by Leon Harmon published in ""The Recognition of Faces"".[9]
The technique is similar to what later became known as photographic mosaics.
 Andy Warhol created digital art using an Amiga where the computer was publicly introduced at the Lincoln Center, New York, in July 1985. An image of Debbie Harry was captured in monochrome from a video camera and digitized into a graphics program called ProPaint. Warhol manipulated the image by adding color using flood fills.[10][11]
 Digital art can be purely computer-generated (such as fractals and algorithmic art) or taken from other sources, such as a scanned photograph or an image drawn using vector graphics software using a mouse or graphics tablet. Artworks are considered digital paintings when created similarly to non-digital paintings but using software on a computer platform and digitally outputting the resulting image as painted on canvas.
 Despite differing viewpoints on digital technology's impact on the arts, a consensus exists within the digital art community about its significant contribution to expanding the creative domain, i.e., that it has greatly broadened the creative opportunities available to professional and non-professional artists alike.[12]
 Digital visual art consists of either 2D visual information displayed on an electronic visual display or information mathematically translated into 3D information viewed through perspective projection on an electronic visual display. The simplest form, 2D computer graphics, reflects how one might draw with a pencil or paper. In this case, however, the image is on the computer screen, and the instrument you draw with might be a tablet stylus or a mouse. What is generated on your screen might appear to be drawn with a pencil, pen, or paintbrush. The second kind is 3D computer graphics, where the screen becomes a window into a virtual environment, where you arrange objects to be ""photographed"" by the computer.
 Typically 2D computer graphics use raster graphics as their primary means of source data representations, whereas 3D computer graphics use vector graphics in the creation of immersive virtual reality installations. A possible third paradigm is to generate art in 2D or 3D entirely through the execution of algorithms coded into computer programs. This can be considered the native art form of the computer, and an introduction to the history of which is available in an interview with computer art pioneer Frieder Nake.[13] Fractal art, Datamoshing, algorithmic art, and real-time generative art are examples.
 3D graphics are created via the process of designing imagery from geometric shapes, polygons, or NURBS curves[14] to create three-dimensional objects and scenes for use in various media such as film, television, print, rapid prototyping, games/simulations, and special visual effects.
 There are many software programs for doing this. The technology can enable collaboration, lending itself to sharing and augmenting by a creative effort similar to the open source movement and the creative commons in which users can collaborate on a project to create art.[15]
 Pop surrealist artist Ray Caesar works in Maya (a 3D modeling software used for digital animation), using it to create his figures as well as the virtual realms in which they exist.
 Computer-generated animations are animations created with a computer from digital models created by 3D artists or procedurally generated. The term is usually applied to works created entirely with a computer. Movies make heavy use of computer-generated graphics; they are called computer-generated imagery (CGI) in the film industry. In the 1990s and early 2000s, CGI advanced enough that, for the first time, it was possible to create realistic 3D computer animation, although films had been using extensive computer images since the mid-70s. A number of modern films have been noted for their heavy use of photo-realistic CGI.[16]
 Digital painting[17] mainly refers to the process of creating paintings on computer software based on computers or graphic tables. Through pixel simulation, digital brushes in digital software (see the software in Digital painting) can imitate traditional painting paints and tools, such as oil, acrylic acid, pastel, charcoal, and airbrush. Users of the software can also customize the pixel size to achieve a unique visual effect (customized brushes).
 Artists have used artificial intelligence to create artwork since at least the 1960s.[18] Since their design in 2014, some artists have created artwork using a generative adversarial network (GAN), which is a machine learning framework that allows two ""algorithms"" to compete with each other and iterate.[19][20] It can be used to generate pictures that have visual effects similar to traditional fine art. The essential idea of image generators is that people can use text descriptions to let AI convert their text into visual picture content. Anyone can turn their language into a painting through a picture generator.[21] And some artists can use image generators to generate their paintings instead of drawing from scratch, and then they use the generated paintings as a basis to improve them and finally create new digital paintings. This greatly reduces the threshold of painting and challenges the traditional definition of painting art.
 Generally, the user can set the input, and the input content includes detailed picture content that the user wants. For example, the content can be a scene's content, characters, weather, character relationships, specific items, etc. It can also include selecting a specific artist style, screen style, image pixel size, brightness, etc. Then picture generators will return several similar pictures[20] generated according to the input (generally, 4 pictures are given now). After receiving the results generated by picture generators, the user can select one picture as a result he wants or let the generator redraw and return to new pictures.
 In addition, it is worth mentioning the whole process: it is also similar to the ""generator"" and ""discriminator"" modules[19] in GANs.
 In both 1991 and 1992, Karl Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution.[22][23][24]
 In 2009, Eric Millikin won the Pulitzer Prize along with several other awards for his artificial intelligence art that was critical of government corruption in Detroit and resulted in the city's mayor being sent to jail.[25][26][27]
 In 2018 Christie's auction house in New York sold an artificial intelligence work, ""Edmond de Bellamy"" for US$432,500. It was created by a collective in Paris named ""Obvious"".[28]
 In 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the ""interests and culture(s) of people of color.""[29]
 Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.[30]
 In 2022, an amateur artist using Midjourney won the first-place $300 prize in a digital art competition at the Colorado State Fair.[31][21]
 Also in 2022, Refik Anadol created an artificial intelligence art installation at the Museum of Modern Art in New York, based on the museum's own collection.[32]
 In contemporary art, the term digital art is used primarily to describe visual art that is made with digital tools, and also is highly computational, and explicitly engages with digital technologies. Art historian Christiane Paul writes that it ""is highly problematic to classify all art that makes use of digital technologies somewhere in its production and dissemination process as digital art since it makes it almost impossible to arrive at any unifying statement about the art form.[38]
 Computer demos are computer programs, usually non-interactive, that produce audiovisual presentations. They are a novel form of art, which emerged as a consequence of home computer revolution in the early 1980s. In the classification of digital art, they can be best described as real-time proceduraly generated animated audio-visuals.
 This form of art does not concentrate only on aesthetics of the final presentation, but also on complexities and skills involved in creating the presentation. As such, it can be fully enjoyed only by persons with a high level of knowledge in the filed of accompanying computer technologies. On the other hand, many of the created pieces of art are primarily aesthetic or amusing, and those can be enjoyed by general public.
 Digital installation art constitutes a broad field of activity and incorporates many forms. Some resemble video installations, particularly large-scale works involving projections and live video capture. By using projection techniques that enhance an audience's impression of sensory envelopment, many digital installations attempt to create immersive environments. Others go even further and attempt to facilitate a complete immersion in virtual realms. This type of installation is generally site-specific, scalable, and without fixed dimensionality, meaning it can be reconfigured to accommodate different presentation spaces.[40]
 Noah Wardrip-Fruin's ""Screen"" (2003) is an example of interactive digital installation art which makes use of a Cave Automatic Virtual Environment to create an interactive experience.[41] Scott Snibbe's ""Boundary Functions"" is an example of augmented reality digital installation art, which response to people who enter the installation by drawing lines between people, indicating their personal space.[39]
 Internet art is digital art that uses the specific characteristics of the internet and is exhibited on the internet.
 Blockchain, and more specifically NFTs, are associated with digital art since the NFTs craze of 2020 and 2021. Digital art is a common use case for NFTs.[42] By minting a piece of digital art the owner of the NFT is proven to be the owner of the art piece.[43] While the technology received many critics and has many flaws related to plagiarism and fraud (due to its almost completely unregulated nature),[44] auction houses, museums and galleries around the world started collaborations and partnerships with digital artists, selling NFTs associated with digital artworks (via NFT platforms) and showcasing those artworks (associated to the respective NFTs) both in virtual galleries and real-life screens, monitors and TVs.[45][46][47]
 In March 2024, Sotheby's presented an auction highlighting significant contributions of digital artists over the previous decade, [48] one of many record-breaking auctions of digital artwork by the auction house. These auctions look broadly at the cultural impact of digital art in the 21-st century and featured work by artists such as Jennifer & Kevin McCoy, Vera Molnár, Claudia Hart, Jonathan Monaghan and Sarah Zucker.[49][50]
 Notable art theorists and historians in this field include Oliver Grau, Jon Ippolito, Christiane Paul, Frank Popper, Jasia Reichardt, Mario Costa, Christine Buci-Glucksmann, Dominique Moulon, Robert C. Morgan, Roy Ascott, Catherine Perret, Margot Lovejoy, Edmond Couchot, Tina Rivers Ryan, Fred Forest and Edward A. Shanken.
 In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. Although the main goal of digitization was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[51]
 Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[52] Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics.[51] Whereas distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
 Whilst 2D and 3D digital art is beneficial as it allows the preservation of history that would otherwise have been destroyed by events like natural disasters and war, there is the issue of who should own these 3D scans – i.e., who should own the digital copyrights.[53]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Digital art', 'Ray Caesar', 'any artistic work or practice that uses digital technology', 'plagiarism and fraud', 'taken from other sources'], 'answer_start': [], 'answer_end': []}"
"Other reasons this message may be displayed:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['this message may be displayed:', 'this message may be displayed:', 'this message may be displayed', 'this message may be displayed:', 'reasons'], 'answer_start': [], 'answer_end': []}"
"
 Fashion design is the art of applying design, aesthetics, clothing construction and natural beauty to clothing and its accessories. It is influenced by culture and different trends, and has varied over time and place. ""A fashion designer creates clothing, including dresses, suits, pants, and skirts, and accessories like shoes and handbags, for consumers. He or she can specialize in clothing, accessory, or jewelry design, or may work in more than one of these areas.""[1]
 'Fashion designers work in a variety of different ways when designing their pieces and accessories such as rings, bracelets, necklaces and earrings. Due to the time required to put a garment out in market, designers must anticipate changes to consumer desires. Fashion designers are responsible for creating looks for individual garments, involving shape, color, fabric, trimming, and more.[2]
 Fashion designers attempt to design clothes that are functional as well as aesthetically pleasing. They consider who is likely to wear a garment and the situations in which it will be worn, and they work within a wide range of materials, colors, patterns, and styles. Though most clothing worn for everyday wear falls within a narrow range of conventional styles, unusual garments are usually sought for special occasions such as evening wear or party dresses.
 Some clothes are made specifically for an individual, as in the case of haute couture or bespoke tailoring. Today, most clothing is designed for the mass market, especially casual and everyday wear, which are commonly known as ready to wear or fast fashion.
 There are different lines of work for designers in the fashion industry. Fashion designers that work full-time for one fashion house, as 'in-house designers', own the designs and may either work alone or as a part of a design team.  Freelance designers who work for themselves, sell their designs to fashion houses, directly to shops, or to clothing manufacturers. There are quite a few fashion designers who choose to set up their own labels, which offers them full control over their designs. While others are self-employed and design for individual clients. Other high-end fashion designers cater to specialty stores or high-end fashion department stores. These designers create original garments, as well as those that follow established fashion trends. Most fashion designers, however, work for apparel manufacturers, creating designs of men's, women's, and children's fashions for the mass market. Large designer brands that have a 'name' as their brand such as Abercrombie & Fitch, Justice, or Juicy are likely to be designed by a team of individual designers under the direction of a design director.
 Fashion designers work in various ways, some start with a vision in their head and later on move into drawing it on paper or computer , while others go directly into draping fabric onto a dress form, also known as a mannequin. The design process is unique to the designer and it is rather intriguing to see the various steps that go into the process. A designer may choose to work with certain apps that are able to help connect all their ideas together and expand their thoughts to create a cohesive design. When a designer is completely satisfied with the fit of the toile (or muslin), they will consult a professional pattern maker who then makes the finished, working version of the pattern out of card or via a computer program. Finally, a sample garment is made up and tested on a model to make sure it is an operational outfit. Fashion design is expressive, the designers create art that may be functional or non-functional.
 Modern Western fashion design is often considered to have started in the 19th century with Charles Frederick Worth who was the first designer to have his label sewn into the garments that he created. Before the former draper set up his maison couture (fashion house) in Paris, clothing design and creation of the garments were handled largely by anonymous seamstresses. At the time high fashion descended from what was popularly worn at royal courts. Worth's success was such that he was able to dictate to his customers what they should wear, instead of following their lead as earlier dressmakers had done. The term couturier was in fact first created in order to describe him. While all articles of clothing from any time period are studied by academics as costume design, only clothing created after 1858 is considered fashion design.[3]
 It was during this period that many design houses began to hire artists to sketch or paint designs for garments. Rather than going straight into manufacturing, the images were shown to clients to gain approval, which saved time and money for the designer. If the client liked their design, the patrons commissioned the garment from the designer, and it was produced for the client in the fashion house. This designer-patron construct launched designers sketching their work rather than putting the completed designs on models.
 Garments produced by clothing manufacturers fall into three main categories, although these may be split up into additional, different types.
 Until the 1950s, fashion clothing was predominately designed and manufactured on a made-to-measure or haute couture basis (French for high-sewing), with each garment being created for a specific client. A couture garment is made to order for an individual customer, and is usually made from high-quality, expensive fabric, sewn with extreme attention to detail and finish, often using time-consuming, hand-executed techniques. Look and fit take priority over the cost of materials and the time it takes to make.[4][5] Due to the high cost of each garment, haute couture makes little direct profit for the fashion houses, but is important for prestige and publicity.[6]
 Ready-to-wear, or prêt-à-porter, clothes are a cross between haute couture and mass market. They are not made for individual customers, but great care is taken in the choice and cut of the fabric. Clothes are made in small quantities to guarantee exclusivity, so they are rather expensive. Ready-to-wear collections are usually presented by fashion houses each season during a period known as Fashion Week. This takes place on a citywide basis and occurs twice a year. The main seasons of Fashion Week include; spring/summer, fall/winter, resort, swim, and bridal.
 Half-way garments are an alternative to ready-to-wear, ""off-the-peg"", or prêt-à-porter fashion. Half-way garments are intentionally unfinished pieces of clothing that encourage co-design between the ""primary designer"" of the garment, and what would usually be considered, the passive ""consumer"".[7] This differs from ready-to-wear fashion, as the consumer is able to participate in the process of making and co-designing their clothing.  During the Make{able} workshop, Hirscher and Niinimaki found that personal involvement in the garment-making process created a meaningful ""narrative"" for the user, which established a person-product attachment and increased the sentimental value of the final product.[7]
 Otto von Busch also explores half-way garments and fashion co-design in his thesis, ""Fashion-able, Hacktivism and engaged Fashion Design"".[8]
 Currently, the fashion industry relies more on mass-market sales. The mass market caters for a wide range of customers, producing ready-to-wear garments using trends set by the famous names in fashion. They often wait around a season to make sure a style is going to catch on before producing their versions of the original look. To save money and time, they use cheaper fabrics and simpler production techniques which can easily be done by machines. The end product can, therefore, be sold much more cheaply.[9][10][11]
 There is a type of design called ""kutch"" originated from the German word kitschig, meaning ""trashy"" or ""not aesthetically pleasing"".  Kitsch can also refer to ""wearing or displaying something that is therefore no longer in fashion"".[12]
 The median annual wages for salaried fashion designers was $74,410 in February of 2023. The middle 50 percent earned an average of 76,700. The lowest 10 percent earned 32,320 and the highest 10 percent earned 130,900.[13] Median annual earnings in May 2008 were $52,860 (£40,730.47) in apparel, piece goods, and notions - the industry employing the largest numbers of fashion designers.[14] In 2016, 23,800 people were counted as fashion designers in the United States.[15]
 Fashion today is a global industry, and most major countries have a fashion industry. Seven countries have established an international reputation in fashion: the United States, France, Italy, United Kingdom, Japan, Germany and Belgium. The ""big four"" fashion capitals of the fashion industry are New York City, Paris, Milan, and London.
 The United States is home to the largest, wealthiest, and most multi-faceted fashion industry. Most fashion houses in the United States are based in New York City, with a high concentration centered in the Garment District neighborhood. On the US west coast, there is also to a lesser extent a significant number of fashion houses in Los Angeles, where a substantial percentage of high fashion clothing manufactured in the United States is actually made. Miami has also emerged as a new fashion hub, especially in regards to swimwear and other beach-oriented fashion. A semi-annual event held every February and September, New York Fashion Week is the oldest of the four major fashion weeks held throughout the world. Parsons The New School for Design, located in the Greenwich Village neighborhood of Lower Manhattan in New York City, is considered one of the top fashion schools in the world. There are numerous fashion magazines published in the United States and distributed to a global readership. Examples include Vogue, Harper's Bazaar, and Cosmopolitan.
 American fashion design is highly diverse, reflecting the enormous ethnic diversity of the population, but is largely dominated by a clean-cut, urban, hip aesthetic, and often favors a more casual style, reflecting the athletic, health-conscious lifestyles of the suburban and urban middle classes. The annual Met Gala ceremony in Manhattan is widely regarded as the world's most prestigious haute couture fashion event and is a venue where fashion designers and their creations are celebrated. Social media is also a place where fashion is presented most often. Some influencers are paid huge amounts of money to promote a product or clothing item, where the business hopes many viewers will buy the product off the back of the advertisement. Instagram is the most popular platform for advertising, but Facebook, Snapchat, Twitter and other platforms are also used.[16] In New York, the LGBT fashion design community contributes very significantly to promulgating fashion trends, and drag celebrities have developed a profound influence upon New York Fashion Week.[17][18]
 Prominent American brands and designers include Calvin Klein, Ralph Lauren, Coach, Nike, Vans, Marc Jacobs, Tommy Hilfiger, DKNY, Tom Ford, Caswell-Massey, Michael Kors, Levi Strauss and Co., Estée Lauder, Revlon, Kate Spade, Alexander Wang, Vera Wang, Victoria's Secret, Tiffany and Co., Converse, Oscar de la Renta, John Varvatos, Anna Sui, Prabal Gurung, Bill Blass, Halston, Carhartt, Brooks Brothers, Stuart Weitzman, Diane von Furstenberg, J. Crew, American Eagle Outfitters, Steve Madden, Abercrombie and Fitch, Juicy Couture, Thom Browne, Guess, Supreme, and The Timberland Company.
 In the late 1980s and early 1990s, Belgian fashion designers brought a new fashion image that mixed East and West, and brought a highly individualised, personal vision on fashion. Well known Belgian designers are the Antwerp Six: Ann Demeulemeester, Dries Van Noten, Dirk Bikkembergs, Dirk Van Saene, Walter Van Beirendonck and Marina Yee, as well as Maison Martin Margiela, Raf Simons, Kris Van Assche, Bruno Pieters, Anthony Vaccarello.[19]
 London has long been the capital of the United Kingdom fashion industry and has a wide range of foreign designs which have integrated with modern British styles. Typical British design is smart but innovative yet recently has become more and more unconventional, fusing traditional styles with modern techniques. Vintage styles play an important role in the British fashion and styling industry. Stylists regularly 'mix and match' the old with the new, which gives British style a unique, bohemian aesthetic. Irish fashion (both design and styling) is also heavily influenced by fashion trends from Britain. Well-known British designers include Thomas Burberry, Alfred Dunhill, Paul Smith, Vivienne Westwood, Stella McCartney, John Galliano, John Richmond, Alexander McQueen, Matthew Williamson, Gareth Pugh, Hussein Chalayan and Neil Barrett.
 Most French fashion houses are in Paris, which is the capital of French fashion. Traditionally, French fashion is chic and stylish, defined by its sophistication, cut, and smart accessories. French fashion is internationally acclaimed.
 Madrid and Barcelona are the main fashion centers in Spain. Spanish fashion is often more conservative and traditional but also more 'timeless' than other fashion cultures. Spaniards are known not to take great risks when dressing.[20][21] Nonetheless, many are the fashion brands and designers coming from Spain.
 The most notable luxury houses are Loewe and Balenciaga. Famous designers include Manolo Blahnik, Elio Berhanyer, Cristóbal Balenciaga, Paco Rabanne, Adolfo Domínguez, Manuel Pertegaz, Jesús del Pozo, Felipe Varela and Agatha Ruiz de la Prada.
 Spain is also home to large fashion brands such as Zara, Massimo Dutti, Bershka, Pull&Bear, Mango, Desigual, Pepe Jeans and Camper.
 Berlin is the centre of fashion in Germany (prominently displayed at Berlin Fashion Week), while Düsseldorf holds Europe's largest fashion trade fairs with Igedo. Other important centres of the scene are Munich, Hamburg, and Cologne. German fashion is known for its elegant lines as well as unconventional young designs and the great variety of styles.
 Most of the Indian fashion houses are in Mumbai, Lakme Fashion Week is considered one of the premier fashion events in the country.[citation needed]
 Milan is Italy's fashion capital. Most of the older Italian couturiers are in Rome. However, Milan and Florence are the Italian fashion capitals, and it is the exhibition venue for their collections. Italian fashion features casual and glamorous elegance. In Italy, Milan Fashion Week takes place twice a year in February and September. Milan Fashion week puts fashion in the spotlight and celebrates it in the heart of Milan with fashion lovers, buyers and media.
 Most Japanese fashion houses are in Tokyo which is home to Tokyo Fashion Week, Asia's largest fashion week. The Japanese look is loose and unstructured (often resulting from complicated cutting), colors tend to the sombre and subtle, and richly textured fabrics. Famous Japanese designers include Kenzo Takada, Issey Miyake, Yohji Yamamoto and Rei Kawakubo.
 Chinese clothing has historically been associated with lower quality both inside and outside China, leading to a stigma on Chinese brands. Due to government censorship, Chinese citizens were only able to access fashion magazines in the 1990s.[22] However, as more and more Chinese designers matriculate from the world's top fashion schools, Chinese designers such as Shushu/Tong and Rui Zhou have made their way into the world's top fashion weeks, and Shanghai has become a fashion hub in China.[23][22] In the early 2020s, Gen Z shoppers pioneered the guochao (Chinese: 国潮; pinyin: Guó cháo) movement, a trend of preferring homegrown designers which incorporate aspects of Chinese history and culture.[23] Hong Kong clothing brand Shanghai Tang's design concept is inspired by Chinese clothing and set out to rejuvenate Chinese fashion of the 1920s and 30s, with a modern twist of the 21st century and its usage of bright colours.[24] Additionally, a revival in interest in traditional Han clothing has led to interest in haute couture clothing with historical Chinese details, particularly around Chinese New Year.[25]
 Fashion in the Soviet Union largely followed general trends of the Western world. However, the state's socialist ideology consistently moderated and influenced these trends. In addition, shortages of consumer goods meant that the general public did not have ready access to pre-made fashion.
 Most of the Swiss fashion houses are in Zürich.[26]  The Swiss look is casual elegant and luxurious with a slight touch of quirkiness. Additionally, it has been greatly influenced by the dance club scene.
 In the development of Mexican indigenous dress, the fabrication was determined by the materials and resources that are available in specific regions, impacting the ""fabric, shape and construction of a people's clothing"".[27] Textiles were created from plant fibers including cotton and agave. Class status differentiated what fabric was worn. Mexican dress was influenced by geometric shapes to create the silhouettes. Huipil a blouse characterized by a ""loose, sleeveless tunic made of two or three joined webs of cloth sewn lengthwise""[27] is an important historical garment, often seen today. After the Spanish Conquest, traditional Mexican clothing shifted to take a Spanish resemblance.
 Mexican indigenous groups rely on specific embroidery and colors to differentiate themselves from each other.[28]
 Mexican Pink is a significant color to the identity of Mexican art and design and general spirit. The term ""Rosa Mexicano"" as described by Ramón Valdiosera was established by prominent figures such as Dolores del Río and designer Ramón Val in New York.[28]
 When newspapers and magazines such as El Imparcial and El Mundo Ilustrado circulated in Mexico, became a significant movement, as it informed the large cities, such as Mexico City, of European fashions. This encouraged the founding of department stores, changing the existent pace of fashion.[29] With access to European fashion and dress, those with high social status relied on adopting those elements to distinguish themselves from the rest. Juana Catarina Romero was a successful entrepreneur and pioneer in this movement.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['costume design', 'Dolores del Río and designer Ramón Val', 'important historical garment', 'culture and different trends', 'cost of materials and the time it takes to make'], 'answer_start': [], 'answer_end': []}"
"
 Interior design is the art and science of enhancing the interior of a building to achieve a healthier and more aesthetically pleasing environment for the people using the space. With a keen eye for detail and a creative flair, an interior designer is someone who plans, researches, coordinates, and manages such enhancement projects. Interior design is a multifaceted profession that includes conceptual development, space planning, site inspections, programming, research, communicating with the stakeholders of a project, construction management, and execution of the design.
 In the past, interiors were put together instinctively as a part of the process of building.[1]
 The profession of interior design has been a consequence of the development of society and the complex architecture that has resulted from the development of industrial processes.
 The pursuit of effective use of space, user well-being and functional design has contributed to the development of the contemporary interior design profession. The profession of interior design is separate and distinct from the role of interior decorator, a term commonly used in the US; the term is less common in the UK, where the profession of interior design is still unregulated and therefore, strictly speaking, not yet officially a profession.
 In ancient India, architects would also function as interior designers. This can be seen from the references of Vishwakarma the architect—one of the gods in Indian mythology. In these architects' design of 17th-century Indian homes, sculptures depicting ancient texts and events are seen inside the palaces, while during the medieval times wall art paintings were a common feature of palace-like mansions in India commonly known as havelis. While most traditional homes have been demolished to make way to modern buildings, there are still around 2000 havelis[2] in the Shekhawati region of Rajashtan that display wall art paintings.
 In ancient Egypt, ""soul houses"" (or models of houses) were placed in tombs as receptacles for food offerings. From these, it is possible to discern details about the interior design of different residences throughout the different Egyptian dynasties, such as changes in ventilation, porticoes, columns, loggias, windows, and doors.[3]
 Painting interior walls has existed for at least 5,000 years, with examples found as far north as the Ness of Brodgar,[4] as have templated interiors, as seen in the associated Skara Brae settlement.[5] It was the Greeks, and later Romans who added co-ordinated, decorative mosaics floors,[6] and templated bath houses, shops, civil offices, Castra (forts) and temple, interiors, in the first millennia BC. With specialised guilds dedicated to producing interior decoration, and formulaic furniture, in buildings constructed to forms defined by Roman architects, such as Vitruvius: De architectura, libri decem (The Ten Books on Architecture).[7][8]
 Throughout the 17th and 18th century and into the early 19th century, interior decoration was the concern of the homemaker, or an employed upholsterer or craftsman who would advise on the artistic style for an interior space. Architects would also employ craftsmen or artisans to complete interior design for their buildings.
 In the mid-to-late 19th century, interior design services expanded greatly, as the middle class in industrial countries grew in size and prosperity and began to desire the domestic trappings of wealth to cement their new status. Large furniture firms began to branch out into general interior design and management, offering full house furnishings in a variety of styles. This business model flourished from the mid-century to 1914, when this role was increasingly usurped by independent, often amateur, designers. This paved the way for the emergence of the professional interior design in the mid-20th century.[9]
 In the 1950s and 1960s, upholsterers began to expand their business remits. They framed their business more broadly and in artistic terms and began to advertise their furnishings to the public. To meet the growing demand for contract interior work on projects such as offices, hotels, and public buildings, these businesses became much larger and more complex, employing builders, joiners, plasterers, textile designers, artists, and furniture designers, as well as engineers and technicians to fulfil the job. Firms began to publish and circulate catalogs with prints for different lavish styles to attract the attention of expanding middle classes.[9]
 As department stores increased in number and size, retail spaces within shops were furnished in different styles as examples for customers. One particularly effective advertising tool was to set up model rooms at national and international exhibitions in showrooms for the public to see. Some of the pioneering firms in this regard were Waring & Gillow, James Shoolbred, Mintons, and Holland & Sons. These traditional high-quality furniture making firms began to play an important role as advisers to unsure middle class customers on taste and style, and began taking out contracts to design and furnish the interiors of many important buildings in Britain.[10]
 This type of firm emerged in America after the Civil War. The Herter Brothers, founded by two German émigré brothers, began as an upholstery warehouse and became one of the first firms of furniture makers and interior decorators. With their own design office and cabinet-making and upholstery workshops, Herter Brothers were prepared to accomplish every aspect of interior furnishing including decorative paneling and mantels, wall and ceiling decoration, patterned floors, and carpets and draperies.[11]
 A pivotal figure in popularizing theories of interior design to the middle class was the architect Owen Jones, one of the most influential design theorists of the nineteenth century.[12] Jones' first project was his most important—in 1851, he was responsible for not only the decoration of Joseph Paxton's gigantic Crystal Palace for the Great Exhibition but also the arrangement of the exhibits within. He chose a controversial palette of red, yellow, and blue for the interior ironwork and, despite initial negative publicity in the newspapers, was eventually unveiled by Queen Victoria to much critical acclaim. His most significant publication was The Grammar of Ornament (1856),[13] in which Jones formulated 37 key principles of interior design and decoration.
 Jones was employed by some of the leading interior design firms of the day; in the 1860s, he worked in collaboration with the London firm Jackson & Graham to produce furniture and other fittings for high-profile clients including art collector Alfred Morrison as well as Ismail Pasha, Khedive of Egypt.
 In 1882, the London Directory of the Post Office listed 80 interior decorators. Some of the most distinguished companies of the period were Crace, Waring & Gillowm and Holland & Sons; famous decorators employed by these firms included Thomas Edward Collcutt, Edward William Godwin, Charles Barry, Gottfried Semper, and George Edmund Street.[14]
 
By the turn of the 20th century, amateur advisors and publications were increasingly challenging the monopoly that the large retail companies had on interior design. English feminist author Mary Haweis wrote a series of widely read essays in the 1880s in which she derided the eagerness with which aspiring middle-class people furnished their houses according to the rigid models offered to them by the retailers.[15] She advocated the individual adoption of a particular style, tailor-made to the individual needs and preferences of the customer: One of my strongest convictions, and one of the first canons of good taste, is that our houses, like the fish's shell and the bird's nest, ought to represent our individual taste and habits. The move toward decoration as a separate artistic profession, unrelated to the manufacturers and retailers, received an impetus with the 1899 formation of the Institute of British Decorators; with John Dibblee Crace as its president, it represented almost 200 decorators around the country.[16] By 1915, the London Directory listed 127 individuals trading as interior decorators, of which 10 were women. Rhoda Garrett and Agnes Garrett were the first women to train professionally as home decorators in 1874. The importance of their work on design was regarded at the time as on a par with that of William Morris. In 1876, their work – Suggestions for House Decoration in Painting, Woodwork and Furniture – spread their ideas on artistic interior design to a wide middle-class audience.[17]
 
By 1900, the situation was described by The Illustrated Carpenter and Builder:[18] Until recently when a man wanted to furnish he would visit all the dealers and select piece by piece of furniture ....Today he sends for a dealer in art furnishings and fittings who surveys all the rooms in the house and he brings his artistic mind to bear on the subject. In America, Candace Wheeler was one of the first woman interior designers and helped encourage a new style of American design. She was instrumental in the development of art courses for women in a number of major American cities and was considered a national authority on home design. An important influence on the new profession was The Decoration of Houses, a manual of interior design written by Edith Wharton with architect Ogden Codman in 1897 in America. In the book, the authors denounced Victorian-style interior decoration and interior design, especially those rooms that were decorated with heavy window curtains, Victorian bric-a-brac, and overstuffed furniture. They argued that such rooms emphasized upholstery at the expense of proper space planning and architectural design and were, therefore, uncomfortable and rarely used. The book is considered a seminal work, and its success led to the emergence of professional decorators working in the manner advocated by its authors, most notably Elsie de Wolfe.[19]
 Elsie De Wolfe was one of the first interior designers. Rejecting the Victorian style she grew up with, she chose a more vibrant scheme, along with more comfortable furniture in the home. Her designs were light, with fresh colors and delicate Chinoiserie furnishings, as opposed to the Victorian preference of heavy, red drapes and upholstery, dark wood and intensely patterned wallpapers. Her designs were also more practical;[20] she eliminated the clutter that occupied the Victorian home, enabling people to entertain more guests comfortably. In 1905, de Wolfe was commissioned for the interior design of the Colony Club on Madison Avenue; its interiors garnered her recognition almost over night.[21][22] She compiled her ideas into her widely read 1913 book, The House in Good Taste.[23]
 In England, Syrie Maugham became a legendary interior designer credited with designing the first all-white room. Starting her career in the early 1910s, her international reputation soon grew; she later expanded her business to New York City and Chicago.[24] Born during the Victorian Era, a time characterized by dark colors and small spaces, she instead designed rooms filled with light and furnished in multiple shades of white and mirrored screens. In addition to mirrored screens, her trademark pieces included: books covered in white vellum, cutlery with white porcelain handles, console tables with plaster palm-frond, shell, or dolphin bases, upholstered and fringed sleigh beds, fur carpets, dining chairs covered in white leather, and lamps of graduated glass balls, and wreaths.[25]
 The interior design profession became more established after World War II. From the 1950s onwards, spending on the home increased. Interior design courses were established, requiring the publication of textbooks and reference sources. Historical accounts of interior designers and firms distinct from the decorative arts specialists were made available. Organisations to regulate education, qualifications, standards and practices, etc. were established for the profession.[23]
 Interior design was previously seen as playing a secondary role to architecture. It also has many connections to other design disciplines, involving the work of architects, industrial designers, engineers, builders, craftsmen, etc. For these reasons, the government of interior design standards and qualifications was often incorporated into other professional organisations that involved design.[23] Organisations such as the Chartered Society of Designers, established in the UK in 1986, and the American Designers Institute, founded in 1938,[26] governed various areas of design.
 It was not until later that specific representation for the interior design profession was developed. The US National Society of Interior Designers was established in 1957, while in the UK the Interior Decorators and Designers Association was established in 1966. Across Europe, other organisations such as The Finnish Association of Interior Architects (1949) were being established and in 1994 the International Interior Design Association was founded.[23]
 Ellen Mazur Thomson, author of Origins of Graphic Design in America (1997), determined that professional status is achieved through education, self-imposed standards and professional gate-keeping organizations.[23] Having achieved this, interior design became an accepted profession.
 Interior design is the art and science of understanding people's behavior to create functional spaces, that are aesthetically pleasing, within a building. Decoration is the furnishing or adorning of a space with decorative elements, sometimes complemented by advice and practical assistance. In short, interior designers may decorate, but decorators do not design.
 Interior designer implies that there is more of an emphasis on planning, functional design and the effective use of space, as compared to interior decorating. An interior designer in fine line design can undertake projects that include arranging the basic layout of spaces within a building as well as projects that require an understanding of technical issues such as window and door positioning, acoustics, and lighting.[1] Although an interior designer may create the layout of a space, they may not alter load-bearing walls without having their designs stamped for approval by a structural engineer. Interior designers often work directly with architects, engineers and contractors.
 Interior designers must be highly skilled in order to create interior environments that are functional, safe, and adhere to building codes, regulations and ADA requirements. They go beyond the selection of color palettes and furnishings and apply their knowledge to the development of construction documents, occupancy loads, healthcare regulations and sustainable design principles, as well as the management and coordination of professional services including mechanical, electrical, plumbing, and life safety—all to ensure that people can live, learn or work in an innocuous environment that is also aesthetically pleasing.
 Someone may wish to specialize and develop technical knowledge specific to one area or type of interior design, such as residential design, commercial design, hospitality design, healthcare design, universal design, exhibition design, furniture design, and spatial branding. 
Interior design is a creative profession that is relatively new, constantly evolving, and often confusing to the public. It is not always an artistic pursuit and can rely on research from many fields to provide a well-trained understanding of how people are often influenced by their environments.
 Color is a powerful design tool in decoration, as well as in interior design, which is the art of composing and coordinating colors together to create a stylish scheme on the interior architecture of the space.[27]
 It can be important to interior designers to acquire a deep experience with colors, understand their psychological effects, and understand the meaning of each color in different locations and situations in order to create suitable combinations for each place.[28]
 Combining colors together could result in creating a state of mind as seen by the observer, and could eventually result in positive or negative effects on them. Colors can make the room feel either more calm, cheerful, comfortable, stressful, or dramatic. Color combinations can make a tiny room seem larger or smaller.[29] So it is for the Interior designer to choose appropriate colors for a place towards achieving how clients would want to look at, and feel in, that space.[28]
 In 2024, red-colored home accessories were popularized on social media and in several design magazines for claiming to enhance interior design. This was coined the Unexpected Red Theory.
 Residential design is the design of the interior of private residences. As this type of design is specific for individual situations, the needs and wants of the individual are paramount in this area of interior design. The interior designer may work on the project from the initial planning stage or may work on the remodeling of an existing structure. It is often a process that takes months to fine-tune and create a space with the vision of the client.[30]
 Commercial design encompasses a wide range of subspecialties.
 Other areas of specialization include amusement and theme park design, museum and exhibition design, exhibit design, event design (including ceremonies, weddings, baby and bridal showers, parties, conventions, and concerts), interior and prop styling, craft styling, food styling, product styling, tablescape design, theatre and performance design, stage and set design, scenic design, and production design for film and television. Beyond those, interior designers, particularly those with graduate education, can specialize in healthcare design, gerontological design, educational facility design, and other areas that require specialized knowledge. Some university programs offer graduate studies in theses and other areas. For example, both Cornell University and the University of Florida offer interior design graduate programs in environment and behavior studies.
 There are various paths that one can take to become a professional interior designer. All of these paths involve some form of training. Working with a successful professional designer is an informal method of training and has previously been the most common method of education. In many states, however, this path alone cannot lead to licensing as a professional interior designer. Training through an institution such as a college, art or design school or university is a more formal route to professional practice.
 In many countries, several university degree courses are now available, including those on interior architecture, taking three or four years to complete.
 A formal education program, particularly one accredited by or developed with a professional organization of interior designers, can provide training that meets a minimum standard of excellence and therefore gives a student an education of a high standard. There are also university graduate and Ph.D. programs available for those seeking further training in a specific design specialization (i.e. gerontological or healthcare design) or those wishing to teach interior design at the university level.
 There are a wide range of working conditions and employment opportunities within interior design. Large and tiny corporations often hire interior designers as employees on regular working hours. Designers for smaller firms and online renovation platforms usually work on a contract or per-job basis. Self-employed designers, who made up 32% of interior designers in 2020,[31] usually work the most hours. Interior designers often work under stress to meet deadlines, stay on budget, and meet clients' needs and wishes.
 In some cases, licensed professionals review the work and sign it before submitting the design for approval by clients or construction permitting. The need for licensed review and signature varies by locality, relevant legislation, and scope of work. Their work can involve significant travel to visit different locations. However, with technology development, the process of contacting clients and communicating design alternatives has become easier and requires less travel.[32]
 The Art Deco style began in Europe in the early years of the 20th century, with the waning of Art Nouveau. The term ""Art Deco"" was taken from the Exposition Internationale des Arts Decoratifs et Industriels Modernes, a world's fair held in Paris in 1925.[33] Art Deco rejected many traditional classical influences in favour of more streamlined geometric forms and metallic color. The Art Deco style influenced all areas of design, especially interior design, because it was the first style of interior decoration to spotlight new technologies and materials.[34]
 Art Deco style is mainly based on geometric shapes, streamlining, and clean lines.[35][36] The style offered a sharp, cool look of mechanized living utterly at odds with anything that came before.[37]
 Art Deco rejected traditional materials of decoration and interior design, opting instead to use more unusual materials such as chrome, glass, stainless steel, shiny fabrics, mirrors, aluminium, lacquer, inlaid wood, sharkskin, and zebra skin.[34] The use of harder, metallic materials was chosen to celebrate the machine age. These materials reflected the dawning modern age that was ushered in after the end of the First World War. The innovative combinations of these materials created contrasts that were very popular at the time – for example the mixing together of highly polished wood and black lacquer with satin and furs.[38] The barber shop in the Austin Reed store in London was designed by P. J. Westwood. It was soon regarded as the trendiest barber shop in Britain due to its use of metallic materials.[37]
 The color themes of Art Deco consisted of metallic color, neutral color, bright color, and black and white. In interior design, cool metallic colors including silver, gold, metallic blue, charcoal grey, and platinum tended to predominate.[35][39] Serge Chermayeff, a Russian-born British designer made extensive use of cool metallic colors and luxurious surfaces in his room schemes. His 1930 showroom design for a British dressmaking firm had a silver-grey background and black mirrored-glass wall panels.[37][40]
 Black and white was also a very popular color scheme during the 1920s and 1930s. Black and white checkerboard tiles, floors and wallpapers were very trendy at the time.[41] As the style developed, bright vibrant colors became popular as well.[42]
 Art Deco furnishings and lighting fixtures had a glossy, luxurious appearance with the use of inlaid wood and reflective finishes. The furniture pieces often had curved edges, geometric shapes, and clean lines.[33][37] Art Deco lighting fixtures tended to make use of stacked geometric patterns.[43]
 Modern design grew out of the decorative arts, mostly from the Art Deco, in the early 20th century.[44] One of the first to introduce this style was Frank Lloyd Wright, who had not become hugely popularized until completing the house called Fallingwater in the 1930s. Modern art reached its peak during the 1950s and '60s, which is why designers and decorators today may refer to modern design as being ""mid-century"".[44] Modern art does not refer to the era or age of design and is not the same as contemporary design, a term used by interior designers for a shifting group of recent styles and trends.[44]
 ""Majlis painting"", also called nagash painting, is the decoration of the majlis, or front parlor of traditional Arabic homes, in the Asir province of Saudi Arabia and adjoining parts of Yemen. These wall paintings, an arabesque form of mural or fresco, show various geometric designs in bright colors: ""Called 'nagash' in Arabic, the wall paintings were a mark of pride for a woman in her house.""[45]
 The geometric designs and heavy lines seem to be adapted from the area's textile and weaving patterns. ""In contrast with the sobriety of architecture and decoration in the rest of Arabia, exuberant color and ornamentation characterize those of Asir. The painting extends into the house over the walls and doors, up the staircases, and onto the furniture itself. When a house is being painted, women from the community help each other finish the job. The building then displays their shared taste and knowledge. Mothers pass these on to their daughters. This artwork is based on a geometry of straight lines and suggests the patterns common to textile weaving, with solid bands of different colors. Certain motifs reappear, such as the triangular mihrab or 'niche' and the palmette. In the past, paint was produced from mineral and vegetable pigments. Cloves and alfalfa yielded green. Blue came from the indigo plant. Red came from pomegranates and a certain mud. Paintbrushes were created from the tough hair found in a goat's tail. Today, however, women use modern manufactured paint to create new looks, which have become an indicator of social and economic change.""[46]
 Women in the Asir province often complete the decoration and painting of the house interior. ""You could tell a family's wealth by the paintings,"" Um Abdullah says: ""If they didn't have much money, the wife could only paint the motholath, the basic straight, simple lines, in patterns of three to six repetitions in red, green, yellow and brown."" When women did not want to paint the walls themselves, they could barter with other women who would do the work. Several Saudi women have become famous as majlis painters, such as Fatima Abou Gahas.[45]
 The interior walls of the home are brightly painted by the women, who work in defined patterns with lines, triangles, squares, diagonals and tree-like patterns. ""Some of the large triangles represent mountains. Zigzag lines stand for water and also for lightning. Small triangles, especially when the widest area is at the top, are found in pre-Islamic representations of female figures. That the small triangles found in the wall paintings in 'Asir are called banat may be a cultural remnant of a long-forgotten past.""[45]
 ""Courtyards and upper pillared porticoes are principal features of the best Nadjdi architecture, in addition to the fine incised plaster wood (jiss) and painted window shutters, which decorate the reception rooms. Good examples of plasterwork can often be seen in the gaping ruins of torn-down buildings- the effect is light, delicate and airy. It is usually around the majlis, around the coffee hearth and along the walls above where guests sat on rugs, against cushions. Doughty wondered if this ""parquetting of jis"", this ""gypsum fretwork... all adorning and unenclosed"" originated from India. However, the Najd fretwork seems very different from that seen in the Eastern Province and Oman, which are linked to Indian traditions, and rather resembles the motifs and patterns found in ancient Mesopotamia. The rosette, the star, the triangle and the stepped pinnacle pattern of dadoes are all ancient patterns, and can be found all over the Middle East of antiquity. Al-Qassim Province seems to be the home of this art, and there it is normally worked in hard white plaster (though what you see is usually begrimed by the smoke of the coffee hearth). In Riyadh, examples can be seen in unadorned clay.[47]
 Interior design has become the subject of television shows. In the United Kingdom, popular interior design and decorating programs include 60 Minute Makeover (ITV), Changing Rooms (BBC), and Selling Houses (Channel 4). Famous interior designers whose work is featured in these programs include Linda Barker and Laurence Llewelyn-Bowen. In the United States, the TLC Network aired a popular program called Trading Spaces, a show based on the UK program Changing Rooms. In addition, both HGTV and the DIY Network also televise many programs about interior design and decorating, featuring the works of a variety of interior designers, decorators, and home improvement experts in a myriad of projects.
 Fictional interior decorators include the Sugarbaker sisters on Designing Women and Grace Adler on Will & Grace. There is also another show called Home MADE. There are two teams and two houses and whoever has the designed and made the worst room, according to the judges, is eliminated. Another show on the Style Network, hosted by Niecy Nash, is Clean House where they re-do messy homes into themed rooms that the clients would like. Other shows include Design on a Dime, Designed to Sell, and The Decorating Adventures of Ambrose Price. The show called Design Star has become more popular through the five seasons that have already aired. The winners of this show end up getting their own TV shows, of which are Color Splash hosted by David Bromstad, Myles of Style hosted by Kim Myles, Paint-Over! hosted by Jennifer Bertrand, The Antonio Treatment hosted by Antonio Ballatore, and finally Secrets from a Stylist hosted by Emily Henderson. Bravo also has a variety of shows that explore the lives of interior designers. These include Flipping Out, which explores the life of Jeff Lewis and his team of designers; Million Dollar Decorators explores the lives of interior designers Nathan Turner, Jeffrey Alan Marks, Mary McDonald, Kathryn Ireland, and Martyn Lawrence Bullard.
 Interior design has also become the subject of radio shows. In the U.S., popular interior design & lifestyle shows include Martha Stewart Living and Living Large featuring Karen Mills. Famous interior designers whose work is featured on these programs include Bunny Williams, Barbara Barry, and Kathy Ireland, among others.
 Many interior design magazines exist to offer advice regarding color palette, furniture, art, and other elements that fall under the umbrella of interior design. These magazine often focus on related subjects to draw a more specific audience. For instance, architecture as a primary aspect of Dwell, while Veranda is well known as a luxury living magazine. Lonny Magazine and the newly relaunched, Domino Magazine, cater to a young, hip, metropolitan audience, and emphasize accessibility and a do-it-yourself (DIY) approach to interior design.
 Other early interior decorators:
 Many of the most famous designers and decorators during the 20th century had no formal training. Some examples include Sister Parish, Robert Denning and Vincent Fourcade, Kerry Joyce, Kelly Wearstler, Stéphane Boudin, Georges Geffroy, Emilio Terry, Carlos de Beistegui, Nina Petronzio, Lorenzo Mongiardino, Mary Jean Thompson and David Nightingale Hicks.
 Notable interior designers in the world today include Scott Salvator, Troy Adams, Jonathan Adler, Michael S. Smith, Martin Brudnizki, Mary Douglas Drysdale, Kelly Hoppen, Kelly Wearstler, Nina Campbell, David Collins, Nate Berkus, Sandra Espinet, Jo Hamilton and Nicky Haslam.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Interior design', 'Linda Barker and Laurence Llewelyn-Bowen', 'Notable interior designers in the world', 'ceremonies, weddings, baby and bridal showers', 'planning, functional design and the effective use of space'], 'answer_start': [], 'answer_end': []}"
"
 Graphic design is a profession,[2] academic discipline[3][4][5] and applied art whose activity consists in projecting visual communications intended to transmit specific messages to social groups, with specific objectives.[6] Graphic design is an interdisciplinary branch of design[1] and of the fine arts. Its practice involves creativity, innovation and lateral thinking using manual or digital tools, where it is usual to use text and graphics to communicate visually.
 The role of the graphic designer in the communication process is that of the encoder or interpreter of the message. They work on the interpretation, ordering, and presentation of visual messages. Usually, graphic design uses the aesthetics of typography and the compositional arrangement of the text, ornamentation, and imagery to convey ideas, feelings, and attitudes beyond what language alone expresses. The design work can be based on a customer's demand, a demand that ends up being established linguistically, either orally or in writing, that is, that graphic design transforms a linguistic message into a graphic manifestation.[7]
 Graphic design has, as a field of application, different areas of knowledge focused on any visual communication system. For example, it can be applied in advertising strategies, or it can also be applied in the aviation world[8] or space exploration.[9][10] In this sense, in some countries graphic design is related as only associated with the production of sketches and drawings, this is incorrect, since visual communication is a small part of a huge range of types and classes where it can be applied.
 With origins in Antiquity and the Middle Ages,[11] graphic design as applied art was initially linked to the boom of the rise of printing in Europe in the 15th century and the growth of consumer culture in the Industrial Revolution. From there it emerged as a distinct profession in the West, closely associated with advertising in the 19th century[12] and its evolution allowed its consolidation in the 20th century. Given the rapid and massive growth in information exchange today, the demand for experienced designers is greater than ever, particularly because of the development of new technologies and the need to pay attention to human factors beyond the competence of the engineers who develop them.[13]
 William Addison Dwiggins is often credited with first using the term ""graphic design"" in a 1922 article,[14] although it appears in a 4 July 1908 issue (volume 9, number 27) of Organized Labor, a publication of the Labor Unions of San Francisco, in an article about technical education for printers:[15]
 An Enterprising Trades Union
… The admittedly high standard of intelligence which prevails among printers is an assurance that with the elemental principles of design at their finger ends many of them will grow in knowledge and develop into specialists in graphic design and decorating. …
 A decade later, the 1917–1918 course catalog of the California School of Arts & Crafts advertised a course titled Graphic Design and Lettering, which replaced one called Advanced Design and Lettering. Both classes were taught by Frederick Meyer.[16]
 In both its lengthy history and in the relatively recent explosion of visual communication in the 20th and 21st centuries, the distinction between advertising, art, graphic design and fine art has disappeared. They share many elements, theories, principles, practices, languages and sometimes the same benefactor or client. In advertising, the ultimate objective is the sale of goods and services. In graphic design, ""the essence is to give order to information, form to ideas, expression, and feeling to artifacts that document the human experience.""[17]
 The definition of the graphic designer profession is relatively recent concerning its preparation, activity, and objectives. Although there is no consensus on an exact date when graphic design emerged, some date it back to the Interwar period. Others understand that it began to be identified as such by the late 19th century.[11]
 It can be argued that graphic communications with specific purposes have their origins in Paleolithic cave paintings and the birth of written language in the third millennium BCE. However, the differences in working methods, auxiliary sciences, and required training are such that it is not possible to clearly identify the current graphic designer with prehistoric man, the 15th-century xylographer, or the lithographer of 1890.
 The diversity of opinions stems from some considering any graphic manifestation as a product of graphic design, while others only recognize those that arise as a result of the application of an industrial production model—visual manifestations that have been ""projected"" to address various needs: productive, symbolic, ergonomic, contextual, among others.
 Nevertheless, the evolution of graphic design as a practice and profession has been closely linked to technological innovations, social needs, and the visual imagination of professionals.[18] Graphic design has been practiced in various forms throughout history; in fact, good examples of graphic design date back to manuscripts from ancient China, Egypt, and Greece. As printing and book production developed in the 15th century, advances in graphic design continued over the subsequent centuries, with composers or typographers often designing pages according to established type.[11]
 By the late 19th century, graphic design emerged as a distinct profession in the West, partly due to the process of labor specialization that occurred there and partly due to the new technologies and business possibilities brought about by the Industrial Revolution. New production methods led to the separation of the design of a communication medium (such as a poster) from its actual production. Increasingly, throughout the 19th and early 20th centuries, advertising agencies, book publishers, and magazines hired art directors who organized all visual elements of communication and integrated them into a harmonious whole, creating an expression appropriate to the content. In 1922, typographer William A. Dwiggins coined the term graphic design to identify the emerging field.[11]
 Throughout the 20th century, the technology available to designers continued to advance rapidly, as did the artistic and commercial possibilities of design. The profession expanded greatly, and graphic designers created, among other things, magazine pages, book covers, posters, CD covers, postage stamps, packaging, brands, signs, advertisements, kinetic titles for TV programs and movies, and websites. By the early 21st century, graphic design had become a global profession as advanced technology and industry spread worldwide.[11]
 In China, during the Tang dynasty (618–907) wood blocks were cut to print on textiles and later to reproduce Buddhist texts. A Buddhist scripture printed in 868 is the earliest known printed book. Beginning in the 11th century in China, longer scrolls and books were produced using movable type printing, making books widely available during the Song dynasty (960–1279).[19]
 In the mid-15th century in Mainz, Germany, Johannes Gutenberg developed a way to reproduce printed pages at a faster pace using movable type made with a new metal alloy[20] that created a revolution in the dissemination of information.[21]
 In 1849, Henry Cole became one of the major forces in design education in Great Britain, informing the government of the importance of design in his Journal of Design and Manufactures. He organized the Great Exhibition as a celebration of modern industrial technology and Victorian design.
 From 1891 to 1896, William Morris' Kelmscott Press was a leader in graphic design associated with the Arts and Crafts movement, creating hand-made books in medieval and Renaissance era style,[22] in addition to wallpaper and textile designs.[23] Morris' work, along with the rest of the Private Press movement, directly influenced Art Nouveau.[24]
 Will H. Bradley became one of the notable graphic designers in the late nineteenth-century due to creating art pieces in various Art Nouveau styles. Bradley created a number of designs as promotions for a literary magazine titled The Chap-Book.[25]
 In 1917, Frederick H. Meyer, director and instructor at the California School of Arts and Crafts, taught a class entitled ""Graphic Design and Lettering"".[26] Raffe's Graphic Design, published in 1927, was the first book to use ""Graphic Design"" in its title.[27] In 1936, author and graphic designer Leon Friend published his book titled ""Graphic Design"" and it is known to be the first piece of literature to cover the topic extensively.[28]
 The signage in the London Underground is a classic design example[29] of the modern era. Although he lacked artistic training, Frank Pick led the Underground Group design and publicity movement. The first Underground station signs were introduced in 1908 with a design of a solid red disk with a blue bar in the center and the name of the station. The station name was in white sans-serif letters. It was in 1916 when Pick used the expertise of Edward Johnston to design a new typeface for the Underground. Johnston redesigned the Underground sign and logo to include his typeface on the blue bar in the center of a red circle.[30]
 In the 1920s, Soviet constructivism applied 'intellectual production' in different spheres of production. The movement saw individualistic art as useless in revolutionary Russia and thus moved towards creating objects for utilitarian purposes. They designed buildings, film and theater sets, posters, fabrics, clothing, furniture, logos, menus, etc.[31]
 Jan Tschichold codified the principles of modern typography in his 1928 book, New Typography.[32] He later repudiated the philosophy he espoused in this book as fascistic, but it remained influential.[citation needed] Tschichold, Bauhaus typographers such as Herbert Bayer and László Moholy-Nagy and El Lissitzky greatly influenced graphic design. They pioneered production techniques[citation needed] and stylistic devices used throughout the twentieth century. The following years saw graphic design in the modern style gain widespread acceptance and application.[33]
 The professional graphic design industry grew in parallel with consumerism. This raised concerns and criticisms, notably from within the graphic design community with the First Things First manifesto. First launched by Ken Garland in 1964, it was re-published as the First Things First 2000 manifesto in 1999 in the magazine Emigre 51[34] stating ""We propose a reversal of priorities in favor of more useful, lasting and democratic forms of communication – a mindshift away from product marketing and toward the exploration and production of a new kind of meaning. The scope of debate is shrinking; it must expand. Consumerism is running uncontested; it must be challenged by other perspectives expressed, in part, through the visual languages and resources of design.""[35]
 Graphic design can have many applications, from road signs to technical schematics and reference manuals. It is often used in branding products and elements of company identity such as logos, colors, packaging, labelling and text.
 From scientific journals to news reporting, the presentation of opinions and facts is often improved with graphics and thoughtful compositions of visual information – known as information design.  With the advent of the web, information designers with experience in interactive tools are increasingly used to illustrate the background to news stories. Information design can include Data and information visualization, which involves using programs to interpret and form data into a visually compelling presentation, and can be tied in with information graphics.
 A graphic design project may involve the creative presentation of existing text, ornament, and images.
 The ""process school"" is concerned with communication; it highlights the channels and media through which messages are transmitted and by which senders and receivers encode and decode these messages. The semiotic school treats a message as a construction of signs which through interaction with receivers, produces meaning; communication as an agent.[citation needed]
 Typography includes type design, modifying type glyphs and arranging type. Type glyphs (characters) are created and modified using illustration techniques. Type arrangement is the selection of typefaces, point size, tracking (the space between all characters used), kerning (the space between two specific characters) and leading (line spacing).
 Typography is performed by typesetters, compositors, typographers, graphic artists, art directors, and clerical workers. Until the digital age, typography was a specialized occupation. Certain fonts communicate or resemble stereotypical notions. For example, the 1942 Report is a font which types text akin to a typewriter or a vintage report.[36]
 Page layout deals with the arrangement of elements (content) on a page, such as image placement, text layout and style. Page design has always been a consideration in printed material and more recently extended to displays such as web pages. Elements typically consist of type (text), images (pictures), and (with print media) occasionally place-holder graphics such as a dieline for elements that are not printed with ink such as die/laser cutting, foil stamping or blind embossing.
 A grid serves as a method of arranging both space and information, allowing the reader to easily comprehend the overall project. Furthermore, a grid functions as a container for information and a means of establishing and maintaining order. Despite grids being utilized for centuries, many graphic designers associate them with Swiss design. The desire for order in the 1940s resulted in a highly systematic approach to visualizing information. However, grids were later regarded as tedious and uninteresting, earning the label of ""designersaur."" Today, grids are once again considered crucial tools for professionals, whether they are novices or veterans.[37]
 In the mid-1980s desktop publishing and graphic art software applications introduced computer image manipulation and creation capabilities that had previously been manually executed. Computers enabled designers to instantly see the effects of layout or typographic changes, and to simulate the effects of traditional media. Traditional tools such as pencils can be useful even when computers are used for finalization; a designer or art director may sketch numerous concepts as part of the creative process.[38] Styluses can be used with tablet computers to capture hand drawings digitally.[39]
 Designers disagree whether computers enhance the creative process.[40] Some designers argue that computers allow them to explore multiple ideas quickly and in more detail than can be achieved by hand-rendering or paste-up.[41] While other designers find the limitless choices from digital design can lead to paralysis or endless iterations with no clear outcome.
 Most designers use a hybrid process that combines traditional and computer-based technologies. First, hand-rendered layouts are used to get approval to execute an idea, then the polished visual product is produced on a computer.
 Graphic designers are expected to be proficient in software programs for image-making, typography and layout. Nearly all of the popular and ""industry standard"" software programs used by graphic designers since the early 1990s are products of Adobe Inc. Adobe Photoshop (a raster-based program for photo editing) and Adobe Illustrator (a vector-based program for drawing) are often used in the final stage. CorelDraw, a vector graphics editing software developed and marketed by Corel Corporation, is also used worldwide. Designers often use pre-designed raster images and vector graphics in their work from online design databases. Raster images may be edited in Adobe Photoshop, vector logos and illustrations in Adobe Illustrator and CorelDraw, and the final product assembled in one of the major page layout programs, such as Adobe InDesign, Serif PagePlus and QuarkXPress.
 Many free and open-source programs are also used by both professionals and casual graphic designers. Inkscape uses Scalable Vector Graphics (SVG) as its primary file format and allows importing and exporting other formats. Other open-source programs used include GIMP for photo-editing and image manipulation, Krita for digital painting, and Scribus for page layout.
 Since the advent of personal computers, many graphic designers have become involved in interface design, in an environment commonly referred to as a Graphical user interface (GUI). This has included web design and software design when end user-interactivity is a design consideration of the layout or interface. Combining visual communication skills with an understanding of user interaction and online branding, graphic designers often work with software developers and web developers to create the look and feel of a web site or software application. An important aspect of interface design is icon design.
 User experience design (UX) is the study, analysis, and development of creating products that provide meaningful and relevant experiences to users. This involves the creation of the entire process of acquiring and integrating the product, including aspects of branding, design, usability, and function. UX design involves creating the interface and interactions for a website or application, and is considered both an act and an art. This profession requires a combination of skills, including visual design, social psychology, development, project management, and most importantly, empathy towards the end-users.[42]
 Experiential graphic design is the application of communication skills to the built environment.[43] This area of graphic design requires practitioners to understand physical installations that have to be manufactured and withstand the same environmental conditions as buildings. As such, it is a cross-disciplinary collaborative process involving designers, fabricators, city planners, architects, manufacturers and construction teams.
 Experiential graphic designers try to solve problems that people encounter while interacting with buildings and space (also called environmental graphic design). Examples of practice areas for environmental graphic designers are wayfinding, placemaking, branded environments, exhibitions and museum displays, public installations and digital environments.
 Graphic design career paths cover all parts of the creative spectrum and often overlap. Workers perform specialized tasks, such as design services, publishing, advertising and public relations. As of 2023, median pay was $50,710 per year.[45] The main job titles within the industry are often country specific. They can include graphic designer, art director, creative director, animator and entry level production artist. Depending on the industry served, the responsibilities may have different titles such as ""DTP associate"" or ""Graphic Artist"". The responsibilities may involve specialized skills such as illustration, photography, animation, visual effects or interactive design.
 Employment in design of online projects was expected to increase by 35% by 2026, while employment in traditional media, such as newspaper and book design, expect to go down by 22%. Graphic designers will be expected to constantly learn new techniques, programs, and methods.[46]
 Graphic designers can work within companies devoted specifically to the industry, such as design consultancies or branding agencies, others may work within publishing, marketing or other communications companies. Especially since the introduction of personal computers, many graphic designers work as in-house designers in non-design oriented organizations.  Graphic designers may also work freelance, working on their own terms, prices, ideas, etc.
 A graphic designer typically reports to the art director, creative director or senior media creative. As a designer becomes more senior, they spend less time designing and more time leading and directing other designers on broader creative activities, such as brand development and corporate identity development. They are often expected to interact more directly with clients, for example taking and interpreting briefs.
 Jeff Howe of Wired Magazine first used the term ""crowdsourcing"" in his 2006 article, ""The Rise of Crowdsourcing.""[47][48] It spans such creative domains as graphic design, architecture, apparel design, writing, illustration, and others. Tasks may be assigned to individuals or a group and may be categorized as convergent or divergent. An example of a divergent task is generating alternative designs for a poster. An example of a convergent task is selecting one poster design. Companies, startups, small businesses and entrepreneurs have all benefitted from design crowdsourcing since it helps them source great graphic designs at a fraction of the budget they used to spend before. Getting a logo design through crowdsourcing being one of the most common. Major companies that operate in the design crowdsourcing space are generally referred to as design contest sites.[citation needed]]
 Graphic design is essential for advertising, branding, and marketing, influencing how people act.  Good graphic design builds strong, recognizable brands, communicates messages clearly, and shapes how consumers see and react to things.
 One way that graphic design influences consumer behavior is through the use of visual elements, such as color, typography, and imagery. Studies have shown that certain colors can evoke specific emotions and behaviors in consumers, and that typography can influence how information is perceived and remembered.[49] For example, serif fonts are often associated with tradition and elegance, while sans-serif fonts are seen as modern and minimalistic. These factors can all impact the way consumers perceive a brand and its messaging.[50]
 Another way that graphic design impacts consumer behavior is through its ability to communicate complex information in a clear and accessible way. For example, infographics and data visualizations can help to distill complex information into a format that is easy to understand and engaging for consumers.[51] This can help to build trust and credibility with consumers, and encourage them to take action.
 Ethics are an important consideration in graphic design, particularly when it comes to accurately representing information and avoiding harmful stereotypes. Graphic designers have a responsibility to ensure that their work is truthful, accurate, and free from any misleading or deceptive elements. This requires a commitment to honesty, integrity, and transparency in all aspects of the design process.
 One of the key ethical considerations in graphic design is the responsibility to accurately represent information. This means ensuring that any claims or statements made in advertising or marketing materials are true and supported by evidence.[52] For example, a company should not use misleading statistics to promote their product or service, or make false claims about its benefits. Graphic designers must take care to accurately represent information in all visual elements, such as graphs, charts, and images, and avoid distorting or misrepresenting data.[53]
 Another important ethical consideration in graphic design is the need to avoid harmful stereotypes. This means avoiding any images or messaging that perpetuate negative or harmful stereotypes based on race, gender, religion, or other characteristics.[54] Graphic designers should strive to create designs that are inclusive and respectful of all individuals and communities, and avoid reinforcing negative attitudes or biases.
 The future of graphic design is likely to be heavily influenced by emerging technologies and social trends. Advancements in areas such as artificial intelligence, virtual and augmented reality, and automation are likely to transform the way that graphic designers work and create designs. Social trends, such as a greater focus on sustainability and inclusivity, are also likely to impact the future of graphic design.[55]
 One area where emerging technologies are likely to have a significant impact on graphic design is in the automation of certain tasks. Machine learning algorithms, for example, can analyze large datasets and create designs based on patterns and trends, freeing up designers to focus on more complex and creative tasks. Virtual and augmented reality technologies may also allow designers to create immersive and interactive experiences for users, blurring the lines between the digital and physical worlds.[56]
 Social trends are also likely to shape the future of graphic design. As consumers become more conscious of environmental issues, for example, there may be a greater demand for designs that prioritize sustainability and minimize waste. Similarly, there is likely to be a growing focus on inclusivity and diversity in design, with designers seeking to create designs that are accessible and representative of a wide range of individuals and communities.[57]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['sustainability and inclusivity', 'novices or veterans', 'ancient China, Egypt, and Greece', 'growth of consumer culture in the Industrial Revolution', 'different areas of knowledge focused on any visual communication system'], 'answer_start': [], 'answer_end': []}"
"Industrial design is a process of design applied to physical products that are to be manufactured by mass production.[1][2] It is the creative act of determining and defining a product's form and features, which takes place in advance of the manufacture or production of the product. Industrial manufacture consists of pre-determined, standardized and repeated, often automated, acts of replication,[3][4] while craft-based design is a process or approach in which the form of the product is determined personally by the product's creator largely concurrent with the act of its production.[5]
 All manufactured products are the result of a design process, but the nature of this process can vary. It can be conducted by an individual or a team, and such a team could include people with varied expertise (e.g. designers, engineers, business experts, etc.). It can emphasize intuitive creativity or calculated scientific decision-making, and often emphasizes a mix of both. It can be influenced by factors as varied as materials, production processes, business strategy, and prevailing social, commercial, or aesthetic attitudes.[3] Industrial design, as an applied art, most often focuses on a combination of aesthetics and user-focused considerations,[6] but also often provides solutions for problems of form, function, physical ergonomics, marketing, brand development, sustainability, and sales.[7]
 For several millennia before the onset of industrialization, design, technical expertise, and manufacturing was often done by individual crafts people, who determined the form of a product at the point of its creation, according to their own manual skill, the requirements of their clients, experience accumulated through their own experimentation, and knowledge passed on to them through training or apprenticeship.[5]
 The division of labour that underlies the practice of industrial design did have precedents in the pre-industrial era.[1] The growth of trade in the medieval period led to the emergence of large workshops in cities such as Florence, Venice, Nuremberg, and Bruges, where groups of more specialized craftsmen made objects with common forms through the repetitive duplication of models which defined by their shared training and technique.[8] Competitive pressures in the early 16th century led to the emergence in Italy and Germany of pattern books: collections of engravings illustrating decorative forms and motifs which could be applied to a wide range of products, and whose creation took place in advance of their application.[8] The use of drawing to specify how something was to be constructed later was first developed by architects and shipwrights during the Italian Renaissance.[9]
 In the 17th century, the growth of artistic patronage in centralized monarchical states such as France led to large government-operated manufacturing operations epitomized by the Gobelins Manufactory, opened in Paris in 1667 by Louis XIV.[8] Here teams of hundreds of craftsmen, including specialist artists, decorators and engravers, produced sumptuously decorated products ranging from tapestries and furniture to metalwork and coaches, all under the creative supervision of the King's leading artist Charles Le Brun.[10] This pattern of large-scale royal patronage was repeated in the court porcelain factories of the early 18th century, such as the Meissen porcelain workshops established in 1709 by the Grand Duke of Saxony, where patterns from a range of sources, including court goldsmiths, sculptors, and engravers, were used as models for the vessels and figurines for which it became famous.[11] As long as reproduction remained craft-based, however, the form and artistic quality of the product remained in the hands of the individual craftsman, and tended to decline as the scale of production increased.[12]
 The emergence of industrial design is specifically linked to the growth of industrialization and mechanization that began with the industrial revolution in Great Britain in the mid 18th century.[1][2] The rise of industrial manufacture changed the way objects were made, urbanization changed patterns of consumption, the growth of empires broadened tastes and diversified markets, and the emergence of a wider middle class created demand for fashionable styles from a much larger and more heterogeneous population.[13]
 The first use of the term ""industrial design"" is often attributed to the industrial designer Joseph Claude Sinel in 1919 (although he himself denied this in interviews), but the discipline predates 1919 by at least a decade. Christopher Dresser is considered among the first independent industrial designers.[14] Industrial design's origins lie in the industrialization of consumer products. For instance, the Deutscher Werkbund, founded in 1907 and a precursor to the Bauhaus, was a state-sponsored effort to integrate traditional crafts and industrial mass-production techniques, to put Germany on a competitive footing with Great Britain and the United States.
 The earliest published use of the term may have been in The Art-Union, 15 September 1840.
 Dyce's Report to the Board of Trade, on Foreign Schools of Design for Manufactures.
 Mr. Dyce's official visit to France, Prussia, and Bavaria, for the purpose of examining the state of schools of design in those countries, will be fresh in the recollection of our readers. His report on this subject was ordered to be printed some few months since, on the motion of Mr. Hume; and it is the sum and substance of this Report that we are now about to lay before our own especial portion of the reading public.
 The school of St. Peter, at Lyons, was founded about 1750, for the instruction of draftsmen employed in preparing patterns for the silk manufacture. It has been much more successful than the Paris school; and having been disorganized by the revolution, was restored by Napoleon and differently constituted, being then erected into an Academy of Fine Art: to which the study of design for silk manufacture was merely attached as a subordinate branch.
 
It appears that all the students who entered the school commence as if they were intended for artists in the higher sense of the word and are not expected to decide as to whether they will devote themselves to the Fine Arts or to Industrial Design, until they have completed their exercises in drawing and painting of the figure from the antique and from the living model. It is for this reason, and from the fact that artists for industrial purposes are both well-paid and highly considered (as being well-instructed men), that so many individuals in France engage themselves in both pursuits.[15] The Practical Draughtsman's Book of Industrial Design by Jacques-Eugène Armengaud was printed in 1853.[16] The subtitle of the (translated) work explains, that it wants to offer a ""complete course of mechanical, engineering, and architectural drawing."" The study of those types of technical drawing, according to Armengaud, belongs to the field of industrial design. This work paved the way for a big expansion in the field of drawing education in France, the United Kingdom, and the United States.
 Robert Lepper helped to establish one of the USA's first industrial design degree programs in 1934 at Carnegie Institute of Technology.[17]
 Product design and industrial design overlap in the fields of user interface design, information design, and interaction design. Various schools of industrial design specialize in one of these aspects, ranging from pure art colleges and design schools (product styling), to mixed programs of engineering and design, to related disciplines such as exhibit design and interior design, to schools that almost completely subordinated aesthetic design to concerns of usage and ergonomics, the so-called functionalist school.[18] Except for certain functional areas of overlap between industrial design and engineering design, the former is considered an applied art[6] while the latter is an applied science.[19] Educational programs in the U.S. for engineering require accreditation by the Accreditation Board for Engineering and Technology (ABET)[20] in contrast to programs for industrial design which are accredited by the National Association of Schools of Art and Design (NASAD).[21] Of course, engineering education requires heavy training in mathematics and physical sciences, which is not typically required in industrial design education.[22]
 Most industrial designers complete a design or related program at a vocational school or university. Relevant programs include graphic design, interior design, industrial design, architectural technology, and drafting. Diplomas and degrees in industrial design are offered at vocational schools and universities worldwide. Diplomas and degrees take two to four years of study. The study results in a Bachelor of Industrial Design (B.I.D.), Bachelor of Science (B.Sc) or Bachelor of Fine Arts (B.F.A.). Afterwards, the bachelor programme can be extended to postgraduate degrees such as Master of Design, Master of Fine Arts and others to a Master of Arts or Master of Science.
 Industrial design studies function and form—and the connection between product, user, and environment. Generally, industrial design professionals work in small scale design, rather than overall design of complex systems such as buildings or ships. Industrial designers don't usually design motors, electrical circuits, or gearing that make machines move, but they may affect technical aspects through usability design and form relationships. Usually, they work with other professionals such as engineers who focus on the mechanical and other functional aspects of the product, assuring functionality and manufacturability, and with marketers to identify and fulfill customer needs and expectations.
 Industrial design (ID) is the professional service of creating and developing concepts and specifications that optimize the function, value and appearance of products and systems for the mutual benefit of both user and manufacturer.
 Industrial Designers Society of America,[23] Design, itself, is often difficult to describe to non-designers because the meaning accepted by the design community is not made of words. Instead, the definition is created as a result of acquiring a critical framework for the analysis and creation of artifacts. One of the many accepted (but intentionally unspecific) definitions of design originates from Carnegie Mellon's School of Design: ""Everyone designs who devises courses of action aimed at changing existing situations into preferred ones.""[24][25] This applies to new artifacts, whose existing state is undefined, and previously created artifacts, whose state stands to be improved.
 Industrial design can overlap significantly with engineering design, and in different countries the boundaries of the two concepts can vary, but in general engineering focuses principally on functionality or utility of products, whereas industrial design focuses principally on aesthetic and user-interface aspects of products. In many jurisdictions this distinction is effectively defined by credentials and/or licensure required to engage in the practice of engineering.[26]  ""Industrial design"" as such does not overlap much with the engineering sub-discipline of industrial engineering, except for the latter's sub-specialty of ergonomics.
 At the 29th General Assembly in Gwangju, South Korea, 2015, the Professional Practise Committee unveiled a renewed definition of industrial design as follows:
""Industrial Design is a strategic problem-solving process that drives innovation, builds business success and leads to a better quality of life through innovative products, systems, services and experiences.""
An extended version of this definition is as follows:
""Industrial Design is a strategic problem-solving process that drives innovation, builds business success and leads to a better quality of life through innovative products, systems, services and experiences. Industrial Design bridges the gap between what is and what's possible. It is a trans-disciplinary profession that harnesses creativity to resolve problems and co-create solutions with the intent of making a product, system, service, experience or a business, better. At its heart, Industrial Design provides a more optimistic way of looking at the future by reframing problems as opportunities. It links innovation, technology, research, business and customers to provide new value and competitive advantage across economic, social and environmental spheres.
Industrial Designers place the human in the centre of the process. They acquire a deep understanding of user needs through empathy and apply a pragmatic, user centric problem solving process to design products, systems, services and experiences. They are strategic stakeholders in the innovation process and are uniquely positioned to bridge varied professional disciplines and business interests. They value the economic, social and environmental impact of their work and their contribution towards co-creating a better quality of life. ""[27]
 Although the process of design may be considered 'creative,' many analytical processes also take place. In fact, many industrial designers often use various design methodologies in their creative process. Some of the processes that are commonly used are user research, sketching, comparative product research, model making, prototyping and testing. These processes are best defined by the industrial designers and/or other team members. Industrial designers often utilize 3D software, computer-aided industrial design and CAD programs to move from concept to production. They may also build a prototype or scaled down sketch models through a 3D printing process or using other materials such as paper, balsa wood, various foams, or clay for modeling. They may then use industrial CT scanning to test for interior defects and generate a CAD model. From this the manufacturing process may be modified to improve the product.
 Product characteristics specified by industrial designers may include the overall form of the object, the location of details with respect to one another, colors, texture, form, and aspects concerning the use of the product. Additionally, they may specify aspects concerning the production process, choice of materials and the way the product is presented to the consumer at the point of sale. The inclusion of industrial designers in a product development process may lead to added value by improving usability, lowering production costs, and developing more appealing products.
 Industrial design may also focus on technical concepts, products, and processes. In addition to aesthetics, usability, and ergonomics, it can also encompass engineering, usefulness, market placement, and other concerns—such as psychology, desire, and the emotional attachment of the user. These values and accompanying aspects that form the basis of industrial design can vary—between different schools of thought, and among practicing designers.
 Industrial design rights are intellectual property rights that make exclusive the visual design of objects that are not purely utilitarian. A design patent would also be considered under this category. An industrial design consists of the creation of a shape, configuration or composition of pattern or color, or combination of pattern and color in three-dimensional form containing aesthetic value. An industrial design can be a two- or three-dimensional pattern used to produce a product, industrial commodity or handicraft. Under the Hague Agreement Concerning the International Deposit of Industrial Designs, a WIPO-administered treaty, a procedure for an international registration exists. An applicant can file for a single international deposit with WIPO or with the national office in a country party to the treaty. The design will then be protected in as many member countries of the treaty as desired.
 A number of industrial designers have made such a significant impact on culture and daily life that their work is documented by historians of social science. Alvar Aalto, renowned as an architect, also designed a significant number of household items, such as chairs, stools, lamps, a tea-cart, and vases. Raymond Loewy was a prolific American designer who is responsible for the Royal Dutch Shell corporate logo, the original BP logo (in use until 2000), the PRR S1 steam locomotive, the Studebaker Starlight (including the later bulletnose), as well as Schick electric razors, Electrolux refrigerators, short-wave radios, Le Creuset French ovens, and a complete line of modern furniture, among many other items.
 Richard Teague, who spent most of his career with the American Motors Corporation, originated the concept of using interchangeable body panels so as to create a wide array of different vehicles using the same stampings. He was responsible for such unique automotive designs as the Pacer, Gremlin, Matador coupe, Jeep Cherokee, and the complete interior of the Eagle Premier.
 Milwaukee's Brooks Stevens was best known for his Milwaukee Road Skytop Lounge car and Oscar Mayer Wienermobile designs, among others.
 Viktor Schreckengost designed bicycles manufactured by Murray bicycles for Murray and Sears, Roebuck and Company. With engineer Ray Spiller, he designed the first truck with a cab-over-engine configuration, a design in use to this day. Schreckengost also founded The Cleveland Institute of Art's school of industrial design.
 Oskar Barnack was a German optical engineer, precision mechanic, industrial designer, and the father of 35mm photography. He developed the Leica, which became the hallmark for photography for 50 years, and remains a high-water mark for mechanical and optical design.[28]
 Charles and Ray Eames were most famous for their pioneering furniture designs,[29] such as the Eames Lounge Chair Wood and Eames Lounge Chair. Other influential designers included Henry Dreyfuss, Eliot Noyes, John Vassos, and Russel Wright.
 Dieter Rams is a German industrial designer closely associated with the consumer products company Braun and the Functionalist school of industrial design.
 German industrial designer Luigi Colani, who designed cars for automobile manufacturers including Fiat, Alfa Romeo, Lancia, Volkswagen, and BMW, was also known to the general public for his unconventional approach to industrial design. He had expanded in numerous areas ranging from mundane household items, instruments and furniture to trucks, uniforms and entire rooms. A grand piano created by Colani, the Pegasus, is manufactured and sold by the Schimmel piano company.
 Many of Apple's recent products were designed by Sir Jonathan Ive.
 
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['ergonomics', 'designers, engineers, business experts', ""the creative act of determining and defining a product's form and features"", 'marketing, brand development, sustainability, and sales', 'applied science'], 'answer_start': [], 'answer_end': []}"
"
 Landscape architecture is the design of outdoor areas, landmarks, and structures to achieve environmental, social-behavioural, or aesthetic outcomes.[2] It involves the systematic design and general engineering of various structures for construction and human use, investigation of existing social, ecological, and soil conditions and processes in the landscape, and the design of other interventions that will produce desired outcomes. 
 The scope of the profession is broad and can be subdivided into several sub-categories including professional or licensed landscape architects who are regulated by governmental agencies and possess the expertise to design a wide range of structures and landfors for human use; landscape design which is not a licensed profession; site planning; stormwater management; erosion control; environmental restoration; public realm, parks, recreation and urban planning; visual resource management; green infrastructure planning and provision; and private estate and residence landscape master planning and design; all at varying scales of design, planning and management. A practitioner in the profession of landscape architecture may be called a landscape architect; however, in jurisdictions where professional licenses are required it is often only those who possess a landscape architect license who can be called a landscape architect.
 Modern landscape architecture is a multi-disciplinary field, incorporating aspects of urban design, architecture, geography, ecology, civil engineering, structural engineering, horticulture, environmental psychology, industrial design, soil sciences, botany, and fine arts. The activities of a landscape architect can range from the creation of public parks and parkways to site planning for campuses and corporate office parks; from the design of residential estates to the design of civil infrastructure; and from the management of large wilderness areas to reclamation of degraded landscapes such as mines or landfills. Landscape architects work on structures and external spaces in the landscape aspect of the design – large or small, urban, suburban and rural, and with ""hard"" (built) and ""soft"" (planted) materials, while integrating ecological sustainability.  
 The most valuable contribution can be made at the first stage of a project to generate ideas with technical understanding and creative flair for the design, organization, and use of spaces. The landscape architect can conceive the overall concept and prepare the master plan, from which detailed design drawings and technical specifications are prepared. They can also review proposals to authorize and supervise contracts for the construction work. Other skills include preparing design impact assessments, conducting environmental assessments and audits, and serving as an expert witness at inquiries on land use issues. The majority of their time will most likely be spent inside an office building designing and preparing models for clients.[citation needed]
 For the period before 1800, the history of landscape gardening (later called landscape architecture) is largely that of master planning and garden design for manor houses, palaces and royal properties. An example is the extensive work by André Le Nôtre for King Louis XIV of France on the Gardens of Versailles. The first person to write of making a landscape was Joseph Addison in 1712. The term landscape architecture was invented by Gilbert Laing Meason in 1828, and John Claudius Loudon (1783–1843) was instrumental in the adoption of the term landscape architecture by the modern profession. He took up the term from Meason and gave it publicity in his Encyclopedias and in his 1840 book on the Landscape Gardening and Landscape Architecture of the Late Humphry Repton.[3]
 John Claudius Loudon was an established and influential horticultural journalist and Scottish landscape architect whose writings were instrumental in shaping Victorian taste in gardens, public parks, and architecture.[4] In the Landscape Gardening and Landscape Architecture of the Late Humphry Repton, Loudon describes two distinct styles of landscape gardening existing at the beginning of the 19th century: geometric and natural.[3] Loudon wrote that each style reflected a different stage of society. The geometric style was “most striking and pleasing,” displaying wealth and taste in an “early state of society” and in “countries where the general scenery was wild, irregular, and natural, and man, comparatively, uncultivated and unrefined.”[3] The natural style was used in “modern times” and in countries where “society is in a higher state of cultivation,"" displaying wealth and taste through the sacrifice of profitable lands to make room for such designs. [3]
 The prominent English landscape designer Humphry Repton (1752-1818) echoed similar ideas in his work and design ideas. In his writings on the use of delineated spaces (e.g. courtyards, terrace walls, fences), Repton states that while the motive for defense no longer exists, the features are still useful in separating ""the gardens, which belong to man, and the forest, or desert, which belongs to the wild denizens.""[3] Repton refers to Indigenous peoples as ""uncivilized human beings, against whom some decided line of defense was absolutely necessary.”[3]
 The practice of landscape architecture spread from the Old to the New World. The term ""landscape architect"" was used as a professional title by Frederick Law Olmsted in the United States in 1863[citation needed] and Andrew Jackson Downing, another early American landscape designer, was editor of The Horticulturist magazine (1846–52). In 1841 his first book, A Treatise on the Theory and Practice of Landscape Gardening, Adapted to North America, was published to a great success; it was the first book of its kind published in the United States.[5] During the latter 19th century, the term landscape architect began to be used by professional landscapes designers, and was firmly established after Frederick Law Olmsted Jr. and Beatrix Jones (later Farrand) with others founded the American Society of Landscape Architects (ASLA) in 1899. IFLA was founded at Cambridge, England, in 1948 with Sir Geoffrey Jellicoe as its first president, representing 15 countries from Europe and North America. Later, in 1978, IFLA's Headquarters were established in Versailles.[6][7][8]
 The variety of the professional tasks that landscape architects collaborate on is very broad, but some examples of project types include:[9]
 Landscape managers use their knowledge of landscape processes to advise on the long-term care and development of the landscape. They often work in forestry, nature conservation and agriculture.[citation needed]
 Landscape scientists have specialist skills such as soil science, hydrology, geomorphology or botany that they relate to the practical problems of landscape work. Their projects can range from site surveys to the ecological assessment of broad areas for planning or management purposes. They may also report on the impact of development or the importance of particular species in a given area.[citation needed]
 Landscape planners are concerned with landscape planning for the location, scenic, ecological and recreational aspects of urban, rural, and coastal land use. Their work is embodied in written statements of policy and strategy, and their remit includes master planning for new developments, landscape evaluations and assessments, and preparing countryside management or policy plans. Some may also apply an additional specialism such as landscape archaeology or law to the process of landscape planning.[citation needed]
 Green roof (or more specifically, vegetative roof) designers design extensive and intensive roof gardens for stormwater management, evapo-transpirative cooling, sustainable architecture, aesthetics, and habitat creation.[10]
 Through the 19th century, urban planning became a focal point and central issue in cities. The combination of the tradition of landscape gardening and the emerging field of urban planning offered landscape architecture an opportunity to serve these needs.[11] In the second half of the century, Frederick Law Olmsted completed a series of parks that continue to have a significant influence on the practices of landscape architecture today. Among these were Central Park in New York City, Prospect Park in Brooklyn, New York and Boston's Emerald Necklace park system. Jens Jensen designed sophisticated and naturalistic urban and regional parks for Chicago, Illinois, and private estates for the Ford family including Fair Lane and Gaukler Point. One of the original eleven founding members of the American Society of Landscape Architects (ASLA), and the only woman, was Beatrix Farrand. She was design consultant for over a dozen universities including: Princeton in Princeton, New Jersey; Yale in New Haven, Connecticut; and the Arnold Arboretum for Harvard in Boston, Massachusetts. Her numerous private estate projects include the landmark Dumbarton Oaks in the Georgetown neighborhood of Washington, D.C.[12] Since that time, other architects – most notably Ruth Havey and Alden Hopkins – changed certain elements of the Farrand design.[citation needed]
 Since this period urban planning has developed into a separate independent profession that has incorporated important contributions from other fields such as civil engineering, architecture and public administration. Urban Planners are qualified to perform tasks independent of landscape architects, and in general, the curriculum of landscape architecture programs do not prepare students to become urban planners.[13]
 Landscape architecture continues to develop as a design discipline and to respond to the various movements in architecture and design throughout the 20th and 21st centuries. Thomas Church was a mid-century landscape architect significant in the profession. Roberto Burle Marx in Brazil combined the International style and native Brazilian plants and culture for a new aesthetic. Innovation continues today solving challenging problems with contemporary design solutions for master planning, landscapes, and gardens.[citation needed]
 Ian McHarg was known for introducing environmental concerns in landscape architecture.[14][15] He popularized a system of analyzing the layers of a site in order to compile a complete understanding of the qualitative attributes of a place. This system became the foundation of today's Geographic Information Systems (GIS). McHarg would give every qualitative aspect of the site a layer, such as the history, hydrology, topography, vegetation, etc.  GIS software is ubiquitously used in the landscape architecture profession today to analyze materials in and on the Earth's surface and is similarly used by urban planners, geographers, forestry and natural resources professionals, etc.[citation needed]
 European nations enabled the widespread circulation of urban planning strategies by transferring landscaping ideas and practices to overseas colonies. The green belt was a popular landscape practice exported by Britain onto colonial territories such as Haifa (1918-1948).[16] Spatial mechanisms like the green belt, implemented through the Haifa Bay Plan and the British ""Grand Model,"" were used to enforce political control and civic order and extend western ideas of progress and development.[16] The Greater London Regional Planning Committee accepted the green belt concept which formed the basis of the 1938 Green Belt Act. The planning prototype demarcated open spaces, distinguished between city and countryside, limited urban growth, and created zoning divisions.[16] It was used extensively in the British colonies to facilitate British rule through the organized division of landscape and populations. [16]
 Indigenous land management practices create constantly changing landscapes through the use of vegetation and natural systems, contrasting with western epistemologies of the discipline that separate ornament from function.[17] The discipline of landscape architecture favors western designs made from structured materials and geometric forms.[17] Landscape architecture history books tend to include projects that contain constructed architectural elements that persist over time, excluding many Indigenous landscape-based designs.[17]
 Landscape architecture textbooks often place Indigenous peoples as a prefix to the official start of the discipline. The widely read landscape history text The Landscape of Man (1964) offers a global history of the designed landscape from past to present, featuring African and other Indigenous peoples in its discussions of paleolithic man between 500,000 and 8,000 BCE in relation to human migration.[17] Indigenous land-management practices are described as archaeological rather than a part of contemporary practice. Gardens in Time (1980) also places Indigenous practice as prehistory at the beginning of the landscape architecture timeline. Authors John and Ray Oldham describe Aborigines of Australia as “survivors of an ancient way of life” who provide an opportunity to examine western Australia as a “meeting place of a prehistoric man.”[17]
 In the late 18th century, the landscapes created by aboriginal land and fire management practices appealed to English settlers in Australia.[17] Journals from the period of early white settlement note the landscape resembling parks and popular designs in English landscape gardens of the same period.[17] In England, these designs were considered sophisticated and celebrated for the intentional sacrifice of usable land. In Australia, the park-like condition was used to justify British control, citing its emptiness and lack of productive use as a basis for the dispossession of Aboriginal people. [17]
 Landscape Architects are generally required to have university or graduate education from an accredited landscape architecture degree program, which can vary in length and degree title. They learn how to create projects from scratch, such as residential or commercial planting and designing outdoor living spaces[18] they are willing to work with others to get a better outcome for the customers when doing a project; they will have to learn the basics of how to create a project on a manner of time and will require to get your license in a certain state to be allowed to work; students of Landscape Architects will learn how to interact with clients and will learn how to explain a design from scratch when giving the final project.[19]
 Landscape architecture has been taught in the University of Manchester since the 1950s. The course in the Manchester School of Architecture enables students to gain various bachelor's and master's degrees, including MLPM(Hons) which is accredited by the Landscape Institute and by the Royal Town Planning Institute.[20]
 In many countries, a professional institute, comprising members of the professional community, exists in order to protect the standing of the profession and promote its interests, and sometimes also regulate the practice of landscape architecture. The standard and strength of legal regulations governing landscape architecture practice varies from nation to nation, with some requiring licensure in order to practice; and some having little or no regulation. In Europe, North America, parts of South America, Australia, India, and New Zealand, landscape architecture is a regulated profession.[21]
 Since 1889, with the arrival of the French architect and urbanist landscaper Carlos Thays, recommended to recreate the National Capital's parks and public gardens, it was consolidated an apprentice and training program in landscaping that eventually became a regulated profession, currently the leading academic institution is the UBA University of Buenos Aires""UBA Facultad de Arquitectura, Diseño y Urbanismo"" (Faculty of Architecture, Design and Urbanism) offering a Bacherlor's degree in Urban Landscaping Design and Planning, the profession itself is regulated by the National Ministry of Urban Planning of Argentina and the Institute of the Buenos Aires Botanical Garden.[citation needed]
 The Australian Institute of Landscape Architects (AILA) provides accreditation of university degrees and non-statutory professional registration for landscape architects. Once recognized by AILA, landscape architects use the title 'Registered Landscape Architect' across the six states and territories within Australia.[citation needed]
 AILA's system of professional recognition is a national system overseen by the AILA National Office in Canberra. To apply for AILA Registration, an applicant usually needs to satisfy a number of pre-requisites, including university qualification, a minimum number years of practice and a record of professional experience.[22]
 Landscape Architecture within Australia covers a broad spectrum of planning, design, management, and research. From specialist design services for government and private sector developments through to specialist professional advice as an expert witness.[citation needed]
 In Canada, landscape architecture, like law and medicine, is a self-regulating profession pursuant to provincial statute.  For example, Ontario's profession is governed by the Ontario Association of Landscape Architects pursuant to the Ontario Association of Landscape Architects Act.  Landscape architects in Ontario, British Columbia, and Alberta must complete the specified components of L.A.R.E (Landscape Architecture Registration Examination) as a prerequisite to full professional standing.
 Provincial regulatory bodies are members of a national organization, the Canadian Society of Landscape Architects / L'Association des Architectes Paysagistes du Canada (CSLA-AAPC), and individual membership in the CSLA-AAPC is obtained through joining one of the provincial or territorial components.[23]
 ISLA (Indonesia Society of Landscape Architects) is the Indonesian society for professional landscape architects formed on 4 February 1978 and is a member of IFLA APR and IFLA World. The main aim is to increase the dignity of the professional members of landscape architects by increasing their activity role in community service, national and international development. The management of IALI consists of National Administrators who are supported by 20 Regional Administrators (Provincial level) and 3 Branch Managers at city level throughout Indonesia.[citation needed]
 Landscape architecture education in Indonesia was held in 18 universities, which graduated D3, Bachelor and Magister graduates. The landscape architecture education incorporate in Association of Indonesian Landscape Architecture Education.[citation needed]
 AIAPP (Associazione Italiana Architettura del Paesaggio) is the Italian association of professional landscape architects formed in 1950 and is a member of IFLA and IFLA Europe (formerly known as EFLA). AIAPP is in the process of contesting this new law which has given the Architects' Association the new title of Architects, Landscape Architects, Planners and Conservationists whether or not they have had any training or experience in any of these fields other than Architecture.
In Italy, there are several different professions involved in landscape architecture:
 The New Zealand Institute of Landscape Architects (NZILA) is the professional body for Landscape Architects in NZ.[24]
 In April 2013, NZILA jointly with AILA, hosted the 50th International Federation of Landscape Architects (IFLA) World Congress in Auckland, New Zealand.  The World Congress is an international conference where Landscape Architects from all around the globe meet to share ideas around a particular topic.[citation needed]
 Within NZ, Members of NZILA when they achieve their professional standing, can use the title Registered Landscape Architect NZILA.[citation needed]
 NZILA provides an education policy and an accreditation process to review education programme providers; currently there are three accredited undergraduate Landscape Architecture programmes in New Zealand. Lincoln University also has an accredited masters programme in landscape architecture.[citation needed]
 Landscape architecture in Norway was established in 1919 at the Norwegian University of Life Sciences (NMBU) at Ås. The Norwegian School of Landscape Architecture at the Faculty of Landscape and Society is responsible for Europe's oldest landscape architecture education on an academic level. The departments areas include design and design of cities and places, garden art history, landscape engineering, greenery, zone planning, site development, place making and place keeping.[citation needed]
 In May 1962, Joane Pim, Ann Sutton, Peter Leutscher and Roelf Botha (considered the forefathers of the profession in South Africa) established the Institute for Landscape Architects, now known as the Institute for Landscape Architecture in South Africa (ILASA).[25] ILASA is a voluntary organisation registered with the South African Council for the Landscape Architectural Profession (SACLAP).[26] It consists of three regional bodies, namely, Gauteng, KwaZula-Natal and the Western Cape. ILASA's mission is to advance the profession of landscape architecture and uphold high standards of professional service to its members, and to represent the profession of landscape architecture in any matter which may affect the interests of the members of the institute. ILASA holds the country's membership with The International Federation of Landscape Architects (IFLA).[27]
 In South Africa, the profession is regulated by SACLAP, established as a statutory council in terms of Section 2 of the South African Council for the Landscape Architectural Profession Act – Act 45 of 2000. The Council evolved out of the Board of Control for Landscape Architects (BOCLASA), which functioned under the Council of Architects in terms of The Architectural Act, Act 73 of 1970. SACLAP's mission is to establish, direct, sustain and ensure a high level of professional responsibilities and ethical conduct within the art and science of landscape architecture with honesty, dignity and integrity in the broad interest of public health, safety and welfare of the community.[citation needed]
 After completion of an accredited under-graduate and/or post-graduate qualification in landscape architecture at either the University of Cape Town or the University of Pretoria, or landscape technology at the Cape Peninsula University of Technology, professional registration is attained via a mandatory mentored candidacy period (minimum of two years) and sitting of the professional registration exam. After successfully completing the exam, the individual is entitled to the status of Professional Landscape Architect or Professional Landscape Technologist.[citation needed]
 Architects Sweden, Sveriges Arkitekter, is the collective trade union and professional organisation for all architects, including landscape architects, in Sweden. The professional body is a member of IFLA (International Federation of Landscape Architects) as well as IFLA Europe.
 As a landscape architect, anyone can become a member of Architects Sweden if they have a national or international university degree that is approved by the association. If the degree is from within the European Union, Architects Sweden approves Landscape architect educations listed by IFLA Europe. For educations outside the EU, the association makes an assessment on a statement from the Swedish Council for Higher Education (UHR).
 The UK's professional body is the Landscape Institute (LI). It is a chartered body that accredits landscape professionals and university courses. At present there are fifteen accredited programmes in the UK. Membership of the LI is available to students, academics and professionals, and there are over 3,000 professionally qualified members.[citation needed]
 The Institute provides services to assist members including support and promotion of the work of landscape architects; information and guidance to the public and industry about the specific expertise offered by those in the profession; and training and educational advice to students and professionals looking to build upon their experience.[citation needed]
 In 2008, the LI launched a major recruitment drive entitled ""I want to be a Landscape Architect"" to encourage the study of Landscape Architecture. The campaign aimed to raise the profile of landscape architecture and highlight its valuable role in building sustainable communities and fighting climate change.[28]
 As of July 2018, the ""I want to be a Landscape Architect"" initiative was replaced by a brand new careers campaign entitled #ChooseLandscape, which aims to raise awareness of landscape as a profession; improve and increase access to landscape education; and inspire young people to choose landscape as a career.[29] This new campaign includes other landscape-related professions such as landscape management, landscape planning, landscape science and urban design.[30]
 In the United States, Landscape Architecture is regulated by individual state governments. For a landscape architect, obtaining licensure requires advanced education and work experience, plus passage of the national examination called The Landscape Architect Registration Examination (L.A.R.E.). Several states require passage of a state exam as well. In the United States licensing is overseen both at the state level, and nationally by the Council of Landscape Architectural Registration Boards (CLARB).  Landscape architecture has been identified as an above-average growth profession by the US Bureau of Labor Statistics and was listed in U.S. News & World Report's list of Best Jobs to Have in 2006, 2007, 2008, 2009 and 2010.[31] The national trade association for United States landscape architects is the American Society of Landscape Architects.  Frederic Law Olmsted, who designed Central Park in New York City, is known as the ""father of American Landscape Architecture"".[32]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['landscape management, landscape planning, landscape science and urban design', 'John and Ray Oldham', 'master planning and garden design for manor houses, palaces and royal properties', 'landscape management, landscape planning, landscape science and urban design', 'design of other interventions that will produce desired outcomes'], 'answer_start': [], 'answer_end': []}"
"Urban design is an approach to the design of buildings and the spaces between them that focuses on specific design processes and outcomes. In addition to designing and shaping the physical features of towns, cities, and regional spaces, urban design considers 'bigger picture' issues of economic, social and environmental value and social design. The scope of a project can range from a local street or public space to an entire city and surrounding areas. Urban designers connect the fields of architecture, landscape architecture and urban planning to better organize physical space and community environments.[1]
 Some important focuses of urban design on this page include its historical impact, paradigm shifts, its interdisciplinary nature, and issues related to urban design.
 Urban design deals with the larger scale of groups of buildings, infrastructure, streets, and public spaces, entire neighbourhoods and districts, and entire cities, with the goal of making urban environments that are equitable, beautiful, performative, and sustainable.[2][3][4]
 Urban design is an interdisciplinary field that utilizes the procedures and the elements of architecture and other related professions, including landscape design, urban planning, civil engineering, and municipal engineering.[5][6] It borrows substantive and procedural knowledge from public administration, sociology, law, urban geography, urban economics and other related disciplines from the social and behavioral sciences, as well as from the natural sciences.[7] In more recent times different sub-subfields of urban design have emerged such as strategic urban design, landscape urbanism, water-sensitive urban design, and sustainable urbanism. Urban design demands an understanding of a wide range of subjects from physical geography to social science, and an appreciation for disciplines, such as real estate development, urban economics, political economy, and social theory.
 Urban design theory deals primarily with the design and management of public space (i.e. the 'public environment', 'public realm' or 'public domain'), and the way public places are used and experienced. Public space includes the totality of spaces used freely on a day-to-day basis by the general public, such as streets, plazas, parks, and public infrastructure. Some aspects of privately owned spaces, such as building facades or domestic gardens, also contribute to public space and are therefore also considered by urban design theory. Important writers on urban design theory include Christopher Alexander, Peter Calthorpe, Gordon Cullen, Andrés Duany, Jane Jacobs, Jan Gehl, Allan B. Jacobs, Kevin Lynch, Aldo Rossi, Colin Rowe, Robert Venturi, William H. Whyte, Camillo Sitte, Bill Hillier (space syntax), and Elizabeth Plater-Zyberk.
 Although contemporary professional use of the term 'urban design' dates from the mid-20th century, urban design as such has been practiced throughout history. Ancient examples of carefully planned and designed cities exist in Asia, Africa, Europe, and the Americas, and are particularly well known within Classical Chinese, Roman, and Greek cultures. Specifically, Hippodamus of Miletus was a famous ancient Greek architect and urban planner, and all around academic that is often considered to be a ""father of European urban planning"", and the namesake of the ""Hippodamian plan"", also known as the grid plan of a city layout.[8]
 European Medieval cities are often, and often erroneously, regarded as exemplars of undesigned or 'organic' city development. There are many examples of considered urban design in the Middle Ages.[9] In England, many of the towns listed in the 9th-century Burghal Hidage were designed on a grid, examples including Southampton, Wareham, Dorset and Wallingford, Oxfordshire, having been rapidly created to provide a defensive network against Danish invaders.[10] 12th century western Europe brought renewed focus on urbanisation as a means of stimulating economic growth and generating revenue.[11] The burgage system dating from that time and its associated burgage plots brought a form of self-organising design to medieval towns.[12]
 Throughout history, the design of streets and deliberate configuration of public spaces with buildings have reflected contemporaneous social norms or philosophical and religious beliefs.[13] Yet the link between designed urban space and the human mind appears to be bidirectional. Indeed, the reverse impact of urban structure upon human behaviour and upon thought is evidenced by both observational study and historical records. There are clear indications of impact through Renaissance urban design on the thought of Johannes Kepler and Galileo Galilei.[14] Already René Descartes in his Discourse on the Method had attested to the impact Renaissance planned new towns had upon his own thought, and much evidence exists that the Renaissance streetscape was also the perceptual stimulus that had led to the development of coordinate geometry.[15]
 The beginnings of modern urban design in Europe are associated with the Renaissance but, especially, with the Age of Enlightenment.[16] Spanish colonial cities were often planned, as were some towns settled by other imperial cultures.[17] These sometimes embodied utopian ambitions as well as aims for functionality and good governance, as with James Oglethorpe's plan for Savannah, Georgia.[18] In the Baroque period the design approaches developed in French formal gardens such as Versailles were extended into urban development and redevelopment. In this period, when modern professional specializations did not exist, urban design was undertaken by people with skills in areas as diverse as sculpture, architecture, garden design, surveying, astronomy, and military engineering. In the 18th and 19th centuries, urban design was perhaps most closely linked with surveyors engineers and architects. The increase in urban populations brought with it problems of epidemic disease, the response to which was a focus on public health, the rise in the UK of municipal engineering and the inclusion in British legislation of provisions such as minimum widths of street in relation to heights of buildings in order to ensure adequate light and ventilation.[citation needed]
 Much of Frederick Law Olmsted's work was concerned with urban design, and the newly formed profession of landscape architecture also began to play a significant role in the late 19th century.[19]
 In the 19th century, cities were industrializing and expanding at a tremendous rate. Private businesses largely dictated the pace and style of this development. The expansion created many hardships for the working poor and concern for public health increased. However, the laissez-faire style of government, in fashion for most of the Victorian era, was starting to give way to a New Liberalism. This gave more power to the public. The public wanted the government to provide citizens, especially factory workers, with healthier environments. Around 1900, modern urban design emerged from developing theories on how to mitigate the consequences of the industrial age.
 The first modern urban planning theorist was Sir Ebenezer Howard. His ideas, although utopian, were adopted around the world because they were highly practical. He initiated the garden city movement.[20] in 1898.
His garden cities were intended to be planned, self-contained communities surrounded by parks. Howard wanted the cities to be proportional with separate areas of residences, industry, and agriculture. Inspired by the Utopian novel Looking Backward and Henry George's work Progress and Poverty, Howard published his book Garden Cities of To-morrow in 1898. His work is an important reference in the history of urban planning.[21] He envisioned the self-sufficient garden city to house 32,000 people on a site of 6,000 acres (2,428 ha). He planned on a concentric pattern with open spaces, public parks, and six radial boulevards, 120 ft (37 m) wide, extending from the center. When it reached full population, Howard wanted another garden city to be developed nearby. He envisaged a cluster of several garden cities as satellites of a central city of 50,000 people, linked by road and rail.[22] His model for a garden city was first created at Letchworth[23] and Welwyn Garden City in Hertfordshire. Howard's movement was extended by Sir Frederic Osborn to regional planning.[23]
 In the early 1900s, urban planning became professionalized. With input from utopian visionaries, civil engineers, and local councilors, new approaches to city design were developed for consideration by decision-makers such as elected officials. In 1899, the Town and Country Planning Association was founded. In 1909, the first academic course on urban planning was offered by the University of Liverpool.[24] Urban planning was first officially embodied in the Housing and Town Planning Act of 1909 Howard's 'garden city' compelled local authorities to introduce a system where all housing construction conformed to specific building standards.[25] In the United Kingdom following this Act, surveyor, civil engineers, architects, and lawyers began working together within local authorities. In 1910, Thomas Adams became the first Town Planning Inspector at the Local Government Board and began meeting with practitioners. In 1914, The Town Planning Institute was established. The first urban planning course in America wasn't established until 1924 at Harvard University. Professionals developed schemes for the development of land, transforming town planning into a new area of expertise.
 In the 20th century, urban planning was changed by the automobile industry. Car-oriented design impacted the rise of 'urban design'. City layouts now revolved around roadways and traffic patterns.
 In June 1928, the International Congresses of Modern Architecture (CIAM) was founded at the Chateau de la Sarraz in Switzerland, by a group of 28 European architects organized by Le Corbusier, Hélène de Mandrot, and Sigfried Giedion. The CIAM was one of many 20th century manifestos meant to advance the cause of ""architecture as a social art"".
 Team X was a group of architects and other invited participants who assembled starting in July 1953 at the 9th Congress of the International Congresses of Modern Architecture (CIAM) and created a schism within CIAM by challenging its doctrinaire approach to urbanism.
 In 1956, the term ""Urban design"" was first used at a series of conferences hosted by Harvard University. The event provided a platform for Harvard's Urban Design program. The program also utilized the writings of famous urban planning thinkers: Gordon Cullen, Jane Jacobs, Kevin Lynch, and Christopher Alexander.
 In 1961, Gordon Cullen published The Concise Townscape. He examined the traditional artistic approach to city design of theorists including Camillo Sitte, Barry Parker, and Raymond Unwin. Cullen also created the concept of 'serial vision'. It defined the urban landscape as a series of related spaces.
 Also in 1961, Jane Jacobs published The Death and Life of Great American Cities. She critiqued the modernism of CIAM (International Congresses of Modern Architecture). Jacobs also claimed crime rates in publicly owned spaces were rising because of the Modernist approach of 'city in the park'. She argued instead for an 'eyes on the street' approach to town planning through the resurrection of main public space precedents (e.g. streets, squares).
 In the same year, Kevin Lynch published The Image of the City. He was seminal to urban design, particularly with regards to the concept of legibility. He reduced urban design theory to five basic elements: paths, districts, edges, nodes, landmarks. He also made the use of mental maps to understand the city popular, rather than the two-dimensional physical master plans of the previous 50 years.
 Other notable works:
 The popularity of these works resulted in terms that become everyday language in the field of urban planning. Aldo Rossi introduced 'historicism' and 'collective memory' to urban design. Rossi also proposed a 'collage metaphor' to understand the collection of new and old forms within the same urban space. Peter Calthorpe developed a manifesto for sustainable urban living via medium-density living. He also designed a manual for building new settlements in his concept of Transit Oriented Development (TOD). Bill Hillier and Julienne Hanson introduced Space Syntax to predict how movement patterns in cities would contribute to urban vitality, anti-social behaviour, and economic success. 'Sustainability', 'livability', and 'high quality of urban components' also became commonplace in the field.
 Today, urban design seeks to create sustainable urban environments with long-lasting structures, buildings, and overall livability. Walkable urbanism is another approach to practice that is defined within the Charter of New Urbanism. It aims to reduce environmental impacts by altering the built environment to create smart cities that support sustainable transport. Compact urban neighborhoods encourage residents to drive less. These neighborhoods have significantly lower environmental impacts when compared to sprawling suburbs.[26] To prevent urban sprawl, Circular flow land use management was introduced in Europe to promote sustainable land use patterns.
 As a result of the recent New Classical Architecture movement, sustainable construction aims to develop smart growth, walkability, architectural tradition, and classical design.[27][28] It contrasts with modernist and globally uniform architecture. In the 1980s, urban design began to oppose the increasing solitary housing estates and suburban sprawl.[29]
Managed Urbanisation with the view to making the urbanising process completely culturally and economically, and environmentally sustainable, and as a possible solution to the urban sprawl, Frank Reale has submitted an interesting concept of Expanding Nodular Development (E.N.D.) that integrates many urban designs and ecological principles, to design and build smaller rural hubs with high-grade connecting freeways, rather than adding more expensive infrastructure to existing big cities and the resulting congestion.
 Throughout the young existence of the Urban Design discipline, many paradigm shifts have occurred that have affected the trajectory of the field regarding theory and practice. These paradigm shifts cover multiple subject areas outside of the traditional design disciplines.
 There have been many different theories and approaches applied to the practice of urban design.
 New Urbanism is an approach that began in the 1980s as a place-making initiative to combat suburban sprawl. Its goal is to increase density by creating compact and complete towns and neighborhoods. The 10 principles of new urbanism are walkability, connectivity, mixed-use and diversity, mixed housing, quality architecture and urban design, traditional neighborhood structure, increased density, smart transportation, sustainability, and quality of life. New urbanism and the developments that it has created are sources of debates within the discipline, primarily with the landscape urbanist approach but also due to its reproduction of idyllic architectural tropes that do not respond to the context. Andres Duany, Elizabeth Plater-Zyberk, Peter Calthorpe, and Jeff Speck are all strongly associated with New Urbanism and its evolution over the years.
 Landscape Urbanism is a theory that first surfaced in the 1990s, arguing that the city is constructed of interconnected and ecologically rich horizontal field conditions, rather than the arrangement of objects and buildings. Charles Waldheim, Mohsen Mostafavi, James Corner, and Richard Weller are closely associated with this theory. Landscape urbanism theorises sites, territories, ecosystems, networks, and infrastructures through landscape practice according to Corner,[30] while applying a dynamic concept to cities as ecosystems that grow, shrink or change phases of development according to Waldheim.[31]
 Everyday Urbanism is a concept introduced by Margaret Crawford and influenced by Henry Lefebvre that describes the everyday lived experience shared by urban residents including commuting, working, relaxing, moving through city streets and sidewalks, shopping, buying, eating food, and running errands. Everyday urbanism is not concerned with aesthetic value. Instead, it introduces the idea of eliminating the distance between experts and ordinary users and forces designers and planners to contemplate a 'shift of power' and address social life from a direct and ordinary perspective.
 Tactical Urbanism (also known as DIY Urbanism, Planning-by-Doing, Urban Acupuncture, or Urban Prototyping) is a city, organizational, or citizen-led approach to neighborhood-building that uses short-term, low-cost, and scalable interventions and policies to catalyze long term change.
 Top-up Urbanism is the theory and implementation of two techniques in urban design: top-down and bottom-up. Top-down urbanism is when the design is implemented from the top of the hierarchy - normally the government or planning department. Bottom-up or grassroots urbanism begins with the people or the bottom of the hierarchy. Top-up means that both methods are used together to make a more participatory design, so it is sure to be comprehensive and well regarded in order to be as successful as possible.
 Infrastructural Urbanism is the study of how the major investments that go into making infrastructural systems can be leveraged to be more sustainable for communities. Instead of the systems being solely about efficiency in both cost and production, infrastructural urbanism strives to utilize these investments to be more equitable for social and environmental issues as well. Linda Samuels is a designer investigating how to accomplish this change in infrastructure in what she calls ""next-generation infrastructure"" which is ""multifunctional; public; visible; socially productive; locally specific, flexible, and adaptable; sensitive to the eco-economy; composed of design prototypes or demonstration projects; symbiotic; technologically smart; and developed collaboratively across disciplines and agencies"".
 Sustainable Urbanism is the study from the 1990s of how a community can be beneficial for the ecosystem, the people, and the economy for which it is associated. It is based on Scott Campbell's planner's triangle which tries to find the balance between economy, equity, and the environment. Its main concept is to try and make cities as self-sufficient as possible while not damaging the ecosystem around them, today with an increased focus on climate stability.[4] A key designer working with sustainable urbanism is Douglas Farr.
 Feminist Urbanism is the study and critique of how the built environment affects genders differently because of patriarchal social and political structures in society. Typically, the people at the table making design decisions are men, so their conception about public space and the built environment relates to their life perspectives and experiences, which do not reflect the same experiences of women or children. Dolores Hayden is a scholar who has researched this topic from 1980 to the present day. Hayden's writing says, “when women, men, and children of all classes and races can identify the public domain as the place where they feel most comfortable as citizens, Americans will finally have homelike urban space.”
 Educational Urbanism is an emerging discipline, at the crossroads of urban planning, educational planning, and pedagogy. An approach that tackles the notion that economic activities, the need for new skills at the workplace, and the spatial configuration of the workplace rely on the spatial reorientation in the design of educational spaces and the urban dimension of educational planning.
 Black Urbanism is an approach in which black communities are active creators, innovators, and authors of the process of designing and creating the neighborhoods and spaces of the metropolitan areas they have done so much to help revive over the past half-century. The goal is not to build black cities for black people but to explore and develop the creative energy that exists in so-called black areas: that has the potential to contribute to the sustainable development of the whole city.
 Underlying the practice of urban design are the many theories about how to best design the city. Each theory makes a unique claim about how to effectively design thriving, sustainable urban environments. Debates over the efficacy of these approaches fill the urban design discourse. Landscape Urbanism and New Urbanism are commonly debated as distinct approaches to addressing suburban sprawl. While Landscape Urbanism proposes landscape as the basic building block of the city and embraces horizontality, flexibility, and adaptability, New Urbanism offers the neighborhood as the basic building block of the city and argues for increased density, mixed uses, and walkability. Opponents of Landscape Urbanism point out that most of its projects are urban parks, and as such, its application is limited. Opponents of New Urbanism claim that its preoccupation with traditional neighborhood structures is nostalgic, unimaginative, and culturally problematic. Everyday Urbanism argues for grassroots neighborhood improvements rather than master-planned, top-down interventions. Each theory elevates the roles of certain professions in the urban design process, further fueling the debate. In practice, urban designers often apply principles from many urban design theories. Emerging from the conversation is a universal acknowledgement of the importance of increased interdisciplinary collaboration in designing the modern city.[32]
 Urban designers work with architects, landscape architects, transportation engineers, urban planners, and industrial designers to reshape the city. Cooperation with public agencies, authorities and the interests of nearby property owners is necessary to manage public spaces. Users often compete over the spaces and negotiate across a variety of spheres. Input is frequently needed from a wide range of stakeholders. This can lead to different levels of participation as defined in Arnstein's Ladder of Citizen Participation.[33]
 While there are some professionals who identify themselves specifically as urban designers, a majority have backgrounds in urban planning, architecture, or landscape architecture. Many collegiate programs incorporate urban design theory and design subjects into their curricula. There is an increasing number of university programs offering degrees in urban design at the post-graduate level.
 Urban design considers:
 The original urban design was thought to be separated from architecture and urban planning. Urban Design has developed to a certain extent, and comes from the foundation of engineering. In Anglo-Saxon countries, it is often considered as a branch under the architecture, urban planning, and landscape architecture and limited as the construction of the urban physical environment. However Urban Design is more integrated into the social science-based, cultural, economic, political, and other aspects. Not only focus on space and architectural group, but also look at the whole city from a broader and more holistic perspective to shape a better living environment. Compared to architecture, the spatial and temporal scale of Urban Design processing is much larger. It deals with neighborhoods, communities, and even the entire city.
 The University of Liverpool's Department of Civic Design is the first urban design school in the world founded in 1909.[34] Following the 1956 Urban Design conference, Harvard University established the first graduate program with urban design in its title, The Master of Architecture in Urban Design, although as a subject taught in universities its history in Europe is far older. Urban design programs explore the built environment from diverse disciplinary backgrounds and points of view. The pedagogically innovative combination of interdisciplinary studios, lecture courses, seminars, and independent study creates an intimate and engaging educational atmosphere in which students thrive and learn. Soon after in 1961, Washington University in St. Louis founded their Master of Urban Design program. Today, twenty urban design programs exist in the United States:
 In the United Kingdom, Master's programmes in Urban Design at University of Manchester or University of Sheffield and Cardiff University or London South Bank University and City Design at the Royal College of Art or Queen's University Belfast are offered.
 The field of urban design holds enormous potential for helping us address today's biggest challenges: an expanding population, mass urbanization, rising inequality, and climate change. In its practice as well as its theories, urban design attempts to tackle these pressing issues. As climate change progresses, urban design can mitigate the results of flooding, temperature changes, and increasingly detrimental storm impacts through a mindset of sustainability and resilience. In doing so, the urban design discipline attempts to create environments that are constructed with longevity in mind, such as zero-carbon cities. Cities today must be designed to minimize resource consumption, waste generation, and pollution while also withstanding the unprecedented impacts of climate change.[4][35][36][37] To be truly resilient, our cities need to be able to not just bounce back from a catastrophic climate event but to bounce forward to an improved state.
 Another issue in this field is that it is often assumed that there are no mothers of planning and urban design. However, this is not the case, many women have made proactive contributions to the field, including the work of Mary Kingsbury Simkhovitch, Florence Kelley, and Lillian Wald, to name a few of whom were prominent leaders in the City Social movement. The City Social was a movement that steamed between the commonly known City Practical and City Beautiful movements. It was a movement that’s main concerns lay with the economic and social equalities regarding urban issues.[38]
 Justice is and will always be a key issue in urban design. As previously mentioned, past urban strategies have caused injustices within communities incapable of being remedied via simple means. As urban designers tackle the issue of justice, they often are required to look at the injustices of the past and must be careful not to overlook the nuances of race, place, and socioeconomic status in their design efforts. This includes ensuring reasonable access to basic services, transportation, and fighting against gentrification and the commodification of space for economic gain. Organizations such as the Divided Cities Initiatives at Washington University in St. Louis and the Just City Lab at Harvard work on promoting justice in urban design.
 Until the 1970s, the design of towns and cities took little account of the needs of people with disabilities. At that time, disabled people began to form movements demanding recognition of their potential contribution if social obstacles were removed. Disabled people challenged the 'medical model' of disability which saw physical and mental problems as an individual 'tragedy' and people with disabilities as 'brave' for enduring them. They proposed instead a 'social model' which said that barriers to disabled people result from the design of the built environment and attitudes of able-bodied people. 'Access Groups' were established composed of people with disabilities who audited their local areas, checked planning applications, and made representations for improvements. The new profession of 'access officer' was established around that time to produce guidelines based on the recommendations of access groups and to oversee adaptations to existing buildings as well as to check on the accessibility of new proposals. Many local authorities now employ access officers who are regulated by the Access Association. A new chapter of the Building Regulations (Part M) was introduced in 1992. Although it was beneficial to have legislation on this issue the requirements were fairly minimal but continue to be improved with ongoing amendments. The Disability Discrimination Act 1995 continues to raise awareness and enforce action on disability issues in the urban environment.
 
The issue of walkability has gained prominence in recent years, not only with the concerns of the aforementioned climate change, but also the health outcomes of residents. Car-centric urban design has an invariably negative effect on such outcomes. With proximity to internal combustion engines, residents tend to suffer from dangerous levels of air pollution which lead to cardiovascular complications ranging from the acute, in hypertension and alterations in heart rate, and the chronic, the outright development of atherosclerosis. More people die from air pollution each year than from car accidents.  This issue has been used to fuel movements for alternative forms of long to mid range transportation such as trains and bicycles, with walking as the primary means of short-range travel. This would bring benefits from two simultaneous avenues. The physical activity from walking, and the lack of particulate matter (carbon dioxide, sulfur dioxide, nitrogen dioxide, etc.) has shown to alleviate and lower the risk of many maladies such as diabetes, hypertension and cardiovascular disease. Physical activity levels from walking are closely related to the abundance of open public spaces, commercial shops, greenery, among others. These attributes also have been stated to contribute to stronger social and emotional health as the open public spaces facilitate more social interaction within communities. This issue is most prevalent in the United States, where the rise of neoliberalism directly and intentionally caused the car-centric infrastructure.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['urban design', 'Johannes Kepler and Galileo Galilei', 'impact', 'historical impact, paradigm shifts, its interdisciplinary nature', 'limited'], 'answer_start': [], 'answer_end': []}"
"
 Art of Central Asia
 Art of East Asia
 Art of South Asia
 Art of Southeast Asia
 Art of Europe
 Art of Africa
 Art of the Americas
 Art of Oceania
 The history of architecture traces the changes in architecture through various traditions, regions, overarching stylistic trends, and dates. The beginnings of all these traditions is thought to be humans satisfying the very basic need of shelter and protection.[1] The term ""architecture"" generally refers to buildings, but in its essence is much broader, including fields we now consider specialized forms of practice, such as urbanism, civil engineering, naval, military,[2] and landscape architecture.
 Trends in architecture were influenced, among other factors, by technological innovations, particularly in the 19th, 20th and 21st centuries. The improvement and/or use of steel, cast iron, tile, reinforced concrete, and glass helped for example Art Nouveau appear and made Beaux Arts more grandiose.[3]
 Humans and their ancestors have been creating various types of shelters for at least hundreds of thousands of years, and shelter-building may have been present early in hominin evolution. All great apes will construct ""nests"" for sleeping, albeit at different frequencies and degrees of complexity. Chimpanzees regularly make nests out of bundles of branches woven together;[4] these vary depending on the weather (nests have thicker bedding when cool and are built with larger, stronger supports in windy or wet weather).[5] Orangutans currently make the most complex nests out of all non-human great apes, complete with roofs, blankets, pillows, and ""bunks"".[6]
 It has been argued that nest-building practices were crucial to the evolution of human creativity and construction skill moreso than tool use, as hominins became required to build nests not just in uniquely adapted circumstances but as forms of signalling.[7] Retaining arboreal features like highly prehensile hands for the expert construction of nests and shelters would have also benefitted early hominins in unpredictable environments and changing climates.[5] Many hominins, especially the earliest ones such as Ardipithecus[8] and Australopithecus[9] retained such features and may have chosen to build nests in trees where available. The development of a ""home base"" 2 million years ago may have also fostered the evolution of constructing shelters or protected caches.[10] Regardless of the complexity of nest-building, early hominins may still have still slept in more or less 'open' conditions, unless the opportunity of a rock shelter was afforded.[7] These rock shelters could be used as-is with little more amendments than nests and hearths, or in the case of established bases —especially among later hominins— they could be personalized with rock art (in the case of Lascaux) or other types of aesthetic structures (in the case of the Bruniquel Cave among the Neanderthals)[11] In cases of sleeping in open ground, Dutch ethologist Adriaan Kortlandt once proposed that hominins could have built temporary enclosures of thorny bushes to deter predators, which he supported using tests that showed lions becoming averse to food if near thorny branches.[12]
 In 2000, archaeologists at the Meiji University in Tokyo claimed to have found 2 pentagonal alignments of post holes on a hillside near the village of Chichibu, interpreting it as two huts dated around 500,000 years old and built by Homo erectus.[13] Currently, the earliest confirmed purpose-built structures are in France at the site of Terra Amata, along with the earliest evidence of artificial fire, c. 400,000 years ago.[14] Due to the perishable nature of shelters of this time, it is difficult to find evidence for dwellings beyond hearths and the stones that may make up a dwelling's foundation. Near Wadi Halfa, Sudan, the Arkin 8 site contains 100,000 year old circles of sandstone that were likely the anchor stones for tents.[15] In eastern Jordan, post hole markings in the soil give evidence to houses made of poles and thatched brush around 20,000 years ago.[16] In areas where bone — especially mammoth bone — is a viable material, evidence of structures preserve much more easily, such as the mammoth-bone dwellings among the Mal'ta-Buret' culture 24–15,000 years ago and at Mezhirich 15,000 years ago. The Upper Paleolithic in general is characterized by the expansion and cultural growth of anatomically modern humans (as well as the cultural growth of Neanderthals, despite their steady extinction at this time), and although we currently lack data for dwellings built before this time, the dwellings of this era begin to more commonly show signs of aesthetic modification, such as at Mezhirich where engraved mammoth tusks may have formed the ""facade"" of a dwelling.[17]
 Architectural advances are an important part of the Neolithic period (10,000-2000 BC), during which some of the major innovations of human history occurred. The domestication of plants and animals, for example, led to both new economics and a new relationship between people and the world, an increase in community size and permanence, a massive development of material culture and new social and ritual solutions to enable people to live together in these communities. New styles of individual structures and their combination into settlements provided the buildings required for the new lifestyle and economy, and were also an essential element of change.[21]
 Although many dwellings belonging to all prehistoric periods and also some clay models of dwellings have been uncovered enabling the creation of faithful reconstructions, they seldom included elements that may relate them to art. Some exceptions are provided by wall decorations and by finds that equally apply to Neolithic and Chalcolithic rites and art.
 In South and Southwest Asia, Neolithic cultures appear soon after 10,000 BC, initially in the Levant (Pre-Pottery Neolithic A and Pre-Pottery Neolithic B) and from there spread eastwards and westwards. There are early Neolithic cultures in Southeast Anatolia, Syria and Iraq by 8000 BC, and food-producing societies first appear in southeast Europe by 7000 BC, and Central Europe by c. 5500 BC (of which the earliest cultural complexes include the Starčevo-Koros (Cris), Linearbandkeramic, and Vinča).[22][23][24][25]
 Neolithic settlements and ""cities"" include:
 Mesopotamia is most noted for its construction of mud-brick buildings and the construction of ziggurats, occupying a prominent place in each city and consisting of an artificial mound, often rising in huge steps, surmounted by a temple. The mound was no doubt to elevate the temple to a commanding position in what was otherwise a flat river valley. The great city of Uruk had a number of religious precincts, containing many temples larger and more ambitious than any buildings previously known.[32]
 The word ziggurat is an anglicized form of the Akkadian word ziqqurratum, the name given to the solid stepped towers of mud brick. It derives from the verb zaqaru, (""to be high""). The buildings are described as being like mountains linking Earth and heaven. The Ziggurat of Ur, excavated by Leonard Woolley, is 64 by 46 meters at base and originally some 12 meters in height with three stories. It was built under Ur-Nammu (circa 2100 B.C.) and rebuilt under Nabonidus (555–539 B.C.), when it was increased in height to probably seven stories.[33]
 Modern imaginings of ancient Egypt are heavily influenced by the surviving traces of monumental architecture. Many formal styles and motifs were established at the dawn of the pharaonic state, around 3100 BC. The most iconic Ancient Egyptian buildings are the pyramids, built during the Old and Middle Kingdoms (c.2600–1800 BC) as tombs for the pharaoh. However, there are also impressive temples, like the Karnak Temple Complex.
 The Ancient Egyptians believed in the afterlife. They also believed that in order for their soul (known as ka) to live eternally in their afterlife, their bodies would have to remain intact for eternity. So, they had to create a way to protect the deceased from damage and grave robbers. This way, the mastaba was born. These were adobe structures with flat roofs, which had underground rooms for the coffin, about 30 m down. Imhotep, an ancient Egyptian priest and architect, had to design a tomb for the Pharaoh Djoser. For this, he placed five mastabas, one above the next, this way creating the first Egyptian pyramid, the Pyramid of Djoser at Saqqara (c.2667–2648 BC), which is a step pyramid. The first smooth-sided one was built by Pharaoh Sneferu, who ruled between c.2613 and 2589 BC. The most imposing one is the Great Pyramid of Giza, made for Sneferu's son: Khufu (c.2589–2566 BC), being the last surviving wonder of the ancient world and the largest pyramid in Egypt. The stone blocks used for pyramids were held together by mortar, and the entire structure was covered with highly polished white limestone, with their tops topped in gold. What we see today is actually the core structure of the pyramid. Inside, narrow passages led to the royal burial chambers. Despite being highly associated with the Ancient Egypt, pyramids have been built by other civilisations too, like the Mayans.
 Due to the lack of resources and a shift in power towards priesthood, ancient Egyptians stepped away from pyramids, and temples became the focal point of cult construction. Just like the pyramids, Ancient Egyptian temples were also spectacular and monumental. They evolved from small shrines made of perishable materials to large complexes, and by the New Kingdom (circa 1550–1070 BC) they have become massive stone structures consisting of halls and courtyards. The temple represented a sort of 'cosmos' in stone, a copy of the original mound of creation on which the god could rejuvenate himself and the world. The entrance consisted of a twin gateway (pylon), symbolizing the hills of the horizon. Inside there were columned halls symbolizing a primeval papyrus thicket. It was followed by a series of hallways of decreasing size, until the sanctuary was reached, where a god's cult statue was placed. Back in ancient times, temples were painted in bright colours, mainly red, blue, yellow, green, orange, and white. Because of the desert climate of Egypt, some parts of these painted surfaces were preserved well, especially in interiors.
 An architectural element specific to ancient Egyptian architecture is the cavetto cornice (a concave moulding), introduced by the end of the Old Kingdom. It was widely used to accentuate the top of almost every formal pharaonic building. Because of how often it was used, it will later decorate many Egyptian Revival buildings and objects.[41][38]
 The first Urban Civilization in the Indian subcontinent is traceable originally to the Indus Valley civilisation mainly in Mohenjodaro and Harappa, now in modern-day Pakistan as well western states of the Republic of India. The earliest settlements are seen during the Neolithic period in Merhgarh, Balochistan. The civilization's cities were noted for their urban planning with baked brick buildings, elaborate drainage and water systems, and handicraft (carnelian products, seal carving). This civilisation transitioned from the Neolithic period into the Chalcolithic period and beyond with their expertise in metallurgy (copper, bronze, lead, and tin).[44] Their urban centres possibly grew to contain between 30,000 and 60,000 individuals,[45] and the civilisation itself may have contained between one and five million individuals.[46]
 Since the advent of the Classical Age in Athens, in the 5th century BC, the Classical way of building has been deeply woven into Western understanding of architecture and, indeed, of civilization itself.[53] From circa 850 BC to circa 300 AD, ancient Greek culture flourished on the Greek mainland, on the Peloponnese, and on the Aegean islands. However, Ancient Greek architecture is best known for its temples, many of which are found throughout the region, and the Parthenon is a prime example of this. Later, they will serve as inspiration for Neoclassical architects during the late 18th and the 19th century. The most well-known temples are the Parthenon and the Erechtheion, both on the Acropolis of Athens. Another type of important Ancient Greek buildings were the theatres. Both temples and theatres used a complex mix of optical illusions and balanced ratios.
 Ancient Greek temples usually consist of a base with continuous stairs of a few steps at each edges (known as crepidoma), a cella (or naos) with a cult statue in it, columns, an entablature, and two pediments, one on the front side and another in the back. By the 4th century BC, Greek architects and stonemasons had developed a system of rules for all buildings known as the orders: the Doric, the Ionic, and the Corinthian. They are most easily recognised by their columns (especially by the capitals). The Doric column is stout and basic, the Ionic one is slimmer and has four scrolls (called volutes) at the corners of the capital, and the Corinthian column is just like the Ionic one, but the capital is completely different, being decorated with acanthus leafs and four scrolls.[47] Besides columns, the frieze was different based on order. While the Doric one has metopes and triglyphs with guttae, Ionic and Corinthian friezes consist of one big continuous band with reliefs.
 Besides the columns, the temples were highly decorated with sculptures, in the pediments, on the friezes, metopes and triglyphs. Ornaments used by Ancient Greek architects and artists include palmettes, vegetal or wave-like scrolls, lion mascarons (mostly on lateral cornices), dentils, acanthus leafs, bucrania, festoons, egg-and-dart, rais-de-cœur, beads, meanders, and acroteria at the corners of the pediments. Pretty often, ancient Greek ornaments are used continuously, as bands. They will later be used in Etruscan, Roman and in the post-medieval styles that tried to revive Greco-Roman art and architecture, like Renaissance, Baroque, Neoclassical etc.
 Looking at the archaeological remains of ancient and medieval buildings it is easy to perceive them as limestone and concrete in a grey taupe tone and make the assumption that ancient buildings were monochromatic. However, architecture was polychromed in much of the Ancient and Medieval world. One of the most iconic Ancient buildings, the Parthenon (c. 447–432 BC) in Athens, had details painted with vibrant reds, blues and greens. Besides ancient temples, Medieval cathedrals were never completely white. Most had colored highlights on capitals and columns.[54] This practice of coloring buildings and artworks was abandoned during the early Renaissance. This is because Leonardo da Vinci and other Renaissance artists, including Michelangelo, promoted a color palette inspired by the ancient Greco-Roman ruins, which because of neglect and constant decay during the Middle Ages, became white despite being initially colorful. The pigments used in the ancient world were delicate and especially susceptible to weathering. Without necessary care, the colors exposed to rain, snow, dirt, and other factors, vanished over time, and this way Ancient buildings and artworks became white, like they are today and were during the Renaissance.[55]
 The architecture of ancient Rome has been one of the most influential in the world. Its legacy is evident throughout the medieval and early modern periods, and Roman buildings continue to be reused in the modern era in both New Classical and Postmodern architecture. It was particularly influenced by Greek and Etruscan styles. A range of temple types was developed during the republican years (509–27 BC), modified from Greek and Etruscan prototypes.
 Wherever the Roman army conquered, they established towns and cities, spreading their empire and advancing their architectural and engineering achievements. While the most important works are to be found in Italy, Roman builders also found creative outlets in the western and eastern provinces, of which the best examples preserved are in modern-day North Africa, Turkey, Syria and Jordan. Extravagant projects appeared, like the Arch of Septimius Severus in Leptis Magna (present-day Libya, built in 216 AD), with broken pediments on all sides, or the Arch of Caracalla in Thebeste (present-day Algeria, built in c.214 AD), with paired columns on all sides, projecting entablatures and medallions with divine busts. Due to the fact that the empire was formed from multiple nations and cultures, some buildings were the product of combining the Roman style with the local tradition. An example is the Palmyra Arch (present-day Syria, built in c.212–220), some of its arches being embellished with a repeated band design consisting of four ovals within a circle around a rosette, which are of Eastern origin.
 Among the many Roman architectural achievements were domes (which were created for temples), baths, villas, palaces and tombs. The most well known example is the one of the Pantheon in Rome, being the largest surviving Roman dome and having a large oculus at its centre. Another important innovation is the rounded stone arch, used in arcades, aqueducts and other structures. Besides the Greek orders (Doric, Ionic and Corinthian), the Romans invented two more. The Tuscan order was influenced by the Doric, but with un-fluted columns and a simpler entablature with no triglyphs or guttae, while the Composite was a mixed order, combining the volutes of the Ionic order capital with the acanthus leaves of the Corinthian order.
 Between 30 and 15 BC, the architect and engineer Marcus Vitruvius Pollio published a major treatise, De architectura, which influenced architects around the world for centuries. As the only treatise on architecture to survive from antiquity, it has been regarded since the Renaissance as the first book on architectural theory, as well as a major source on the canon of classical architecture.[60]
 Just like the Greeks, the Romans built amphiteatres too. The largest amphitheatre ever built, the Colosseum in Rome, could hold around 50,000 spectators. Another iconic Roman structure that demonstrates their precision and technological advancement is the Pont du Gard in southern France, the highest surviving Roman aqueduct.[61][57]
 From over 3,000 years before the Europeans 'discovered' America, complex societies had already been established across North, Central and South America. The most complex ones were in Mesoamerica, notably the Mayans, the Olmecs and the Aztecs, but also Incas in South America. Structures and buildings were often aligned with astronomical features or with the cardinal directions.
 Much of the Mesoamerican architecture developed through cultural exchange – for example the Aztecs learnt much from earlier Mayan architecture. Many cultures built entire cities, with monolithic temples and pyramids decoratively carved with animals, gods and kings. Most of these cities had a central plaza with governmental buildings and temples, plus public ball courts, or tlachtli, on raised platforms. Just like in ancient Egypt, here were built pyramids too, being generally stepped. They were probably not used as burial chambers, but had important religious sites at the top.[64] They had few rooms, as interiors mattered less than the ritual presence of these imposing structures and the public ceremonies they hosted; so, platforms, altars, processional stairs, statuary, and carving were all important.[67]
 Inca architecture originated from the Tiwanaku styles, founded in the 2nd century B.C.E.. The Incas used topography and land materials in their designs, with the capital city of Cuzco still containing many examples. The famous Machu Picchu royal estate is a surviving example, along with Sacsayhuamán and Ollantaytambo. The Incas also developed a road system along the western continent, placing their distinctive architecture along the way, visually asserting their imperial rule along the frontier. Other groups such as the Muisca did not construct grand architecture of stone based materials, but rather made of materials like wood and clay.
 After the fall of the Indus Valley, South Asian architecture entered the Dharmic period which saw the development of Ancient Indian architectural styles which further developed into various unique forms in the Middle Ages, along with the combination of Islamic styles, and later, other global traditions.
 Buddhist architecture developed in the Indian subcontinent during the 4th and 2nd century BC, and spread first to China and then further across Asia. Three types of structures are associated with the religious architecture of early Buddhism: monasteries (viharas), places to venerate relics (stupas), and shrines or prayer halls (chaityas, also called chaitya grihas), which later came to be called temples in some places. The most iconic Buddhist type of building is the stupa, which consists of a domed structure containing relics, used as a place of meditation to commemorate Buddha. The dome symbolised the infinite space of the sky.[68]
 Buddhism had a significant influence on Sri Lankan architecture after its introduction,[69] and ancient Sri Lankan architecture was mainly religious, with over 25 styles of Buddhist monasteries.[70] Monasteries were designed using the Manjusri Vasthu Vidya Sastra, which outlines the layout of the structure.
 After the fall of the Gupta empire, Buddhism mainly survived in Bengal under the Palas,[71] and has had a significant impact on pre-Islamic Bengali architecture of that period.[72]
 Across the Indian subcontinent, Hindu architecture evolved from simple rock-cut cave shrines to monumental temples. From the 4th to 5th centuries AD, Hindu temples were adapted to the worship of different deities and regional beliefs, and by the 6th or 7th centuries larger examples had evolved into towering brick or stone-built structures that symbolise the sacred five-peaked Mount Meru. Influenced by early Buddhist stupas, the architecture was not designed for collective worship, but had areas for worshippers to leave offerings and perform rituals.[73]
 Many Indian architectural styles for structures such as temples, statues, homes, markets, gardens and planning are as described in Hindu texts.[74][75] The architectural guidelines survive in Sanskrit manuscripts and in some cases also in other regional languages. These include the Vastu shastras, Shilpa Shastras, the Brihat Samhita, architectural portions of the Puranas and the Agamas, and regional texts such as the Manasara among others.[76][77]
 Since this architectural style emerged in the classical period, it has had a considerable influence on various medieval architectural styles like that of the Gurjaras, Dravidians, Deccan, Odias, Bengalis, and the Assamese.
 This style of North Indian architecture has been observed in Hindu as well as Jain places of worship and congregation. It emerged in the 11th to 13th centuries under the Chaulukya (Solanki) period.[79] It eventually became more popular among the Jain communities who spread it in the greater region and across the world.[80] These structures have the unique features like a large number of projections on external walls with sharply carved statues, and several urushringa spirelets on the main shikhara.
 The Himalayas are inhabited by various people groups including the Paharis, Sino-Tibetans, Kashmiris, and many more groups. Being from different religious and ethnic backgrounds, the architecture has also had multiple influences. Considering the logistical difficulties and slower pace of life in the Himalayas, artisans have that the time to make intricate wood carvings and paintings accompanied by ornamental metal work and stone sculptures that are reflected in religious as well as civic and military buildings. These styles exist in different forms from Tibet and Kashmir to Assam and Nagaland.[81] A common feature is observed in the slanted layered roofs on temples, mosques, and civic buildings.[82]
 This is an architectural style that emerged in the southern part of the Indian subcontinent and in Sri Lanka. These include Hindu temples with a unique style that involves a shorter pyramidal tower over the garbhagriha or sanctuary called a vimana, where the north has taller towers, usually bending inwards as they rise, called shikharas. These also include secular buildings that may or may not have slanted roofs based on the geographical region. In the Tamil country, this style is influenced by the Sangam period as well as the styles of the great dynasties that ruled it. This style varies in the region to its west in Kerala that is influenced by geographic factors like western trade and the monsoons which result in sloped roofs.[85] Further north, the Karnata Dravida style varies based on the diversity of influences, often relaying much about the artistic trends of the rulers of twelve different dynasties.[86]
 The ancient Kalinga region corresponds to the present-day eastern Indian areas of Odisha, West Bengal and northern Andhra Pradesh. Its architecture reached a peak between the 9th and 12th centuries under the patronage of the Somavamsi dynasty of Odisha. Lavishly sculpted with hundreds of figures, Kalinga temples usually feature repeating forms such as horseshoes. Within the protective walls of the temple complex are three main buildings with distinctive curved towers called deul or deula and prayer halls called jagmohan.[88]
 
 Chinese and Confucian culture has had a significant influence on the art and architecture in the Sinosphere (mainly Vietnam, Korea, Japan).[89]
 What is recognised today as Chinese culture has its roots in the Neolithic period (10,000–2000 BC), covering the cultural sites of Yangshao, Longshan, and Liangzhu in central China. Sections of present-day north-east China also contain sites of the Neolithic Hongshan culture that manifested aspects of proto-Chinese culture. Native Chinese belief systems included naturalistic, animistic and hero worship. In general, open-air platforms (tan, or altar) were used for worshipping naturalistic deities, such as the gods of wind and earth, whereas formal buildings (miao, or temple) were for heroes and deceased ancestors.
 Most early buildings in China were timber structures. Columns with sets of brackets on the face of the buildings, mostly in even numbers, made the central intercolumnal space the largest interior opening. Heavily tiled roofs sat squarely on the timber building with walls constructed in brick or pounded earth.
 The transmission of Buddhism into China around the 1st century AD led to a new era of religious practices, and so to new building types. Places of worship in form of cave temples appeared in China, based on Indian rock-cut ones. Another new building type introduced by Buddhism was the Chinese form of stupa (ta) or pagoda. In India, stupas were erected to commemorate well-known people or teachers: consequently, the Buddhist tradition adapted the structure to remember the great teacher, the Buddha. In The Chinese pagoda shared a similar symbolism with the Indian stupa and was built with sponsorship mainly from imperial patrons who hoped to gain earthly merits for the next life. Buddhism reached its peak from the 6th to the 8th centuries when there was an unprecedented number of monasteries thought China. More than 4,600 official and 40,000 unofficial monasteries were built. They varies in size by the number of cloisters they contained, ranging from 6 to 120. Each cloister consisted of a main stand-alone building – a hall, pagoda of pavilion – and was surrounded by a covered corridor in a rectangular compounded served by a gate building.[90]
 Korean architecture, especially post Choson period showcases Ming-Qing influences.[91]
 Traditionally, Japanese architecture was made of wood and fusuma (sliding doors) in place of walls, allowing internal space to be altered to suit different purposes. The introduction of Buddhism in the mid 6th century, via the neighbouring Korean kingdom of Paekche, initiated large-scale wooden temple building with an emphasis on simplicity, and much of the architecture was imported from China and other Asian cultures. By the end of this century, Japan was constructing Continental-style monasteries, notably the temple, known as Horyu-ji in Ikaruga.[92] In contrast with Western architecture, Japanese structures rarely use stone, except for specific elements such as foundations. Walls are light, thin, never load-bearing and often movable.[60]
 From the start of the 9th century to the early 15th century, Khmer kings rules over a vad Hindu-Buddhist empire in Southeast Asia. Angkor, in present-day Cambodia, was its capital city, and most of its surviving buildings are east-facing stone temples, many of them constructed in pyramidal, tiered form consisting of five square structures with towers, or prasats, that represent the sacred five-peaked Mount Meru of Hindu, Jain and Buddhist doctrine. As the residences of gods, temples were made of durable materials such as sandstone, brick or laterite, a clay-like substance that dries hard.[94]
 Cham architecture in Vietnam also follows a similar style.[93]
 Traditional Sub-Saharan African architecture is diverse, varying significantly across regions. Included among traditional house types, are huts, sometimes consisting of one or two rooms, as well as various larger and more complex structures.
 In much of West Africa, rectangular houses with peaked roofs and courtyards, sometimes consisting of several rooms and courtyards, are also traditionally found (sometimes decorated, with adobe reliefs as among the Ashanti of Ghana,[96][97] or carved pillars as among the Yoruba people of Nigeria, especially in palaces and the dwellings of the wealthy)[98] Besides the regular rectangular type of dwelling with a sharp roof, widespread in West Africa and Madagascar, there also other types of houses: beehive houses made from a circle of stones topped with a domed roof, and the round one, with a cone-shaped roof. The first type, which also existed in America, is characteristic especially for Southern Africa. These were used by Bantu-speaking groups in southern and parts of east Africa, which was made with mud, poles, thatch, and cow dung (rectangular houses were more common among the Bantu-speaking peoples of the greater Congo region and central Africa). The round hut with a cone-shaped roof is widespread especially in Sudan and Eastern Africa, but is also present in Colombia and New Caledonia, as well as in the Western Sudan and Sahel regions of west Africa, where they are sometimes arranged into compounds.[99] A distinct style of traditional wooden architecture exists among the Grassland peoples of Cameroon, such as the Bamileke.
 In several West African societies, including the kingdom of Benin (and of other Edo peoples), and the kingdoms of the Yoruba, Hausa, at sites like Jenne-Jeno (a pre-Islamic city in Mali),[100][101] and elsewhere, towns and cities were surrounded by large walls of mud brick or adobe,[102] and sometimes by monumental moats and earthworks, such as Sungbo's Eredo (in the Nigerian Yoruba kingdom of Ijebu) and the Walls of Benin (of the Nigerian Kingdom of Benin).[103][104] In medieval southern Africa, a tradition existed of fortified stone settlements such as Great Zimbabwe and Khami.
 The famed Benin City of southwest Nigeria (capital of the Kingdom of Benin) destroyed by the Punitive Expedition, was a large complex of homes in coursed clay, with hipped roofs of shingles or palm leaves. The Palace had a sequence of ceremonial rooms, and was decorated with brass plaques. It was surrounded by a monumental complex of earthworks and walls whose construction is thought to have begun by the early Middle Ages.[103][104][105][106]
 In the Western Sahel region, Islamic influence was a major contributing factor to architectural development from the later ages of the Kingdom of Ghana. At Kumbi Saleh, locals lived in domed-shaped dwellings in the king's section of the city, surrounded by a great enclosure. Traders lived in stone houses in a section which possessed 12 beautiful mosques, as described by al-bakri, with one centered on Friday prayer.[107] The king is said to have owned several mansions, one of which was sixty-six feet long, forty-two feet wide, contained seven rooms, was two stories high, and had a staircase; with the walls and chambers filled with sculpture and painting.[108]
 Sahelian architecture initially grew from the two cities of Djenné and Timbuktu. The Sankore Mosque in Timbuktu, constructed from mud on timber, was similar in style to the Great Mosque of Djenné. The rise of kingdoms in the West African coastal region produced architecture which drew on indigenous traditions, utilizing wood, mud-brick and adobe. Though later acquiring Islamic influences, the style also had roots in local pre-Islamic building styles, such as those found in ancient settlements like Jenne-Jeno, Dia, Mali, and Dhar Tichitt,[109] some of which employed a traditional sahelian style of cylindrical mud brick.[100]
 Ethiopian architecture (including modern-day Eritrea) expanded from the Aksumite style and incorporated new traditions with the expansion of the Ethiopian state. Styles incorporated more wood and rounder structures in domestic architecture in the center of the country and the south, and these stylistic influences were manifested in the construction of churches and monasteries. Throughout the medieval period, Aksumite architecture and influences and its monolithic tradition persisted, with its influence strongest in the early medieval (Late Aksumite) and Zagwe periods (when the rock-cut monolithic churches of Lalibela were carved). Throughout the medieval period, and especially from the 10th to 12th centuries, churches were hewn out of rock throughout Ethiopia, especially during the northernmost region of Tigray, which was the heart of the Aksumite Empire. The most famous example of Ethiopian rock-hewn architecture are the eleven monolithic churches of Lalibela, carved out of the red volcanic tuff found around the town.[110] During the early modern period in Ethiopia, the absorption of new diverse influences such as Baroque, Arab, Turkish and Gujarati style began with the arrival of Portuguese Jesuit missionaries in the 16th and 17th centuries.
 Most Oceanic buildings consist of huts, made of wood and other vegetal materials. Art and architecture have often been closely connected—for example, storehouses and meetinghouses are often decorated with elaborate carvings—and so they are presented together in this discussion. The architecture of the Pacific Islands was varied and sometimes large in scale. Buildings reflected the structure and preoccupations of the societies that constructed them, with considerable symbolic detail. Technically, most buildings in Oceania were no more than simple assemblages of poles held together with cane lashings; only in the Caroline Islands were complex methods of joining and pegging known. Fakhua shen, Taboa shen and Kuhua shen (the shen triplets) designed the first oceanian architecture.
 An important Oceanic archaeological site is Nan Madol from the Federated States of Micronesia. Nan Madol was the ceremonial and political seat of the Saudeleur Dynasty, which united Pohnpei's estimated 25,000 people until about 1628.[111] Set apart between the main island of Pohnpei and Temwen Island, it was a scene of human activity as early as the first or second century AD. By the 8th or 9th century, islet construction had started, with construction of the distinctive megalithic architecture beginning 1180–1200 AD.[112]
 Due to the extent of the Islamic conquests, Islamic architecture encompasses a wide range of architectural styles from the foundation of Islam (7th century) to the present day. Early Islamic architecture was influenced by Roman, Byzantine, Persian, Mesopotamian architecture and all other lands which the Early Muslim conquests conquered in the 7th and 8th centuries.[118][119] Further east, it was also influenced by Chinese and Indian architecture as Islam spread to Southeast Asia. This wide and long history has given rise to many local architectural styles, including but not limited to: Umayyad, Abbasid, Persian, Moorish, Fatimid, Mamluk, Ottoman, Indo-Islamic (particularly Mughal), Sino-Islamic and Sahelian architecture.
 Some distinctive structures in Islamic architecture are mosques, madrasas, tombs, palaces, baths, and forts. Notable types of Islamic religious architecture include hypostyle mosques, domed mosques and mausoleums, structures with vaulted iwans, and madrasas built around central courtyards. In secular architecture, major examples of preserved historic palaces include the Alhambra and the Topkapi Palace. Islam does not encourage the worship of idols; therefore the architecture tends to be decorated with Arabic calligraphy (including Qur'anic verses or other poetry) and with more abstract motifs such as geometric patterns, muqarnas, and arabesques, as opposed to illustrations of scenes and stories.[120][121][122][123]
 Surviving examples of medieval secular architecture mainly served for defense across various parts of Europe. Castles and fortified walls provide the most notable remaining non-religious examples of medieval architecture. New types of civic, military, as well as religious buildings of new styles begin to pop up in this region during this period.
 Byzantine architects built city walls, palaces, hippodromes, bridges, aqueducts, and churches. They built many types of churches, including the basilica (the most widespread type, and the one that reached the greatest development). After the early period, the most common layout was the cross-in-square with five domes, also found in Moscow, Novgorod or Kiev, as well as in Romania, Bulgaria, Serbia, North Macedonia and Albania. Through modifications and adaptations of local inspiration, the Byzantine style will be used as the main source of inspiration for architectural styles in all Eastern Orthodox countries.[129] For example, in Romania, the Brâncovenesc style is highly based on Byzantine architecture, but also has individual Romanian characteristics.
 Just as the Parthenon is the most famous building of Ancient Greek architecture, Hagia Sophia remains the iconic church of Orthodox Christianity. In Greek and Roman temples, the exterior was the most important part of the temple, where sacrifices were made; the interior, where the cult statue of the deity to whom the temple was built was kept, often had limited access by the general public. But Christian liturgies are held in the interior of the churches, Byzantine exteriors usually have little if any ornamentation.[130]
 Byzantine architecture often featured marble columns, coffered ceilings and sumptuous decoration, including the extensive use of mosaics with golden backgrounds.[131] The building material used by Byzantine architects was no longer marble, which was very appreciated by the Ancient Greeks. They used mostly stone and brick, and also thin alabaster sheets for windows.[132] Mosaics were used to cover brick walls, and any other surface where fresco would not resist. Good examples of mosaics from the proto-Byzantine era are in Hagios Demetrios in Thessaloniki (Greece), the Basilica of Sant'Apollinare Nuovo and the Basilica of San Vitale, both in Ravenna (Italy), and Hagia Sophia in Istanbul.
 From the very beginning of the formation of feudal relations, the architecture and urban planning of Armenia entered a new stage. The ancient Armenian cities experienced economic decline; only Artashat and Tigranakert retained their importance. The importance of the cities of Dvin and Karin (Erzurum) increased. The construction of the city of Arshakavan by the king of Great Armenia Arshak II was not completely completed. Christianity brought to life a new architecture of religious buildings, which was initially nourished by the traditions of the old, ancient architecture.
 Churches of the 4th-5th centuries are mainly basilicas (Kasakh, 4th-5th centuries, Ashtarak, 5th century, Akhts, 4th century, Yeghvard, 5th century). Some basilicas of Armenian architecture belong to the so-called “Western type” of basilica churches. Of these, the most famous are the churches of Tekor (5th century), Yererouk (IV-V centuries), Dvin (470), Tsitsernavank (IV-5 centuries). The three-nave Yereruyk basilica stands on a 6-step stylobate, presumably built on the site of an earlier pre-Christian temple. The basilicas of Karnut (5th century), Yeghvard (5th century), Garni (IV century), Zovuni (5th century), Tsaghkavank (VI century), Dvina (553–557), Tallinn (5th century) have also been preserved c.), Tanaat (491), Jarjaris (IV-V centuries), Lernakert (IV-V centuries), etc.[136]
 The term 'Romanesque' is rooted in the 19th century, when it was coined to describe medieval churches built from the 10th to 12th century, before the rise of steeply pointed arches, flying buttresses and other Gothic elements. This style of architecture emerged nearly simultaneously in multiple countries (France, Germany, Italy, Spain).[142] For 19th-century critics, the Romanesque reflected the architecture of stonemasons who evidently admired the heavy barrel vaults and intricate carved capitals of the ancient Romans, but whose own architecture was considered derivative and degenerate, lacking the sophistication of their classical models.
 Scholars in the 21st century are less inclined to understand the architecture of this period as a 'failure' to reproduce the achievements of the past, and are far more likely to recognise its profusion of experimental forms, as a series of creative new inventions. At the time, however, research has questioned the value of Romanesque as a stylistic term. On the surface, it provides a convenient designation for buildings that share a common vocabulary of rounded arches and thick stone masonry, and appear in between the Carolingian revival of classical antiquity in the 9th century and the swift evolution of Gothic architecture after the second half of the 12th century. One problem, however, is that the term encompasses a broad array of regional variations, some with closer links to Rome than others. It should also be noted that the distinction between Romanesque architecture and its immediate predecessors and followers is not at all clear. There is little evidence that medieval viewers were concerned with the stylistic distinctions that we observe today, making the slow evolution of medieval architecture difficult to separate into neat chronological categories. Nevertheless, Romanesque remains a useful word despite its limitations, because it reflects a period of intensive building activity that maintained some continuity with the classical past, but freely reinterpreted ancient forms in a new distinctive manner.[21]
 Romanesque cathedrals can be easily differentiated from Gothic and Byzantine ones, since they are characterized by the wide use of thick piers and columns, round arches and severity. Here, the possibilities of the round-arch arcade in both a structural and a spatial sense were once again exploited to the full. Unlike the sharp pointed arch of the later Gothic, the Romanesque round arch required the support of massive piers and columns. In comparison to Byzantine churches, Romanesque ones tend to lack complex ornamentation both on the exterior and interior. An example of this is the Périgueux Cathedral (Périgueux, France), built in the early 12th century and designed on the model of St. Mark's Basilica in Venice, but lacking mosaics, leaving its interior very austere and minimalistic.[143]
 Gothic architecture began with a series of experiments, which were conducted to fulfil specific requests by patrons and to accommodate the ever-growing number of pilgrims visiting sites that housed precious relics. Pilgrims in the high Middle Ages (circa 1000 to 1250 AD) increasingly travelled to well-known pilgrimage sites, but also to local sites where local and national saints were reputed to have performed miracles. The churches and monasteries housing important relics therefore wanted to heighten the popularity of their respective saints and build appropriate shrines for them. These shrines were not merely gem-encrusted reliquaries, but more importantly took the form of powerful architectural settings characterised by coloured light emitting from the large areas of stained glass. The use of stained glass, however, is not the only defining element of Gothic architecture and neither are the pointed arch, the ribbed vault, the rose window or the flying buttress, as many of these elements were used in one way or another in preceding architectural traditions. It was rather the combination and constant refinement of these elements, along with the quick response to the rapidly changing building techniques of the time, that fuelled the Gothic movement in architecture.
 Consequently, it is difficult to point to one element or the exact place where Gothic first emerged; however, it is traditional to initiate a discussion of Gothic architecture with the Basilica of St Denis (circa 1135–1344) and its patrons, Abbot Suger, who began to rebuild the west front and the choir of the church. As he wrote in his De Administratione, the old building could no longer accommodate the large volumes of pilgrims who were coming to venerate the relics of St Denis, and the solution for this twofold: a west façade with three large portals and the innovative new choir, which combined an ambulatory with radiating chapels that were unique as they were not separated by walls. Instead a row of slim columns was inserted between the chapels and the choir arcade to support the rib vaults. The result enabled visitors to circulate around the altar and come within reach of the relics without actually disrupting the altar space, while also experiencing the large stained-glass windows within the chapels. As confirmed by Suger, the desire for more stained-glass was not necessarily to bring more daylight into the building but rather to fill the space with a continuous ray of colorful light, rather like mosaics or precious stones, which would make the wall vanish. The demand for ever more stained-glass windows and the search for techniques that would support them are constant throughout the development of Gothic architecture, as is evident in the writings of Suger, who was fascinated by the mystical quality of such lighting.[21]
 Brick Gothic was a specific style of Gothic architecture common in Northeast and Central Europe especially in the regions in and around the Baltic Sea, which do not have resources of standing rock. The buildings are essentially built using bricks.
 During the Renaissance, Italy consisted of many states, and intense rivalry between them generated an increase in technical and artistic developments. The Medici Family, an Italian banking family and political dynasty, is famous for its financial support of Renaissance art and architecture.
 The period began in around 1452, when the architect and humanist Leon Battista Alberti (1404–1472) completed his treatise De Re Aedificatoria (On the Art of Building) after studying the ancient ruins of Rome and Vitruvius's De Architectura. His writings covered numerous subjects, including history, town planning, engineering, sacred geometry, humanism and philosophies of beauty, and set out the key elements of architecture and its ideal proportions. In the last decades of the 15th century, artists and architects began to visit Rome to study the ruins, especially the Colosseum and the Pantheon. They left behind precious records of their studies in the form of drawings. While humanist interest in Rome had been building up over more than a century (dating back at least to Petrarch in the 14th century), antiquarian considerations of monuments had focused on literary, epigraphic and historical information rather than on the physical remains. Although some artists and architects, such as Filippo Brunelleschi (1377–1446), Donatello (circa 1386–1466) and Leon Battista Alberti, are reported to have made studies of Roman sculpture and ruins, almost no direct evidence of this work survives. By the 1480s, prominent architects, such as Francesco di Giorgio (1439–1502) and Giuliano da Sangallo (circa 1445–1516), were making numerous studies of ancient monuments, undertaken in ways that demonstrated that the process of transforming the model into a new design had already begun. In many cases, drawing ruins in their fragmentary state necessitated a leap of imagination, as Francesco himself readily admitted in his annotation to his reconstruction of the Campidoglio, noting 'largely imagined by me, since very little can be understood from the ruins.[156]
 Soon, grand buildings were constructed in Florence using the new style, like the Pazzi Chapel (1441–1478) or the Palazzo Pitti (1458–1464). The Renaissance begun in Italy, but slowly spread to other parts of Europe, with varying interpretations.[149]
 Since Renaissance art is an attempt of reviving Ancient Rome's culture, it uses pretty much the same ornaments as the Ancient Greek and Roman. However, because most if not all resources that Renaissance artists had were Roman, Renaissance architecture and applied arts widely use certain motifs and ornaments that are specific to Ancient Rome. The most iconic one is the margent, a vertical arrangement of flowers, leaves or hanging vines, used at pilasters. Another ornament associated with the Renaissance is the round medallion, containing a profile of a person, similar with Ancient cameos. Renaissance, Baroque, Rococo, and other post-medieval styles use putti (chubby baby angels) much more often compared to Greco-Roman art and architecture. An ornament reintroduced during the Renaissance, that was of Ancient Roman descent, that will also be used in later styles, is the cartouche, an oval or oblong design with a slightly convex surface, typically edged with ornamental scrollwork.
 The Baroque emerged from the Counter Reformation as an attempt by the Catholic Church in Rome to convey its power and to emphasize the magnificence of God. The Baroque and its late variant the Rococo were the first truly global styles in the arts. Dominating more than two centuries of art and architecture in Europe, Latin America and beyond from circa 1580 to circa 1800. Born in the painting studios of Bologna and Rome in the 1580s and 1590s, and in Roman sculptural and architectural ateliers in the second and third decades of the 17th century, the Baroque spread swiftly throughout Italy, Spain and Portugal, Flanders, France, the Netherlands, England, Scandinavia, and Russia, as well as to central and eastern European centres from Munich (Germany) to Vilnius (Lithuania). The Portuguese, Spanish and French empires and the Dutch treading network had a leading role in spreading the two styles into the Americas and colonial Africa and Asia, to places such as Lima, Mozambique, Goa and the Philippines.[166] Due to its spread in regions with different architectural traditions, multiple kinds of Baroque appeared based on location, different in some aspects, but similar overall. For example, French Baroque appeared severe and detached by comparison, preempting Neoclassicism and the architecture of the Age of Enlightenment.[157] Hybrid Native American/European Baroque architecture first appeared in South America (as opposed to Mexico) in the late 17th century, after the indigenous symbols and styles that characterize this unusual variant of Baroque had been kept alive over the preceding century in other media, a very good example of this being the Jesuit Church in Arequipa (Peru).[167]
 The first Baroque buildings were cathedrals, churches and monasteries, soon joined by civic buildings, mansions, and palaces. Being characterized by dynamism, for the first time walls, façades and interiors curved,[168] a good example being San Carlo alle Quattro Fontane in Rome. Baroque architects took the basic elements of Renaissance architecture, including domes and colonnades, and made them higher, grander, more decorated, and more dramatic. The interior effects were often achieved with the use of quadratura, or trompe-l'œil painting combined with sculpture: the eye is drawn upward, giving the illusion that one is looking into the heavens. Clusters of sculpted angels and painted figures crowd the ceiling. Light was also used for dramatic effect; it streamed down from cupolas and was reflected from an abundance of gilding. Solomonic columns were often used, to give an illusion of upwards motion and other decorative elements occupied every available space. In Baroque palaces, grand stairways became a central element.[169] Besides architecture, Baroque painting and sculpture are characterized by dynamism too. This is in contrast with how static and peaceful Renaissance art is.
 Besides the building itself, the space where it was placed had a role too. Both Baroque and Rococo buildings try to seize viewers' attention and to dominate their surroundings, whether on a small scale such as the San Carlo alle Quattro Fontane in Rome, or on a massive one, like the new facade of the Santiago de Compostela Cathedral, designed to tower over the city. A manifestation of power and authority on the grandest scale, Baroque urban planning and renewal was promoted by the church and the state alike. It was the first era since antiquity to experience mass migration into cities, and urban planners took idealistic measures to regulate them. The most notable early example was Domenico Fontana's restructuring of Rome's street plan of Pope Sixtus V. Architects had experimented with idealized city schemes since the early Renaissance, examples being Leon Battista Alberti (1404–1472) planning a centralized model city, with streets leading to a central piazza, or Filarete (Antonio di Pietro Aver(u)lino, c. 1400-c. 1469) designing a round city named Sforzinda (1451–1456) that he based on parts of the human body in the idea that a healthy city should reflect the physiognomy of its inhabitants. However, none of these idealistic cities has ever been built. In fact, few such projects were put into practice in Europe as new cities were prohibitively costly and existing urban areas, with existing churches and palaces, could not be demolished. Only in the Americas, where architects often had a clean space to work with, were such cities possible, as in Lima (Peru) or Buenos Aires (Argentina). The earliest Baroque ideal city is Zamość, built north-east of Kraków (Poland) by the Italian architect Bernardo Morando (c. 1540-1600), being a centralized town focusing on a square with radiating streets. Where entire cities could not be rebuilt, patrons and architects compensated by creating spacious and symmetrical squares, often with avenues and radiating out at perpendicular angles and focusing on a fountain, statue or obelisk. A good example of this is the Place des Vosges (formerly Place Royale), commissioned by Henry IV probably after plans by Baptiste du Cerceau (1545–1590). The most famous Baroque space in the world is Gianlorenzo Bernini's St. Peter's Square in Rome.[170] Similar with ideal urban planning, Baroque gardens are characterized by straight and readapting avenues, with geometric spaces.
 The name Rococo derives from the French word rocaille, which describes shell-covered rock-work, and coquille, meaning seashell. Rococo architecture is fancy and fluid, accentuating asymmetry, with an abundant use of curves, scrolls, gilding and ornaments. The style enjoyed great popularity with the ruling elite of Europe during the first half of the 18th century. It developed in France out of a new fashion in interior decoration, and spread across Europe.[175] Domestic Rococo abandoned Baroque's high moral tone, its weighty allegories and its obsession with legitimacy: in fact, its abstract forms and carefree, pastoral subjects related more to notions of refuge and joy that created a more forgiving atmosphere for polite conversations. Rococo rooms are typically smaller than their Baroque counterparts, reflecting a movement towards domestic intimacy. Even the grander salons used for entertaining were more modest in scale, as social events involved smaller numbers of guests.
 Characteristic of the style were Rocaille motifs derived from the shells, icicles and rock-work or grotto decoration. Rocaille arabesques were mostly abstract forms, laid out symmetrically over and around architectural frames. A favourite motif was the scallop shell, whose top scrolls echoed the basic S and C framework scrolls of the arabesques and whose sinuous ridges echoed the general curvilinearity of the room decoration. While few Rococo exteriors were built in France, a number of Rococo churches are found in southern Germany.[176] Other widely-user motifs in decorative arts and interior architecture include: acanthus and other leaves, birds, bouquets of flowers, fruits, elements associated with love (putti, quivers with arrows ans arrowed hearts) trophies of arms, putti, medallions with faces, many many flowers, and Far Eastern elements (pagodes, dragons, monkeys, bizarre flowers, bamboo, and Chinese people).[177] Pastel colours were widely used, like light blue, mint green or pink. Rococo designers also loved mirrors (the more the better), an example being the Hall of Mirrors of the Amalienburg (Munich, Germany), by Johann Baptist Zimmermann. Generally, mirrors are also featured above fireplaces.
 The interactions between East and West brought on by colonialist exploration have had an impact on aesthetics. Because of being something rare and new to Westerners, some non-European styles were really appreciated during the 17th, 18th and 19th centuries. Some nobles and kings built little structures inspired by these styles in the gardens of their palaces, or fully decorated a handful of rooms of palaces like this. Because of not fully understanding the origins and principles that govern these exotic aesthetics, Europeans sometimes created hybrids of the style which they tried to replicate and which were the trends at that time. A good example of this is chinoiserie, a Western decorative style, popular during the 18th century, that was heavily inspired by Chinese arts, but also by Rococo at the same time. Because traveling to China or other Far Eastern countries was something hard at that time and so remained mysterious to most Westerners, European imagination were fuelled by perceptions of Asia as a place of wealth and luxury, and consequently patrons from emperors to merchants vied with each other in adorning their living quarters with Asian goods and decorating them in Asian styles. Where Asian objects were hard to obtain, European craftsmen and painters stepped up to fill the demand, creating a blend of Rococo forms and Asian figures, motifs and techniques.
 Chinese art was not the only foreign style with which Europeans experimented. Another was the Islamic one. Examples of this include the Garden Mosque of the Schwetzingen Palace in Germany (the only surviving example of an 18th-century European garden mosque), the Royal Pavilion in Brighton, or the Moorish Revival buildings from the 19th and early 20th centuries, with horseshoe arches and brick patterns. When it come to the Orient, Europeans also had an interest for the culture of Ancient Egypt. Compared to other cases of exoticism, the one with the land of pharaohs is the oldest one, since Ancient Greeks and Romans had this interest during Antiquity. The main periods when Egyptian Revival monuments were erected were the early 19th century, with Napoleon's military campaigns in Egypt, and the 1920s, when the Tomb of Tutankhamun was discovered in 1922, which caused an Egyptomania that lead to Art Deco sometimes using motifs inspired by Ancient Egypt. During the late 18th and early 19th century, Neoclassicism sometimes mixed Greco-Roman elements with Egyptian ones. Because of its association with pharaohs, death and eternity, multiple Egyptian Revival tombs or cemetery entry gates were built in this style. Besides mortuary structures, other buildings in this style include certain synagogues, like the Karlsruhe Synagogue or some Empire monuments built during the reign of Nepoleon, such as the Egyptian portico of the Hôtel Beauharnais or the Fontaine du Fellah. During the 1920s and 1930s, Pre-Columbian Mesoamerican architecture was of great interest for some American architects, particularly what the Mayans built. Several of Frank Lloyd Wright's California houses were erected in a Mayan Revival style, while other architects combined Mayan motifs with Art Deco ones.[186]
 Neoclassical architecture focused on Ancient Greek and Roman details, plain, white walls and grandeur of scale. Compared to the previous styles, Baroque and Rococo, Neoclassical exteriors tended to be more minimalist, featuring straight and angular lines, but being still ornamented. The style's clean lines and sense of balance and proportion worked well for grand buildings (such as the Panthéon in Paris) and for smaller structures alike (such as the Petit Trianon).
 Excavations during the 18th century at Pompeii and Herculaneum, which had both been buried under volcanic ash during the 79 AD eruption of Mount Vesuvius, inspired a return to order and rationality, largely thanks to the writings of Johann Joachim Winckelmann.[197][198] In the mid-18th century, antiquity was upheld as a standard for architecture as never before. Neoclassicism was a fundamental investigation of the very bases of architectural form and meaning. In the 1750s, an alliance between archaeological exploration and architectural theory started, which will continue in the 19th century. Marc-Antoine Laugier wrote in 1753 that 'Architecture owes all that is perfect to the Greeks'.[199]
 The style was adopted by progressive circles in other countries such as Sweden and Russia. Federal-style architecture is the name for the classicizing architecture built in North America between c. 1780 and 1830, and particularly from 1785 to 1815. This style shares its name with its era, the Federal Period. The term is also used in association with furniture design in the United States of the same time period. The style broadly corresponds to the middle-class classicism of Biedermeier style in the German-speaking lands, Regency style in Britain and to the French Empire style. In Central and Eastern Europe, the style is usually referred to as Classicism (German: Klassizismus, Russian: Классицизм), while the newer Revival styles of the 19th century until today are called neoclassical.
 Étienne-Louis Boullée (1728–1799) was a visionary architect of the period. His utopian projects, never built, included a monument to Isaac Newton (1784) in the form of an immense dome, with an oculus allowing the light to enter, giving the impression of a sky full of stars. His project for an enlargement of the Royal Library (1785) was even more dramatic, with a gigantic arch sheltering the collection of books. While none of his projects were ever built, the images were widely published and inspired architects of the period to look outside the traditional forms.[200]
 Similarly with the Renaissance and Baroque periods, during the Neoclassical one urban theories of how a good city should be appeared too. Enlightenment writers of the 18th century decried the problems of Paris at that time, the biggest one being the big number of narrow medieval streets crowded with modest houses. Voltaire openly criticized the failure of the French Royal administration to initiate public works, improve the quality of life in towns, and stimulate the economy. 'It is time for those who rule the most opulent capital in Europe to make it the most comfortable and the most magnificent of cities. There must be public markets, fountains which actually provide water and regular pavements. The narrow and infected streets must be widened, monuments that cannot be seen must be revealed and new ones built for all to see', Voltaire insisted in a polemical essay on 'The Embellishments of Paris' in 1749. In the same year, Étienne La Font de Saint-Yenne, criticized how Louis XIV's great east façade of the Louvre, was all but hidden from views by a dense quarter of modest houses. Voltaire also said that in order to transform Paris into a city that could rival ancient Rome, it was necessary to demolish more than it was to built. 'Our towns are still what they were, a mass of houses crowded together haphazardly without system, planning or design', Marc-Antoine Laugier complained in 1753. Writing a decade later, Pierre Patte promoted an urban reform in quest of health, social order, and security, launching at the same time a medical and organic metaphor which compared the operations of urban design to those of the surgeons. With bad air and lack of fresh water its current state was pathological, Patte asserted, calling for fountains to be placed at principal intersections and markets. Squares are recommended promote the circulation of air, and for the same reason houses on the city's bridges should be demolished. He also criticized the location of hospitals next to markets and protested continued burials in overcrowded city churchyards.[201] Besides cities, new ideas of how a garden should be appeared in 18th century England, making place for the English landscape garden (aka jardin à l'anglaise), characterized by an idealized view of nature, and the use of Greco-Roman or Gothic ruins, bridges, and other picturesque architecture, designed to recreate an idyllic pastoral landscape. It was the opposite of the symmetrical and geometrically planned Baroque garden (aka jardin à la française).
 The 19th century was dominated by a wide variety of stylistic revivals, variations, and interpretations. Revivalism in architecture is the use of visual styles that consciously echo the style of a previous architectural era. Modern-day Revival styles can be summarized within New Classical architecture, and sometimes under the umbrella term traditional architecture.
 The idea that architecture might represent the glory of kingdoms can be traced to the dawn of civilisation, but the notion that architecture can bear the stamp of national character is a modern idea, that appeared in the 18th century historical thinking and given political currency in the wake of the French Revolution. As the map of Europe was repeatedly changing, architecture was used to grant the aura of a glorious past to even the most recent nations. In addition to the credo of universal Classicism, two new, and often contradictory, attitudes on historical styles existed in the early 19th century. Pluralism promoted the simultaneous use of the expanded range of style, while Revivalism held that a single historical model was appropriate for modern architecture. Associations between styles and building types appeared, for example: Egyptian for prisons, Gothic for churches, or Renaissance Revival for banks and exchanges. These choices were the result of other associations: the pharaohs with death and eternity, the Middle Ages with Christianity, or the Medici family with the rise of banking and modern commerce.
 Whether their choice was Classical, medieval, or Renaissance, all revivalists shared the strategy of advocating a particular style based on national history, one of the great enterprises of historians in the early 19th century. Only one historic period was claimed to be the only one capable of providing models grounded in national traditions, institutions, or values. Issues of style became matters of state.[204]
 The most well-known Revivalist style is the Gothic Revival one, that appeared in the mid-18th century in the houses of a number of wealthy antiquarians in England, a notable example being the Strawberry Hill House. German Romantic writers and architects were the first to promote Gothic as a powerful expression of national character, and in turn use it as a symbol of national identity in territories still divided. Johann Gottfried Herder posed the question 'Why should we always imitate foreigners, as if we were Greeks or Romans?'.[205]
 In art and architecture history, the term Orientalism refers to the works of the Western artists who specialized in Oriental subjects, produced from their travels in Western Asia, during the 19th century. In that time, artists and scholars were described as Orientalists, especially in France.
 In India, during the British Raj, a new style, Indo-Saracenic, (also known as Indo-Gothic, Mughal-Gothic, Neo-Mughal, or Hindoo style) was getting developed, which incorporated varying degrees of Indian elements into the Western European style. The Churches and convents of Goa are another example of the blending of traditional Indian styles with western European architectural styles. Most Indo-Saracenic public buildings were constructed between 1858 and 1947, with the peaking at 1880.[206] The style has been described as ""part of a 19th-century movement to project themselves as the natural successors of the Mughals"".[207] They were often built for modern functions such as transport stations, government offices, and law courts. It is much more evident in British power centres in the subcontinent like Mumbai, Chennai, and Kolkata.[208]
 The Beaux-Arts style takes its name from the École des Beaux-Arts in Paris, where it developed and where many of the main exponents of the style studied. Due to the fact that international students studied here, there are buildings from the second half of the 19th century and the early 20th century of this type all over the world, designed by architects like Charles Girault, Thomas Hastings, Ion D. Berindey or Petre Antonescu. Today, from Bucharest to Buenos Aires and from San Francisco to Brussels, the Beaux-Arts style survives in opera houses, civic structures, university campuses commemorative monuments, luxury hotels and townhouses. The style was heavily influenced by the Paris Opéra House (1860–1875), designed by Charles Garnier, the masterpiece of the 19th century renovation of Paris, dominating its entire neighbourhood and continuing to astonish visitors with its majestic staircase and reception halls. The Opéra was an aesthetic and societal turning point in French architecture. Here, Garnier showed what he called a style actuel, which was influenced by the spirit of the time, aka Zeitgeist, and reflected the designer's personal taste.
 Beaux-Arts façades were usually imbricated, or layered with overlapping classical elements or sculpture. Often façades consisted of a high rusticated basement level, after it a few floors high level, usually decorated with pilasters or columns, and at the top an attic level and/or the roof. Beaux-Arts architects were often commissioned to design monumental civic buildings symbolic of the self-confidence of the town or city. The style aimed for a Baroque opulence through lavishly decorated monumental structures that evoked Louis XIV's Versailles. However, it was not just a revival of the Baroque, being more of a synthesis of Classicist styles, like Renaissance, Baroque, Rococo, Neoclassicism etc.[216][217][218]
 Because of the Industrial Revolution and the new technologies it brought, new types of buildings have appeared. By 1850 iron was quite present in dailylife at every scale, from mass-produced decorative architectural details and objects of apartment buildings and commercial buildings to train sheds. A well-known 19th century glass and iron building is the Crystal Palace from Hyde Park (London), built in 1851 to house the Great Exhibition, having an appearance similar with a greenhouse. Its scale was daunting.
 The marketplace pioneered novel uses of iron and glass to create an architecture of display and consumption that made the temporary display of the world fairs a permanent feature of modern urban life. Just after a year after the Crystal Palace was dismantaled, Aristide Boucicaut opened what historians of mass consumption have labelled the first department store, Le Bon Marché in Paris. As the store expanded, its exterior took on the form of a public monument, being highly decorated with French Renaissance Revival motifs. The entrances advanced subtly onto the pavemenet, hoping to captivate the attention of potential customers. Between 1872 and 1874, the interior was remodelled by Louis-Charles Boileau, in collaboration with the young engineering firm of Gustave Eiffel. In place of the open courtyard required to permit more daylight into the interior, the new building focused around three skylight atria.[224]
 Popular in many countries from the early 1890s until the outbreak of World War I in 1914, Art Nouveau was an influential although relatively brief art and design movement and philosophy. Despite being a short-lived fashion, it paved the way for the modern architecture of the 20th century. Between c. 1870 and 1900, a crisis of historicism occurred, during which the historicist culture was critiqued, one of the voices being Friedrich Nietzsche in 1874, who diagnosed 'a malignant historical fervour' as one of the crippling symptoms of a modern culture burdened by archaeological study and faith in the laws of historical progression.
 Focusing on natural forms, asymmetry, sinuous lines and whiplash curves, architects and designers aimed to escape the excessively ornamental styles and historical replications, popular during the 19th century. However, the style was not completely new, since Art Nouveau artists drew on a huge range of influences, particularly Beaux-Arts architecture, the Arts and Crafts movement, aestheticism and Japanese art. Buildings used materials associated in the 19th century with modernity, such as cast-iron and glass. A good example of this is the Paris Metro entrance at Porte Dauphine by Hector Guimard (1900). Its cast-iron and glass canopy is as much sculpture as it is architecture. In Paris, Art Nouveau was even called Le Style Métro by some. The interest for stylized organic forms of ornamentation originated in the mid 19th century, when it was promoted in The Grammar of Ornament (1854), a pattern book by British architect Owen Jones (architect) (1809–1874).
 Whiplash curves and sinuous organic lines are its most familiar hallmarks, however the style can not be summarized only to them, since its forms are much more varied and complex. The movement displayed many national interpretations. Depending on where it manifested, it was inspired by Celtic art, Gothic Revival, Rococo Revival, and Baroque Revival. In Hungary, Romania and Poland, for example, Art Nouveau incorporated folkloric elements. This is true especially in Romania, because it facilitated the appearance of the Romanian Revival style, which draws inspiration from Brâncovenesc architecture and traditional peasant houses and objects. The style also had different names, depending on countries. In Britain it was known as Modern Style, in the Netherlands as Nieuwe Kunst, in Germany and Austria as Jugendstil, in Italy as Liberty style, in Romania as Arta 1900, and in Japan as Shiro-Uma. It would be wrong to credit any particular place as the only one where the movement appeared, since it seems to have arisen in multiple locations.[233][234][235][236]
 Rejecting ornament and embracing minimalism and modern materials, Modernist architecture appeared across the world in the early 20th century. Art Nouveau paved the way for it, promoting the idea of non-historicist styles. It developed initially in Europe, focusing on functionalism and the avoidance of decoration. Modernism reached its peak during the 1930s and 1940s with the Bauhaus and the International Style, both characterised by asymmetry, flat roofs, large ribbon windows, metal, glass, white rendering and open-plan interiors.[240]
 Art Deco, named retrospectively after an exhibition held in Paris in 1925, originated in France as a luxurious, highly decorated style. It then spread quickly throughout the world - most dramatically in the United States - becoming more streamlined and modernistic through the 1930s. The style was pervasive and popular, finding its way into the design of everything from jewellery to film sets, from the interiors of ordinary homes to cinemas, luxury streamliners and hotels. Its exuberance and fantasy captured the spirit of the 'roaring 20s' and provided an escape from the realities of the Great Depression during the 1930s.[246]
 Although it ended with the start of World War II, its appeal has endured. Despite that it is an example of modern architecture, elements of the style drew on ancient Egyptian, Greek, Roman, African, Aztec and Japanese influences, but also on Futurism, Cubism and the Bauhaus. Bold colours were often applied on low-reliefs. Predominant materials include chrome plating, brass, polished steel and aluminium, inlaid wood, stone and stained glass.
 The International Style emerged in Europe after World War I, influenced by recent movements, including De Stijl and Streamline Moderne, and had a close relationship to the Bauhaus. The antithesis of nearly every other architectural movement that preceded it, the International Style eliminated extraneous ornament and used modern industrial materials such as steel, glass, reinforced concrete and chrome plating. Rectilinear, flat-roofed, asymmetrical and white, it became a symbol of modernity across the world. It seemed to offer a crisp, clean, rational future after the horrors of war. Named by the architect Philip Johnson and historian Henry-Russell Hitchcock (1903–1987) in 1932, the movement was epitomized by Charles-Edouard Jeanneret, or Le Corbusier and was clearly expressed in his statement that 'a house is a machine for living in'.[251]
 Based on social equality, Brutalism was inspired by Le Corbusier's 1947-1952 Unité d'habitation in Marseilles. It seems the term was originally coined by Swedish architect Hans Asplund (1921–1994), but Le Corbusier's use of the description béton brut, meaning raw concrete, for his choice of material for the Unité d'habitation was particularly influential. The style flourished from the 1950s to the mid-1970s, mainly using concrete, which although new in itself, was unconventional when exposed on facades. Before Brutalism, concrete was usually hidden beneath other materials.[257]
 Not one definable style, Postmodernism is an eclectic mix of approaches that appeared in the late 20th century in reaction against Modernism, which was increasingly perceived as monotonous and conservative. As with many movements, a complete antithesis to Modernism developed. In 1966, the architect Robert Venturi (1925–2018) had published his book, Complexity and Contradiction in Architecture, which praised the originality and creativity of Mannerist and Baroque architecture of Rome, and encouraged more ambiguity and complexity in contemporary design. Complaining about the austerity and tedium of so many smooth steel and glass Modernist buildings, and in deliberate denunciation of the famous Modernist 'Less is more', Venturi stated 'Less is a bore'. His theories became a majore influence on the development of Postmodernism.[258]
 Deconstructivism in architecture is a development of postmodern architecture that began in the late 1980s. It is characterized by ideas of fragmentation, non-linear processes of design, an interest in manipulating ideas of a structure's surface or skin, and apparent non-Euclidean geometry,[270] (i.e., non-rectilinear shapes) which serve to distort and dislocate some of the elements of architecture, such as structure and envelope. The finished visual appearance of buildings that exhibit the many deconstructivist ""styles"" is characterised by a stimulating unpredictability and a controlled chaos.
 Important events in the history of the Deconstructivist movement include the 1982 Parc de la Villette architectural design competition (especially the entry from the French philosopher Jacques Derrida and the American architect Peter Eisenman[271] and Bernard Tschumi's winning entry), the Museum of Modern Art's 1988 Deconstructivist Architecture exhibition in New York, organized by Philip Johnson and Mark Wigley, and the 1989 opening of the Wexner Center for the Arts in Columbus, designed by Peter Eisenman. The New York exhibition featured works by Frank Gehry, Daniel Libeskind, Rem Koolhaas, Peter Eisenman, Zaha Hadid, Coop Himmelblau, and Bernard Tschumi. Since the exhibition, many of the architects who were associated with Deconstructivism have distanced themselves from the term. Nonetheless, the term has stuck and has now, in fact, come to embrace a general trend within contemporary architecture.
 Dependencies and other territories
 Dependencies and other territories
 Territories
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Earth and heaven', 'Lavishly sculpted with hundreds of figures', 'local architectural styles', 'Futurism, Cubism and the Bauhaus', 'precision and technological advancement'], 'answer_start': [], 'answer_end': []}"
"An architectural style is a classification of buildings (and nonbuilding structures) based on a set of characteristics and features, including overall appearance, arrangement of the components, method of construction, building materials used, form, size, structural design, and regional character.[1]
 Architectural styles are frequently associated with a historical epoch (Renaissance style), geographical location (Italian Villa style), or an earlier architectural style (Neo-Gothic style), [1] and are influenced by the corresponding broader artistic style and the ""general human condition"". Heinrich Wölfflin even declared an analogy between a building and a costume: an ""architectural style reflects the attitude and the movement of people in the period concerned.[2]
 The 21st century construction uses a multitude of styles that are sometimes lumped together as a ""contemporary architecture"" based on the common trait of extreme reliance on computer-aided architectural design (cf. Parametricism).[citation needed]
 Folk architecture (also ""vernacular architecture"") is not a style, but an application of local customs to small-scale construction without clear identity of the builder.[3][4]
 The concept of architectural style is studied in the architectural history as one of the approaches (""style and period"") that are used to organize the history of architecture (Leach lists five other approaches as ""biography, geography and culture, type, technique, theme and analogy"").[5] Style provides an additional relationship between otherwise disparate buildings, thus serving as a ""protection against chaos"".[6]
 The concept of style was foreign to architects until the 18th century. Prior to the era of Enlightenment, the architectural form was mostly considered timeless, either as a divine revelation or an absolute truth derived from the laws of nature, and a great architect was the one who understood this ""language"". The new interpretation of history declared each historical period to be a stage of growth for the humanity (cf. Johann Gottfried Herder's Volksgeist that much later developed into Zeitgeist). This approach allowed to classify architecture of each age as an equally valid approach, ""style"" (the use of the word in this sense became established by the mid-18th century).[7]
 Style has been subject of an extensive debate since at least the 19th century.[8] Many architects argue that the notion of ""style"" cannot adequately describe the contemporary architecture, is obsolete and ridden with historicism. In their opinion, by concentrating on the appearance of the building, style classification misses the hidden from view ideas that architects had put into the form.[9] Studying history of architecture without reliance on styles usually relies on a ""canon"" of important architects and buildings. The lesser objects in this approach do not deserve attention: ""A bicycle shed is a building; Lincoln Cathedral is a piece of architecture"" (Nikolaus Pevsner, 1943).[10] Nonetheless, the traditional and popular approach to the architectural history is through chronology of styles,[11] with changes reflecting the evolution of materials, economics, fashions, and beliefs.
 Works of architecture are unlikely to be preserved for their aesthetic value alone; with practical re-purposing, the original intent of the original architect, sometimes his very identity, can be forgotten, and the building style becomes ""an indispensable historical tool"".[12]
 Styles emerge from the history of a society. At any time several styles may be fashionable, and when a style changes it usually does so gradually, as architects learn and adapt to new ideas. The new style is sometimes only a rebellion against an existing style, such as postmodern architecture (meaning ""after modernism""), which in 21st century has found its own language and split into a number of styles which have acquired other names.[citation needed]
 Architectural styles often spread to other places, so that the style at its source continues to develop in new ways while other countries follow with their own twist. For instance, Renaissance ideas emerged in Italy around 1425 and spread to all of Europe over the next 200 years, with the French, German, English, and Spanish Renaissances showing recognisably the same style, but with unique characteristics. An architectural style may also spread through colonialism, either by foreign colonies learning from their home country, or by settlers moving to a new land. One example is the Spanish missions in California, brought by Spanish priests in the late 18th century and built in a unique style.[citation needed]
 After an architectural style has gone out of fashion, revivals and re-interpretations may occur. For instance, classicism has been revived many times and found new life as neoclassicism. Each time it is revived, it is different. The Spanish mission style was revived 100 years later as the Mission Revival, and that soon evolved into the Spanish Colonial Revival.[citation needed]
 Early writing on the subjects of architectural history, since the works of Vitruvius in the 1st century B.C., treated architecture as a patrimony that was passed on to the next generation of architects by their forefathers.[13] Giorgio Vasari in the 16th century shifted the narrative to biographies of the great artists in his ""Lives of the Most Excellent Painters, Sculptors, and Architects"".[14]
 Constructing schemes of the period styles of historic art and architecture was a major concern of 19th century scholars in the new and initially mostly German-speaking field of art history. Important writers on the broad theory of style including Carl Friedrich von Rumohr, Gottfried Semper, and Alois Riegl in his Stilfragen of 1893, with Heinrich Wölfflin and Paul Frankl continued the debate into the 20th century.[15] Paul Jacobsthal and Josef Strzygowski are among the art historians who followed Riegl in proposing grand schemes tracing the transmission of elements of styles across great ranges in time and space. This type of art history is also known as formalism, or the study of forms or shapes in art. Wölfflin declared the goal of formalism as German: Kunstgeschichtliche Grundbegriffe, ""art history without names"", where an architect's work has a place in history that is independent of its author. The subject of study no longer was the ideas that Borromini borrowed from Maderno who in turn learned from Michelangelo, instead the questions now were about the continuity and changes observed when the architecture transitioned from Renaissance to Baroque.[16]
 Semper, Wölfflin, and Frankl, and later Ackerman, had backgrounds in the history of architecture, and like many other terms for period styles, ""Romanesque"" and ""Gothic"" were initially coined to describe architectural styles, where major changes between styles can be clearer and more easy to define, not least because style in architecture is easier to replicate by following a set of rules than style in figurative art such as painting. Terms originated to describe architectural periods were often subsequently applied to other areas of the visual arts, and then more widely still to music, literature and the general culture.[17]  In architecture stylistic change often follows, and is made possible by, the discovery of new techniques or materials, from the Gothic rib vault to modern metal and reinforced concrete construction. A major area of debate in both art history and archaeology has been the extent to which stylistic change in other fields like painting or pottery is also a response to new technical possibilities, or has its own impetus to develop (the kunstwollen of Riegl), or changes in response to social and economic factors affecting patronage and the conditions of the artist, as current thinking tends to emphasize, using less rigid versions of Marxist art history.[18]
 Although style was well-established as a central component of art historical analysis, seeing it as the over-riding factor in art history had fallen out of fashion by World War II, as other ways of looking at art were developing,[19] and a reaction against the emphasis on style developing; for Svetlana Alpers, ""the normal invocation of style in art history is a depressing affair indeed"".[20] According to James Elkins ""In the later 20th century criticisms of style were aimed at further reducing the Hegelian elements of the concept while retaining it in a form that could be more easily controlled"".[21]
 Multiple aesthetic and social factors forced architects in the middle of the 19th century to design the new buildings using a selection of styles patterned after the historical ones (working ""in every style or none""), style definition became a practical matter. The choice of an appropriate style was subject of elaborate discussions; for example, the Cambridge Camden Society had argued that the churches in the new British colonies should be built in the Norman style, so that the local architects and builders can go through the paces repeating the architectural history of England.[22]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['The choice of an appropriate style', '""in every style or none', 'Renaissance style', 'continuity and changes', 'continuity and changes'], 'answer_start': [], 'answer_end': []}"
"Building material is material used for construction. Many naturally occurring substances, such as clay, rocks, sand, wood, and even twigs and leaves, have been used to construct buildings and other structures, like bridges. Apart from naturally occurring materials, many man-made products are in use, some more and some less synthetic. The manufacturing of building materials is an established industry in many countries and the use of these materials is typically segmented into specific specialty trades, such as carpentry, insulation, plumbing, and roofing work. They provide the make-up of habitats and structures including homes.[1]
 In history, there are trends in building materials from being natural to becoming more human-made and composite; biodegradable to imperishable; indigenous (local) to being transported globally; repairable to disposable; chosen for increased levels of fire-safety, and improved seismic resistance. These trends tend to increase the initial and long-term economic, ecological, energy, and social costs of building materials.
 The initial economic cost of building materials is the purchase price. This is often what governs decision making about what materials to use. Sometimes people take into consideration the energy savings or durability of the materials and see the value of paying a higher initial cost in return for a lower lifetime cost. For example, an asphalt shingle roof costs less than a metal roof to install, but the metal roof will last longer so the lifetime cost is less per year. Some materials may require more care than others, maintaining costs specific to some materials may also influence the final decision. Risks when considering lifetime cost of a material is if the building is damaged such as by fire or wind, or if the material is not as durable as advertised. The cost of materials should be taken into consideration to bear the risk to buy combustive materials to enlarge the lifetime. It is said that, ""if it must be done, it must be done well"".
 Pollution costs can be macro and micro. The macro, environmental pollution of extraction industries building materials rely on such as mining, petroleum, and logging produce environmental damage at their source and in transportation of the raw materials, manufacturing, transportation of the products, retailing, and installation. An example of the micro aspect of pollution is the off-gassing of the building materials in the building or indoor air pollution. Red List building materials are materials found to be harmful. Also the carbon footprint, the total set of greenhouse gas emissions produced in the life of the material. A life-cycle analysis also includes the reuse, recycling, or disposal of construction waste. Two concepts in building which account for the ecological economics of building materials are green building and sustainable development.
 The initial energy costs include the amount of energy consumed to produce, deliver and install the material. The long term energy cost is the economic, ecological, and social costs of continuing to produce and deliver energy to the building for its use, maintenance, and eventual removal. The initial embodied energy of a structure is the energy consumed to extract, manufacture, deliver, install, the materials. The lifetime embodied energy continues to grow with the use, maintenance, and reuse/recycling/disposal of the building materials themselves and how the materials and design help minimize the life-time energy consumption of the structure.
 Social costs are injury and health of the people producing and transporting the materials and potential health problems of the building occupants if there are problems with the building biology. Globalization has had significant impacts on people both in terms of jobs, skills, and self-sufficiency are lost when manufacturing facilities are closed and the cultural aspects of where new facilities are opened. Aspects of fair trade and labor rights are social costs of global building material manufacturing.
 Bio-based materials (especially plant-based materials) are used in a variety of building applications, including load-bearing, filling, insulating, and plastering materials.[2] These materials vary in structure depending on the formulation used.[2][3] Plant fibres can be combined with binders and then used in construction to provide thermal, hydric or structural functions. The behaviour of concrete based on plant fibre is mainly governed by the amount of the fibre constituting the material. Several studies have shown that increasing the amount of these plant particles increases porosity, moisture buffering capacity, and maximum absorbed water content on the one side, while decreasing density, thermal conductivity, and compressive strength on the other.
 Plant-based materials are largely derived from renewable resources and mainly use co-products from agriculture or the wood industry. When used as insulation materials, most bio-based materials exhibit (unlike most other insulation materials) hygroscopic behaviour, combining high water vapour permeability and moisture regulation.[4]
 Brush structures are built entirely from plant parts and were used in primitive cultures such as Native Americans and[5] pygmy peoples in Africa.[6] These are built mostly with branches, twigs and leaves, and bark, similar to a beaver's lodge. These were variously named wikiups, lean-tos, and so forth.
 An extension on the brush building idea is the wattle and daub process in which clay soils or dung, usually cow, are used to fill in and cover a woven brush structure. This gives the structure more thermal mass and strength. Wattle and daub is one of the oldest building techniques.[7] Many older timber frame buildings incorporate wattle and daub as non load bearing walls between the timber frames.
 Snow and occasionally ice,[8] were used by the Inuit peoples for igloos and snow is used to build a shelter called a quinzhee. Ice has also been used for ice hotels as a tourist attraction in northern climates.[9]
 Clay based buildings usually come in two distinct types. One being when the walls are made directly with the mud mixture, and the other being walls built by stacking air-dried building blocks called mud bricks.
 Other uses of clay in building is combined with straws to create light clay, wattle and daub, and mud plaster.
 Wet-laid, or damp, walls are made by using the mud or clay mixture directly without forming blocks and drying them first. The amount of and type of each material in the mixture used leads to different styles of buildings. The deciding factor is usually connected with the quality of the soil being used. Larger amounts of clay are usually employed in building with cob, while low-clay soil is usually associated with sod house or sod roof construction. The other main ingredients include more or less sand/gravel and straw/grasses. Rammed earth is both an old and newer take on creating walls, once made by compacting clay soils between planks by hand; nowadays forms and mechanical pneumatic compressors are used.[10]
 Soil, and especially clay, provides good thermal mass; it is very good at keeping temperatures at a constant level. Homes built with earth tend to be naturally cool in the summer heat and warm in cold weather. Clay holds heat or cold, releasing it over a period of time like stone. Earthen walls change temperature slowly, so artificially raising or lowering the temperature can use more resources than in say a wood built house, but the heat/coolness stays longer.[10]
 People building with mostly dirt and clay, such as cob, sod, and adobe, created homes that have been built for centuries in western and northern Europe, Asia, as well as the rest of the world, and continue to be built, though on a smaller scale. Some of these buildings have remained habitable for hundreds of years.[11][12]
 Mud-bricks, also known by their Spanish name adobe are ancient building materials with evidence dating back thousands of years BC. Compressed earth blocks are a more modern type of brick used for building more frequently in industrialized society since the building blocks can be manufactured off site in a centralized location at a brickworks and transported to multiple building locations. These blocks can also be monetized more easily and sold.
 Structural mud bricks are almost always made using clay, often clay soil and a binder are the only ingredients used, but other ingredients can include sand, lime, concrete, stone and other binders. The formed or compressed block is then air dried and can be laid dry or with a mortar or clay slip.
 Sand is used with cement, and sometimes lime, to make mortar for masonry work and plaster. Sand is also used as a part of the concrete mix. An important low-cost building material in countries with high sand content soils is the Sandcrete block, which is weaker but cheaper than fired clay bricks.[13]
 Rock structures have existed for as long as history can recall. It is the longest-lasting building material available, and is usually readily available. There are many types of rock, with differing attributes that make them better or worse for particular uses. Rock is a very dense material so it gives a lot of protection; its main drawback as a building material is its weight and the difficulty of working it. Its energy density is both an advantage and disadvantage. Stone is hard to warm without consuming considerable energy but, once warm, its thermal mass means that can retain heat for useful periods of time.[14]
 Dry-stone walls and huts have been built for as long as humans have put one stone on top of another. Eventually, different forms of mortar were used to hold the stones together, cement being the most commonplace now.
 The granite-strewn uplands of Dartmoor National Park, United Kingdom, for example, provided ample resources for early settlers. Circular huts were constructed from loose granite rocks throughout the Neolithic and early Bronze Age, and the remains of an estimated 5,000 can still be seen today. Granite continued to be used throughout the Medieval period (see Dartmoor longhouse) and into modern times. Slate is another stone type, commonly used as roofing material in the United Kingdom and other parts of the world where it is found.
 Stone buildings can be seen in most major cities, and some civilizations built predominantly with stone, such as the Egyptian and Aztec pyramids and the structures of the Inca civilization.
 Thatch is one of the oldest of building materials known. ""Thatch"" is another word for ""grass""; grass is a good insulator and easily harvested. Many African tribes have lived in homes made completely of grasses and sand year-round. In Europe, thatch roofs on homes were once prevalent but the material fell out of favor as industrialization and improved transport increased the availability of other materials. Today, though, the practice is undergoing a revival. In the Netherlands, for instance, many new buildings have thatched roofs with special ridge tiles on top.
 Wood has been used as a building material for thousands of years in its natural state. Today, engineered wood is becoming very common in industrialized countries.
 Wood is a product of trees, and sometimes other fibrous plants, used for construction purposes when cut or pressed into lumber and timber, such as boards, planks and similar materials. It is a  generic building material and is used in building just about any type of structure in most climates.  Wood can be very flexible under loads, keeping strength while bending, and is incredibly strong when compressed vertically. There are many differing qualities to the different types of wood, even among same tree species. This means specific species are better suited for various uses than others. And growing conditions are important for deciding quality.
 ""Timber"" is the term used for construction purposes except the term ""lumber"" is used in the United States. Raw wood (a log, trunk, bole) becomes timber when the wood has been ""converted"" (sawn, hewn, split) in the forms of minimally-processed logs stacked on top of each other, timber frame construction, and light-frame construction. The main problems with timber structures are fire risk and moisture-related problems.[citation needed]
 In modern times softwood is used as a lower-value bulk material, whereas hardwood is usually used for finishings and furniture. Historically timber frame structures were built with oak in western Europe, recently douglas fir has become the most popular wood for most types of structural building.
 Many families or communities, in rural areas, have a personal woodlot from which the family or community will grow and harvest trees to build with or sell. These lots are tended to like a garden. This was much more prevalent in pre-industrial times, when laws existed as to the amount of wood one could cut at any one time to ensure there would be a supply of timber for the future, but is still a viable form of  agriculture.
 Bricks are made in a similar way to mud-bricks except without the fibrous binder such as straw and are fired (""burned"" in a brick clamp or kiln) after they have air-dried to permanently harden them. Kiln fired clay bricks are a ceramic material. Fired bricks can be solid or have hollow cavities to aid in drying and make them lighter and easier to transport. The individual bricks are placed upon each other in courses using mortar. Successive courses being used to build up walls, arches, and other architectural elements. Fired brick walls are usually substantially thinner than cob/adobe while keeping the same vertical strength. They require more energy to create but are easier to transport and store, and are lighter than stone blocks. Romans extensively used fired brick of a shape and type now called Roman bricks.[15] Building with brick gained much popularity in the mid-18th century and 19th centuries. This was due to lower costs with increases in brick[16] manufacturing and fire-safety in the ever crowding cities.
 The cinder block supplemented or replaced fired bricks in the late 20th century often being used for the inner parts of masonry walls and by themselves.
 Structural clay tiles (clay blocks) are clay or terracotta and typically are perforated with holes.
 Cement bonded composites are made of hydrated cement paste that binds wood, particles, or fibers to make pre-cast building components. Various fiberous materials, including paper, fiberglass, and carbon-fiber have been used as binders.
 Wood and natural fibers are composed of various soluble organic compounds like carbohydrates, glycosides and phenolics. These compounds are known to retard cement setting. Therefore, before using a wood in making cement bonded composites, its compatibility with cement is assessed.
 Wood-cement compatibility is the ratio of a parameter related to the property of a wood-cement composite to that of a neat cement paste. The compatibility is often expressed as a percentage value. To determine wood-cement compatibility, methods based on different properties are used, such as, hydration characteristics, strength, interfacial bond and morphology. Various methods are used by researchers such as the measurement of hydration characteristics of a cement-aggregate mix;[17][18][19] the comparison of the mechanical properties of cement-aggregate mixes[20][21] and the visual assessment of microstructural properties of the wood-cement mixes.[22] It has been found that the hydration test by measuring the change in hydration temperature with time is the most convenient method. Recently, Karade et al.[23] have reviewed these methods of compatibility assessment and suggested a method based on the ‘maturity concept’ i.e. taking in consideration both time and temperature of cement hydration reaction. Recent work on aging of lignocellulosic materials in the cement paste showed hydrolysis of hemicelluloses and lignin[24] that affects the interface between particles or fibers and concrete and causes degradation.[25]
 Bricks were laid in lime mortar from the time of the Romans until supplanted by Portland cement mortar in the early 20th century. Cement blocks also sometimes are filled with grout or covered with a parge coat.
 Concrete is a composite building material made from the combination of aggregate and a binder such as cement.  The most common form of concrete is Portland cement concrete, which consists of mineral aggregate (generally gravel and sand), portland cement and water.
 After mixing, the cement hydrates and eventually hardens into a stone-like material. When used in the generic sense, this is the material referred to by the term ""concrete"".
 For a concrete construction of any size, as concrete has a rather low tensile strength, it is generally strengthened using steel rods or bars (known as rebars). This strengthened concrete is then referred to as reinforced concrete. In order to minimise any air bubbles, that would weaken the structure, a vibrator is used to eliminate any air that has been entrained when the liquid concrete mix is poured around the ironwork. Concrete has been the predominant building material in the modern age due to its longevity, formability, and ease of transport. Recent advancements, such as insulating concrete forms, combine the concrete forming and other construction steps (installation of insulation). All materials must be taken in required proportions as described in standards.
 The tent is the home of choice among nomadic groups all over the world. Two well-known types include the conical teepee and the circular yurt. The tent has been revived as a major construction technique with the development of tensile architecture and synthetic fabrics. Modern buildings can be made of flexible material such as fabric membranes, and supported by a system of steel cables, rigid or internal, or by air pressure.
 Recently, synthetic polystyrene or polyurethane foam has been used in combination with structural materials, such as concrete. It is lightweight, easily shaped, and an excellent insulator. Foam is usually used as part of a structural insulated panel, wherein the foam is sandwiched between wood or cement or insulating concrete forms.
 Glassmaking is considered an art form as well as an industrial process or material.
 Clear windows have been used since the invention of glass to cover small openings in a building. Glass panes provided humans with the ability to both let light into rooms while at the same time keeping inclement weather outside.
 Glass is generally made from mixtures of sand and silicates, in a very hot fire stove called a kiln, and is very brittle. Additives are often included the mixture used to produce glass with shades of colors or various characteristics (such as bulletproof glass or lightbulbs).
 The use of glass in architectural buildings has become very popular in the modern culture. Glass ""curtain walls"" can be used to cover the entire facade of a building, or it can be used to span over a wide roof structure in a ""space frame"". These uses though require some sort of frame to hold sections of glass together, as glass by itself is too brittle and would require an overly large kiln to be used to span such large areas by itself.
 Glass bricks were invented in the early 20th century.
 Gypsum concrete is a mixture of gypsum plaster and fibreglass rovings. Although plaster and fibres fibrous plaster have been used for many years, especially for ceilings, it was not until the early 1990s that serious studies of the strength and qualities of a walling system Rapidwall, using a mixture of gypsum plaster and 300mm plus fibreglass rovings, were investigated. With an abundance of gypsum (naturally occurring and by-product chemical FGD and phospho gypsums) available worldwide, Gypsum concrete-based building products, which are fully recyclable, offer significant environmental benefits.
 Metal is used as structural framework for larger buildings such as skyscrapers, or as an external surface covering. There are many types of metals used for building. Metal figures quite prominently in prefabricated structures such as the Quonset hut, and can be seen used in most cosmopolitan cities. It requires a great deal of human labor to produce metal, especially in the large amounts needed for the building industries. Corrosion is metal's prime enemy when it comes to longevity.
 The term plastics covers a range of synthetic or semi-synthetic organic condensation or polymerization products that can be molded or extruded into objects, films, or fibers. Their name is derived from the fact that in their semi-liquid state they are malleable, or have the property of plasticity. Plastics vary immensely in heat tolerance, hardness, and resiliency. Combined with this adaptability, the general uniformity of composition and lightness of plastics ensures their use in almost all industrial applications today. High performance plastics such as ETFE have become an ideal building material due to its high abrasion resistance and chemical inertness. Notable buildings that feature it include: the Beijing National Aquatics Center and the Eden Project biomes.[26]
 Building papers and membranes are used for many reasons in construction. One of the oldest building papers is red rosin paper which was known to be in use before 1850 and was used as an underlayment in exterior walls, roofs, and floors and for protecting a jobsite during construction. Tar paper was invented late in the 19th century and was used for similar purposes as rosin paper and for gravel roofs. Tar paper has largely fallen out of use supplanted by asphalt felt paper. Felt paper has been supplanted in some uses by synthetic underlayments, particularly in roofing by synthetic underlayments and siding by housewraps.
 There are a wide variety of damp proofing and waterproofing membranes used for roofing, basement waterproofing, and geomembranes.
 Fired clay bricks have been used since the time of the Romans. Special tiles are used for roofing, siding, flooring, ceilings,  pipes, flue liners, and more.
 A relatively new category of building materials, living building materials are materials that are either composed of, or created by a living organism; or materials that behave in a manner that's reminiscent of such. Potential use cases include self-healing materials, and materials that replicate (reproduce) rather than be manufactured.
 In the market place, the term ""building products"" often refers to ready-made particles or sections made from various materials, that are fitted in architectural hardware and decorative hardware parts of a building. The list of building products excludes the building materials used to construct the building architecture and supporting fixtures, like windows, doors, cabinets, millwork components, etc. Building products, rather, support and make building materials work in a modular fashion.
 ""Building products"" may also refer to items used to put such hardware together, such as caulking, glues, paint, and anything else bought for the purpose of constructing a building.
 To facilitate and optimize the use of new materials and up-to-date technologies, ongoing research is being undertaken to improve efficiency, productivity and competitiveness in world markets.[citation needed]
 Material research and development may be commercial, academical or both, and can be conducted at any scale. An example of a building material prototyping facility is the open source Forty Walls House in Australia, where up to 40 new sustainable materials are being rapidly prototyped and tested simultaneously in a building that is permanently occupied and monitored.[27]
 Rapid prototyping allows researchers to develop and test materials quickly, making adjustments and solving issues during the process. Rather than developing materials theoretically and then testing them, only to discover fundamental flaws, rapid prototypes allow for comparatively quick development and testing, shortening the time to market for a new materials to a matter of months, rather than years.[28]
 In 2017, buildings and construction together consumed 36% of the final energy produced globally while being responsible for 39% of the global energy related CO2 emissions.[29] The shares from the construction industry alone were 6% and 11% respectively. Energy consumption during building material production is a dominant contributor to the construction industry's overall share, predominantly due to the use of electricity during production. Embodied energy of relevant building materials in the US are provided in the table below.[30]
  Architecture portal
  Media related to Building materials at Wikimedia Commons
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['fire risk and moisture-related problems', 'The shares from the construction industry alone were 6% and 11%', 'trends in building materials from being natural to becoming more human-made and composite', 'fire risk and moisture-related problems', 'energy savings or durability of the materials'], 'answer_start': [], 'answer_end': []}"
"
 Construction is a general term meaning the art and science of forming objects, systems, or organizations.[1] It comes from the Latin word constructio (from com- ""together"" and struere ""to pile up"") and Old French construction.[2] To 'construct' is a verb: the act of building, and the noun is construction: how something is built or the nature of its structure.
 In its most widely used context, construction covers the processes involved in delivering buildings, infrastructure, industrial facilities, and associated activities through to the end of their life. It typically starts with planning, financing, and design that continues until the asset is built and ready for use. Construction also covers repairs and maintenance work, any works to expand, extend and improve the asset, and its eventual demolition, dismantling or decommissioning.
 The construction industry contributes significantly to many countries' gross domestic products (GDP). Global expenditure on construction activities was about $4 trillion in 2012. In 2022, expenditure on the construction industry exceeded $11 trillion a year, equivalent to about 13 percent of global GDP. This spending was forecasted to rise to around $14.8 trillion in 2030.[3]
 The construction industry promotes economic development and brings many non-monetary benefits to many countries, but it is one of the most hazardous industries. For example, about 20% (1,061) of US industry fatalities in 2019 happened in construction.[4]
 The first huts and shelters were constructed by hand or with simple tools. As cities grew during the Bronze Age, a class of professional craftsmen, like bricklayers and carpenters, appeared. Occasionally, slaves were used for construction work. In the Middle Ages, the artisan craftsmen were organized into guilds. In the 19th century, steam-powered machinery appeared, and later, diesel- and electric-powered vehicles such as cranes, excavators and bulldozers.
 Fast-track construction has been increasingly popular in the 21st century. Some estimates suggest that 40% of construction projects are now fast-track construction.[5]
 Broadly, there are three sectors of construction: buildings, infrastructure and industrial:[6]
 The industry can also be classified into sectors or markets.[7] For example, Engineering News-Record (ENR), a US-based construction trade magazine, has compiled and reported data about the size of design and construction contractors. In 2014, it split the data into nine market segments: transportation, petroleum, buildings, power, industrial, water, manufacturing, sewage/waste, telecom, hazardous waste, and a tenth category for other projects.[8] ENR used data on transportation, sewage, hazardous waste and water to rank firms as heavy contractors.[9]
 The Standard Industrial Classification and the newer North American Industry Classification System classify companies that perform or engage in construction into three subsectors: building construction, heavy and civil engineering construction, and specialty trade contractors. There are also categories for professional services firms (e.g., engineering, architecture, surveying, project management).[10][11]
 Building construction is the process of adding structures to areas of land, also known as real property sites. Typically, a project is instigated by or with the owner of the property (who may be an individual or an organisation); occasionally, land may be compulsorily purchased from the owner for public use.[12]
 Residential construction may be undertaken by individual land-owners (self-built), by specialist housebuilders, by property developers, by general contractors, or by providers of public or social housing (e.g.: local authorities, housing associations). Where local zoning or planning policies allow, mixed-use developments may comprise both residential and non-residential construction (e.g.: retail, leisure, offices, public buildings, etc.).
 Residential construction practices, technologies, and resources must conform to local building authority's regulations and codes of practice. Materials readily available in the area generally dictate the construction materials used (e.g.: brick versus stone versus timber). Costs of construction on a per square meter (or per square foot) basis for houses can vary dramatically based on site conditions, access routes, local regulations, economies of scale (custom-designed homes are often more expensive to build) and the availability of skilled tradespeople.[citation needed]
 Depending upon the type of building, non-residential building construction can be procured by a wide range of private and public organisations, including local authorities, educational and religious bodies, transport undertakings, retailers, hoteliers, property developers, financial institutions and other private companies. Most construction in these sectors is undertaken by general contractors.
 Civil engineering covers the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, tunnels, airports, water and sewerage systems, pipelines, and railways.[13][14] Some general contractors have expertise in civil engineering; civil engineering contractors are firms dedicated to work in this sector, and may specialise in particular types of infrastructure.
 Industrial construction includes offshore construction (mainly of energy installations: oil and gas platforms, wind power), mining and quarrying, refineries, breweries, distilleries and other processing plants, power stations, steel mills, warehouses and factories.
 Some construction projects are small renovations or repair jobs, like repainting or fixing leaks, where the owner may act as designer, paymaster and laborer for the entire project. However, more complex or ambitious projects usually require additional multi-disciplinary expertise and manpower, so the owner may commission one or more specialist businesses to undertake detailed planning, design, construction and handover of the work. Often the owner will appoint one business to oversee the project (this may be a designer, a contractor, a construction manager, or other advisors); such specialists are normally appointed for their expertise in project delivery and construction management and will help the owner define the project brief, agree on a budget and schedule, liaise with relevant public authorities, and procure materials and the services of other specialists (the supply chain, comprising subcontractors and materials suppliers). Contracts are agreed for the delivery of services by all businesses, alongside other detailed plans aimed at ensuring legal, timely, on-budget and safe delivery of the specified works.
 Design, finance, and legal aspects overlap and interrelate. The design must be not only structurally sound and appropriate for the use and location, but must also be financially possible to build, and legal to use. The financial structure must be adequate to build the design provided and must pay amounts that are legally owed. Legal structures integrate design with other activities and enforce financial and other construction processes.
 These processes also affect procurement strategies. Clients may, for example, appoint a business to design the project, after which a competitive process is undertaken to appoint a lead contractor to construct the asset (design–bid–build); they may appoint a business to lead both design and construction (design-build); or they may directly appoint a designer, contractor and specialist subcontractors (construction management).[15] Some forms of procurement emphasize collaborative relationships (partnering, alliancing) between the client, the contractor, and other stakeholders within a construction project, seeking to ameliorate often highly competitive and adversarial industry practices.
 Construction or refurbishment work in a ""live"" environment (where residents or businesses remain living in or operating on the site) requires particular care, planning and communication.[16]
 When applicable, a proposed construction project must comply with local land-use planning policies including zoning and building code requirements. A project will normally be assessed (by the 'authority having jurisdiction, AHJ, typically the municipality where the project will be located) for its potential impacts on neighbouring properties, and upon existing infrastructure (transportation, social infrastructure, and utilities including water supply, sewerage, electricity, telecommunications, etc.). Data may be gathered through site analysis, site surveys and geotechnical investigations. Construction normally cannot start until planning permission has been granted, and may require preparatory work to ensure relevant infrastructure has been upgraded before building work can commence. Preparatory works will also include surveys of existing utility lines to avoid damage-causing outages and other hazardous situations.
 Some legal requirements come from malum in se considerations, or the desire to prevent indisputably bad phenomena, e.g. explosions or bridge collapses. Other legal requirements come from malum prohibitum considerations, or factors that are a matter of custom or expectation, such as isolating businesses from a business district or residences from a residential district. An attorney may seek changes or exemptions in the law that governs the land where the building will be built, either by arguing that a rule is inapplicable (the bridge design will not cause a collapse), or that the custom is no longer needed (acceptance of live-work spaces has grown in the community).[17]
 During the construction of a building, a municipal building inspector usually inspects the ongoing work periodically to ensure that construction adheres to the approved plans and the local building code. Once construction is complete, any later changes made to a building or other asset that affect safety, including its use, expansion, structural integrity, and fire protection, usually require municipality approval.
 Depending on the type of project, mortgage bankers, accountants, and cost engineers may participate in creating an overall plan for the financial management of a construction project. The presence of the mortgage banker is highly likely, even in relatively small projects since the owner's equity in the property is the most obvious source of funding for a building project. Accountants act to study the expected monetary flow over the life of the project and to monitor the payouts throughout the process. Professionals including cost engineers, estimators and quantity surveyors apply expertise to relate the work and materials involved to a proper valuation.
 Financial planning ensures adequate safeguards and contingency plans are in place before the project is started, and ensures that the plan is properly executed over the life of the project. Construction projects can suffer from preventable financial problems.[18] Underbids happen when builders ask for too little money to complete the project. Cash flow problems exist when the present amount of funding cannot cover the current costs for labour and materials; such problems may arise even when the overall budget is adequate, presenting a temporary issue. Cost overruns with government projects have occurred when the contractor identified change orders or project changes that increased costs, which are not subject to competition from other firms as they have already been eliminated from consideration after the initial bid.[19] Fraud is also an issue of growing significance within construction.[20]
 Large projects can involve highly complex financial plans and often start with a conceptual cost estimate performed by a building estimator. As portions of a project are completed, they may be sold, supplanting one lender or owner for another, while the logistical requirements of having the right trades and materials available for each stage of the building construction project carry forward. Public–private partnerships (PPPs) or private finance initiatives (PFIs) may also be used to help deliver major projects. According to McKinsey in 2019, the ""vast majority of large construction projects go over budget and take 20% longer than expected"".[21]
 A construction project is a complex net of construction contracts and other legal obligations, each of which all parties must carefully consider. A contract is the exchange of a set of obligations between two or more parties, and provides structures to manage issues. For example, construction delays can be costly, so construction contracts set out clear expectations and clear paths to manage delays. Poorly drafted contracts can lead to confusion and costly disputes.
 At the start of a project, legal advisors seek to identify ambiguities and other potential sources of trouble in the contract structures, and to present options for preventing problems. During projects, they work to avoid and resolve conflicts that arise. In each case, the lawyer facilitates an exchange of obligations that matches the reality of the project.
 Design-bid-build is the most common and well-established method of construction procurement. In this arrangement, the architect, engineer or builder acts for the client as the project coordinator. They design the works, prepare specifications and design deliverables (models, drawings, etc.), administer the contract, tender the works, and manage the works from inception to completion. In parallel, there are direct contractual links between the client and the main contractor, who, in turn, has direct contractual relationships with subcontractors. The arrangement continues until the project is ready for handover.
 Design-build became more common from the late 20th century, and involves the client contracting a single entity to provide design and construction. In some cases, the design-build package can also include finding the site, arranging funding and applying for all necessary statutory consents. Typically, the client invites several Design & Build (D&B) contractors to submit proposals to meet the project brief and then selects a preferred supplier. Often this will be a consortium involving a design firm and a contractor (sometimes more than one of each). In the United States, departments of transportation usually use design-build contracts as a way of progressing projects where states lack the skills or resources, particularly for very large projects.[22]
 In a construction management arrangement, the client enters into separate contracts with the designer (architect or engineer), a construction manager, and individual trade contractors. The client takes on the contractual role, while the construction or project manager provides the active role of managing the separate trade contracts, and ensuring that they complete all work smoothly and effectively together. This approach is often used to speed up procurement processes, to allow the client greater flexibility in design variation throughout the contract, to enable the appointment of individual work contractors, to separate contractual responsibility on each individual throughout the contract, and to provide greater client control.
 In the industrialized world, construction usually involves the translation of designs into reality. Most commonly (i.e.: in a design-bid-build project), the design team is employed by (i.e. in contract with) the property owner. Depending upon the type of project, a design team may include architects, civil engineers, mechanical engineers, electrical engineers, structural engineers, fire protection engineers, planning consultants, architectural consultants, and archaeological consultants. A 'lead designer' will normally be identified to help coordinate different disciplinary inputs to the overall design. This may be aided by integration of previously separate disciplines (often undertaken by separate firms) into multi-disciplinary firms with experts from all related fields,[23] or by firms establishing relationships to support design-build processes.
 The increasing complexity of construction projects creates the need for design professionals trained in all phases of a project's life-cycle and develop an appreciation of the asset as an advanced technological system requiring close integration of many sub-systems and their individual components, including sustainability. For buildings, building engineering is an emerging discipline that attempts to meet this new challenge.
 Traditionally, design has involved the production of sketches, architectural and engineering drawings, and specifications. Until the late 20th century, drawings were largely hand-drafted; adoption of computer-aided design (CAD) technologies then improved design productivity, while the 21st-century introduction of building information modeling (BIM) processes has involved the use of computer-generated models that can be used in their own right or to generate drawings and other visualisations as well as capturing non-geometric data about building components and systems.
 On some projects, work on-site will not start until design work is largely complete; on others, some design work may be undertaken concurrently with the early stages of on-site activity (for example, work on a building's foundations may commence while designers are still working on the detailed designs of the building's internal spaces). Some projects may include elements that are designed for off-site construction (see also prefabrication and modular building) and are then delivered to the site ready for erection, installation or assembly.
 Once contractors and other relevant professionals have been appointed and designs are sufficiently advanced, work may commence on the project site. Typically, a construction site will include a secure perimeter to restrict unauthorised access, site access control points, office and welfare accommodation for personnel from the main contractor and other firms involved in the project team, and storage areas for materials, machinery and equipment. According to the McGraw-Hill Dictionary of Architecture and Construction's definition, construction may be said to have started when the first feature of the permanent structure has been put in place, such as pile driving, or the pouring of slabs or footings.[24]
 Commissioning is the process of verifying that all subsystems of a new building (or other assets) work as intended to achieve the owner's project requirements and as designed by the project's architects and engineers.
 A period after handover (or practical completion) during which the owner may identify any shortcomings in relation to the building specification ('defects'), with a view to the contractor correcting the defect.[25]
 Maintenance involves functional checks, servicing, repairing or replacing of necessary devices, equipment, machinery, building infrastructure, and supporting utilities in industrial, business, governmental, and residential installations.[26][27]
 Demolition is the discipline of safely and efficiently tearing down buildings and other artificial structures. Demolition contrasts with deconstruction, which involves taking a building apart while carefully preserving valuable elements for reuse purposes (recycling – see also circular economy).
 The output of the global construction industry was worth an estimated $10.8 trillion in 2017, and in 2018 was forecast to rise to $12.9 trillion by 2022,[28] and to around $14.8 trillion in 2030.[3] As a sector, construction accounts for more than 10% of global GDP (in developed countries, construction comprises 6–9% of GDP),[29] and employs around 7% of the total employed workforce around the globe[30] (accounting for over 273 million full- and part-time jobs in 2014).[31] Since 2010,[32] China has been the world's largest single construction market.[33] The United States is the second largest construction market with a 2018 output of $1.581 trillion.[34]
 Construction is a major source of employment in most countries; high reliance on small businesses, and under-representation of women are common traits. For example:
 According to McKinsey research, productivity growth per worker in construction has lagged behind many other industries across different countries including in the United States and in European countries. In the United States, construction productivity per worker has declined by half since the 1960s.[59]
 The twenty-five largest countries in the world by construction GVA (2018)[60]
 Some workers may be engaged in manual labour[61] as unskilled or semi-skilled workers; they may be skilled tradespeople; or they may be supervisory or managerial personnel. Under safety legislation in the United Kingdom, for example, construction workers are defined as people ""who work for or under the control of a contractor on a construction site"";[62] in Canada, this can include people whose work includes ensuring conformance with building codes and regulations, and those who supervise other workers.[63]
 Laborers comprise a large grouping in most national construction industries. In the United States, for example, in May 2021 the construction sector employed just over 7.5 million people, of whom just over 820,000 were laborers, while 573,000 were carpenters, 508,000 were electricians, 258,000 were equipment operators and 230,000 were construction managers.[64] Like most business sectors, there is also substantial white-collar employment in construction – 681,000 US workers were recorded by the United States Department of Labor as in 'office and administrative support occupations' in May 2021.[65]
 Large-scale construction requires collaboration across multiple disciplines. A project manager normally manages the budget on the job, and a construction manager, design engineer, construction engineer or architect supervises it. Those involved with the design and execution must consider zoning requirements and legal issues, environmental impact of the project, scheduling, budgeting and bidding, construction site safety, availability and transportation of building materials, logistics, and inconvenience to the public, including those caused by construction delays.
 Some models and policy-making organisations promote the engagement of local labour in construction projects as a means of tackling social exclusion and addressing skill shortages. In the UK, the Joseph Rowntree Foundation reported in 2000 on 25 projects which had aimed to offer training and employment opportunities for locally based school leavers and unemployed people.[66] The Foundation published ""a good practice resource book"" in this regard at the same time.[67] Use of local labour and local materials were specified for the construction of the Danish Storebaelt bridge, but there were legal issues which were challenged in court and addressed by the European Court of Justice in 1993. The court held that a contract condition requiring use of local labour and local materials was incompatible with EU treaty principles.[68] Later UK guidance noted that social and employment clauses, where used, must be compatible with relevant EU regulation.[69] Employment of local labour was identified as one of several social issues which could potentially be incorporated in a sustainable procurement approach, although the interdepartmental Sustainable Procurement Group recognised that ""there is far less scope to incorporate [such] social issues in public procurement than is the case with environmental issues"".[70]
 There are many routes to the different careers within the construction industry. There are three main tiers of construction workers based on educational background and training, which vary by country:
 Unskilled and semi-skilled workers provide general site labor, often have few or no construction qualifications, and may receive basic site training.
 Skilled tradespeople have typically served apprenticeships (sometimes in labor unions) or received technical training; this group also includes on-site managers who possess extensive knowledge and experience in their craft or profession. Skilled manual occupations include carpenters, electricians, plumbers, ironworkers, heavy equipment operators and masons, as well as those involved in project management. In the UK these require further education qualifications, often in vocational subject areas, undertaken either directly after completing compulsory education or through ""on the job"" apprenticeships.[71]
 Professional, technical and managerial personnel often have higher education qualifications, usually graduate degrees, and are trained to design and manage construction processes. These roles require more training as they demand greater technical knowledge, and involve more legal responsibility. Example roles (and qualification routes) include:
 Construction is one of the most dangerous occupations in the world, incurring more occupational fatalities than any other sector in both the United States and in the European Union.[4][72] In the US in 2019, 1,061, or about 20%, of worker fatalities in private industry occurred in construction.[4] In 2017, more than a third of US construction fatalities (366 out of 971 total fatalities) were the result of falls;[73] in the UK, half of the average 36 fatalities per annum over a five-year period to 2021 were attributed to falls from height.[74] Proper safety equipment such as harnesses, hard hats and guardrails and procedures such as securing ladders and inspecting scaffolding can curtail the risk of occupational injuries in the construction industry.[75] Other major causes of fatalities in the construction industry include electrocution, transportation accidents, and trench cave-ins.[76]
 Other safety risks for workers in construction include hearing loss due to high noise exposure, musculoskeletal injury, chemical exposure, and high levels of stress.[77] Besides that, the high turnover of workers in construction industry imposes a huge challenge of accomplishing the restructuring of work practices in individual workplaces or with individual workers.[citation needed] Construction has been identified by the National Institute for Occupational Safety and Health (NIOSH) as a priority industry sector in the National Occupational Research Agenda (NORA) to identify and provide intervention strategies regarding occupational health and safety issues.[78][79] A study conducted in 2022 found “significant effect of air pollution exposure on construction-related injuries and fatalities”, especially with the exposure of nitrogen dioxide.[80]
 Sustainability is an aspect of ""green building"", defined by the United States Environmental Protection Agency (EPA) as ""the practice of creating structures and using processes that are environmentally responsible and resource-efficient throughout a building's life-cycle from siting to design, construction, operation, maintenance, renovation and deconstruction.""[81]
 The construction industry may require transformation at pace and at scale if it is to successfully contribute to achieving the target set out in The Paris Agreement of limiting global temperature rise to 1.5C above industrial levels.[82][83] The World Green Building Council has stated the buildings and infrastructure around the world can reach 40% less embodied carbon emissions but that this can only be achieved through urgent transformation.[84][85]
 Conclusions from industry leaders have suggested that the net zero transformation is likely to be challenging for the construction industry, but it does present an opportunity. Action is demanded from governments, standards bodies, the construction sector, and the engineering profession to meet the decarbonising targets.[86]
 In 2021, the National Engineering Policy Centre published its report Decarbonising Construction: Building a new net zero industry,[86] which outlined key areas to decarbonise the construction sector and the wider built environment. This report set out around 20 different recommendations to transform and decarbonise the construction sector, including recommendations for engineers, the construction industry and decision makers, plus outlined six-overarching ‘system levers’ where action taken now will result in rapid decarbonisation of the construction sector.[86] These levels are:
 Progress is being made internationally to decarbonise the sector including improvements to sustainable procurement practice such as the CO2 performance ladder in the Netherlands and the Danish Partnership for Green Public Procurement.[87][88] There are also now demonstrations of applying the principles of circular economy practices in practice such as Circl, ABN AMRO's sustainable pavilion and the Brighton Waste House.[89][90][91]
  Architecture portal
 Engineering portal
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Construction', 'a construction manager, design engineer, construction engineer or architect', 'Fraud', 'Danish Partnership for Green Public Procurement', 'applying the principles of circular economy practices'], 'answer_start': [], 'answer_end': []}"
"
 Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewage systems, pipelines, structural components of buildings, and railways.[1][2]
 Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering,[3] and it is defined to distinguish non-military engineering from military engineering.[4] Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.[5]
 Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.[6]
 Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.[7]
 One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes' principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.[8]
 
Engineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in ancient Egypt, the Indus Valley civilization, and Mesopotamia (ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing. Until modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably.[9] The construction of pyramids in Egypt (c. 2700–2500 BC) were some of the first instances of large structure constructions. Other ancient historic civil engineering constructions include the Qanat water management system in modern-day Iran (the oldest is older than 3000 years and longer than 71 kilometres (44 mi),[10]) the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC)[11] and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.
 In the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering.[4] In 1747, the first institution for the teaching of civil engineering, the École Nationale des Ponts et Chaussées was established in France; and more examples followed in other European countries, like Spain.[12] The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse.[3][11] In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.
 
In 1818 the Institution of Civil Engineers was founded in London,[13] and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as: the art of directing the great sources of power in nature for the use and convenience of man, as the means of production and of traffic in states, both for external and internal trade, as applied in the construction of roads, bridges, aqueducts, canals, river navigation and docks for internal intercourse and exchange, and in the construction of ports, harbours, moles, breakwaters and lighthouses, and in the art of navigation by artificial power for the purposes of commerce, and in the construction and application of machinery, and in the drainage of cities and towns.[14] The first private college to teach civil engineering in the United States was Norwich University, founded in 1819 by Captain Alden Partridge.[15] The first degree in civil engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835.[16][17] The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905.[18]
 In the UK during the early 19th century, the division between civil engineering and military engineering (served by the Royal Military Academy, Woolwich), coupled with the demands of the Industrial Revolution, spawned new engineering education initiatives: the Class of Civil Engineering and Mining was founded at King's College London in 1838, mainly as a response to the growth of the railway system and the need for more qualified engineers, the private College for Civil Engineers in Putney was established in 1839, and the UK's first Chair of Engineering was established at the University of Glasgow in 1840.
 Civil engineers typically possess an academic degree in civil engineering. The length of study is three to five years, and the completed degree is designated as a bachelor of technology, or a bachelor of engineering. The curriculum generally includes classes in physics, mathematics, project management, design and specific topics in civil engineering. After taking basic courses in most sub-disciplines of civil engineering, they move on to specialize in one or more sub-disciplines at advanced levels. While an undergraduate degree (BEng/BSc) normally provides successful students with industry-accredited qualifications, some academic institutions offer post-graduate degrees (MEng/MSc), which allow students to further specialize in their particular area of interest.[19] In most countries, a bachelor's degree in engineering represents the first step towards professional certification, and a professional body certifies the degree program. After completing a certified degree program, the engineer must satisfy a range of requirements including work experience and exam requirements before being certified. Once certified, the engineer is designated as a professional engineer (in the United States, Canada and South Africa), a chartered engineer (in most Commonwealth countries), a chartered professional engineer (in Australia and New Zealand), or a European engineer (in most countries of the European Union). There are international agreements between relevant professional bodies to allow engineers to practice across national borders.
 The benefits of certification vary depending upon location. For example, in the United States and Canada, ""only a licensed professional engineer may prepare, sign and seal, and submit engineering plans and drawings to a public authority for approval, or seal engineering work for public and private clients.""[20] This requirement is enforced under provincial law such as the Engineers Act in Quebec.[21] No such legislation has been enacted in other countries including the United Kingdom. In Australia, state licensing of engineers is limited to the state of Queensland. Almost all certifying bodies maintain a code of ethics which all members must abide by.[22]
 Engineers must obey contract law in their contractual relationships with other parties. In cases where an engineer's work fails, they may be subject to the law of tort of negligence, and in extreme cases, criminal charges.[23] An engineer's work must also comply with numerous other rules and regulations such as building codes and environmental law.
 There are a number of sub-disciplines within the broad field of civil engineering. General civil engineers work closely with surveyors and specialized civil engineers to design grading, drainage, pavement, water supply, sewer service, dams, electric and communications supply. General civil engineering is also referred to as site engineering, a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another. Site engineers spend time visiting project sites, meeting with stakeholders, and preparing construction plans. Civil engineers apply the principles of geotechnical engineering, structural engineering, environmental engineering, transportation engineering and construction engineering to residential, commercial, industrial and public works projects of all sizes and levels of construction.
 Coastal engineering is concerned with managing coastal areas. In some jurisdictions, the terms sea defense and coastal protection mean defense against flooding and erosion, respectively. Coastal defense is the more traditional term, but coastal management has become popular as well.
 Construction engineering involves planning and execution, transportation of materials, site development based on hydraulic, environmental, structural and geotechnical engineering. As construction firms tend to have higher business risk than other types of civil engineering firms do, construction engineers often engage in more business-like transactions, for example, drafting and reviewing contracts, evaluating logistical operations, and monitoring prices of supplies.
 Earthquake engineering involves designing structures to withstand hazardous earthquake exposures. Earthquake engineering is a sub-discipline of structural engineering. The main objectives of earthquake engineering are[24] to understand interaction of structures on the shaky ground; foresee the consequences of possible earthquakes; and design, construct and maintain structures to perform at earthquake in compliance with building codes.
 Environmental engineering is the contemporary term for sanitary engineering, though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering. Public health engineering and environmental health engineering are other terms being used.
 Environmental engineering deals with treatment of chemical, biological, or thermal wastes, purification of water and air, and remediation of contaminated sites after waste disposal or accidental contamination. Among the topics covered by environmental engineering are pollutant transport, water purification, waste water treatment, air pollution, solid waste treatment, recycling, and hazardous waste management. Environmental engineers administer pollution reduction, green engineering, and industrial ecology. Environmental engineers also compile information on environmental consequences of proposed actions.
 Forensic engineering is the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property. The consequences of failure are dealt with by the law of product liability. The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery. The subject is applied most commonly in civil law cases, although it may be of use in criminal law cases. Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component, or to assist a court in determining the facts of an accident. It can also involve investigation of intellectual property claims, especially patents.
 Geotechnical engineering studies rock and soil supporting civil engineering systems. Knowledge from the field of soil science, materials science, mechanics, and hydraulics is applied to safely and economically design foundations, retaining walls, and other structures. Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geo-environmental engineering.[25][26]
 Identification of soil properties presents challenges to geotechnical engineers. Boundary conditions are often well defined in other branches of civil engineering, but unlike steel or concrete, the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation. Furthermore, soil exhibits nonlinear (stress-dependent) strength, stiffness, and dilatancy (volume change associated with application of shear stress), making studying soil mechanics all the more difficult.[25] Geotechnical engineers frequently work with professional geologists,  Geological Engineering professionals and soil scientists.[27]
 Materials science is closely related to civil engineering. It studies fundamental characteristics of materials, and deals with ceramics such as concrete and mix asphalt concrete, strong metals such as aluminum and steel, and thermosetting polymers including polymethylmethacrylate (PMMA) and carbon fibers.
 Materials engineering involves protection and prevention (paints and finishes). Alloying combines two types of metals to produce another metal with desired properties. It incorporates elements of applied physics and chemistry. With recent media attention on nanoscience and nanotechnology, materials engineering has been at the forefront of academic research. It is also an important part of forensic engineering and failure analysis.
 Site development, also known as site planning, is focused on the planning and development potential of a site as well as addressing possible impacts from permitting issues and environmental challenges.[28]
 Structural engineering is concerned with the structural design and structural analysis of buildings, bridges, towers, flyovers (overpasses), tunnels, off shore structures like oil and gas fields in the sea, aerostructure and other structures. This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads, and then designing the structure to successfully support and resist those loads. The loads can be self weight of the structures, other dead load, live loads, moving (wheel) load, wind load, earthquake load, load from temperature change etc. The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for (to be serviceable). Due to the nature of some loading conditions, sub-disciplines within structural engineering have emerged, including wind engineering and earthquake engineering.[29]
 Design considerations will include strength, stiffness, and stability of the structure when subjected to loads which may be static, such as furniture or self-weight, or dynamic, such as wind, seismic, crowd or vehicle loads, or transitory, such as temporary construction loads or impact. Other considerations include cost, constructibility, safety, aesthetics and sustainability.
 Surveying is the process by which a surveyor measures certain dimensions that occur on or near the surface of the Earth. Surveying equipment such as levels and theodolites are used for accurate measurement of angular deviation, horizontal, vertical and slope distances. With computerisation, electronic distance measurement (EDM), total stations, GPS surveying and laser scanning have to a large extent supplanted traditional instruments. Data collected by survey measurement is converted into a graphical representation of the Earth's surface in the form of a map. This information is then used by civil engineers, contractors and realtors to design from, build on, and trade, respectively. Elements of a structure must be sized and positioned in relation to each other and to site boundaries and adjacent structures.
 Although surveying is a distinct profession with separate qualifications and licensing arrangements, civil engineers are trained in the basics of surveying and mapping, as well as geographic information systems. Surveyors also lay out the routes of railways, tramway tracks, highways, roads, pipelines and streets as well as position other infrastructure, such as harbors, before construction.
 
In the United States, Canada, the United Kingdom and most Commonwealth countries land surveying is considered to be a separate and distinct profession. Land surveyors are not considered to be engineers, and have their own professional associations and licensing requirements. The services of a licensed land surveyor are generally required for boundary surveys (to establish the boundaries of a parcel using its legal description) and subdivision plans (a plot or map based on a survey of a parcel of land, with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads), both of which are generally referred to as Cadastral surveying. Construction surveying is generally performed by specialized technicians. Unlike land surveyors, the resulting plan does not have legal status. Construction surveyors perform the following tasks:
 Transportation engineering is concerned with moving people and goods efficiently, safely, and in a manner conducive to a vibrant community. This involves specifying, designing, constructing, and maintaining transportation infrastructure which includes streets, canals, highways, rail systems, airports, ports, and mass transit. It includes areas such as transportation design, transportation planning, traffic engineering, some aspects of urban engineering, queueing theory, pavement engineering, Intelligent Transportation System (ITS), and infrastructure management.
 Municipal engineering is concerned with municipal infrastructure. This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure. In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services. It can also include the optimization of waste collection and bus service networks. Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously, and managed by the same municipal authority.  Municipal engineers may also design the site civil works for large buildings, industrial plants or campuses (i.e. access roads, parking lots, potable water supply, treatment or pretreatment of waste water, site drainage, etc.)
 Water resources engineering is concerned with the collection and management of water (as a natural resource). As a discipline it therefore combines elements of hydrology, environmental science, meteorology, conservation, and resource management. This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground (aquifers) and above ground (lakes, rivers, and streams) resources. Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into, through, or out of a facility. Although the actual design of the facility may be left to other engineers.
 Hydraulic engineering is concerned with the flow and conveyance of fluids, principally water. This area of civil engineering is intimately related to the design of pipelines, water supply network, drainage facilities (including bridges, dams, channels, culverts, levees, storm sewers), and canals. Hydraulic engineers design these facilities using the concepts of fluid pressure, fluid statics, fluid dynamics, and hydraulics, among others. Civil engineering systems is a discipline that promotes the use of systems thinking to manage complexity and change in civil engineering within its wider public context. It posits that the proper development of civil engineering infrastructure requires a holistic, coherent understanding of the relationships between all of the important factors that contribute to successful projects while at the same time emphasizing the importance of attention to technical detail. Its purpose is to help integrate the entire civil engineering project life cycle from conception, through planning, designing, making, operating to decommissioning.[30][31]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['municipal infrastructure', 'stonemasons and carpenters', 'advances in the understanding of physics and mathematics throughout history', 'accidents in operation of vehicles or machinery', 'cost, constructibility, safety, aesthetics and sustainability'], 'answer_start': [], 'answer_end': []}"
"
 Mechanical engineering is the study of physical machines that may involve force and movement. It is an engineering branch that combines engineering physics and mathematics principles with materials science, to design, analyze, manufacture, and maintain mechanical systems.[1] It is one of the oldest and broadest of the engineering branches.
 Mechanical engineering requires an understanding of core areas including mechanics, dynamics, thermodynamics, materials science, design, structural analysis, and electricity. In addition to these core principles, mechanical engineers use tools such as computer-aided design (CAD), computer-aided manufacturing (CAM), computer-aided engineering (CAE), and product lifecycle management to design and analyze manufacturing plants, industrial equipment and machinery, heating and cooling systems, transport systems, motor vehicles, aircraft, watercraft, robotics, medical devices, weapons, and others.[2][3]
 Mechanical engineering emerged as a field during the Industrial Revolution in Europe in the 18th century; however, its development can be traced back several thousand years around the world. In the 19th century, developments in physics led to the development of mechanical engineering science. The field has continually evolved to incorporate advancements; today mechanical engineers are pursuing developments in such areas as composites, mechatronics, and nanotechnology. It also overlaps with aerospace engineering, metallurgical engineering, civil engineering, structural engineering, electrical engineering, manufacturing engineering, chemical engineering, industrial engineering, and other engineering disciplines to varying amounts. Mechanical engineers may also work in the field of biomedical engineering, specifically with biomechanics, transport phenomena, biomechatronics, bionanotechnology, and modelling of biological systems.
 The application of mechanical engineering can be seen in the archives of various ancient and medieval societies. The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times.[4] The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC.[5] The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale,[6] and to move large objects in ancient Egyptian technology.[7] The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia circa 3000 BC.[6] The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC.[8]
 The Sakia was developed in the Kingdom of Kush during the 4th century BC. It relied on animal power reducing the tow on the requirement of human energy.[9] Reservoirs in the form of Hafirs were developed in Kush to store water and boost irrigation.[10] Bloomeries and blast furnaces were developed during the seventh century BC in Meroe.[11][12][13][14] Kushite sundials applied mathematics in the form of advanced trigonometry.[15][16]
 The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.[17] In ancient Greece, the works of Archimedes (287–212 BC) influenced mechanics in the Western tradition. The geared Antikythera mechanisms was an Analog computer invented around the 2nd century BC.[18]
 In Roman Egypt, Heron of Alexandria (c. 10–70 AD) created the first steam-powered device (Aeolipile).[19] In China, Zhang Heng (78–139 AD) improved a water clock and invented a seismometer, and Ma Jun (200–265 AD) invented a chariot with differential gears. The medieval Chinese horologist and engineer Su Song (1020–1101 AD) incorporated an escapement mechanism into his astronomical clock tower two centuries before escapement devices were found in medieval European clocks. He also invented the world's first known endless power-transmitting chain drive.[20]
 The cotton gin was invented in India by the 6th century AD,[21] and the spinning wheel was invented in the Islamic world by the early 11th century,[22] Dual-roller gins appeared in India and China between the 12th and 14th centuries.[23] The worm gear roller gin appeared in the Indian subcontinent during the early Delhi Sultanate era of the 13th to 14th centuries.[24]
 During the Islamic Golden Age (7th to 15th century), Muslim inventors made remarkable contributions in the field of mechanical technology. Al-Jazari, who was one of them, wrote his famous Book of Knowledge of Ingenious Mechanical Devices in 1206 and presented many mechanical designs.
 In the 17th century, important breakthroughs in the foundations of mechanical engineering occurred in England and the Continent. The Dutch mathematician and physicist Christiaan Huygens invented the pendulum clock in 1657, which was the first reliable timekeeper for almost 300 years, and published a work dedicated to clock designs and the theory behind them.[25][26] In England, Isaac Newton formulated Newton's Laws of Motion and developed the calculus, which would become the mathematical basis of physics. Newton was reluctant to publish his works for years, but he was finally persuaded to do so by his colleagues, such as Edmond Halley. Gottfried Wilhelm Leibniz, who earlier designed a mechanical calculator, is also credited with developing the calculus during the same time period.[27]
 During the early 19th century Industrial Revolution, machine tools were developed in England, Germany, and Scotland. This allowed mechanical engineering to develop as a separate field within engineering. They brought with them manufacturing machines and the engines to power them.[28] The first British professional society of mechanical engineers was formed in 1847 Institution of Mechanical Engineers, thirty years after the civil engineers formed the first such professional society Institution of Civil Engineers.[29] On the European continent, Johann von Zimmermann (1820–1901) founded the first factory for grinding machines in Chemnitz, Germany in 1848.
 In the United States, the American Society of Mechanical Engineers (ASME) was formed in 1880, becoming the third such professional engineering society, after the American Society of Civil Engineers (1852) and the American Institute of Mining Engineers (1871).[30] The first schools in the United States to offer an engineering education were the United States Military Academy in 1817, an institution now known as Norwich University in 1819, and Rensselaer Polytechnic Institute in 1825. Education in mechanical engineering has historically been based on a strong foundation in mathematics and science.[31]
 Degrees in mechanical engineering are offered at various universities worldwide. Mechanical engineering programs typically take four to five years of study depending on the place and university and result in a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Science Engineering (B.Sc.Eng.), Bachelor of Technology (B.Tech.), Bachelor of Mechanical Engineering (B.M.E.), or Bachelor of Applied Science (B.A.Sc.) degree, in or with emphasis in mechanical engineering. In Spain, Portugal and most of South America, where neither B.S. nor B.Tech. programs have been adopted, the formal name for the degree is ""Mechanical Engineer"", and the course work is based on five or six years of training. In Italy the course work is based on five years of education, and training, but in order to qualify as an Engineer one has to pass a state exam at the end of the course. In Greece, the coursework is based on a five-year curriculum.[32]
 In the United States, most undergraduate mechanical engineering programs are accredited by the Accreditation Board for Engineering and Technology (ABET) to ensure similar course requirements and standards among universities. The ABET web site lists 302 accredited mechanical engineering programs as of 11 March 2014.[33] Mechanical engineering programs in Canada are accredited by the Canadian Engineering Accreditation Board (CEAB),[34] and most other countries offering engineering degrees have similar accreditation societies.
 In Australia, mechanical engineering degrees are awarded as Bachelor of Engineering (Mechanical) or similar nomenclature, although there are an increasing number of specialisations. The degree takes four years of full-time study to achieve. To ensure quality in engineering degrees, Engineers Australia accredits engineering degrees awarded by Australian universities in accordance with the global Washington Accord. Before the degree can be awarded, the student must complete at least 3 months of on the job work experience in an engineering firm.[35] Similar systems are also present in South Africa and are overseen by the Engineering Council of South Africa (ECSA).
 In India, to become an engineer, one needs to have an engineering degree like a B.Tech. or B.E., have a diploma in engineering, or by completing a course in an engineering trade like fitter from the Industrial Training Institute (ITIs) to receive a ""ITI Trade Certificate"" and also pass the All India Trade Test (AITT) with an engineering trade conducted by the National Council of Vocational Training (NCVT) by which one is awarded a ""National Trade Certificate"". A similar system is used in Nepal.[36]
 Some mechanical engineers go on to pursue a postgraduate degree such as a Master of Engineering, Master of Technology, Master of Science, Master of Engineering Management (M.Eng.Mgt. or M.E.M.), a Doctor of Philosophy in engineering (Eng.D. or Ph.D.) or an engineer's degree. The master's and engineer's degrees may or may not include research. The Doctor of Philosophy includes a significant research component and is often viewed as the entry point to academia.[37] The Engineer's degree exists at a few institutions at an intermediate level between the master's degree and the doctorate.
 Standards set by each country's accreditation society are intended to provide uniformity in fundamental subject material, promote competence among graduating engineers, and to maintain confidence in the engineering profession as a whole. Engineering programs in the U.S., for example, are required by ABET to show that their students can ""work professionally in both thermal and mechanical systems areas.""[38] The specific courses required to graduate, however, may differ from program to program. Universities and institutes of technology will often combine multiple subjects into a single class or split a subject into multiple classes, depending on the faculty available and the university's major area(s) of research.
 The fundamental subjects required for mechanical engineering usually include:
 Mechanical engineers are also expected to understand and be able to apply basic concepts from chemistry, physics, tribology, chemical engineering, civil engineering, and electrical engineering. All mechanical engineering programs include multiple semesters of mathematical classes including calculus, and advanced mathematical concepts including differential equations, partial differential equations, linear algebra, differential geometry, and statistics, among others.
 In addition to the core mechanical engineering curriculum, many mechanical engineering programs offer more specialized programs and classes, such as control systems, robotics, transport and logistics, cryogenics, fuel technology, automotive engineering, biomechanics, vibration, optics and others, if a separate department does not exist for these subjects.[41]
 Most mechanical engineering programs also require varying amounts of research or community projects to gain practical problem-solving experience. In the United States it is common for mechanical engineering students to complete one or more internships while studying, though this is not typically mandated by the university. Cooperative education is another option. Future work skills[42] research puts demand on study components that feed student's creativity and innovation.[43]
 Mechanical engineers research, design, develop, build, and test mechanical and thermal devices, including tools, engines, and machines.
 Mechanical engineers typically do the following:
 Mechanical engineers design and oversee the manufacturing of many products ranging from medical devices to new batteries. They also design power-producing machines such as electric generators, internal combustion engines, and steam and gas turbines as well as power-using machines, such as refrigeration and air-conditioning systems.
 Like other engineers, mechanical engineers use computers to help create and analyze designs, run simulations and test how a machine is likely to work.
 Engineers may seek license by a state, provincial, or national government. The purpose of this process is to ensure that engineers possess the necessary technical knowledge, real-world experience, and knowledge of the local legal system to practice engineering at a professional level. Once certified, the engineer is given the title of Professional Engineer (United States, Canada, Japan, South Korea, Bangladesh and South Africa), Chartered Engineer (in the United Kingdom, Ireland, India and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (much of the European Union).
 In the U.S., to become a licensed Professional Engineer (PE), an engineer must pass the comprehensive FE (Fundamentals of Engineering) exam, work a minimum of 4 years as an Engineering Intern (EI) or Engineer-in-Training (EIT), and pass the ""Principles and Practice"" or PE (Practicing Engineer or Professional Engineer) exams. The requirements and steps of this process are set forth by the National Council of Examiners for Engineering and Surveying (NCEES), composed of engineering and land surveying licensing boards representing all U.S. states and territories.
 In the UK, current graduates require a BEng plus an appropriate master's degree or an integrated MEng degree, a minimum of 4 years post graduate on the job competency development and a peer-reviewed project report to become a Chartered Mechanical Engineer (CEng, MIMechE) through the Institution of Mechanical Engineers. CEng MIMechE can also be obtained via an examination route administered by the City and Guilds of London Institute.[44]
 In most developed countries, certain engineering tasks, such as the design of bridges, electric power plants, and chemical plants, must be approved by a professional engineer or a chartered engineer. ""Only a licensed engineer, for instance, may prepare, sign, seal and submit engineering plans and drawings to a public authority for approval, or to seal engineering work for public and private clients.""[45] This requirement can be written into state and provincial legislation, such as in the Canadian provinces, for example the Ontario or Quebec's Engineer Act.[46]
 In other countries, such as Australia, and the UK, no such legislation exists; however, practically all certifying bodies maintain a code of ethics independent of legislation, that they expect all members to abide by or risk expulsion.[47]
 The total number of engineers employed in the U.S. in 2015 was roughly 1.6 million. Of these, 278,340 were mechanical engineers (17.28%), the largest discipline by size.[48] In 2012, the median annual income of mechanical engineers in the U.S. workforce was $80,580. The median income was highest when working for the government ($92,030), and lowest in education ($57,090).[49] In 2014, the total number of mechanical engineering jobs was projected to grow 5% over the next decade.[50] As of 2009, the average starting salary was $58,800 with a bachelor's degree.[51]
 The field of mechanical engineering can be thought of as a collection of many mechanical engineering science disciplines. Several of these subdisciplines which are typically taught at the undergraduate level are listed below, with a brief explanation and the most common application of each. Some of these subdisciplines are unique to mechanical engineering, while others are a combination of mechanical engineering and one or more other disciplines. Most work that a mechanical engineer does uses skills and techniques from several of these subdisciplines, as well as specialized subdisciplines. Specialized subdisciplines, as used in this article, are more likely to be the subject of graduate studies or on-the-job training than undergraduate research. Several specialized subdisciplines are discussed in this section.
 Mechanics is, in the most general sense, the study of forces and their effect upon matter. Typically, engineering mechanics is used to analyze and predict the acceleration and deformation (both elastic and plastic) of objects under known forces (also called loads) or stresses. Subdisciplines of mechanics include
 Mechanical engineers typically use mechanics in the design or analysis phases of engineering. If the engineering project were the design of a vehicle, statics might be employed to design the frame of the vehicle, in order to evaluate where the stresses will be most intense. Dynamics might be used when designing the car's engine, to evaluate the forces in the pistons and cams as the engine cycles. Mechanics of materials might be used to choose appropriate materials for the frame and engine. Fluid mechanics might be used to design a ventilation system for the vehicle (see HVAC), or to design the intake system for the engine.
 Mechatronics is a combination of mechanics and electronics. It is an interdisciplinary branch of mechanical engineering, electrical engineering and software engineering that is concerned with integrating electrical and mechanical engineering to create hybrid automation systems. In this way, machines can be automated through the use of electric motors, servo-mechanisms, and other electrical systems in conjunction with special software. A common example of a mechatronics system is a CD-ROM drive. Mechanical systems open and close the drive, spin the CD and move the laser, while an optical system reads the data on the CD and converts it to bits. Integrated software controls the process and communicates the contents of the CD to the computer.
 Robotics is the application of mechatronics to create robots, which are often used in industry to perform tasks that are dangerous, unpleasant, or repetitive. These robots may be of any shape and size, but all are preprogrammed and interact physically with the world. To create a robot, an engineer typically employs kinematics (to determine the robot's range of motion) and mechanics (to determine the stresses within the robot).
 Robots are used extensively in industrial automation engineering. They allow businesses to save money on labor, perform tasks that are either too dangerous or too precise for humans to perform them economically, and to ensure better quality. Many companies employ assembly lines of robots, especially in Automotive Industries and some factories are so robotized that they can run by themselves. Outside the factory, robots have been employed in bomb disposal, space exploration, and many other fields. Robots are also sold for various residential applications, from recreation to domestic applications.[53]
 Structural analysis is the branch of mechanical engineering (and also civil engineering) devoted to examining why and how objects fail and to fix the objects and their performance. Structural failures occur in two general modes: static failure, and fatigue failure. Static structural failure occurs when, upon being loaded (having a force applied) the object being analyzed either breaks or is deformed plastically, depending on the criterion for failure. Fatigue failure occurs when an object fails after a number of repeated loading and unloading cycles. Fatigue failure occurs because of imperfections in the object: a microscopic crack on the surface of the object, for instance, will grow slightly with each cycle (propagation) until the crack is large enough to cause ultimate failure.[54]
 Failure is not simply defined as when a part breaks, however; it is defined as when a part does not operate as intended. Some systems, such as the perforated top sections of some plastic bags, are designed to break. If these systems do not break, failure analysis might be employed to determine the cause.
 Structural analysis is often used by mechanical engineers after a failure has occurred, or when designing to prevent failure. Engineers often use online documents and books such as those published by ASM[55] to aid them in determining the type of failure and possible causes.
 Once theory is applied to a mechanical design, physical testing is often performed to verify calculated results. Structural analysis may be used in an office when designing parts, in the field to analyze failed parts, or in laboratories where parts might undergo controlled failure tests.
 Thermodynamics is an applied science used in several branches of engineering, including mechanical and chemical engineering. At its simplest, thermodynamics is the study of energy, its use and transformation through a system.[56] Typically, engineering thermodynamics is concerned with changing energy from one form to another. As an example, automotive engines convert chemical energy (enthalpy) from the fuel into heat, and then into mechanical work that eventually turns the wheels.
 Thermodynamics principles are used by mechanical engineers in the fields of heat transfer, thermofluids, and energy conversion. Mechanical engineers use thermo-science to design engines and power plants, heating, ventilation, and air-conditioning (HVAC) systems, heat exchangers, heat sinks, radiators, refrigeration, insulation, and others.[57]
 Drafting or technical drawing is the means by which mechanical engineers design products and create instructions for manufacturing parts. A technical drawing can be a computer model or hand-drawn schematic showing all the dimensions necessary to manufacture a part, as well as assembly notes, a list of required materials, and other pertinent information.[58] A U.S. mechanical engineer or skilled worker who creates technical drawings may be referred to as a drafter or draftsman. Drafting has historically been a two-dimensional process, but computer-aided design (CAD) programs now allow the designer to create in three dimensions.
 Instructions for manufacturing a part must be fed to the necessary machinery, either manually, through programmed instructions, or through the use of a computer-aided manufacturing (CAM) or combined CAD/CAM program. Optionally, an engineer may also manually manufacture a part using the technical drawings. However, with the advent of computer numerically controlled (CNC) manufacturing, parts can now be fabricated without the need for constant technician input. Manually manufactured parts generally consist of spray coatings, surface finishes, and other processes that cannot economically or practically be done by a machine.
 Drafting is used in nearly every subdiscipline of mechanical engineering, and by many other branches of engineering and architecture. Three-dimensional models created using CAD software are also commonly used in finite element analysis (FEA) and computational fluid dynamics (CFD).
 Many mechanical engineering companies, especially those in industrialized nations, have incorporated computer-aided engineering (CAE) programs into their existing design and analysis processes, including 2D and 3D solid modeling computer-aided design (CAD). This method has many benefits, including easier and more exhaustive visualization of products, the ability to create virtual assemblies of parts, and the ease of use in designing mating interfaces and tolerances.
 Other CAE programs commonly used by mechanical engineers include product lifecycle management (PLM) tools and analysis tools used to perform complex simulations. Analysis tools may be used to predict product response to expected loads, including fatigue life and manufacturability. These tools include finite element analysis (FEA), computational fluid dynamics (CFD), and computer-aided manufacturing (CAM).
 Using CAE programs, a mechanical design team can quickly and cheaply iterate the design process to develop a product that better meets cost, performance, and other constraints. No physical prototype need be created until the design nears completion, allowing hundreds or thousands of designs to be evaluated, instead of a relative few. In addition, CAE analysis programs can model complicated physical phenomena which cannot be solved by hand, such as viscoelasticity, complex contact between mating parts, or non-Newtonian flows.
 As mechanical engineering begins to merge with other disciplines, as seen in mechatronics, multidisciplinary design optimization (MDO) is being used with other CAE programs to automate and improve the iterative design process. MDO tools wrap around existing CAE processes, allowing product evaluation to continue even after the analyst goes home for the day. They also use sophisticated optimization algorithms to more intelligently explore possible designs, often finding better, innovative solutions to difficult multidisciplinary design problems.
 Mechanical engineers are constantly pushing the boundaries of what is physically possible in order to produce safer, cheaper, and more efficient machines and mechanical systems. Some technologies at the cutting edge of mechanical engineering are listed below (see also exploratory engineering).
 Micron-scale mechanical components such as springs, gears, fluidic and heat transfer devices are fabricated from a variety of substrate materials such as silicon, glass and polymers like SU8. Examples of MEMS components are the accelerometers that are used as car airbag sensors, modern cell phones, gyroscopes for precise positioning and microfluidic devices used in biomedical applications.
 Friction stir welding, a new type of welding, was discovered in 1991 by The Welding Institute (TWI). The innovative steady state (non-fusion) welding technique joins materials previously un-weldable, including several aluminum alloys. It plays an important role in the future construction of airplanes, potentially replacing rivets. Current uses of this technology to date include welding the seams of the aluminum main Space Shuttle external tank, Orion Crew Vehicle, Boeing Delta II and Delta IV Expendable Launch Vehicles and the SpaceX Falcon 1 rocket, armor plating for amphibious assault ships, and welding the wings and fuselage panels of the new Eclipse 500 aircraft from Eclipse Aviation among an increasingly growing pool of uses.[59][60][61]
 Composites or composite materials are a combination of materials which provide different physical characteristics than either material separately. Composite material research within mechanical engineering typically focuses on designing (and, subsequently, finding applications for) stronger or more rigid materials while attempting to reduce weight, susceptibility to corrosion, and other undesirable factors. Carbon fiber reinforced composites, for instance, have been used in such diverse applications as spacecraft and fishing rods.
 Mechatronics is the synergistic combination of mechanical engineering, electronic engineering, and software engineering. The discipline of mechatronics began as a way to combine mechanical principles with electrical engineering. Mechatronic concepts are used in the majority of electro-mechanical systems.[62] Typical electro-mechanical sensors used in mechatronics are strain gauges, thermocouples, and pressure transducers.
 At the smallest scales, mechanical engineering becomes nanotechnology—one speculative goal of which is to create a molecular assembler to build molecules and materials via mechanosynthesis. For now that goal remains within exploratory engineering. Areas of current mechanical engineering research in nanotechnology include nanofilters,[63] nanofilms,[64] and nanostructures,[65] among others.
 Finite Element Analysis is a computational tool used to estimate stress, strain, and deflection of solid bodies. It uses a mesh setup with user-defined sizes to measure physical quantities at a node. The more nodes there are, the higher the precision.[66] This field is not new, as the basis of Finite Element Analysis (FEA) or Finite Element Method (FEM) dates back to 1941. But the evolution of computers has made FEA/FEM a viable option for analysis of structural problems. Many commercial software applications such as NASTRAN, ANSYS, and ABAQUS are widely used in industry for research and the design of components. Some 3D modeling and CAD software packages have added FEA modules. In the recent times, cloud simulation platforms like SimScale are becoming more common.
 Other techniques such as finite difference method (FDM) and finite-volume method (FVM) are employed to solve problems relating heat and mass transfer, fluid flows, fluid surface interaction, etc.
 Biomechanics is the application of mechanical principles to biological systems, such as humans, animals, plants, organs, and cells.[67] Biomechanics also aids in creating prosthetic limbs and artificial organs for humans. Biomechanics is closely related to engineering, because it often uses traditional engineering sciences to analyze biological systems. Some simple applications of Newtonian mechanics and/or materials sciences can supply correct approximations to the mechanics of many biological systems.
 In the past decade, reverse engineering of materials found in nature such as bone matter has gained funding in academia. The structure of bone matter is optimized for its purpose of bearing a large amount of compressive stress per unit weight.[68] The goal is to replace crude steel with bio-material for structural design.
 Over the past decade the Finite element method (FEM) has also entered the Biomedical sector highlighting further engineering aspects of Biomechanics. FEM has since then established itself as an alternative to in vivo surgical assessment and gained the wide acceptance of academia. The main advantage of Computational Biomechanics lies in its ability to determine the endo-anatomical response of an anatomy, without being subject to ethical restrictions.[69] This has led FE modelling to the point of becoming ubiquitous in several fields of Biomechanics while several projects have even adopted an open source philosophy (e.g. BioSpine).
 Computational fluid dynamics, usually abbreviated as CFD, is a branch of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions.[70] With high-speed supercomputers, better solutions can be achieved. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as turbulent flows. Initial validation of such software is performed using a wind tunnel with the final validation coming in full-scale testing, e.g. flight tests.
 Acoustical engineering is one of many other sub-disciplines of mechanical engineering and is the application of acoustics. Acoustical engineering is the study of Sound and Vibration. These engineers work effectively to reduce noise pollution in mechanical devices and in buildings by soundproofing or removing sources of unwanted noise. The study of acoustics can range from designing a more efficient hearing aid, microphone, headphone, or recording studio to enhancing the sound quality of an orchestra hall. Acoustical engineering also deals with the vibration of different mechanical systems.[71]
 Manufacturing engineering, aerospace engineering and automotive engineering are grouped with mechanical engineering at times. A bachelor's degree in these areas will typically have a difference of a few specialized classes.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['mechanical engineering', 'Mechanical engineers', 'two-dimensional process', 'Sound and Vibration', 'application of mechanical principles to biological systems'], 'answer_start': [], 'answer_end': []}"
"
 Electrical engineering is an engineering discipline concerned with the study, design, and application of equipment, devices, and systems which use electricity, electronics, and electromagnetism. It emerged as an identifiable occupation in the latter half of the 19th century after the commercialization of the electric telegraph, the telephone, and electrical power generation, distribution, and use.
 Electrical engineering is divided into a wide range of different fields, including computer engineering, systems engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, photovoltaic cells, electronics, and optics and photonics. Many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics/control, and electrical materials science.[a]
 Electrical engineers typically hold a degree in electrical engineering, electronic or electrical and electronic engineering. Practicing engineers may have professional certification and be members of a professional body or an international standards organization. These include the International Electrotechnical Commission (IEC), the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET, formerly the IEE).
 Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from circuit theory to the management skills of a project manager. The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to sophisticated design and manufacturing software.
 Electricity has been a subject of scientific interest since at least the early 17th century. William Gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. He is credited with establishing the term ""electricity"".[1] He also designed the versorium: a device that detects the presence of statically charged objects. In 1762 Swedish professor Johan Wilcke invented a device later named electrophorus that produced a static electric charge. By 1800 Alessandro Volta had developed the voltaic pile, a forerunner of the electric battery.[2]
 In the 19th century, research into the subject started to intensify. Notable developments in this century include the work of Hans Christian Ørsted, who discovered in 1820 that an electric current produces a magnetic field that will deflect a compass needle; of William Sturgeon, who in 1825 invented the electromagnet; of Joseph Henry and Edward Davy, who invented the electrical relay in 1835; of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor; of Michael Faraday, the discoverer of electromagnetic induction in 1831; and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise Electricity and Magnetism.[3]
 In 1782, Georges-Louis Le Sage developed and presented in Berlin probably the world's first form of electric telegraphy, using 24 different wires, one for each letter of the alphabet. This telegraph connected two rooms. It was an electrostatic telegraph that moved gold leaf through electrical conduction.
 In 1795, Francisco Salva Campillo proposed an electrostatic telegraph system. Between 1803 and 1804, he worked on electrical telegraphy, and in 1804, he presented his report at the Royal Academy of Natural Sciences and Arts of Barcelona. Salva's electrolyte telegraph system was very innovative though it was greatly influenced by and based upon two discoveries made in Europe in 1800—Alessandro Volta's electric battery for generating an electric current and William Nicholson and Anthony Carlyle's electrolysis of water.[4] Electrical telegraphy may be considered the first example of electrical engineering.[5] Electrical engineering became a profession in the later 19th century.  Practitioners had created a global electric telegraph network, and the first professional electrical engineering institutions were founded in the UK and the US to support the new discipline. Francis Ronalds created an electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity.[6][7] Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort.[8] By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy.
 Practical applications and advances in such fields created an increasing need for standardized units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893.[9] The publication of these standards formed the basis of future advances in standardization in various industries, and in many countries, the definitions were immediately recognized in relevant legislation.[10]
 During these years, the study of electricity was largely considered to be a subfield of physics since early electrical technology was considered electromechanical in nature. The Technische Universität Darmstadt founded the world's first department of electrical engineering in 1882 and introduced the first-degree course in electrical engineering in 1883.[11] The first electrical engineering degree program in the United States was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross, [12] though it was Cornell University to produce the world's first electrical engineering graduates in 1885.[13] The first course in electrical engineering was taught in 1883 in Cornell's Sibley College of Mechanical Engineering and Mechanic Arts.[14]
 In about 1885, Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States.[15] In the same year, University College London founded the first chair of electrical engineering in Great Britain.[16] Professor Mendell P. Weinbach at University of Missouri established the electrical engineering department in 1886.[17] Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world.
 During these decades the use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts—direct current (DC)—to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Károly Zipernowsky, Ottó Bláthy and Miksa Déri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley, Jr. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown.[18] Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering.[19][20] The spread in the use of AC set off in the United States what has been called the war of the currents between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.[21]
 During the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including the possibility of invisible airborne waves (later called ""radio waves""). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these ""Hertzian waves"" into a purpose built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of 2,100 miles (3,400 km).[22]
 Millimetre wave communication was first investigated by Jagadish Chandra Bose during 1894–1896, when he reached an extremely high frequency of up to 60 GHz in his experiments.[23] He also introduced the use of semiconductor junctions to detect radio waves,[24] when he patented the radio crystal detector in 1901.[25][26]
 In 1897, Karl Ferdinand Braun introduced the cathode-ray tube as part of an oscilloscope, a crucial enabling technology for electronic television.[27] John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.[28]
 In 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer.[29][30] In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936.[31]
 In 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts.  In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer.[32][33] In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives.[34]
 In 1948, Claude Shannon published ""A Mathematical Theory of Communication"" which mathematically describes the passage of information with uncertainty (electrical noise).
 The first working transistor was a point-contact transistor invented by John Bardeen and Walter Houser Brattain while working under William Shockley at the Bell Telephone Laboratories (BTL) in 1947.[35] They then invented the bipolar junction transistor in 1948.[36] While early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis,[37] they opened the door for more compact devices.[38]
 The first integrated circuits were the hybrid integrated circuit invented by Jack Kilby at Texas Instruments in 1958 and the monolithic integrated circuit chip invented by Robert Noyce at Fairchild Semiconductor in 1959.[39]
 The MOSFET (metal–oxide–semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at BTL in 1959.[40][41][42] It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[37] It revolutionized the electronics industry,[43][44] becoming the most widely used electronic device in the world.[41][45][46]
 The MOSFET made it possible to build high-density integrated circuit chips.[41] The earliest experimental MOS IC chip to be fabricated was built by Fred Heiman and Steven Hofstein at RCA Laboratories in 1962.[47] MOS technology enabled Moore's law, the doubling of transistors on an IC chip every two years, predicted by Gordon Moore in 1965.[48] Silicon-gate MOS technology was developed by Federico Faggin at Fairchild in 1968.[49] Since then, the MOSFET has been the basic building block of modern electronics.[42][50][51] The mass-production of silicon MOSFETs and MOS integrated circuit chips, along with continuous MOSFET scaling miniaturization at an exponential pace (as predicted by Moore's law), has since led to revolutionary changes in technology, economy, culture and thinking.[52]
 The Apollo program which culminated in landing astronauts on the Moon with Apollo 11 in 1969 was enabled by NASA's adoption of advances in semiconductor electronic technology, including MOSFETs in the Interplanetary Monitoring Platform (IMP)[53][54] and silicon integrated circuit chips in the Apollo Guidance Computer (AGC).[55]
 The development of MOS integrated circuit technology in the 1960s led to the invention of the microprocessor in the early 1970s.[56][57] The first single-chip microprocessor was the Intel 4004, released in 1971.[56]  The Intel 4004 was designed and realized by Federico Faggin at Intel with his silicon-gate MOS technology,[56] along with Intel's Marcian Hoff and Stanley Mazor and Busicom's Masatoshi Shima.[58] The microprocessor led to the development of microcomputers and personal computers, and the microcomputer revolution.
 One of the properties of electricity is that it is very useful for energy transmission as well as for information transmission. These were also the first areas in which electrical engineering was developed. Today, electrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes, certain fields, such as electronic engineering and computer engineering, are considered disciplines in their own right.
 Power & Energy engineering deals with the generation, transmission, and distribution of electricity as well as the design of a range of related devices.[59] These include transformers, electric generators, electric motors, high voltage engineering, and power electronics. In many regions of the world, governments maintain an electrical network called a power grid that connects a variety of generators together with users of their energy. Users purchase electrical energy from the grid, avoiding the costly exercise of having to generate their own. Power engineers may work on the design and maintenance of the power grid as well as the power systems that connect to it.[60] Such systems are called on-grid power systems and may supply the grid with additional power, draw power from the grid, or do both. Power engineers may also work on systems that do not connect to the grid, called off-grid power systems, which in some cases are preferable to on-grid systems. 
 Telecommunications engineering focuses on the transmission of information across a communication channel such as a coax cable, optical fiber or free space.[61] Transmissions across free space require information to be encoded in a carrier signal to shift the information to a carrier frequency suitable for transmission; this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation.[62] The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.
 Once the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength.[63][64] Typically, if the power of the transmitted signal is insufficient once the signal arrives at the receiver's antenna(s), the information contained in the signal will be corrupted by noise, specifically static.
 Control engineering focuses on the modeling of a diverse range of dynamic systems and the design of controllers that will cause these systems to behave in the desired manner.[65] To implement such controllers, electronics control engineers may use electronic circuits, digital signal processors, microcontrollers, and programmable logic controllers (PLCs). Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles.[66] It also plays an important role in industrial automation.
 Control engineers often use feedback when designing control systems. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system which adjusts the motor's power output accordingly.[67] Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.
 Control engineers also work in robotics to design autonomous systems using control algorithms which interpret sensory feedback to control actuators that move robots such as autonomous vehicles, autonomous drones and others used in a variety of industries.[68]
 Electronic engineering involves the design and testing of electronic circuits that use the properties of components such as resistors, capacitors, inductors, diodes, and transistors to achieve a particular functionality.[60] The tuned circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit. Another example to research is a pneumatic signal conditioner.
 Prior to the Second World War, the subject was commonly known as radio engineering and basically was restricted to aspects of communications and radar, commercial radio, and early television.[60] Later, in post-war years, as consumer devices began to be developed, the field grew to include modern television, audio systems, computers, and microprocessors. In the mid-to-late 1950s, the term radio engineering gradually gave way to the name electronic engineering.
 Before the invention of the integrated circuit in 1959,[69] electronic circuits were constructed from discrete components that could be manipulated by humans. These discrete circuits consumed much space and power and were limited in speed, although they are still common in some applications. By contrast, integrated circuits packed a large number—often millions—of tiny electrical components, mainly transistors,[70] into a small chip around the size of a coin. This allowed for the powerful computers and other electronic devices we see today.
 Microelectronics engineering deals with the design and microfabrication of very small electronic circuit components for use in an integrated circuit or sometimes for use on their own as a general electronic component.[71] The most common microelectronic components are semiconductor transistors, although all main electronic components (resistors, capacitors etc.) can be created at a microscopic level.
 Nanoelectronics is the further scaling of devices down to nanometer levels. Modern devices are already in the nanometer regime, with below 100 nm processing having been standard since around 2002.[72]
 Microelectronic components are created by chemically fabricating wafers of semiconductors such as silicon (at higher frequencies, compound semiconductors like gallium arsenide and indium phosphide) to obtain the desired transport of electronic charge and control of current. The field of microelectronics involves a significant amount of chemistry and material science and requires the electronic engineer working in the field to have a very good working knowledge of the effects of quantum mechanics.[73]
 Signal processing deals with the analysis and manipulation of signals.[74] Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information. For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error detection and error correction of digitally sampled signals.[75]
 Signal processing is a very mathematically oriented and intensive area forming the core of digital signal processing and it is rapidly expanding with new applications in every field of electrical engineering such as communications, control, radar, audio engineering, broadcast engineering, power electronics, and biomedical engineering as many already existing analog systems are replaced with their digital counterparts. Analog signal processing is still important in the design of many control systems.
 DSP processor ICs are found in many types of modern electronic devices, such as digital television sets,[76] radios, hi-fi audio equipment, mobile phones, multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, missile guidance systems, radar systems, and telematics systems. In such products, DSP may be responsible for noise reduction, speech recognition or synthesis, encoding or decoding digital media, wirelessly transmitting or receiving data, triangulating positions using GPS, and other kinds of image processing, video processing, audio processing, and speech processing.[77]
 Instrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow, and temperature.[78] The design of such instruments requires a good understanding of physics that often extends beyond electromagnetic theory. For example, flight instruments measure variables such as wind speed and altitude to enable pilots the control of aircraft analytically. Similarly, thermocouples use the Peltier-Seebeck effect to measure the temperature difference between two points.[79]
 Often instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant.[80] For this reason, instrumentation engineering is often viewed as the counterpart of control.
 Computer engineering deals with the design of computers and computer systems. This may involve the design of new hardware. Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline.[81] Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of embedded devices including video game consoles and DVD players. Computer engineers are involved in many hardware and software aspects of computing.[82] Robots are one of the applications of computer engineering.
 Photonics and optics deals with the generation, transmission, amplification, modulation, detection, and analysis of electromagnetic radiation. The application of optics deals with design of optical instruments such as lenses, microscopes, telescopes, and other equipment that uses the properties of electromagnetic radiation. Other prominent applications of optics include electro-optical sensors and measurement systems, lasers, fiber-optic communication systems, and optical disc systems (e.g. CD and DVD). Photonics builds heavily on optical technology, supplemented with modern developments such as optoelectronics (mostly involving semiconductors), laser systems, optical amplifiers and novel materials (e.g. metamaterials).
 Mechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. Such combined systems are known as electromechanical systems and have widespread adoption. Examples include automated manufacturing systems,[83] heating, ventilation and air-conditioning systems,[84] and various subsystems of aircraft and automobiles.[85]
Electronic systems design is the subject within electrical engineering that deals with the multi-disciplinary design issues of complex electrical and mechanical systems.[86]
 The term mechatronics is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. Already, such small devices, known as microelectromechanical systems (MEMS), are used in automobiles to tell airbags when to deploy,[87] in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. In the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.[88]
 In aerospace engineering and robotics, an example is the most recent electric propulsion and ion propulsion.
 Electrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology,[89] or electrical and electronic engineering.[90][91] The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science, depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering.[92] Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.
 At many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a Bachelor of Engineering (Electrical and Electronic), but in others, electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.[93]
 Some electrical engineers choose to study for a postgraduate degree such as a Master of Engineering/Master of Science (MEng/MSc), a Master of Engineering Management, a Doctor of Philosophy (PhD) in Engineering, an Engineering Doctorate (Eng.D.), or an Engineer's degree. The master's and engineer's degrees may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy and Engineering Doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. In the United Kingdom and some other European countries, Master of Engineering is often considered to be an undergraduate degree of slightly longer duration than the Bachelor of Engineering rather than a standalone postgraduate degree.[94]
 In most countries, a bachelor's degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body.[95] After completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada and South Africa), Chartered engineer or Incorporated Engineer (in India, Pakistan, the United Kingdom, Ireland and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).
 The advantages of licensure vary depending upon location. For example, in the United States and Canada ""only a licensed engineer may seal engineering work for public and private clients"".[96] This requirement is enforced by state and provincial legislation such as Quebec's Engineers Act.[97] In other countries, no such legislation exists. Practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion.[98] In this way these organizations play an important role in maintaining ethical standards for the profession. Even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. In cases where an engineer's work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. An engineer's work must also comply with numerous other rules and regulations, such as building codes and legislation pertaining to environmental law.
 Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET).  The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually.[99] The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe.[100][101] Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.[102]
 In Australia, Canada, and the United States, electrical engineers make up around 0.25% of the labor force.[b]
 From the Global Positioning System to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. They design, develop, test, and supervise the deployment of electrical systems and electronic devices. For example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.[106]
 Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.
 Although most electrical engineers will understand basic circuit theory (that is, the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.[107]
 A wide range of instrumentation is used by electrical engineers.  For simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice.  Where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument.  In RF engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used.  In some disciplines, safety can be a particular concern with instrumentation.  For instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids.[108] Power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different.[109] Many disciplines of electrical engineering use tests specific to their discipline.  Audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise.  Likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.
 For many engineers, technical work accounts for only a fraction of the work they do. A lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules.[110] Many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.
 The workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, on board a Naval ship, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.[111]
 Electrical engineering has an intimate relationship with the physical sciences.  For instance, the physicist Lord Kelvin played a major role in the engineering of the first transatlantic telegraph cable.[112] Conversely, the engineer Oliver Heaviside produced major work on the mathematics of transmission on telegraph cables.[113] Electrical engineers are often required on major science projects.  For instance, large particle accelerators such as CERN need electrical engineers to deal with many aspects of the project including the power distribution, the instrumentation, and the manufacture and installation of the superconducting electromagnets.[114][115]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['electromagnetism', 'Charles Steinmetz and Oliver Heaviside', 'electromagnetism', 'electricity, electronics, and electromagnetism', 'revolutionary changes in technology, economy, culture and thinking'], 'answer_start': [], 'answer_end': []}"
"Chemical engineering is an engineering field which deals with the study of operation and design of chemical plants as well as methods of improving production. Chemical engineers develop economical commercial processes to convert raw materials into useful products. Chemical engineering uses principles of chemistry, physics, mathematics, biology, and economics to efficiently use, produce, design, transport and transform energy and materials. The work of chemical engineers can range from the utilization of nanotechnology and nanomaterials in the laboratory to large-scale industrial processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products. Chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, modeling, control engineering, chemical reaction engineering, nuclear engineering, biological engineering,  construction specification, and operating instructions.
 Chemical engineers typically hold a degree in Chemical Engineering or Process Engineering. Practicing engineers may have professional certification and be accredited members of a professional body. Such bodies include the Institution of Chemical Engineers (IChemE) or the American Institute of Chemical Engineers (AIChE). A degree in chemical engineering is directly linked with all of the other engineering disciplines, to various extents.
 A 1996 article cites James F. Donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid.[1] In the same paper, however, George E. Davis, an English consultant, was credited with having coined the term.[2] Davis also tried to found a Society of Chemical Engineering, but instead, it was named the Society of Chemical Industry (1881), with Davis as its first secretary.[3][4] The History of Science in United States: An Encyclopedia puts the use of the term around 1890.[5] ""Chemical engineering"", describing the use of mechanical equipment in the chemical industry, became common vocabulary in England after 1850.[6] By 1910, the profession, ""chemical engineer,"" was already in common use in Britain and the United States.[7]
 In the 1940s, it became clear that unit operations alone were insufficient in developing chemical reactors. While the predominance of unit operations in chemical engineering courses in Britain and the United States continued until the 1960s, transport phenomena started to receive greater focus.[8] Along with other novel concepts, such as process systems engineering (PSE), a ""second paradigm"" was defined.[9][10] Transport phenomena gave an analytical approach to chemical engineering[11] while PSE focused on its synthetic elements, such as those of a control system and process design.[12] Developments in chemical engineering before and after World War II were mainly incited by the petrochemical industry;[13] however, advances in other fields were made as well. Advancements in biochemical engineering in the 1940s, for example, found application in the pharmaceutical industry, and allowed for the mass production of various antibiotics, including penicillin and streptomycin.[14] Meanwhile, progress in polymer science in the 1950s paved way for the ""age of plastics"".[15]
 Concerns regarding large-scale chemical manufacturing facilities' safety and environmental impact were also raised during this period. Silent Spring, published in 1962, alerted its readers to the harmful effects of DDT, a potent insecticide.[16] The 1974 Flixborough disaster in the United Kingdom resulted in 28 deaths, as well as damage to a chemical plant and three nearby villages.[17] 1984 Bhopal disaster in India resulted in almost 4,000 deaths.[citation needed] These incidents, along with other incidents, affected the reputation of the trade as industrial safety and environmental protection were given more focus.[18] In response, the IChemE required safety to be part of every degree course that it accredited after 1982. By the 1970s, legislation and monitoring agencies were instituted in various countries, such as France, Germany, and the United States.[19] In time, the systematic application of safety principles to chemical and other process plants began to be considered a specific discipline, known as process safety.[20]
 Advancements in computer science found applications for designing and managing plants, simplifying calculations and drawings that previously had to be done manually. The completion of the Human Genome Project is also seen as a major development, not only advancing chemical engineering but genetic engineering and genomics as well.[21] Chemical engineering principles were used to produce DNA sequences in large quantities.[22]
 Chemical engineering involves the application of several principles. Key concepts are presented below.
 Chemical engineering design concerns the creation of plans, specifications, and economic analyses for pilot plants, new plants, or plant modifications. Design engineers often work in a consulting role, designing plants to meet clients' needs. Design is limited by several factors, including funding, government regulations, and safety standards. These constraints dictate a plant's choice of process, materials, and equipment.[23]
 Plant construction is coordinated by project engineers and project managers,[24] depending on the size of the investment. A chemical engineer may do the job of project engineer full-time or part of the time, which requires additional training and job skills or act as a consultant to the project group. In the USA the education of chemical engineering graduates from the Baccalaureate programs accredited by ABET do not usually stress project engineering education, which can be obtained by specialized training, as electives, or from graduate programs. Project engineering jobs are some of the largest employers for chemical engineers.[25]
 A unit operation is a physical step in an individual chemical engineering process. Unit operations (such as crystallization, filtration, drying and evaporation) are used to prepare reactants, purifying and separating its products, recycling unspent reactants, and controlling energy transfer in reactors.[26] On the other hand, a unit process is the chemical equivalent of a unit operation. Along with unit operations, unit processes constitute a process operation. Unit processes (such as nitration, hydrogenation, and oxidation involve the conversion of materials by biochemical, thermochemical and other means. Chemical engineers responsible for these are called process engineers.[27]
 Process design requires the definition of equipment types and sizes as well as how they are connected and the materials of construction. Details are often printed on a Process Flow Diagram which is used to control the capacity and reliability of a new or existing chemical factory.
 Education for chemical engineers in the first college degree 3 or 4 years of study stresses the principles and practices of process design. The same skills are used in existing chemical plants to evaluate the efficiency and make recommendations for improvements.
 Modeling and analysis of transport phenomena is essential for many industrial applications. Transport phenomena involve fluid dynamics, heat transfer and mass transfer, which are governed mainly by momentum transfer, energy transfer and transport of chemical species, respectively. Models often involve separate considerations for macroscopic, microscopic and molecular level phenomena. Modeling of transport phenomena, therefore, requires an understanding of applied mathematics.[28]
 Chemical engineers ""develop economic ways of using materials and energy"".[30] Chemical engineers use chemistry and engineering to turn raw materials into usable products, such as medicine, petrochemicals, and plastics on a large-scale, industrial setting. They are also involved in waste management and research.[31][32] Both applied and research facets could make extensive use of computers.[29]
 Chemical engineers may be involved in industry or university research where they are tasked with designing and performing experiments, by scaling up theoretical chemical reactions, to create better and safer methods for production, pollution control, and resource conservation. They may be involved in designing and constructing plants as a project engineer. Chemical engineers serving as project engineers use their knowledge in selecting optimal production methods and plant equipment to minimize costs and maximize safety and profitability. After plant construction, chemical engineering project managers may be involved in equipment upgrades, troubleshooting, and daily operations in either full-time or consulting roles. [33]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['waste management and research', 'James F. Donnelly', 'using materials and energy', 'industrial safety and environmental protection', 'funding, government regulations, and safety standards'], 'answer_start': [], 'answer_end': []}"
"Aerospace engineering is the primary field of engineering concerned with the development of aircraft and spacecraft.[3]  It has two major and overlapping branches: aeronautical engineering and astronautical engineering.  Avionics engineering is similar, but deals with the electronics side of aerospace engineering.
 ""Aeronautical engineering"" was the original term for the field. As flight technology advanced to include vehicles operating in outer space, the broader term ""aerospace engineering"" has come into use.[4]  Aerospace engineering, particularly the astronautics branch, is often colloquially referred to as ""rocket science"".[5][a]
 Flight vehicles are subjected to demanding conditions such as those caused by changes in atmospheric pressure and temperature, with structural loads applied upon vehicle components. Consequently, they are usually the products of various technological and engineering disciplines including aerodynamics, Air propulsion, avionics, materials science, structural analysis and manufacturing. The interaction between these technologies is known as aerospace engineering. Because of the complexity and number of disciplines involved, aerospace engineering is carried out by teams of engineers, each having their own specialized area of expertise.[7]
 The origin of aerospace engineering can be traced back to the aviation pioneers around the late 19th to early 20th centuries, although the work of Sir George Cayley dates from the last decade of the 18th to the mid-19th century. One of the most important people in the history of aeronautics[8] and a pioneer in aeronautical engineering,[9] Cayley is credited as the first person to separate the forces of lift and drag, which affect any atmospheric flight vehicle.[10]
 Early knowledge of aeronautical engineering was largely empirical, with some concepts and skills imported from other branches of engineering.[11] Some key elements, like fluid dynamics, were understood by 18th-century scientists.[12]
 In December 1903, the Wright Brothers performed the first sustained, controlled flight of a powered, heavier-than-air aircraft, lasting 12 seconds. The 1910s saw the development of aeronautical engineering through the design of World War I military aircraft.
 Between World Wars I and II, great leaps were made in the field, accelerated by the advent of mainstream civil aviation. Notable airplanes of this era include the Curtiss JN 4, the Farman F.60 Goliath, and Fokker Trimotor. Notable military airplanes of this period include the Mitsubishi A6M Zero, the Supermarine Spitfire and the Messerschmitt Bf 109 from Japan, United Kingdom, and Germany respectively. A significant development in aerospace engineering came with the first operational Jet engine-powered airplane, the Messerschmitt Me 262 which entered service in 1944 towards the end of the Second World War.[13]
 The first definition of aerospace engineering appeared in February 1958,[4] considering the Earth's atmosphere and outer space as a single realm, thereby encompassing both aircraft (aero) and spacecraft (space) under the newly coined term aerospace.
 In response to the USSR launching the first satellite, Sputnik, into space on October 4, 1957, U.S. aerospace engineers launched the first American satellite on January 31, 1958. The National Aeronautics and Space Administration was founded in 1958 as a response to the Cold War. In 1969, Apollo 11, the first human space mission to the moon took place. It saw three astronauts enter orbit around the Moon, with two, Neil Armstrong and Buzz Aldrin, visiting the lunar surface. The third astronaut, Michael Collins, stayed in orbit to rendezvous with Armstrong and Aldrin after their visit.[14]
 An important innovation came on January 30, 1970, when the Boeing 747 made its first commercial flight from New York to London. This aircraft made history and became known as the ""Jumbo Jet"" or ""Whale""[15] due to its ability to hold up to 480 passengers.[16]
 Another significant development in aerospace engineering came in 1976, with the development of the first passenger supersonic aircraft, the Concorde. The development of this aircraft was agreed upon by the French and British on November 29, 1962.[17]
 On December 21, 1988, the Antonov An-225 Mriya cargo aircraft commenced its first flight. It holds the records for the world's heaviest aircraft, heaviest airlifted cargo, and longest airlifted cargo, and has the widest wingspan of any aircraft in operational service.[18]
 On October 25, 2007, the Airbus A380 made its maiden commercial flight from Singapore to Sydney, Australia. This aircraft was the first passenger plane to surpass the Boeing 747 in terms of passenger capacity, with a maximum of 853. Though development of this aircraft began in 1988 as a competitor to the 747, the A380 made its first test flight in April 2005.[19]
 Some of the elements of aerospace engineering are:[20][21]
 The basis of most of these elements lies in theoretical physics, such as fluid dynamics for aerodynamics or the equations of motion for flight dynamics. There is also a large empirical component. Historically, this empirical component was derived from testing of scale models and prototypes, either in wind tunnels or in the free atmosphere. More recently, advances in computing have enabled the use of computational fluid dynamics to simulate the behavior of the fluid, reducing time and expense spent on wind-tunnel testing. Those studying hydrodynamics or hydroacoustics often obtain degrees in aerospace engineering.
 Additionally, aerospace engineering addresses the integration of all components that constitute an aerospace vehicle (subsystems including power, aerospace bearings, communications, thermal control, life support system, etc.) and its life cycle (design, temperature, pressure, radiation, velocity, lifetime).
 Aerospace engineering may be studied at the advanced diploma, bachelor's, master's, and Ph.D. levels in aerospace engineering departments at many universities, and in mechanical engineering departments at others. A few departments offer degrees in space-focused astronautical engineering.  Some institutions differentiate between aeronautical and astronautical engineering.  Graduate degrees are offered in advanced or specialty areas for the aerospace industry.
 A background in chemistry, physics, computer science and mathematics is important for students pursuing an aerospace engineering degree.[23]
 The term ""rocket scientist"" is sometimes used to describe a person of great intelligence since rocket science is seen as a practice requiring great mental ability, especially technically and mathematically. The term is used ironically in the expression ""It's not rocket science"" to indicate that a task is simple.[24] Strictly speaking, the use of ""science"" in ""rocket science"" is a misnomer since science is about understanding the origins, nature, and behavior of the universe; engineering is about using scientific and engineering principles to solve problems and develop new technology.[5][6] The more etymologically correct version of this phrase would be ""rocket engineer"". However, ""science"" and ""engineering"" are often misused as synonyms.[5][6][25]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['development of aircraft and spacecraft', 'chemistry, physics, computer science and mathematics', 'One of the most important people in the history of aeronautics', 'aeronautical engineering and astronautical engineering', 'a task is simple'], 'answer_start': [], 'answer_end': []}"
"Marine engineering is the engineering of boats, ships, submarines, and any other marine vessel. Here it is also taken to include the engineering of other ocean systems and structures – referred to in certain academic and professional circles as ""ocean engineering"".
 Marine engineering applies a number of engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and ocean systems.[1] It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, as well as coastal and offshore structures.
 Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
 In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton's Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe. Around 50 years later the steam powered paddle wheels had a peak with the creation of the Great Eastern, which was as big as one of the cargo ships of today, 700 feet in length, weighing 22,000 tons. Paddle steamers would become the front runners of the steamship industry for the next thirty years till the next type of propulsion came around.[2]
 There are many ways to become a Marine Engineer, but all include a university or college degree. Primarily, training includes a Bachelor of Engineering (B.Eng. or B.E.), 
 Bachelor of Science (B.Sc. or B.S.), 
 Bachelor of Technology (B.Tech.), 
 Bachelor of Technology Management and Marine Engineering (B.TecMan & MarEng)
 Bachelor of Applied Science (B.A.Sc.) in Marine Engineering. 
 
Depending on the country and jurisdiction, to be licensed as a Marine engineer, a Master's degree; 
 Master of Engineering (M.Eng.), 
 Master of Science (M.Sc or M.S.)  
 Master of Applied Science (M.A.Sc.) 
 may be required. There are also Marine engineers who have come from other disciplines, e.g., from engineering fields like Mechanical Engineering, Civil Engineering, Electrical Engineering, Geomatics Engineering, Environmental Engineering or from science fields like Geology, Geophysics, Physics, Geomatics, Earth Science, Mathematics, However, this path requires taking a graduate degree such as M.Eng, M.S., M.Sc. or M.A.Sc. in Marine Engineering after graduating from a different quantitative undergraduate program to be qualified as a Marine engineer.
 The fundamental subjects of Marine engineering study usually include:
 In the engineering of seagoing vessels, naval architecture is concerned with the overall design of the ship and its propulsion through the water, while marine engineering ensures that the ship systems function as per the design.[3] Although they have distinctive disciplines, naval architects and marine engineers often work side-by-side.
 Ocean engineering is concerned with other structures and systems in or adjacent to the ocean, including offshore platforms, coastal structures such as piers and harbors, and other ocean systems such as ocean wave energy conversion and underwater life-support systems.[4] This in fact makes ocean engineering a distinctive field from marine engineering, which is concerned with the design and application of shipboard systems specifically.[5] However, on account of its similar nomenclature and multiple overlapping core disciplines (e.g. hydrodynamics, hydromechanics, and materials science), ""ocean engineering"" sometimes operates under the umbrella term of ""marine engineering"", especially in industry and academia outside of the U.S. The same combination has been applied to the rest of this article.
 Oceanography is a scientific field concerned with the acquisition and analysis of data to characterize the ocean. Although separate disciplines, marine engineering and oceanography are closely intertwined: marine engineers often use data gathered by oceanographers to inform their design and research, and oceanographers use tools designed by marine engineers (more specifically, oceanographic engineers) to advance their understanding and exploration of the ocean.[6]
 Marine engineering incorporates many aspects of mechanical engineering. One manifestation of this relationship lies in the design of shipboard propulsion systems. Mechanical engineers design the main propulsion plant, the powering and mechanization aspects of the ship functions such as steering, anchoring, cargo handling, heating, ventilation, air conditioning interior and exterior communication, and other related requirements. Electrical power generation and electrical power distribution systems are typically designed by their suppliers; the only design responsibility of the marine engineering is installation.
 Furthermore, an understanding of mechanical engineering topics such as fluid dynamics, fluid mechanics, linear wave theory, strength of materials, structural mechanics, and structural dynamics is essential to a marine engineer's repertoire of skills. These and other mechanical engineering subjects serve as an integral component of the marine engineering curriculum.[7]
 Civil engineering concepts play in an important role in many marine engineering projects such as the design and construction of ocean structures, ocean bridges and tunnels, and port/harbor design.
 Marine engineering often deals in the fields of electrical engineering and robotics, especially in applications related to employing deep-sea cables and UUVs.
 A series of transoceanic fiber optic cables are responsible for connecting much of the world's communication via the internet, carrying as much as 99 percent of total global internet and signal traffic. These cables must be engineered to withstand deep-sea environments that are remote and often unforgiving, with extreme pressures and temperatures as well as potential interference by fishing, trawling, and sea life.
 The use of unmanned underwater vehicles (UUVs) stands to benefit from the use of autonomous algorithms and networking. Marine engineers aim to learn how advancements in autonomy and networking can be used to enhance existing UUV technologies and facilitate the development of more capable underwater vehicles.
 A knowledge of marine engineering proves useful in the field of petroleum engineering, as hydrodynamics and seabed integration serve as key elements in the design and maintenance of offshore oil platforms.
 Marine construction is the process of building structures in or adjacent to large bodies of water, usually the sea. These structures can be built for a variety of purposes, including transportation, energy production, and recreation. Marine construction can involve the use of a variety of building materials, predominantly steel and concrete. Some examples of marine structures include ships, offshore platforms, moorings, pipelines, cables, wharves, bridges, tunnels, breakwaters and docks.
 In the same way that civil engineers design to accommodate wind loads on building and bridges, marine engineers design to accommodate a ship or submarine struck by waves millions of times over the course of the vessel's life. These load conditions are also found in marine construction and coastal engineering
 Any seagoing vessel has the constant need for hydrostatic stability. A naval architect, like an airplane designer, is concerned with stability. What makes the naval architect's job unique is that a ship operates in two fluids simultaneously: water and air. Even after a ship has been designed and put to sea, marine engineers face the challenge of balancing cargo, as stacking containers vertically increases the mass of the ship and shifts the center of gravity higher. The weight of fuel also presents a problem, as the pitch of the ship may cause the liquid to shift, resulting in an imbalance. In some vessels, this offset will be counteracted by storing water inside larger ballast tanks. Marine engineers are responsible for the task of balancing and tracking the fuel and ballast water of a ship. Floating offshore structures have similar constraints.
 The saltwater environment faced by seagoing vessels makes them highly susceptible to corrosion. In every project, marine engineers are concerned with surface protection and preventing galvanic corrosion. Corrosion can be inhibited through cathodic protection by introducing pieces of metal (e.g. zinc) to serve as a ""sacrificial anode"" in the corrosion reaction. This causes the metal to corrode instead of the ship's hull. Another way to prevent corrosion is by sending a controlled amount of low DC current through the ship's hull, thereby changing the hull's electrical charge and delaying the onset of electro-chemical corrosion. Similar problems are encountered in coastal and offshore structures.
 Anti-fouling is the process of eliminating obstructive organisms from essential components of seawater systems. Depending on the nature and location of marine growth, this process is performed in a number of different ways:
 
The burning of marine fuels releases harmful pollutants into the atmosphere. Ships burn marine diesel in addition to heavy fuel oil. Heavy fuel oil, being the heaviest of refined oils, releases sulfur dioxide when burned. Sulfur dioxide emissions have the potential to raise atmospheric and ocean acidity causing harm to marine life. However, heavy fuel oil may only be burned in international waters due to the pollution created. It is commercially advantageous due to the cost effectiveness compared to other marine fuels. It is prospected that heavy fuel oil will be phased out of commercial use by the year 2020 (Smith, 2018).[10] Water, oil, and other substances collect at the bottom of the ship in what is known as the bilge. Bilge water is pumped overboard, but must pass a pollution threshold test of 15 ppm (parts per million) of oil to be discharged. Water is tested and either discharged if clean or recirculated to a holding tank to be separated before being tested again. The tank it is sent back to, the oily water separator, utilizes gravity to separate the fluids due to their viscosity. Ships over 400 gross tons are required to carry the equipment to separate oil from bilge water. Further, as enforced by MARPOL, all ships over 400 gross tons and all oil tankers over 150 gross tons are required to log all oil transfers in an oil record book (EPA, 2011).[11]
 Cavitation is the process of forming an air bubble in a liquid due to the vaporization of that liquid cause by an area of low pressure. This area of low pressure lowers the boiling point of a liquid allowing it to vaporize into a gas. Cavitation can take place in pumps, which can cause damage to the impeller that moves the fluids through the system. Cavitation is also seen in propulsion. Low pressure pockets form on the surface of the propeller blades as its revolutions per minute increase (IIMS, 2015).[12] Cavitation on the propeller causes a small but violent implosion which could warp the propeller blade. To remedy the issue, more blades allow the same amount of propulsion force but at a lower rate of revolutions. This is crucial for submarines as the propeller needs to keep the vessel relatively quiet to stay hidden. With more propeller blades, the vessel is able to achieve the same amount of propulsion force at lower shaft revolutions.
 The following categories provide a number of focus areas in which marine engineers direct their efforts.
 In designing systems that operate in the arctic (especially scientific equipment such as meteorological instrumentation and oceanographic buoys), marine engineers must overcome an array of design challenges. Equipment must be able to operate at extreme temperatures for prolonged periods of time, often with little to no maintenance. This creates the need for exceptionally temperature-resistant materials and durable precision electronic components.[citation needed]
 Coastal engineering applies a mixture of civil engineering and other disciplines to create coastal solutions for areas along or near the ocean. In protecting coastlines from wave forces, erosion, and sea level rise, marine engineers must consider whether they will use a ""gray"" infrastructure solution - such as a breakwater, culvert, or sea wall made from rocks and concrete - or a ""green"" infrastructure solution that incorporates aquatic plants, mangroves, and/or marsh ecosystems.[13] It has been found that gray infrastructure costs more to build and maintain, but it may provide better protection against ocean forces in high-energy wave environments.[14] A green solution is generally less expensive and more well-integrated with local vegetation, but may be susceptible to erosion or damage if executed improperly.[15] In many cases engineers will select a hybrid approach that combines elements of both gray and green solutions.[16]
 The design of underwater life-support systems such as underwater habitats presents a unique set of challenges requiring a detailed knowledge of pressure vessels, diving physiology, and thermodynamics.
 Marine engineers may design or make frequent use of unmanned underwater vehicles, which operate underwater without a human aboard. UUVs often perform work in locations which would be otherwise impossible or difficult to access by humans due to a number of environmental factors (e.g. depth, remoteness, and/or temperature). UUVs can be remotely operated by humans, like in the case of remotely operated vehicles, semi-autonomous, or autonomous.
 The development of oceanographic sciences, subsea engineering and the ability to detect, track and destroy submarines (anti-submarine warfare) required the parallel development of a host of marine scientific instrumentation and sensors. Visible light is not transferred far underwater, so the medium for transmission of data is primarily acoustic. High-frequency sound is used to measure the depth of the ocean, determine the nature of the seafloor, and detect submerged objects. The higher the frequency, the higher the definition of the data that is returned. Sound Navigation and Ranging or SONAR was developed during the First World War to detect submarines, and has been greatly refined through to the present day. Submarines similarly use sonar equipment to detect and target other submarines and surface ships, and to detect submerged obstacles such as seamounts that pose a navigational obstacle. Simple echo-sounders point straight down and can give an accurate reading of ocean depth (or look up at the underside of sea-ice).
More advanced echo sounders use a fan-shaped beam or sound, or multiple beams to derive highly detailed images of the ocean floor. High power systems can penetrate the soil and seabed rocks to give information about the geology of the seafloor, and are widely used in geophysics for the discovery of hydrocarbons, or for engineering survey. 
For close-range underwater communications, optical transmission is possible, mainly using blue lasers. These have a high bandwidth compared with acoustic systems, but the range is usually only a few tens of metres, and ideally at night. 
As well as acoustic communications and navigation, sensors have been developed to measure ocean parameters such as temperature, salinity, oxygen levels and other properties including nitrate levels, levels of trace chemicals and environmental DNA. The industry trend has been towards smaller, more accurate and more affordable systems so that they can be purchased and used by university departments and small companies as well as large corporations, research organisations and governments. The sensors and instruments are fitted to autonomous and remotely-operated systems as well as ships, and are enabling these systems to take on tasks that hitherto required an expensive human-crewed platform.
Manufacture of marine sensors and instruments mainly takes place in Asia, Europe and North America. Products are advertised in specialist journals, and through Trade Shows such as Oceanology International and Ocean Business which help raise awareness of the products.
 In every coastal and offshore project, environmental sustainability is an important consideration for the preservation of ocean ecosystems and natural resources. Instances in which marine engineers benefit from knowledge of environmental engineering include creation of fisheries, clean-up of oil spills, and creation of coastal solutions.[17]
 A number of systems designed fully or in part by marine engineers are used offshore - far away from coastlines.
 The design of offshore oil platforms involves a number of marine engineering challenges. Platforms must be able to withstand ocean currents, wave forces, and saltwater corrosion while remaining structurally integral and fully anchored into the seabed. Additionally, drilling components must be engineered to handle these same challenges with a high factor of safety to prevent oil leaks and spills from contaminating the ocean.
 Offshore wind farms encounter many similar marine engineering challenges to oil platforms. They provide a source of renewable energy with a higher yield than wind farms on land, while encountering less resistance from the general public (see NIMBY).[18]
 Marine engineers continue to investigate the possibility of ocean wave energy as a viable source of power for distributed or grid applications. Many designs have been proposed and numerous prototypes have been built, but the problem of harnessing wave energy in a cost-effective manner remains largely unresolved.[19]
 A marine engineer may also deal with the planning, creation, expansion, and modification of port and harbor designs. Harbors can be natural or artificial and protect anchored ships from wind, waves, and currents.[20] Ports can be defined as a city, town, or place where ships are moored, loaded, or unloaded. Ports typically reside within a harbor and are made up of one or more individual terminals that handle a particular cargo including passengers, bulk cargo, or containerized cargo.[21] Marine engineers plan and design various types of marine terminals and structures found in ports, and they must understand the loads imposed on these structures over the course of their lifetime.
 Marine salvage techniques are continuously modified and improved to recover shipwrecks. Marine engineers use their skills to assist at some stages of this process.
 With a diverse engineering background, marine engineers work in a variety of industry jobs across every field of math, science, technology, and engineering. A few companies such as Oceaneering International and Van Oord specialize in marine engineering, while other companies consult marine engineers for specific projects. Such consulting commonly occurs in the oil industry, with companies such as ExxonMobil and BP hiring marine engineers to manage aspects of their offshore drilling projects.
 Marine engineering lends itself to a number of military applications – mostly related to the Navy. The United States Navy's Seabees, Civil Engineer Corps, and Engineering Duty Officers often perform work related to marine engineering. Military contractors (especially those in naval shipyards) and the Army Corps of Engineers play a role in certain marine engineering projects as well.
 In 2012, the average annual earnings for marine engineers in the U.S. were $96,140 with average hourly earnings of $46.22.[22] As a field, marine engineering is predicted to grow approximately 12% from 2016 to 2026. Currently, there are about 8,200 naval architects and marine engineers employed, however, this number is expected to increase to 9,200 by 2026 (BLS, 2017).[23] This is due at least in part to the critical role of the shipping industry on the global market supply chain; 80% of the world's trade by volume is done overseas by close to 50,000 ships, all of which require marine engineers aboard and shoreside (ICS, 2017).[24] Additionally, offshore energy continues to grow, and a greater need exists for coastal solutions due to sea level rise.
 Maritime universities are dedicated to teaching and training students in maritime professions. Marine engineers generally have a bachelor's degree in marine engineering, marine engineering technology, or marine systems engineering. Practical training is valued by employers alongside the bachelor's degree.
 A number of institutions - including MIT,[26] UC Berkeley,[27] the U.S. Naval Academy,[28] and Texas A&M University[29] - offer a four-year Bachelor of Science degree specifically in ocean engineering. Accredited programs consist of basic undergraduate math and science subjects such as calculus, statistics, chemistry, and physics; fundamental engineering subjects such as statics, dynamics, electrical engineering, and thermodynamics; and more specialized subjects such as ocean structural analysis, hydromechanics, and coastal management.
 Graduate students in ocean engineering take classes on more advanced, in-depth subjects while conducting research to complete a graduate-level thesis. The Massachusetts Institute of Technology offers master's and PhD degrees specifically in ocean engineering.[30] Additionally, MIT co-hosts a joint program with the Woods Hole Oceanographic Institution for students studying ocean engineering and other ocean-related topics at the graduate level.[31][32]
 Journals about ocean engineering include Ocean Engineering,[33] the IEEE Journal of Oceanic Engineering[34] and the Journal of Waterway, Port, Coastal, and Ocean Engineering.[35]
 Conferences in the field of marine engineering include the IEEE Oceanic Engineering Society's OCEANS Conference and Exposition[36] and the European Wave and Tidal Energy Conference (EWTEC).[37]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['ocean engineering', 'Oceaneering International and Van Oord', 'design and application of shipboard systems specifically', 'ocean structural analysis, hydromechanics, and coastal management', 'application of shipboard systems specifically'], 'answer_start': [], 'answer_end': []}"
"Mining in the engineering discipline is the extraction of minerals from the ground. Mining engineering is associated with many other disciplines, such as mineral processing, exploration, excavation, geology, metallurgy, geotechnical engineering and surveying. A mining engineer may manage any phase of mining operations, from exploration and discovery of the mineral resources, through feasibility study, mine design, development of plans, production and operations to mine closure.[not verified in body]
 From prehistoric times to the present, mining has played a significant role in the existence of the human race. Since the beginning of civilization, people have used stone and ceramics and, later, metals found on or close to the Earth's surface. These were used to manufacture early tools and weapons. For example, high-quality flint found in northern France and southern England were used to set fire and break rock.[1] Flint mines have been found in chalk areas where seams of the stone were followed underground by shafts and galleries. The oldest known mine on the archaeological record is the ""Lion Cave"" in Eswatini. At this site, which radiocarbon dating indicates to be about 43,000 years old, paleolithic humans mined mineral hematite, which contained iron and was ground to produce the red pigment ochre.[2][3]
 The ancient Romans were innovators of mining engineering. They developed large-scale mining methods, such as the use of large volumes of water brought to the minehead by aqueducts for hydraulic mining. The exposed rock was then attacked by fire-setting, where fires were used to heat the rock, which would be quenched with a stream of water. The thermal shock cracked the rock, enabling it to be removed. In some mines, the Romans utilized water-powered machinery such as reverse overshot water-wheels. These were used extensively in the copper mines at Rio Tinto in Spain, where one sequence comprised 16 such wheels arranged in pairs, lifting water about 80 feet (24 m).[4]
 Black powder was first used in mining in Banská Štiavnica, Kingdom of Hungary (present-day Slovakia) in 1627.[5]  This allowed blasting of rock and earth to loosen and reveal ore veins, which was much faster than fire-setting. The Industrial Revolution saw further advances in mining technologies, including improved explosives and steam-powered pumps, lifts, and drills.
 Becoming an accredited mining engineer requires a university or college degree. Training includes a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Technology (B.Tech.) or Bachelor of Applied Science (B.A.Sc.) in mining engineering. Depending on the country and jurisdiction, to be licensed as a mining engineer may require a Master of Engineering (M.Eng.), Master of Science (M.Sc or M.S.) or Master of Applied Science (M.A.Sc.) degree.
 Some mining engineers who have come from other disciplines, primarily from engineering fields (e.g.: mechanical, civil, electrical, geomatics or environmental engineering) or from science fields (e.g.: geology, geophysics, physics, geomatics, earth science, or mathematics), typically completing a graduate degree such as M.Eng, M.S., M.Sc. or M.A.Sc. in mining engineering after graduating from a different quantitative undergraduate program.
 The fundamental subjects of mining engineering study usually include:
 In the United States, about 14 universities offer a B.S. degree in mining and mineral engineering. The top rated universities[according to whom?] include Michigan Technological University, South Dakota School of Mines and Technology, Virginia Tech, the University of Kentucky, the University of Arizona,  Pennsylvania State University, and Colorado School of Mines.[6] Most of these universities offer M.S. and Ph.D. degrees.
 In Canada, there are 19 undergraduate degree programs in mining engineering or equivalent.[7] McGill University Faculty of Engineering offers both undergraduate (B.Sc., B.Eng.) and graduate (M.Sc., Ph.D.) degrees in Mining Engineering.[8][9] and the University of British Columbia in Vancouver offers a Bachelor of Applied Science (B.A.Sc.) in Mining Engineering[10] and also graduate degrees (M.A.Sc. or M.Eng and Ph.D.) in Mining Engineering.[11][promotion?]
 In Europe, most programs are integrated (B.S. plus M.S. into one) after the Bologna Process and take five years to complete. In Portugal, the University of Porto offers an M.Eng. in Mining and Geo-Environmental Engineering[12] and in Spain the Technical University of Madrid offers degrees in Mining Engineering with tracks in Mining Technology, Mining Operations, Fuels and Explosives, Metallurgy.[13] In the United Kingdom, The Camborne School of Mines offers a wide choice of BEng and MEng degrees in Mining engineering and other Mining related disciplines. This is done through the University of Exeter.[14] In Romania, the University of Petroșani (formerly known as the Petroşani Institute of Mines, or rarely as the Petroşani Institute of Coal) is the only university that offers a degree in Mining Engineering, Mining Surveying or Underground Mining Constructions, albeit, after the closure of Jiu Valley coal mines, those degrees had fallen out of interest for most high-school graduates.[15]
 In South Africa, leading institutions include the University of Pretoria, offering a 4-year Bachelor of Engineering (B.Eng in Mining Engineering) as well as post-graduate studies in various specialty fields such as rock engineering and numerical modelling, explosives engineering, ventilation engineering, underground mining methods and mine design;[16] and the University of the Witwatersrand offering a 4-year Bachelor of Science in Engineering (B.Sc.(Eng.)) in Mining Engineering[17] as well as graduate programs (M.Sc.(Eng.) and Ph.D.) in Mining Engineering.[18]
 Some mining engineers go on to pursue Doctorate degree programs such as Doctor of Philosophy (Ph.D., DPhil), Doctor of Engineering (D.Eng., Eng.D.). These programs involve a significant original research component and are usually seen as entry points into academia.
 In the Russian Federation, 85 universities across all federal districts are training specialists for the mineral resource sector. 36 universities are training specialists for extracting and processing solid minerals (mining). 49 are training specialists for extracting, primary processing, and transporting liquid and gaseous minerals (oil and gas). 37 are training specialists for geological exploration (applied geology, geological exploration). Among the universities that train specialists for the mineral resource sector, 7 are federal universities, and 13 are national research universities of Russia.[19] Personnel training for the mineral resource sector in Russian universities is currently carried out in the following main specializations of training (specialist's degree): ""Applied Geology"" with the qualification of mining engineer (5 years of training);  ""Geological Exploration"" with the qualification of mining engineer (5 years of training); ""Mining"" with the qualification of mining engineer (5.5 years of training); ""Physical Processes in Mining or Oil and Gas Production"" with the qualification of mining engineer (5.5 years of training); ""Oil and Gas Engineering and Technologies"" with the qualification of mining engineer (5.5 years of training). Universities develop and implement the main professional educational programs of higher education in the directions and specializations of training by forming their profile (name of the program). For example, within the framework of the specialization ""Mining"", universities often adhere to the classical names of the programs ""Open-pit mining"", ""Underground mining of mineral deposits"", ""Surveying"", ""Mineral enrichment"", ""Mining machines"", ""Technological safety and mine rescue"", ""Mine and underground construction"", ""Blasting work"", ""Electrification of the mining industry"", etc. In the last ten years, under the influence of various factors, new names of programs have begun to appear, such as: ""Mining and geological information systems"", ""Mining ecology"", etc. Thus, universities, using their freedom to form new training programs for specialists, can look to the future and try to foresee new professions of mining engineers. After the specialist's degree, you can immediately enroll in postgraduate school (analog of Doctorate degree programs, four years of training).[19]
 Mining salaries are usually determined by the level of skill required, where the position is, and what kind of organization the engineer works for.[citation needed]
 Mining engineers in India earn relatively high salaries in comparison to many other professions,[20] with an average salary of $15,250 [relevant?]. However, in comparison to mining engineer salaries in other regions, such as Canada, the United States, Australia, and the United Kingdom, Indian salaries are low.  In the United States, there are an estimated 6,150 employed mining engineers, with a mean yearly wage of US$103,710.[21]
 As there is considerable capital expenditure required for mining operations, an array of pre-mining activities are normally carried out to assess whether a mining operation would be worthwhile.
 Mineral exploration is the process of locating minerals and assessing their concentrations (grade) and quantities (tonnage), to determine if they are commercially viable ores for mining. Mineral exploration is much more intensive, organized, involved, and professional than mineral prospecting – though it frequently utilizes services exploration, enlisting geologists and surveyors in the necessary pre-feasibility study of the possible mining operation. Mineral exploration and estimation of the reserve can determine the profitability conditions and advocate the form and type of mining required.[citation needed]
 Mineral discovery can be made from research of mineral maps, academic geological reports, or government geological reports. Other sources of information include property assays and local word of mouth. Mineral research usually includes sampling and analyzing sediments, soil, and drill cores. Soil sampling and analysis is one of the most popular mineral exploration tools.[22][23] Other common tools include satellite and aerial surveys or airborne geophysics, including magnetometric and gamma-spectrometric maps.[24] Unless the mineral exploration is done on public property, the owners of the property may play a significant role in the exploration process and might be the original discoverers of the mineral deposit.[25]
 After a prospective mineral is located, the mining geologist and engineer determine the ore properties. This may involve chemical analysis of the ore to determine the sample's composition. Once the mineral properties are identified, the next step is determining the quantity of the ore. This involves determining the extent of the deposit and the purity of the ore.[26] The geologist drills additional core samples to find the limits of the deposit or seam and estimates the quantity of valuable material present.
 Once the mineral identification and reserve amount are reasonably determined, the next step is to determine the feasibility of recovering the mineral deposit. A preliminary survey shortly after the discovery of the deposit examines the market conditions, such as the supply and demand of the mineral, the amount of ore needed to be moved to recover a certain quantity of that mineral, and analysis of the cost associated with the operation. This pre-feasibility study determines whether the mining project is likely to be profitable; if so, a more in-depth analysis of the deposit is undertaken. After the full extent of the ore body is known and has been examined by engineers, the feasibility study examines the cost of initial capital investment, methods of extraction, the cost of operation, an estimated length of time to pay back the investment, the gross revenue and net profit margin, any possible resale price of the land, the total life of the reserve, the full value of the account, investment in future projects, and the property owner or owners' contract. In addition, environmental impact, reclamation, possible legal ramifications, and all government permitting are considered.[27][28] These steps of analysis determine whether the mining company and its investors should proceed with the extraction of the minerals or whether the project should be abandoned. The mining company may decide to sell the rights to the reserve to a third party rather than develop it themselves. Alternatively, the decision to proceed with extraction may be postponed indefinitely until market conditions become favorable.
 Mining engineers working in an established mine may work as an engineer for operations improvement, further mineral exploration, and operation capitalization by determining where in the mine to add equipment and personnel. The engineer may also work in supervision and management or as an equipment and mineral salesperson. In addition to engineering and operations, the mining engineer may work as an environmental, health, and safety manager or design engineer.
 The act of mining requires different methods of extraction depending on the mineralogy, geology, and location of the resources. Characteristics such as mineral hardness, the mineral stratification, and access to that mineral will determine the method of extraction.
 Generally, mining is either done from the surface or underground. Mining can also occur with surface and covert operations on the same reserve. Mining activity varies as to what method is employed to remove the mineral.
 Surface mining comprises 90% of the world's mineral tonnage output. Also called open pit mining, surface mining removes minerals in formations near the surface. Ore retrieval is done by material removal from the land in its natural state. Surface mining often alters the land's characteristics, shape, topography, and geological makeup.
 Surface mining involves quarrying and excavating minerals through cutting, cleaving, and breaking machinery. Explosives are usually used to facilitate breakage. Hard rocks such as limestone, sand, gravel, and slate are generally quarried into benches.
 Using mechanical shovels, track dozers, and front-end loaders, strip mining is done on softer minerals such as clays and phosphate removed. Smoother coal seams can also be extracted this way.
 With placer mining, dredge mining can also remove minerals from the bottoms of lakes, rivers, streams, and even the ocean. In addition, in-situ mining can be done from the surface using dissolving agents on the ore body and retrieving the ore via pumping. The pumped material is then set to leach for further processing. Hydraulic mining is utilized as water jets to wash away either overburden or the ore itself.[29]
 Legal attention to health and safety in mining began in the late 19th century. In the 20th century, it progressed to a comprehensive and stringent codification of enforcement and mandatory health and safety regulation. In whatever role, a mining engineer must follow all mine safety laws.
 The United States Congress, through the passage of the Federal Mine Safety and Health Act of 1977, known as the Miner's Act, created the Mine Safety and Health Administration (MSHA) under the US Department of Labor. The act provides miners with rights against retaliation for reporting violations, consolidated regulation of coal mines with metallic and nonmetallic mines, and created the independent Federal Mine Safety and Health Review Commission to review violations reported to MSHA.[31]
 The act codified in Code of Federal Regulations § 30 (CFR § 30) covers all miners at an active mine. When a mining engineer works at an active mine, they are subject to the same rights, violations, mandatory health and safety regulations, and compulsory training as any other worker at the mine. The mining engineer can be legally identified as a ""miner"".[32]
 The act establishes the rights of miners. The miner may report at any time a hazardous condition and request an inspection. The miners may elect a miners' representative to participate during an inspection, pre-inspection meeting, and post-inspection conference. The miners and miners' representatives shall be paid for their time during all inspections and investigations.[33]
 Waste and uneconomic material generated from the mineral extraction process are the primary source of pollution in the vicinity of mines. Mining activities, by their nature, cause a disturbance of the natural environment in and around which the minerals are located. Mining engineers should therefore be concerned not only with the production and processing of mineral commodities but also with the mitigation of damage to the environment both during and after mining as a result of the change in the mining area.
  This article incorporates text by Petrov, V. L. available under the CC BY 4.0 license.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Mining engineering', 'geologists and surveyors', 'Mining in the engineering discipline is the extraction of minerals from the ground', 'rights, violations, mandatory health and safety regulations', 'environmental impact, reclamation, possible legal ramifications, and all government permitting'], 'answer_start': [], 'answer_end': []}"
"
 Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]
 Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. 
 The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.
 There are many types of robots; they are used in many different environments and for many different uses. Although diverse in application and form, they all share three basic aspects when it comes to their design and construction:
 As more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed ""assembly robots"". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a ""welding robot"" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as ""heavy-duty robots"".[3]
 Current and potential applications include:
 At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[15] 
Potential power sources could be:
 Actuators are the ""muscles"" of a robot, the parts which convert stored energy into movement.[16] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.
 The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.
 Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.
 Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[17] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[18] and walking humanoid robots.[19][20]
 The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[21] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[22] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[23] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.
 Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[24][25][26]
 Muscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[27][28]
 EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[29] and to enable new robots to float,[30] fly, swim or walk.[31]
 Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[32] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[33] These motors are already available commercially and being used on some robots.[34][35]
 Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact ""muscle"" might allow future robots to outrun and outjump humans.[36]
 Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.
 Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[37][38] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.
 Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[39]
 Other common forms of sensing in robotics use lidar, radar, and sonar.[40] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.
 A definition of robotic manipulation has been provided by Matt Mason as: ""manipulation refers to an agent's control of its environment through selective contact"".[41]
 Robots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[42] while the ""arm"" is referred to as a manipulator.[43] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[44]
 One of the most common types of end-effectors are ""grippers"". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[45] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[46] Hands that are of a mid-level complexity include the Delft hand.[47][48] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.
 Suction end-effectors, powered by vacuum generators, are very simple astrictive[49] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.
 Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.
 Suction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.
 Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[50] and the Schunk hand.[51] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[52]
 For simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.
 Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[53] Many different balancing robots have been designed.[54] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[55]
 A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's ""Ballbot"" which is the approximate height and width of a person, and Tohoku Gakuin University's ""BallIP"".[56] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[57]
 Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[58][59] or by rotating the outer shells of the sphere.[60][61] These have also been referred to as an orb bot[62] or a ball bot.[63][64]
 Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.
 Tank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot ""Urbie"".[65]
 Walking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[66] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[67][68] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:
 The zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[69] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[70][71][72] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.
 Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[73] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[74] A quadruped was also demonstrated which could trot, run, pace, and bound.[75] For a full list of these robots, see the MIT Leg Lab Robots page.[76]
 A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[77] This technique was recently demonstrated by Anybots' Dexter Robot,[78] which is so stable, it can even jump.[79] Another example is the TU Delft Flame.
 Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[80][81]
 A modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[82] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.
 BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[83] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.
 Mammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[84] Examples of bat inspired BFRs include Bat Bot[85] and the DALER.[86] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[86] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[84] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[84]
 Bird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[87] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[87] An example of a raptor inspired BFR is the prototype by Savastano et al.[88] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[89]
 Insect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[90] and a dragonfly inspired BFR is the prototype by Hu et al.[91] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[92] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.
 A class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional ""opposed x-wing fashion"" while ""blowing"" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.
 Several snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[93] The Japanese ACM-R5 snake robot[94] can even navigate both on land and in water.[95]
 A small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[96] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[97]
 Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[98] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[99] and Stickybot.[100]
 China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named ""Speedy Freelander"". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[40]
 It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[101] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[102] Notable examples are the Essex University Computer Science Robotic Fish G9,[103] and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion.[104] The Aqua Penguin,[105] designed and built by Festo of Germany, copies the streamlined shape and propulsion by front ""flippers"" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.
 In 2014, iSplash-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[106] This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[107] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[108]
 Sailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos[109] built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.
 The mechanical structure of a robot must be controlled to perform tasks.[110] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[111] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.
 The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[110][111][112]
 At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a ""cognitive"" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[110] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.
 Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[111] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[113] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[112] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[112] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[112] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[114] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was developed by Michael Short and colleagues at the University of Sunderland in the UK in 2000 (pictured right).[112] The robot was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[114][115]
 Control systems may also have varying levels of autonomy.
 Another classification takes into account the interaction between human control and the machine motions.
 
 Computer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.
 In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.
 Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' ""eyes"" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.
 There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.
 Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[118] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.
 The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[119] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[120] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[120]
 Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[121] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[122] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first ""voice input system"" which recognized ""ten digits spoken by a single user with 100% accuracy"" in 1952.[123] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[124] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[125]
 Other hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[126] making it necessary to develop the emotional component of robotic voice through various techniques.[127][128] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[129][130] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[131] It was programmed to teach students in The Bronx, New York.[131]
 One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate ""down the road, then turn right"". It is likely that gestures will make up a part of the interaction between humans and robots.[132] A great many systems have been developed to recognize human hand gestures.[133]
 Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[134] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[135] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[136]
 Artificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[137]
 Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[138] Nevertheless, researchers are trying to create robots which appear to have a personality:[139][140] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[141]
 Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.
 
 Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.
 To describe the level of advancement of a robot, the term ""Generation Robots"" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[142]
 The study of motion can be divided into kinematics and dynamics.[143] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.
 In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for ""optimal"" performance and ways to optimize design, structure, and control of robots must be developed and implemented.
 Open source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.
 Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[144] and to explore the nature of evolution.[145] Because the process often requires many generations of robots to be simulated,[146] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[147] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]
 Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.
 Swarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [118]
 There has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[148]
 The main venues for robotics research are the international conferences ICRA and IROS.
 Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[151] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[152] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.
 Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[153] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation ""over some unspecified number of years"".[154] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[155] In a 2016 article in The Guardian, Stephen Hawking stated ""The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining"".[156]
 According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[157]
 A discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[158]
 The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[159]
 Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the ""man-robot merger"". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic ""human-robot collaboration"".
 In the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[160][161] aiming to protect employees from the risk of working with collaborative robots will have to be revised.
 Great user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[162]
 It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[163] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.
 Robotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.    
 Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.
 Robotics careers are widely predicted to grow during in the 21st century, as robots replace more manual and intellectual human work. Workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.
 In 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.
 Fully autonomous robots only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately, and more reliably than humans. They are also employed in some jobs that are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery,[164] weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.[165]
 
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['cybernetics', 'Wallbot[99] and Stickybot', 'it drives the non-conservative passivity bounds', 'people and other obstacles that are not stationary', 'social policy, not AI, causes unemployment'], 'answer_start': [], 'answer_end': []}"
"
 Automation describes a wide range of technologies that reduce human intervention in processes, mainly by predetermining decision criteria, subprocess relationships, and related actions, as well as embodying those predeterminations in machines.[1][2] Automation has been achieved by various means including mechanical, hydraulic, pneumatic, electrical, electronic devices, and computers, usually in combination. Complicated systems, such as modern factories, airplanes, and ships typically use combinations of all of these techniques. The benefit of automation includes labor savings, reducing waste, savings in electricity costs, savings in material costs, and improvements to quality, accuracy, and precision.
 Automation includes the use of various equipment and control systems such as machinery, processes in factories, boilers,[3] and heat-treating ovens, switching on telephone networks, steering, stabilization of ships, aircraft and other applications and vehicles with reduced human intervention.[4] Examples range from a household thermostat controlling a boiler to a large industrial control system with tens of thousands of input measurements and output control signals. Automation has also found a home in the banking industry. It can range from simple on-off control to multi-variable high-level algorithms in terms of control complexity.
 In the simplest type of an automatic control loop, a controller compares a measured value of a process with a desired set value and processes the resulting error signal to change some input to the process, in such a way that the process stays at its set point despite disturbances. This closed-loop control is an application of negative feedback to a system. The mathematical basis of control theory was begun in the 18th century and advanced rapidly in the 20th. The term automation, inspired by the earlier word automatic (coming from automaton), was not widely used before 1947, when Ford established an automation department.[5] It was during this time that the industry was rapidly adopting feedback controllers, which were introduced in the 1930s.[6]
 The World Bank's World Development Report of 2019 shows evidence that the new industries and jobs in the technology sector outweigh the economic effects of workers being displaced by automation.[7] Job losses and downward mobility blamed on automation have been cited as one of many factors in the resurgence of nationalist, protectionist and populist politics in the US, UK and France, among other countries since the 2010s.[8][9][10][11][12]
 It was a preoccupation of the Greeks and Arabs (in the period between about 300 BC and about 1200 AD) to keep accurate track of time. In Ptolemaic Egypt, about 270 BC, Ctesibius described a float regulator for a water clock, a device not unlike the ball and cock in a modern flush toilet. This was the earliest feedback-controlled mechanism.[13] The appearance of the mechanical clock in the 14th century made the water clock and its feedback control system obsolete.
 The Persian Banū Mūsā brothers, in their Book of Ingenious Devices (850 AD), described a number of automatic controls.[14] Two-step level controls for fluids, a form of discontinuous variable structure controls, were developed by the Banu Musa brothers.[15] They also described a feedback controller.[16][17] The design of feedback control systems up through the Industrial Revolution was by trial-and-error, together with a great deal of engineering intuition. It was not until the mid-19th century that the stability of feedback control systems was analyzed using mathematics, the formal language of automatic control theory.[citation needed]
 The centrifugal governor was invented by Christiaan Huygens in the seventeenth century, and used to adjust the gap between millstones.[18][19][20]
 The introduction of prime movers, or self-driven machines advanced grain mills, furnaces, boilers, and the steam engine created a new requirement for automatic control systems including temperature regulators (invented in 1624; see Cornelius Drebbel), pressure regulators (1681), float regulators (1700) and speed control devices. Another control mechanism was used to tent the sails of windmills. It was patented by Edmund Lee in 1745.[21] Also in 1745, Jacques de Vaucanson invented the first automated loom. Around 1800, Joseph Marie Jacquard created a punch-card system to program looms.[22]
 In 1771 Richard Arkwright invented the first fully automated spinning mill driven by water power, known at the time as the water frame.[23] An automatic flour mill was developed by Oliver Evans in 1785, making it the first completely automated industrial process.[24][25]
 A centrifugal governor was used by Mr. Bunce of England in 1784 as part of a model steam crane.[26][27] The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one at a flour mill Boulton & Watt were building.[21] The governor could not actually hold a set speed; the engine would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped with this governor were not suitable for operations requiring constant speed, such as cotton spinning.[21]
 Several improvements to the governor, plus improvements to valve cut-off timing on the steam engine, made the engine suitable for most industrial uses before the end of the 19th century. Advances in the steam engine stayed well ahead of science, both thermodynamics and control theory.[21] The governor received relatively little scientific attention until James Clerk Maxwell published a paper that established the beginning of a theoretical basis for understanding control theory.
 Relay logic was introduced with factory electrification, which underwent rapid adaption from 1900 through the 1920s. Central electric power stations were also undergoing rapid growth and the operation of new high-pressure boilers, steam turbines and electrical substations created a large demand for instruments and controls. Central control rooms became common in the 1920s, but as late as the early 1930s, most process controls were on-off. Operators typically monitored charts drawn by recorders that plotted data from instruments. To make corrections, operators manually opened or closed valves or turned switches on or off. Control rooms also used color-coded lights to send signals to workers in the plant to manually make certain changes.[28]
 The development of the electronic amplifier during the 1920s, which was important for long-distance telephony, required a higher signal-to-noise ratio, which was solved by negative feedback noise cancellation. This and other telephony applications contributed to the control theory. In the 1940s and 1950s, German mathematician Irmgard Flügge-Lotz developed the theory of discontinuous automatic controls, which found military applications during the Second World War to fire control systems and aircraft navigation systems.[6]
 Controllers, which were able to make calculated changes in response to deviations from a set point rather than on-off control, began being introduced in the 1930s. Controllers allowed manufacturing to continue showing productivity gains to offset the declining influence of factory electrification.[29]
 Factory productivity was greatly increased by electrification in the 1920s. U.S. manufacturing productivity growth fell from 5.2%/yr 1919–29 to 2.76%/yr 1929–41. Alexander Field notes that spending on non-medical instruments increased significantly from 1929 to 1933 and remained strong thereafter.[29]
 The First and Second World Wars saw major advancements in the field of mass communication and signal processing. Other key advances in automatic controls include differential equations, stability theory and system theory (1938), frequency domain analysis (1940), ship control (1950), and stochastic analysis (1941).
 Starting in 1958, various systems based on solid-state[30][31] digital logic modules for hard-wired programmed logic controllers (the predecessors of programmable logic controllers [PLC]) emerged to replace electro-mechanical relay logic in industrial control systems for process control and automation, including early Telefunken/AEG Logistat, Siemens Simatic, Philips/Mullard/Valvo [de] Norbit, BBC Sigmatronic, ACEC Logacec, Akkord [de] Estacord, Krone Mibakron, Bistat, Datapac, Norlog, SSR, or Procontic systems.[30][32][33][34][35][36]
 In 1959 Texaco's Port Arthur Refinery became the first chemical plant to use digital control.[37]
Conversion of factories to digital control began to spread rapidly in the 1970s as the price of computer hardware fell.
 The automatic telephone switchboard was introduced in 1892 along with dial telephones. By 1929, 31.9% of the Bell system was automatic.[38]: 158   Automatic telephone switching originally used vacuum tube amplifiers and electro-mechanical switches, which consumed a large amount of electricity. Call volume eventually grew so fast that it was feared the telephone system would consume all electricity production, prompting Bell Labs to begin research on the transistor.[39]
 The logic performed by telephone switching relays was the inspiration for the digital computer.
The first commercially successful glass bottle-blowing machine was an automatic model introduced in 1905.[40] The machine, operated by a two-man crew working 12-hour shifts, could produce 17,280 bottles in 24 hours, compared to 2,880 bottles made by a crew of six men and boys working in a shop for a day. The cost of making bottles by machine was 10 to 12 cents per gross compared to $1.80 per gross by the manual glassblowers and helpers.
 Sectional electric drives were developed using control theory. Sectional electric drives are used on different sections of a machine where a precise differential must be maintained between the sections. In steel rolling, the metal elongates as it passes through pairs of rollers, which must run at successively faster speeds. In paper making paper, the sheet shrinks as it passes around steam-heated drying arranged in groups, which must run at successively slower speeds. The first application of a sectional electric drive was on a paper machine in 1919.[41] One of the most important developments in the steel industry during the 20th century was continuous wide strip rolling, developed by Armco in 1928.[42]
 Before automation, many chemicals were made in batches. In 1930, with the widespread use of instruments and the emerging use of controllers, the founder of Dow Chemical Co. was advocating continuous production.[43]
 Self-acting machine tools that displaced hand dexterity so they could be operated by boys and unskilled laborers were developed by James Nasmyth in the 1840s.[44] Machine tools were automated with Numerical control (NC) using punched paper tape in the 1950s. This soon evolved into computerized numerical control (CNC).
 Today extensive automation is practiced in practically every type of manufacturing and assembly process. Some of the larger processes include electrical power generation, oil refining, chemicals, steel mills, plastics, cement plants, fertilizer plants, pulp and paper mills, automobile and truck assembly, aircraft production, glass manufacturing, natural gas separation plants, food and beverage processing, canning and bottling and manufacture of various kinds of parts. Robots are especially useful in hazardous applications like automobile spray painting. Robots are also used to assemble electronic circuit boards. Automotive welding is done with robots and automatic welders are used in applications like pipelines.
 With the advent of the space age in 1957, controls design, particularly in the United States, turned away from the frequency-domain techniques of classical control theory and backed into the differential equation techniques of the late 19th century, which were couched in the time domain. During the 1940s and 1950s, German mathematician Irmgard Flugge-Lotz developed the theory of discontinuous automatic control, which became widely used in hysteresis control systems such as navigation systems, fire-control systems, and electronics. Through Flugge-Lotz and others, the modern era saw time-domain design for nonlinear systems (1961), navigation (1960), optimal control and estimation theory (1962), nonlinear control theory (1969), digital control and filtering theory (1974), and the personal computer (1983).
 Perhaps the most cited advantage of automation in industry is that it is associated with faster production and cheaper labor costs. Another benefit could be that it replaces hard, physical, or monotonous work.[45] Additionally, tasks that take place in hazardous environments or that are otherwise beyond human capabilities can be done by machines, as machines can operate even under extreme temperatures or in atmospheres that are radioactive or toxic. They can also be maintained with simple quality checks. However, at the time being, not all tasks can be automated, and some tasks are more expensive to automate than others. Initial costs of installing the machinery in factory settings are high, and failure to maintain a system could result in the loss of the product itself.
 Moreover, some studies seem to indicate that industrial automation could impose ill effects beyond operational concerns, including worker displacement due to systemic loss of employment and compounded environmental damage; however, these findings are both convoluted and controversial in nature, and could potentially be circumvented.[46]
 The main advantages of automation are:
 Automation primarily describes machines replacing human action, but it is also loosely associated with mechanization, machines replacing human labor. Coupled with mechanization, extending human capabilities in terms of size, strength, speed, endurance, visual range & acuity, hearing frequency & precision, electromagnetic sensing & effecting, etc., advantages include:[48]
 The main disadvantages of automation are:
 The paradox of automation says that the more efficient the automated system, the more crucial the human contribution of the operators. Humans are less involved, but their involvement becomes more critical. Lisanne Bainbridge, a cognitive psychologist, identified these issues notably in her widely cited paper ""Ironies of Automation.""[49] If an automated system has an error, it will multiply that error until it is fixed or shut down. This is where human operators come in.[50] A fatal example of this was Air France Flight 447, where a failure of automation put the pilots into a manual situation they were not prepared for.[51]
 Many roles for humans in industrial processes presently lie beyond the scope of automation. Human-level pattern recognition, language comprehension, and language production ability are well beyond the capabilities of modern mechanical and computer systems (but see Watson computer). Tasks requiring subjective assessment or synthesis of complex sensory data, such as scents and sounds, as well as high-level tasks such as strategic planning, currently require human expertise. In many cases, the use of humans is more cost-effective than mechanical approaches even where the automation of industrial tasks is possible. Therefore, algorithmic management as the digital rationalization of human labor instead of its substitution has emerged as an alternative technological strategy.[53] Overcoming these obstacles is a theorized path to post-scarcity economics.[54]
 Increased automation often causes workers to feel anxious about losing their jobs as technology renders their skills or experience unnecessary. Early in the Industrial Revolution, when inventions like the steam engine were making some job categories expendable, workers forcefully resisted these changes. Luddites, for instance, were English textile workers who protested the introduction of weaving machines by destroying them.[55] More recently, some residents of Chandler, Arizona, have slashed tires and pelted rocks at self-driving car, in protest over the cars' perceived threat to human safety and job prospects.[56]
 The relative anxiety about automation reflected in opinion polls seems to correlate closely with the strength of organized labor in that region or nation. For example, while a study by the Pew Research Center indicated that 72% of Americans are worried about increasing automation in the workplace, 80% of Swedes see automation and artificial intelligence (AI) as a good thing, due to the country's still-powerful unions and a more robust national safety net.[57]
 In the U.S., 47% of all current jobs have the potential to be fully automated by 2033, according to the research of experts Carl Benedikt Frey and Michael Osborne. Furthermore, wages and educational attainment appear to be strongly negatively correlated with an occupation's risk of being automated.[58] Even highly skilled professional jobs like a lawyer, doctor, engineer, journalist are at risk of automation.[59]
 Prospects are particularly bleak for occupations that do not presently require a university degree, such as truck driving.[60] Even in high-tech corridors like Silicon Valley, concern is spreading about a future in which a sizable percentage of adults have little chance of sustaining gainful employment.[61] ""In The Second Machine Age, Erik Brynjolfsson and Andrew McAfee argue that ""...there's never been a better time to be a worker with special skills or the right education, because these people can use technology to create and capture value. However, there's never been a worse time to be a worker with only 'ordinary' skills and abilities to offer, because computers, robots, and other digital technologies are acquiring these skills and abilities at an extraordinary rate.""[62] As the example of Sweden suggests, however, the transition to a more automated future need not inspire panic, if there is sufficient political will to promote the retraining of workers whose positions are being rendered obsolete.
 According to a 2020 study in the Journal of Political Economy, automation has robust negative effects on employment and wages: ""One more robot per thousand workers reduces the employment-to-population ratio by 0.2 percentage points and wages by 0.42%.""[63]
 Research by Carl Benedikt Frey and Michael Osborne of the Oxford Martin School argued that employees engaged in ""tasks following well-defined procedures that can easily be performed by sophisticated algorithms"" are at risk of displacement, and 47% of jobs in the US were at risk. The study, released as a working paper in 2013 and published in 2017, predicted that automation would put low-paid physical occupations most at risk, by surveying a group of colleagues on their opinions.[64] However, according to a study published in McKinsey Quarterly[65] in 2015 the impact of computerization in most cases is not the replacement of employees but the automation of portions of the tasks they perform.[66] The methodology of the McKinsey study has been heavily criticized for being intransparent and relying on subjective assessments.[67] The methodology of Frey and Osborne has been subjected to criticism, as lacking evidence, historical awareness, or credible methodology.[68][69] Additionally, the Organisation for Economic Co-operation and Development (OECD) found that across the 21 OECD countries, 9% of jobs are automatable.[70]
 The Obama administration pointed out that every 3 months ""about 6 percent of jobs in the economy are destroyed by shrinking or closing businesses, while a slightly larger percentage of jobs are added.""[71] A recent MIT economics study of automation in the U.S. from 1990 to 2007 found that there may be a negative impact on employment and wages when robots are introduced to an industry. When one robot is added per one thousand workers, the employment to population ratio decreases between 0.18 and 0.34 percentages and wages are reduced by 0.25–0.5 percentage points. During the time period studied, the US did not have many robots in the economy which restricts the impact of automation. However, automation is expected to triple (conservative estimate) or quadruple (a generous estimate) leading these numbers to become substantially higher.[72]
 Based on a formula by Gilles Saint-Paul, an economist at Toulouse 1 University, the demand for unskilled human capital declines at a slower rate than the demand for skilled human capital increases.[73] In the long run and for society as a whole it has led to cheaper products, lower average work hours, and new industries forming (i.e., robotics industries, computer industries, design industries). These new industries provide many high salary skill-based jobs to the economy. By 2030, between 3 and 14 percent of the global workforce will be forced to switch job categories due to automation eliminating jobs in an entire sector. While the number of jobs lost to automation is often offset by jobs gained from technological advances, the same type of job loss is not the same one replaced and that leading to increasing unemployment in the lower-middle class. This occurs largely in the US and developed countries where technological advances contribute to higher demand for highly skilled labor but demand for middle-wage labor continues to fall. Economists call this trend ""income polarization"" where unskilled labor wages are driven down and skilled labor is driven up and it is predicted to continue in developed economies.[74]
 Unemployment is becoming a problem in the U.S. due to the exponential growth rate of automation and technology. According to Kim, Kim, and Lee (2017:1), ""[a] seminal study by Frey and Osborne in 2013 predicted that 47% of the 702 examined occupations in the U.S. faced a high risk of decreased employment rate within the next 10–25 years as a result of computerization."" As many jobs are becoming obsolete, which is causing job displacement, one possible solution would be for the government to assist with a universal basic income (UBI) program. UBI would be a guaranteed, non-taxed income of around 1000 dollars per month, paid to all U.S. citizens over the age of 21. UBI would help those who are displaced take on jobs that pay less money and still afford to get by. It would also give those that are employed with jobs that are likely to be replaced by automation and technology extra money to spend on education and training on new demanding employment skills. UBI, however, should be seen as a short-term solution as it doesn't fully address the issue of income inequality which will be exacerbated by job displacement.
 Lights-out manufacturing is a production system with no human workers, to eliminate labor costs.
 Lights out manufacturing grew in popularity in the U.S. when General Motors in 1982 implemented humans ""hands-off"" manufacturing to ""replace risk-averse bureaucracy with automation and robots"". However, the factory never reached full ""lights out"" status.[75]
 The expansion of lights out manufacturing requires:[76]
 The costs of automation to the environment are different depending on the technology, product or engine automated. There are automated engines that consume more energy resources from the Earth in comparison with previous engines and vice versa.[citation needed] Hazardous operations, such as oil refining, the manufacturing of industrial chemicals, and all forms of metal working, were always early contenders for automation.[dubious  – discuss][citation needed]
 The automation of vehicles could prove to have a substantial impact on the environment, although the nature of this impact could be beneficial or harmful depending on several factors. Because automated vehicles are much less likely to get into accidents compared to human-driven vehicles, some precautions built into current models (such as anti-lock brakes or laminated glass) would not be required for self-driving versions. Removing these safety features would also significantly reduce the weight of the vehicle, thus increasing fuel economy and reducing emissions per mile. Self-driving vehicles are also more precise concerning acceleration and breaking, and this could contribute to reduced emissions. Self-driving cars could also potentially utilize fuel-efficient features such as route mapping that can calculate and take the most efficient routes. Despite this potential to reduce emissions, some researchers theorize that an increase in the production of self-driving cars could lead to a boom in vehicle ownership and use. This boom could potentially negate any environmental benefits of self-driving cars if a large enough number of people begin driving personal vehicles more frequently.[77]
 Automation of homes and home appliances is also thought to impact the environment, but the benefits of these features are also questioned. A study of energy consumption of automated homes in Finland showed that smart homes could reduce energy consumption by monitoring levels of consumption in different areas of the home and adjusting consumption to reduce energy leaks (e.g. automatically reducing consumption during the nighttime when activity is low). This study, along with others, indicated that the smart home's ability to monitor and adjust consumption levels would reduce unnecessary energy usage. However, new research suggests that smart homes might not be as efficient as non-automated homes. A more recent study has indicated that, while monitoring and adjusting consumption levels do decrease unnecessary energy use, this process requires monitoring systems that also consume a significant amount of energy. This study suggested that the energy required to run these systems is so much so that it negates any benefits of the systems themselves, resulting in little to no ecological benefit.[78]
 Another major shift in automation is the increased demand for flexibility and convertibility in manufacturing processes. Manufacturers are increasingly demanding the ability to easily switch from manufacturing Product A to manufacturing Product B without having to completely rebuild the production lines. Flexibility and distributed processes have led to the introduction of Automated Guided Vehicles with Natural Features Navigation.
 Digital electronics helped too. Former analog-based instrumentation was replaced by digital equivalents which can be more accurate and flexible, and offer greater scope for more sophisticated configuration, parametrization, and operation. This was accompanied by the fieldbus revolution which provided a networked (i.e. a single cable) means of communicating between control systems and field-level instrumentation, eliminating hard-wiring.
 Discrete manufacturing plants adopted these technologies fast. The more conservative process industries with their longer plant life cycles have been slower to adopt and analog-based measurement and control still dominate. The growing use of Industrial Ethernet on the factory floor is pushing these trends still further, enabling manufacturing plants to be integrated more tightly within the enterprise, via the internet if necessary. Global competition has also increased demand for Reconfigurable Manufacturing Systems.[79]
 Engineers can now have numerical control over automated devices. The result has been a rapidly expanding range of applications and human activities. Computer-aided technologies (or CAx) now serve as the basis for mathematical and organizational tools used to create complex systems. Notable examples of CAx include computer-aided design (CAD software) and computer-aided manufacturing (CAM software). The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry.[80]
 Information technology, together with industrial machinery and processes, can assist in the design, implementation, and monitoring of control systems. One example of an industrial control system is a programmable logic controller (PLC). PLCs are specialized hardened computers which are frequently used to synchronize the flow of inputs from (physical) sensors and events with the flow of outputs to actuators and events.[81]
 Human-machine interfaces (HMI) or computer human interfaces (CHI), formerly known as man-machine interfaces, are usually employed to communicate with PLCs and other computers. Service personnel who monitor and control through HMIs can be called by different names. In the industrial process and manufacturing environments, they are called operators or something similar. In boiler houses and central utility departments, they are called stationary engineers.[82]
 Different types of automation tools exist:
 Host simulation software (HSS) is a commonly used testing tool that is used to test the equipment software. HSS is used to test equipment performance concerning factory automation standards (timeouts, response time, processing time).[83]
 Cognitive automation, as a subset of AI, is an emerging genus of automation enabled by cognitive computing. Its primary concern is the automation of clerical tasks and workflows that consist of structuring unstructured data.[84] Cognitive automation relies on multiple disciplines: natural language processing, real-time computing, machine learning algorithms, big data analytics, and evidence-based learning.[85]
 According to Deloitte, cognitive automation enables the replication of human tasks and judgment ""at rapid speeds and considerable scale.""[86] Such tasks include:
 Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate in 3D modeling.[87]  Ai CAD libraries could also be developed using linked open data of schematics  and diagrams.[88]  Ai CAD assistants are used as tools to help streamline workflow.[89]
 Technologies like solar panels, wind turbines, and other renewable energy sources—together with smart grids, micro-grids, battery storage—can automate power production.
 Many agricultural operations are automated with machinery and equipment to improve their diagnosis, decision-making and/or performing. Agricultural automation can relieve the drudgery of agricultural work, improve the timeliness and precision of agricultural operations, raise productivity and resource-use efficiency, build resilience, and improve food quality and safety.[90] Increased productivity can free up labour, allowing agricultural households to spend more time elsewhere.[91]
 The technological evolution in agriculture has resulted in progressive shifts to digital equipment and robotics.[90] Motorized mechanization using engine power automates the performance of agricultural operations such as ploughing and milking.[92] With digital automation technologies, it also becomes possible to automate diagnosis and decision-making of agricultural operations.[90] For example, autonomous crop robots can harvest and seed crops, while drones can gather information to help automate input application.[91] Precision agriculture often employs such automation technologies[91]
 Motorized mechanization has generally increased in recent years.[93] Sub-Saharan Africa is the only region where the adoption of motorized mechanization has stalled over the past decades.[94][91]
 Automation technologies are increasingly used for managing livestock, though evidence on adoption is lacking. Global automatic milking system sales have increased over recent years,[95] but adoption is likely mostly in Northern Europe,[96] and likely almost absent in low- and middle-income countries.[97][91] Automated feeding machines for both cows and poultry also exist, but data and evidence regarding their adoption trends and drivers is likewise scarce.[91][93]
 Many supermarkets and even smaller stores are rapidly introducing self-checkout systems reducing the need for employing checkout workers. In the U.S., the retail industry employs 15.9 million people as of 2017 (around 1 in 9 Americans in the workforce). Globally, an estimated 192 million workers could be affected by automation according to research by Eurasia Group.[98]
 Online shopping could be considered a form of automated retail as the payment and checkout are through an automated online transaction processing system, with the share of online retail accounting jumping from 5.1% in 2011 to 8.3% in 2016. [citation needed] However, two-thirds of books, music, and films are now purchased online. In addition, automation and online shopping could reduce demands for shopping malls, and retail property, which in the USA is currently estimated to account for 31% of all commercial property or around 7 billion square feet (650 million square metres). Amazon has gained much of the growth in recent years for online shopping, accounting for half of the growth in online retail in 2016.[98] Other forms of automation can also be an integral part of online shopping, for example, the deployment of automated warehouse robotics such as that applied by Amazon using Kiva Systems.
 The food retail industry has started to apply automation to the ordering process; McDonald's has introduced touch screen ordering and payment systems in many of its restaurants, reducing the need for as many cashier employees.[99] The University of Texas at Austin has introduced fully automated cafe retail locations.[100] Some Cafes and restaurants have utilized mobile and tablet ""apps"" to make the ordering process more efficient by customers ordering and paying on their device.[101] Some restaurants have automated food delivery to tables of customers using a Conveyor belt system. The use of robots is sometimes employed to replace waiting staff.[102]
 Automation in construction is the combination of methods, processes, and systems that allow for greater machine autonomy in construction activities. Construction automation may have multiple goals, including but not limited to, reducing jobsite injuries, decreasing activity completion times, and assisting with quality control and quality assurance.[103]
  Automated mining involves the removal of human labor from the mining process.[104] The mining industry is currently in the transition towards automation. Currently, it can still require a large amount of human capital, particularly in the third world where labor costs are low so there is less incentive for increasing efficiency through automation.
 The Defense Advanced Research Projects Agency (DARPA) started the research and development of automated visual surveillance and monitoring (VSAM) program, between 1997 and 1999, and airborne video surveillance (AVS) programs, from 1998 to 2002. Currently, there is a major effort underway in the vision community to develop a fully-automated tracking surveillance system. Automated video surveillance monitors people and vehicles in real-time within a busy environment. Existing automated surveillance systems are based on the environment they are primarily designed to observe, i.e., indoor, outdoor or airborne, the number of sensors that the automated system can handle and the mobility of sensors, i.e., stationary camera vs. mobile camera. The purpose of a surveillance system is to record properties and trajectories of objects in a given area, generate warnings or notify the designated authorities in case of occurrence of particular events.[105]
 
As demands for safety and mobility have grown and technological possibilities have multiplied, interest in automation has grown. Seeking to accelerate the development and introduction of fully automated vehicles and highways, the U.S. Congress authorized more than $650 million over six years for intelligent transport systems (ITS) and demonstration projects in the 1991 Intermodal Surface Transportation Efficiency Act (ISTEA). Congress legislated in ISTEA that:[106] [T]he Secretary of Transportation shall develop an automated highway and vehicle prototype from which future fully automated intelligent vehicle-highway systems can be developed. Such development shall include research in human factors to ensure the success of the man-machine relationship. The goal of this program is to have the first fully automated highway roadway or an automated test track in operation by 1997. This system shall accommodate the installation of equipment in new and existing motor vehicles. Full automation commonly defined as requiring no control or very limited control by the driver; such automation would be accomplished through a combination of sensor, computer, and communications systems in vehicles and along the roadway. Fully automated driving would, in theory, allow closer vehicle spacing and higher speeds, which could enhance traffic capacity in places where additional road building is physically impossible, politically unacceptable, or prohibitively expensive. Automated controls also might enhance road safety by reducing the opportunity for driver error, which causes a large share of motor vehicle crashes. Other potential benefits include improved air quality (as a result of more-efficient traffic flows), increased fuel economy, and spin-off technologies generated during research and development related to automated highway systems.[107]
 Automated waste collection trucks prevent the need for as many workers as well as easing the level of labor required to provide the service.[108]
 Business process automation (BPA) is the technology-enabled automation of complex business processes.[109] It can help to streamline a business for simplicity, achieve digital transformation, increase service quality, improve service delivery or contain costs. BPA consists of integrating applications, restructuring labor resources and using software applications throughout the organization. Robotic process automation (RPA; or RPAAI for self-guided RPA 2.0) is an emerging field within BPA and uses AI. BPAs can be implemented in a number of business areas including marketing, sales and workflow.
 Home automation (also called domotics) designates an emerging practice of increased automation of household appliances and features in residential dwellings, particularly through electronic means that allow for things impracticable, overly expensive or simply not possible in recent past decades. The rise in the usage of home automation solutions has taken a turn reflecting the increased dependency of people on such automation solutions. However, the increased comfort that gets added through these automation solutions is remarkable.[110]
 Automation is essential for many scientific and clinical applications.[111] Therefore, automation has been extensively employed in laboratories. From as early as 1980 fully automated laboratories have already been working.[112] However, automation has not become widespread in laboratories due to its high cost. This may change with the ability of integrating low-cost devices with standard laboratory equipment.[113][114] Autosamplers are common devices used in laboratory automation.
 Logistics automation is the application of computer software or automated machinery to improve the efficiency of logistics operations. Typically this refers to operations within a warehouse or distribution center, with broader tasks undertaken by supply chain engineering systems and enterprise resource planning systems.
 Industrial automation deals primarily with the automation of manufacturing, quality control, and material handling processes. General-purpose controllers for industrial processes include programmable logic controllers, stand-alone I/O modules, and computers. Industrial automation is to replace the human action and manual command-response activities with the use of mechanized equipment and logical programming commands. One trend is increased use of machine vision[115] to provide automatic inspection and robot guidance functions, another is a continuing increase in the use of robots. Industrial automation is simply required in industries.
 The rise of industrial automation is directly tied to the ""Fourth Industrial Revolution"", which is better known now as Industry 4.0. Originating from Germany, Industry 4.0 encompasses numerous devices, concepts, and machines,[116] as well as the advancement of the industrial internet of things (IIoT). An ""Internet of Things is a seamless integration of diverse physical objects in the Internet through a virtual representation.""[117] These new revolutionary advancements have drawn attention to the world of automation in an entirely new light and shown ways for it to grow to increase productivity and efficiency in machinery and manufacturing facilities. Industry 4.0 works with the IIoT and software/hardware to connect in a way that (through communication technologies) add enhancements and improve manufacturing processes. Being able to create smarter, safer, and more advanced manufacturing is now possible with these new technologies. It opens up a manufacturing platform that is more reliable, consistent, and efficient than before. Implementation of systems such as SCADA is an example of software that takes place in Industrial Automation today. SCADA is a supervisory data collection software, just one of the many used in Industrial Automation.[118] Industry 4.0 vastly covers many areas in manufacturing and will continue to do so as time goes on.[116]
 Industrial robotics is a sub-branch in industrial automation that aids in various manufacturing processes. Such manufacturing processes include machining, welding, painting, assembling and material handling to name a few.[119] Industrial robots use various mechanical, electrical as well as software systems to allow for high precision, accuracy and speed that far exceed any human performance. The birth of industrial robots came shortly after World War II as the U.S. saw the need for a quicker way to produce industrial and consumer goods.[120] Servos, digital logic and solid-state electronics allowed engineers to build better and faster systems and over time these systems were improved and revised to the point where a single robot is capable of running 24 hours a day with little or no maintenance. In 1997, there were 700,000 industrial robots in use, the number has risen to 1.8M in 2017[121] In recent years, AI with robotics is also used in creating an automatic labeling solution, using robotic arms as the automatic label applicator, and AI for learning and detecting the products to be labelled.[122]
 Industrial automation incorporates programmable logic controllers in the manufacturing process. Programmable logic controllers (PLCs) use a processing system which allows for variation of controls of inputs and outputs using simple programming. PLCs make use of programmable memory, storing instructions and functions like logic, sequencing, timing, counting, etc. Using a logic-based language, a PLC can receive a variety of inputs and return a variety of logical outputs, the input devices being sensors and output devices being motors, valves, etc. PLCs are similar to computers, however, while computers are optimized for calculations, PLCs are optimized for control tasks and use in industrial environments. They are built so that only basic logic-based programming knowledge is needed and to handle vibrations, high temperatures, humidity, and noise. The greatest advantage PLCs offer is their flexibility. With the same basic controllers, a PLC can operate a range of different control systems. PLCs make it unnecessary to rewire a system to change the control system. This flexibility leads to a cost-effective system for complex and varied control systems.[123]
 PLCs can range from small ""building brick"" devices with tens of I/O in a housing integral with the processor, to large rack-mounted modular devices with a count of thousands of I/O, and which are often networked to other PLC and SCADA systems.
 They can be designed for multiple arrangements of digital and analog inputs and outputs (I/O), extended temperature ranges, immunity to electrical noise, and resistance to vibration and impact. Programs to control machine operation are typically stored in battery-backed-up or non-volatile memory.
 It was from the automotive industry in the USA that the PLC was born. Before the PLC, control, sequencing, and safety interlock logic for manufacturing automobiles was mainly composed of relays, cam timers, drum sequencers, and dedicated closed-loop controllers. Since these could number in the hundreds or even thousands, the process for updating such facilities for the yearly model change-over was very time-consuming and expensive, as electricians needed to individually rewire the relays to change their operational characteristics.
 When digital computers became available, being general-purpose programmable devices, they were soon applied to control sequential and combinatorial logic in industrial processes. However, these early computers required specialist programmers and stringent operating environmental control for temperature, cleanliness, and power quality. To meet these challenges, the PLC was developed with several key attributes. It would tolerate the shop-floor environment, it would support discrete (bit-form) input and output in an easily extensible manner, it would not require years of training to use, and it would permit its operation to be monitored. Since many industrial processes have timescales easily addressed by millisecond response times, modern (fast, small, reliable) electronics greatly facilitate building reliable controllers, and performance could be traded off for reliability.[124]
 Agent-assisted automation refers to automation used by call center agents to handle customer inquiries. The key benefit of agent-assisted automation is compliance and error-proofing. Agents are sometimes not fully trained or they forget or ignore key steps in the process. The use of automation ensures that what is supposed to happen on the call actually does, every time. There are two basic types: desktop automation and automated voice solutions.
 Fundamentally, there are two types of control loop: open-loop control (feedforward), and closed-loop control (feedback).
 In open-loop control, the control action from the controller is independent of the ""process output"" (or ""controlled process variable""). A good example of this is a central heating boiler controlled only by a timer, so that heat is applied for a constant time, regardless of the temperature of the building. The control action is the switching on/off of the boiler, but the controlled variable should be the building temperature, but is not because this is open-loop control of the boiler, which does not give closed-loop control of the temperature.
 In closed loop control, the control action from the controller is dependent on the process output. In the case of the boiler analogy this would include a thermostat to monitor the building temperature, and thereby feed back a signal to ensure the controller maintains the building at the temperature set on the thermostat. A closed loop controller therefore has a feedback loop which ensures the controller exerts a control action to give a process output the same as the ""reference input"" or  ""set point"". For this reason, closed loop controllers are also called feedback controllers.[125]
 The definition of a closed loop control system according to the British Standards Institution is ""a control system possessing monitoring feedback, the deviation signal formed as a result of this feedback being used to control the action of a final control element in such a way as to tend to reduce the deviation to zero.""[126]
 One of the simplest types of control is on-off control. An example is a thermostat used on household appliances which either open or close an electrical contact. (Thermostats were originally developed as true feedback-control mechanisms rather than the on-off common household appliance thermostat.)
 Sequence control, in which a programmed sequence of discrete operations is performed, often based on system logic that involves system states. An elevator control system is an example of sequence control.
 A proportional–integral–derivative controller (PID controller) is a control loop feedback mechanism (controller) widely used in industrial control systems.
 In a PID loop, the controller continuously calculates an error value 



e
(
t
)


{\displaystyle e(t)}

 as the difference between a desired setpoint and a measured process variable and applies a correction based on proportional, integral, and derivative terms, respectively (sometimes denoted P, I, and D) which give their name to the controller type.
 The theoretical understanding and application date from the 1920s, and they are implemented in nearly all analog control systems; originally in mechanical controllers, and then using discrete electronics and latterly in industrial process computers.
 Sequential control may be either to a fixed sequence or to a logical one that will perform different actions depending on various system states. An example of an adjustable but otherwise fixed sequence is a timer on a lawn sprinkler.
 States refer to the various conditions that can occur in a use or sequence scenario of the system. An example is an elevator, which uses logic based on the system state to perform certain actions in response to its state and operator input. For example, if the operator presses the floor n button, the system will respond depending on whether the elevator is stopped or moving, going up or down, or if the door is open or closed, and other conditions.[128]
 Early development of sequential control was relay logic, by which electrical relays engage electrical contacts which either start or interrupt power to a device. Relays were first used in telegraph networks before being developed for controlling other devices, such as when starting and stopping industrial-sized electric motors or opening and closing solenoid valves. Using relays for control purposes allowed event-driven control, where actions could be triggered out of sequence, in response to external events. These were more flexible in their response than the rigid single-sequence cam timers. More complicated examples involved maintaining safe sequences for devices such as swing bridge controls, where a lock bolt needed to be disengaged before the bridge could be moved, and the lock bolt could not be released until the safety gates had already been closed.
 The total number of relays and cam timers can number into the hundreds or even thousands in some factories. Early programming techniques and languages were needed to make such systems manageable, one of the first being ladder logic, where diagrams of the interconnected relays resembled the rungs of a ladder. Special computers called programmable logic controllers were later designed to replace these collections of hardware with a single, more easily re-programmed unit.
 In a typical hard-wired motor start and stop circuit (called a control circuit) a motor is started by pushing a ""Start"" or ""Run"" button that activates a pair of electrical relays. The ""lock-in"" relay locks in contacts that keep the control circuit energized when the push-button is released. (The start button is a normally open contact and the stop button is a normally closed contact.) Another relay energizes a switch that powers the device that throws the motor starter switch (three sets of contacts for three-phase industrial power) in the main power circuit. Large motors use high voltage and experience high in-rush current, making speed important in making and breaking contact. This can be dangerous for personnel and property with manual switches. The ""lock-in"" contacts in the start circuit and the main power contacts for the motor are held engaged by their respective electromagnets until a ""stop"" or ""off"" button is pressed, which de-energizes the lock in relay.[129]
 Commonly interlocks are added to a control circuit. Suppose that the motor in the example is powering machinery that has a critical need for lubrication. In this case, an interlock could be added to ensure that the oil pump is running before the motor starts. Timers, limit switches, and electric eyes are other common elements in control circuits.
 Solenoid valves are widely used on compressed air or hydraulic fluid for powering actuators on mechanical components. While motors are used to supply continuous rotary motion, actuators are typically a better choice for intermittently creating a limited range of movement for a mechanical component, such as moving various mechanical arms, opening or closing valves, raising heavy press-rolls, applying pressure to presses.
 Computers can perform both sequential control and feedback control, and typically a single computer will do both in an industrial application. Programmable logic controllers (PLCs) are a type of special-purpose microprocessor that replaced many hardware components such as timers and drum sequencers used in relay logic–type systems. General-purpose process control computers have increasingly replaced stand-alone controllers, with a single computer able to perform the operations of hundreds of controllers. Process control computers can process data from a network of PLCs, instruments, and controllers to implement typical (such as PID) control of many individual variables or, in some cases, to implement complex control algorithms using multiple inputs and mathematical manipulations. They can also analyze data and create real-time graphical displays for operators and run reports for operators, engineers, and management.
 Control of an automated teller machine (ATM) is an example of an interactive process in which a computer will perform a logic-derived response to a user selection based on information retrieved from a networked database. The ATM process has similarities with other online transaction processes. The different logical responses are called scenarios. Such processes are typically designed with the aid of use cases and flowcharts, which guide the writing of the software code. The earliest feedback control mechanism was the water clock invented by Greek engineer Ctesibius (285–222 BC).
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the Greeks and Arabs', 'Carl Benedikt Frey and Michael Osborne', 'Today extensive automation is practiced in practically every type of manufacturing and assembly process', 'predeterminations in machines', 'Humans are less involved, but their involvement becomes more critical'], 'answer_start': [], 'answer_end': []}"
"
 Telecommunication, often used in its plural form, is the transmission of information with an immediacy comparable to face-to-face communication. As such, slow communications technologies like postal mail and pneumatic tubes are excluded from the definition.[1][2] Many transmission media have been used for telecommunications throughout history, from smoke signals, beacons, semaphore telegraphs, signal flags, and optical heliographs  to wires and empty space made to carry electromagnetic signals. These paths of transmission may be divided into communication channels for multiplexing, allowing for a single medium to transmit several concurrent communication sessions. Several methods of long-distance communication before the modern era used sounds like coded drumbeats, the blowing of horns, and whistles. Long-distance technologies invented during the 20th and 21st centuries generally use electric power, and include the telegraph, telephone, television, and radio.
 Early telecommunication networks used metal wires as the medium for transmitting signals. These networks were used for telegraphy and telephony for many decades. In the first decade of the 20th century, a revolution in wireless communication began with breakthroughs including those made in radio communications by Guglielmo Marconi, who won the 1909 Nobel Prize in Physics. Other early pioneers in electrical and electronic telecommunications include co-inventors of the telegraph Charles Wheatstone and Samuel Morse, numerous inventors and developers of the telephone including Antonio Meucci and Alexander Graham Bell, inventors of radio Edwin Armstrong and Lee de Forest, as well as inventors of television like Vladimir K. Zworykin, John Logie Baird and Philo Farnsworth.
 Since the 1960s, the proliferation of digital technologies has meant that voice communications have gradually been supplemented by data. The physical limitations of metallic media prompted the development of optical fibre.[3][4][5] The Internet, a technology independent of any given medium, has provided global access to services for individual users and further reduced location and time limitations on communications.
 Telecommunication is a compound noun of the Greek prefix tele- (τῆλε), meaning distant, far off, or afar,[6] and the Latin verb communicare, meaning to share. Its modern use is adapted from the French,[7] because its written use was recorded in 1904 by the French engineer and novelist Édouard Estaunié.[8][9] Communication was first used as an English word in the late 14th century. It comes from Old French comunicacion (14c., Modern French communication), from Latin communicationem (nominative communication), noun of action from past participle stem of communicare, ""to share, divide out; communicate, impart, inform; join, unite, participate in,"" literally, ""to make common"", from communis"".[10]
 At the 1932 Plenipotentiary Telegraph Conference and the International Radiotelegraph Conference in Madrid, the two organizations merged to form the International Telecommunication Union (ITU).[11] They defined telecommunication as ""any telegraphic or telephonic communication of signs, signals, writing, facsimiles and sounds of any kind, by wire, wireless or other systems or processes of electric signaling or visual signaling (semaphores).""
 The definition was later reconfirmed, according to Article 1.3 of the ITU Radio Regulations, which defined it as ""Any transmission, emission or reception of signs, signals, writings, images and sounds or intelligence of any nature by wire, radio, optical, or other electromagnetic systems"".
 Homing pigeons have been used throughout history by different cultures. Pigeon post had Persian roots and was later used by the Romans to aid their military. Frontinus claimed Julius Caesar used pigeons as messengers in his conquest of Gaul.[12]
The Greeks also conveyed the names of the victors at the Olympic Games to various cities using homing pigeons.[13] In the early 19th century, the Dutch government used the system in Java and Sumatra. And in 1849, Paul Julius Reuter started a pigeon service to fly stock prices between Aachen and Brussels, a service that operated for a year until the gap in the telegraph link was closed.[14]
 In the Middle Ages, chains of beacons were commonly used on hilltops as a means of relaying a signal. Beacon chains suffered the drawback that they could only pass a single bit of information, so the meaning of the message such as ""the enemy has been sighted"" had to be agreed upon in advance. One notable instance of their use was during the Spanish Armada, when a beacon chain relayed a signal from Plymouth to London.[15]
 In 1792, Claude Chappe, a French engineer, built the first fixed visual telegraphy system (or semaphore line) between Lille and Paris.[16] However semaphore suffered from the need for skilled operators and expensive towers at intervals of ten to thirty kilometres (six to nineteen miles). As a result of competition from the electrical telegraph, the last commercial line was abandoned in 1880.[17]
 On July 25, 1837, the first commercial electrical telegraph was demonstrated by English inventor Sir William Fothergill Cooke and English scientist Sir Charles Wheatstone.[18][19] Both inventors viewed their device as ""an improvement to the [existing] electromagnetic telegraph"" and not as a new device.[20]
 Samuel Morse independently developed a version of the electrical telegraph that he unsuccessfully demonstrated on September 2, 1837. His code was an important advance over Wheatstone's signaling method. The first transatlantic telegraph cable was successfully completed on July 27, 1866, allowing transatlantic telecommunication for the first time.[21]
 The conventional telephone was patented by Alexander Bell in 1876. Elisha Gray also filed a caveat for it in 1876. Gray abandoned his caveat and because he did not contest Bell's priority, the examiner approved Bell's patent on March 3, 1876. Gray had filed his caveat for the variable resistance telephone, but Bell was the first to document the idea and test it in a telephone.[88][22] Antonio Meucci invented a device that allowed the electrical transmission of voice over a line nearly 30 years before in 1849, but his device was of little practical value because it relied on the electrophonic effect requiring users to place the receiver in their mouths to ""hear"".[23] The first commercial telephone services were set up by the Bell Telephone Company in 1878 and 1879 on both sides of the Atlantic in the cities of New Haven and London.[24][25]
 In 1894, Italian inventor Guglielmo Marconi began developing a wireless communication using the then-newly discovered phenomenon of radio waves, showing by 1901 that they could be transmitted across the Atlantic Ocean.[26] This was the start of wireless telegraphy by radio. On 17 December 1902, a transmission from the Marconi station in Glace Bay, Nova Scotia, Canada, became the world's first radio message to cross the Atlantic from North America. In 1904, a commercial service was established to transmit nightly news summaries to subscribing ships, which incorporated them into their onboard newspapers.[27]
 World War I accelerated the development of radio for military communications. After the war, commercial radio AM broadcasting began in the 1920s and became an important mass medium for entertainment and news. World War II again accelerated the development of radio for the wartime purposes of aircraft and land communication, radio navigation, and radar.[28] Development of stereo FM broadcasting of radio began in the 1930s in the United States and the 1940s in the United Kingdom,[29] displacing AM as the dominant commercial standard in the 1970s.[30]
 On March 25, 1925, John Logie Baird demonstrated the transmission of moving pictures at the London department store Selfridges. Baird's device relied upon the Nipkow disk and thus became known as the mechanical television. It formed the basis of experimental broadcasts done by the British Broadcasting Corporation beginning on 30 September 1929.[31] However, for most of the 20th century, televisions depended on the cathode ray tube invented by Karl Braun. The first version of such a television to show promise was produced by Philo Farnsworth and demonstrated to his family on 7 September 1927.[32] After World War II, interrupted experiments resumed and television became an important home entertainment broadcast medium.
 The type of device known as a thermionic tube or thermionic valve uses thermionic emission of electrons from a heated cathode for a number of fundamental electronic functions such as signal amplification and current rectification.
 The simplest vacuum tube, the diode invented in 1904 by John Ambrose Fleming, contains only a heated electron-emitting cathode and an anode. Electrons can only flow in one direction through the device—from the cathode to the anode. Adding one or more control grids within the tube enables the current between the cathode and anode to be controlled by the voltage on the grid or grids.[33] These devices became a key component of electronic circuits for the first half of the 20th century and were crucial to the development of radio, television, radar, sound recording and reproduction, long-distance telephone networks, and analogue and early digital computers. While some applications had used earlier technologies such as the spark gap transmitter for radio or mechanical computers for computing, it was the invention of the thermionic vacuum tube that made these technologies widespread and practical, leading to the creation of electronics.[34]
 In the 1940s, the invention of semiconductor devices made it possible to produce solid-state devices, which are smaller, cheaper, and more efficient, reliable, and durable than thermionic tubes. Starting in the mid-1960s, thermionic tubes were replaced with the transistor. Thermionic tubes still have some applications for certain high-frequency amplifiers.
 On 11 September 1940, George Stibitz transmitted problems for his Complex Number Calculator in New York using a teletype and received the computed results back at Dartmouth College in New Hampshire.[35] This configuration of a centralized computer (mainframe) with remote dumb terminals remained popular well into the 1970s. In the 1960s, Paul Baran and, independently, Donald Davies started to investigate packet switching, a technology that sends a message in portions to its destination asynchronously without passing it through a centralized mainframe. A four-node network emerged on 5 December 1969, constituting the beginnings of the ARPANET, which by 1981 had grown to 213 nodes.[36] ARPANET eventually merged with other networks to form the Internet. While Internet development was a focus of the Internet Engineering Task Force (IETF) who published a series of Request for Comments documents, other networking advancements occurred in industrial laboratories, such as the local area network (LAN) developments of Ethernet (1983), Token Ring (1984)[citation needed]and Star network topology.
 The effective capacity to exchange information worldwide through two-way telecommunication networks grew from 281 petabytes (PB) of optimally compressed information in 1986 to 471 PB in 1993 to 2.2 exabytes (EB) in 2000 to 65 EB in 2007.[37] This is the informational equivalent of two newspaper pages per person per day in 1986, and six entire newspapers per person per day by 2007.[38] Given this growth, telecommunications play an increasingly important role in the world economy and the global telecommunications industry was about a $4.7 trillion sector in 2012.[39][40] The service revenue of the global telecommunications industry was estimated to be $1.5 trillion in 2010, corresponding to 2.4% of the world's gross domestic product (GDP).[39]
 Modern telecommunication is founded on a series of key concepts that experienced progressive development and refinement in a period of well over a century:
 Telecommunication technologies may primarily be divided into wired and wireless methods. Overall, a basic telecommunication system consists of three main parts that are always present in some form or another:
 In a radio broadcasting station, the station's large power amplifier is the transmitter and the broadcasting antenna is the interface between the power amplifier and the free space channel. The free space channel is the transmission medium and the receiver's antenna is the interface between the free space channel and the receiver. Next, the radio receiver is the destination of the radio signal, where it is converted from electricity to sound.
 Telecommunication systems are occasionally ""duplex"" (two-way systems) with a single box of electronics working as both the transmitter and a receiver, or a transceiver (e.g., a mobile phone).[41] The transmission electronics and the receiver electronics within a transceiver are quite independent of one another. This can be explained by the fact that radio transmitters contain power amplifiers that operate with electrical powers measured in watts or kilowatts, but radio receivers deal with radio powers measured in microwatts or nanowatts. Hence, transceivers have to be carefully designed and built to isolate their high-power circuitry and their low-power circuitry from each other to avoid interference.
 Telecommunication over fixed lines is called point-to-point communication because it occurs between a transmitter and a receiver. Telecommunication through radio broadcasts is called broadcast communication because it occurs between a powerful transmitter and numerous low-power but sensitive radio receivers.[41]
 Telecommunications in which multiple transmitters and multiple receivers have been designed to cooperate and share the same physical channel are called multiplex systems. The sharing of physical channels using multiplexing often results in significant cost reduction. Multiplexed systems are laid out in telecommunication networks and multiplexed signals are switched at nodes through to the correct destination terminal receiver.
 Communications signals can be sent by analogue signals or digital signals via analogue communication systems or digital communication systems. Analogue signals vary continuously with respect to the information, while digital signals encode information as a set of discrete values (e.g., a set of ones and zeroes).[42] During propagation and reception, information contained in analogue signals is degraded by undesirable physical noise. Commonly, the noise in a communication system can be expressed as adding or subtracting from the desirable signal in a random way. This form of noise is called additive noise, with the understanding that the noise can be negative or positive at different instances.
 Unless the additive noise disturbance exceeds a certain threshold, the information contained in digital signals will remain intact. Their resistance to noise represents a key advantage of digital signals over analogue signals. However, digital systems fail catastrophically when noise exceeds the system's ability to autocorrect. On the other hand, analogue systems fail gracefully: as noise increases, the signal becomes progressively more degraded but still usable. Also, digital transmission of continuous data unavoidably adds quantization noise to the output. This can be reduced, but not eliminated, only at the expense of increasing the channel bandwidth requirement.
 The term ""channel"" has two different meanings. In one meaning, a channel is the physical medium that carries a signal between the transmitter and the receiver. Examples of this include the atmosphere for sound communications, glass optical fibres for some kinds of optical communications, coaxial cables for communications by way of the voltages and electric currents in them, and free space for communications using visible light, infrared waves, ultraviolet light, and radio waves. Coaxial cable types are classified by RG type or ""radio guide"", terminology derived from World War II. The various RG designations are used to classify the specific signal transmission applications.[43] This last channel is called the ""free space channel"". The sending of radio waves from one place to another has nothing to do with the presence or absence of an atmosphere between the two. Radio waves travel through a perfect vacuum just as easily as they travel through air, fog, clouds, or any other kind of gas.
 The other meaning of the term ""channel"" in telecommunications is seen in the phrase communications channel, which is a subdivision of a transmission medium so that it can be used to send multiple streams of information simultaneously. For example, one radio station can broadcast radio waves into free space at frequencies in the neighbourhood of 94.5 MHz (megahertz) while another radio station can simultaneously broadcast radio waves at frequencies in the neighbourhood of 96.1 MHz. Each radio station would transmit radio waves over a frequency bandwidth of about 180 kHz (kilohertz), centred at frequencies such as the above, which are called the ""carrier frequencies"". Each station in this example is separated from its adjacent stations by 200 kHz, and the difference between 200 kHz and 180 kHz (20 kHz) is an engineering allowance for the imperfections in the communication system.
 In the example above, the ""free space channel"" has been divided into communications channels according to frequencies, and each channel is assigned a separate frequency bandwidth in which to broadcast radio waves. This system of dividing the medium into channels according to frequency is called ""frequency-division multiplexing"".  Another term for the same concept is ""wavelength-division multiplexing"", which is more commonly used in optical communications when multiple transmitters share the same physical medium.
 Another way of dividing a communications medium into channels is to allocate each sender a recurring segment of time (a ""time slot"", for example, 20 milliseconds out of each second), and to allow each sender to send messages only within its own time slot. This method of dividing the medium into communication channels is called ""time-division multiplexing"" (TDM), and is used in optical fibre communication. Some radio communication systems use TDM within an allocated FDM channel. Hence, these systems use a hybrid of TDM and FDM.
 The shaping of a signal to convey information is known as modulation. Modulation can be used to represent a digital message as an analogue waveform. This is commonly called ""keying""—a term derived from the older use of Morse Code in telecommunications—and several keying techniques exist (these include phase-shift keying, frequency-shift keying, and amplitude-shift keying). The ""Bluetooth"" system, for example, uses phase-shift keying to exchange information between various devices.[44][45] In addition, there are combinations of phase-shift keying and amplitude-shift keying which is called (in the jargon of the field) ""quadrature amplitude modulation"" (QAM) that are used in high-capacity digital radio communication systems.
 Modulation can also be used to transmit the information of low-frequency analogue signals at higher frequencies. This is helpful because low-frequency analogue signals cannot be effectively transmitted over free space. Hence the information from a low-frequency analogue signal must be impressed into a higher-frequency signal (known as the ""carrier wave"") before transmission. There are several different modulation schemes available to achieve this [two of the most basic being amplitude modulation (AM) and frequency modulation (FM)]. An example of this process is a disc jockey's voice being impressed into a 96 MHz carrier wave using frequency modulation (the voice would then be received on a radio as the channel ""96 FM"").[46] In addition, modulation has the advantage that it may use frequency division multiplexing (FDM).
 A telecommunications network is a collection of transmitters, receivers, and communications channels that send messages to one another. Some digital communications networks contain one or more routers that work together to transmit information to the correct user. An analogue communications network consists of one or more switches that establish a connection between two or more users. For both types of networks, repeaters may be necessary to amplify or recreate the signal when it is being transmitted over long distances. This is to combat attenuation that can render the signal indistinguishable from the noise.[47]
Another advantage of digital systems over analogue is that their output is easier to store in memory, i.e., two voltage states (high and low) are easier to store than a continuous range of states.
 Telecommunication has a significant social, cultural and economic impact on modern society. In 2008, estimates placed the telecommunication industry's revenue at US$4.7 trillion or just under three per cent of the gross world product (official exchange rate).[39] Several following sections discuss the impact of telecommunication on society.
 On the microeconomic scale, companies have used telecommunications to help build global business empires. This is self-evident in the case of online retailer Amazon.com but, according to academic Edward Lenert, even the conventional retailer Walmart has benefited from better telecommunication infrastructure compared to its competitors.[48] In cities throughout the world, home owners use their telephones to order and arrange a variety of home services ranging from pizza deliveries to electricians. Even relatively poor communities have been noted to use telecommunication to their advantage. In Bangladesh's Narsingdi District, isolated villagers use cellular phones to speak directly to wholesalers and arrange a better price for their goods. In Côte d'Ivoire, coffee growers share mobile phones to follow hourly variations in coffee prices and sell at the best price.[49]
 On the macroeconomic scale, Lars-Hendrik Röller and Leonard Waverman suggested a causal link between good telecommunication infrastructure and economic growth.[50][51] Few dispute the existence of a correlation although some argue it is wrong to view the relationship as causal.[52]
 Because of the economic benefits of good telecommunication infrastructure, there is increasing worry about the inequitable access to telecommunication services amongst various countries of the world—this is known as the digital divide. A 2003 survey by the International Telecommunication Union (ITU) revealed that roughly a third of countries have fewer than one mobile subscription for every 20 people and one-third of countries have fewer than one land-line telephone subscription for every 20 people. In terms of Internet access, roughly half of all countries have fewer than one out of 20 people with Internet access. From this information, as well as educational data, the ITU was able to compile an index that measures the overall ability of citizens to access and use information and communication technologies.[53] Using this measure, Sweden, Denmark and Iceland received the highest ranking while the African countries Niger, Burkina Faso and Mali received the lowest.[54]
 Telecommunication has played a significant role in social relationships. Nevertheless, devices like the telephone system were originally advertised with an emphasis on the practical dimensions of the device (such as the ability to conduct business or order home services) as opposed to the social dimensions. It was not until the late 1920s and 1930s that the social dimensions of the device became a prominent theme in telephone advertisements. New promotions started appealing to consumers' emotions, stressing the importance of social conversations and staying connected to family and friends.[55]
 Since then the role that telecommunications has played in social relations has become increasingly important. In recent years,[when?] the popularity of social networking sites has increased dramatically. These sites allow users to communicate with each other as well as post photographs, events and profiles for others to see. The profiles can list a person's age, interests, sexual preference and relationship status. In this way, these sites can play important role in everything from organising social engagements to courtship.[56]
 Prior to social networking sites, technologies like short message service (SMS) and the telephone also had a significant impact on social interactions. In 2000, market research group Ipsos MORI reported that 81% of 15- to 24-year-old SMS users in the United Kingdom had used the service to coordinate social arrangements and 42% to flirt.[57]
 In cultural terms, telecommunication has increased the public's ability to access music and film. With television, people can watch films they have not seen before in their own home without having to travel to the video store or cinema. With radio and the Internet, people can listen to music they have not heard before without having to travel to the music store.
 Telecommunication has also transformed the way people receive their news. A 2006 survey (right table) of slightly more than 3,000 Americans by the non-profit Pew Internet and American Life Project in the United States the majority specified television or radio over newspapers.
 Telecommunication has had an equally significant impact on advertising. TNS Media Intelligence reported that in 2007, 58% of advertising expenditure in the United States was spent on media that depend upon telecommunication.[59]
 Many countries have enacted legislation which conforms to the International Telecommunication Regulations established by the International Telecommunication Union (ITU), which is the ""leading UN agency for information and communication technology issues"".[60] In 1947, at the Atlantic City Conference, the ITU decided to ""afford international protection to all frequencies registered in a new international frequency list and used in conformity with the Radio Regulation"". According to the ITU's Radio Regulations adopted in Atlantic City, all frequencies referenced in the International Frequency Registration Board, examined by the board and registered on the International Frequency List ""shall have the right to international protection from harmful interference"".[61]
 From a global perspective, there have been political debates and legislation regarding the management of telecommunication and broadcasting. The history of broadcasting discusses some debates in relation to balancing conventional communication such as printing and telecommunication such as radio broadcasting.[62] The onset of World War II brought on the first explosion of international broadcasting propaganda.[62] Countries, their governments, insurgents, terrorists, and militiamen have all used telecommunication and broadcasting techniques to promote propaganda.[62][63] Patriotic propaganda for political movements and colonization started the mid-1930s. In 1936, the BBC broadcast propaganda to the Arab World to partly counter similar broadcasts from Italy, which also had colonial interests in North Africa.[62] Modern political debates in telecommunication include the reclassification of broadband Internet service as a telecommunications service (also called net neutrality),[64][65] regulation of phone spam,[66][67] and expanding affordable broadband access.[68]
 According to data collected by Gartner[69][70] and Ars Technica[71] sales of main consumer's telecommunication equipment worldwide in millions of units was:
 In a telephone network, the caller is connected to the person to whom they wish to talk by switches at various telephone exchanges. The switches form an electrical connection between the two users and the setting of these switches is determined electronically when the caller dials the number. Once the connection is made, the caller's voice is transformed to an electrical signal using a small microphone in the caller's handset. This electrical signal is then sent through the network to the user at the other end where it is transformed back into sound by a small speaker in that person's handset.
 As of 2015[update], the landline telephones in most residential homes are analogue—that is, the speaker's voice directly determines the signal's voltage.[72] Although short-distance calls may be handled from end-to-end as analogue signals, increasingly telephone service providers are transparently converting the signals to digital signals for transmission. The advantage of this is that digitized voice data can travel side by side with data from the Internet and can be perfectly reproduced in long-distance communication (as opposed to analogue signals that are inevitably impacted by noise).
 Mobile phones have had a significant impact on telephone networks. Mobile phone subscriptions now outnumber fixed-line subscriptions in many markets. Sales of mobile phones in 2005 totalled 816.6 million with that figure being almost equally shared amongst the markets of Asia/Pacific (204 m), Western Europe (164 m), CEMEA (Central Europe, the Middle East and Africa) (153.5 m), North America (148 m) and Latin America (102 m).[73] In terms of new subscriptions over the five years from 1999, Africa has outpaced other markets with 58.2% growth.[74] Increasingly these phones are being serviced by systems where the voice content is transmitted digitally such as GSM or W-CDMA with many markets choosing to deprecate analog systems such as AMPS.[75]
 There have also been dramatic changes in telephone communication behind the scenes. Starting with the operation of TAT-8 in 1988, the 1990s saw the widespread adoption of systems based on optical fibres. The benefit of communicating with optical fibres is that they offer a drastic increase in data capacity. TAT-8 itself was able to carry 10 times as many telephone calls as the last copper cable laid at that time and today's optical fibre cables are able to carry 25 times as many telephone calls as TAT-8.[76] This increase in data capacity is due to several factors: First, optical fibres are physically much smaller than competing technologies. Second, they do not suffer from crosstalk which means several hundred of them can be easily bundled together in a single cable.[77] Lastly, improvements in multiplexing have led to an exponential growth in the data capacity of a single fibre.[78][79]
 Assisting communication across many modern optical fibre networks is a protocol known as Asynchronous Transfer Mode (ATM). The ATM protocol allows for the side-by-side data transmission mentioned in the second paragraph. It is suitable for public telephone networks because it establishes a pathway for data through the network and associates a traffic contract with that pathway. The traffic contract is essentially an agreement between the client and the network about how the network is to handle the data; if the network cannot meet the conditions of the traffic contract it does not accept the connection. This is important because telephone calls can negotiate a contract so as to guarantee themselves a constant bit rate, something that will ensure a caller's voice is not delayed in parts or cut off completely.[80] There are competitors to ATM, such as Multiprotocol Label Switching (MPLS), that perform a similar task and are expected to supplant ATM in the future.[81][82]
 In a broadcast system, the central high-powered broadcast tower transmits a high-frequency electromagnetic wave to numerous low-powered receivers. The high-frequency wave sent by the tower is modulated with a signal containing visual or audio information. The receiver is then tuned so as to pick up the high-frequency wave and a demodulator is used to retrieve the signal containing the visual or audio information. The broadcast signal can be either analogue (signal is varied continuously with respect to the information) or digital (information is encoded as a set of discrete values).[41][83]
 The broadcast media industry is at a critical turning point in its development, with many countries moving from analogue to digital broadcasts. This move is made possible by the production of cheaper, faster and more capable integrated circuits. The chief advantage of digital broadcasts is that they prevent a number of complaints common to traditional analogue broadcasts. For television, this includes the elimination of problems such as snowy pictures, ghosting and other distortion. These occur because of the nature of analogue transmission, which means that perturbations due to noise will be evident in the final output. Digital transmission overcomes this problem because digital signals are reduced to discrete values upon reception and hence small perturbations do not affect the final output. In a simplified example, if a binary message 1011 was transmitted with signal amplitudes [1.0 0.0 1.0 1.0] and received with signal amplitudes [0.9 0.2 1.1 0.9] it would still decode to the binary message 1011— a perfect reproduction of what was sent. From this example, a problem with digital transmissions can also be seen in that if the noise is great enough it can significantly alter the decoded message. Using forward error correction a receiver can correct a handful of bit errors in the resulting message but too much noise will lead to incomprehensible output and hence a breakdown of the transmission.[84][85]
 In digital television broadcasting, there are three competing standards that are likely to be adopted worldwide. These are the ATSC, DVB and ISDB standards; the adoption of these standards thus far is presented in the captioned map. All three standards use MPEG-2 for video compression. ATSC uses Dolby Digital AC-3 for audio compression, ISDB uses Advanced Audio Coding (MPEG-2 Part 7) and DVB has no standard for audio compression but typically uses MPEG-1 Part 3 Layer 2.[86][87] The choice of modulation also varies between the schemes. In digital audio broadcasting, standards are much more unified with practically all countries choosing to adopt the Digital Audio Broadcasting standard (also known as the Eureka 147 standard). The exception is the United States which has chosen to adopt HD Radio. HD Radio, unlike Eureka 147, is based upon a transmission method known as in-band on-channel transmission that allows digital information to ""piggyback"" on normal AM or FM analog transmissions.[88]
 However, despite the pending switch to digital, analog television remains being transmitted in most countries. An exception is the United States that ended analog television transmission (by all but the very low-power TV stations) on 12 June 2009[89] after twice delaying the switchover deadline. Kenya also ended analog television transmission in December 2014 after multiple delays. For analogue television, there were three standards in use for broadcasting colour TV (see a map on adoption here). These are known as PAL (German designed), NTSC (American designed), and SECAM (French-designed). For analogue radio, the switch to digital radio is made more difficult by the higher cost of digital receivers.[90] The choice of modulation for analogue radio is typically between amplitude (AM) or frequency modulation (FM). To achieve stereo playback, an amplitude modulated subcarrier is used for stereo FM, and quadrature amplitude modulation is used for stereo AM or C-QUAM.
 The Internet is a worldwide network of computers and computer networks that communicate with each other using the Internet Protocol (IP).[91] Any computer on the Internet has a unique IP address that can be used by other computers to route information to it. Hence, any computer on the Internet can send a message to any other computer using its IP address. These messages carry with them the originating computer's IP address allowing for two-way communication. The Internet is thus an exchange of messages between computers.[92]
 It is estimated that 51% of the information flowing through two-way telecommunications networks in the year 2000 were flowing through the Internet (most of the rest (42%) through the landline telephone). By 2007 the Internet clearly dominated and captured 97% of all the information in telecommunication networks (most of the rest (2%) through mobile phones).[37] As of 2008[update], an estimated 21.9% of the world population has access to the Internet with the highest access rates (measured as a percentage of the population) in North America (73.6%), Oceania/Australia (59.5%) and Europe (48.1%).[93] In terms of broadband access, Iceland (26.7%), South Korea (25.4%) and the Netherlands (25.3%) led the world.[94]
 The Internet works in part because of protocols that govern how the computers and routers communicate with each other. The nature of computer network communication lends itself to a layered approach where individual protocols in the protocol stack run more-or-less independently of other protocols. This allows lower-level protocols to be customized for the network situation while not changing the way higher-level protocols operate. A practical example of why this is important is because it allows an Internet browser to run the same code regardless of whether the computer it is running on is connected to the Internet through an Ethernet or Wi-Fi connection. Protocols are often talked about in terms of their place in the OSI reference model (pictured on the right), which emerged in 1983 as the first step in an unsuccessful attempt to build a universally adopted networking protocol suite.[95]
 For the Internet, the physical medium and data link protocol can vary several times as packets traverse the globe. This is because the Internet places no constraints on what physical medium or data link protocol is used. This leads to the adoption of media and protocols that best suit the local network situation. In practice, most intercontinental communication will use the Asynchronous Transfer Mode (ATM) protocol (or a modern equivalent) on top of optic fibre. This is because for most intercontinental communication the Internet shares the same infrastructure as the public switched telephone network.
 At the network layer, things become standardized with the Internet Protocol (IP) being adopted for logical addressing. For the World Wide Web, these ""IP addresses"" are derived from the human-readable form using the Domain Name System (e.g., 72.14.207.99 is derived from Google.com). At the moment, the most widely used version of the Internet Protocol is version four but a move to version six is imminent.[96]
 At the transport layer, most communication adopts either the Transmission Control Protocol (TCP) or the User Datagram Protocol (UDP). TCP is used when it is essential every message sent is received by the other computer whereas UDP is used when it is merely desirable. With TCP, packets are retransmitted if they are lost and placed in order before they are presented to higher layers. With UDP, packets are not ordered nor retransmitted if lost. Both TCP and UDP packets carry port numbers with them to specify what application or process the packet should be handled by.[97] Because certain application-level protocols use certain ports, network administrators can manipulate traffic to suit particular requirements. Examples are to restrict Internet access by blocking the traffic destined for a particular port or to affect the performance of certain applications by assigning priority.
 Above the transport layer, there are certain protocols that are sometimes used and loosely fit in the session and presentation layers, most notably the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. These protocols ensure that data transferred between two parties remains completely confidential.[98] Finally, at the application layer, are many of the protocols Internet users would be familiar with such as HTTP (web browsing), POP3 (e-mail), FTP (file transfer), IRC (Internet chat), BitTorrent (file sharing) and XMPP (instant messaging).
 Voice over Internet Protocol (VoIP) allows data packets to be used for synchronous voice communications. The data packets are marked as voice-type packets and can be prioritized by the network administrators so that the real-time, synchronous conversation is less subject to contention with other types of data traffic which can be delayed (i.e., file transfer or email) or buffered in advance (i.e., audio and video) without detriment. That prioritization is fine when the network has sufficient capacity for all the VoIP calls taking place at the same time and the network is enabled for prioritization, i.e., a private corporate-style network, but the Internet is not generally managed in this way and so there can be a big difference in the quality of VoIP calls over a private network and over the public Internet.[99]
 Despite the growth of the Internet, the characteristics of local area networks (LANs)—computer networks that do not extend beyond a few kilometres—remain distinct. This is because networks on this scale do not require all the features associated with larger networks and are often more cost-effective and efficient without them. When they are not connected with the Internet, they also have the advantages of privacy and security. However, purposefully lacking a direct connection to the Internet does not provide assured protection from hackers, military forces, or economic powers. These threats exist if there are any methods for connecting remotely to the LAN.
 Wide area networks (WANs) are private computer networks that may extend for thousands of kilometres. Once again, some of their advantages include privacy and security. Prime users of private LANs and WANs include armed forces and intelligence agencies that must keep their information secure and secret.
 In the mid-1980s, several sets of communication protocols emerged to fill the gaps between the data-link layer and the application layer of the OSI reference model. These included AppleTalk, IPX, and NetBIOS with the dominant protocol set during the early 1990s being IPX due to its popularity with MS-DOS users. TCP/IP existed at this point, but it was typically only used by large government and research facilities.[100]
 As the Internet grew in popularity and its traffic was required to be routed into private networks, the TCP/IP protocols replaced existing local area network technologies. Additional technologies, such as DHCP, allowed TCP/IP-based computers to self-configure in the network. Such functions also existed in the AppleTalk/ IPX/ NetBIOS protocol sets.[101]
 Whereas Asynchronous Transfer Mode (ATM) or Multiprotocol Label Switching (MPLS) are typical data-link protocols for larger networks such as WANs; Ethernet and Token Ring are typical data-link protocols for LANs. These protocols differ from the former protocols in that they are simpler, e.g., they omit features such as quality of service guarantees, and offer medium access control. Both of these differences allow for more economical systems.[102]
 Despite the modest popularity of Token Ring in the 1980s and 1990s, virtually all LANs now use either wired or wireless Ethernet facilities. At the physical layer, most wired Ethernet implementations use copper twisted-pair cables (including the common 10BASE-T networks). However, some early implementations used heavier coaxial cables and some recent implementations (especially high-speed ones) use optical fibres.[103] When optic fibres are used, the distinction must be made between multimode fibres and single-mode fibres. Multimode fibres can be thought of as thicker optical fibres that are cheaper to manufacture devices for, but that suffer from less usable bandwidth and worse attenuation—implying poorer long-distance performance.[104]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Telecommunication', 'multimode fibres and single-mode fibres', 'The history of broadcasting discusses some debates in relation to balancing conventional communication', ""a person's age, interests, sexual preference and relationship status"", 'signal transmission applications'], 'answer_start': [], 'answer_end': []}"
"
 Computer science is the study of computation, information, and automation.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software).[4][5][6]
 Algorithms and data structures are central to computer science.[7]
The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.
 The fundamental concern of computer science is determining what can and cannot be automated.[2][8][3][9][10] The Turing Award is generally recognized as the highest distinction in computer science.[11][12]
 The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[16]
 Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[17] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[18] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[19] He started developing this machine in 1834, and ""in less than two years, he had sketched out many of the salient features of the modern computer"".[20] ""A crucial step was the adoption of a punched card system derived from the Jacquard loom""[20] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[21] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[22] the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics,[23] and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic.[24][25] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine,[26] on which commands could be typed and the results printed automatically.[27] In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[28] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as ""Babbage's dream come true"".[29]
 
During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.[30] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.[31] Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.[32] Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[33][34] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962.[35] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights. Although first proposed in 1956,[36] the term ""computer science"" appears in a 1959 article in Communications of the ACM,[37]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[38] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[37]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[39] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[40] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[41] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
 In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist.[42] Three months later in the same journal, comptologist was suggested, followed next year by hypologist.[43] The term computics has also been suggested.[44] In Europe, terms derived from contracted translations of the expression ""automatic information"" (e.g. ""informazione automatica"" in Italian) or ""information and mathematics"" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh).[45] ""In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.""[46]
 A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that ""computer science is no more about computers than astronomy is about telescopes.""[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.
 Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.[33] Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[36]
 The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term ""software engineering"" means, and how computer science is defined.[47] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[48]
 The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
 
Despite the word science in its name, there is debate over whether or not computer science is a discipline of science,[49] mathematics,[50] or engineering.[51] Allen Newell and Herbert A. Simon argued in 1975,  Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[51]  It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[51] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[51] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[51]
 Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods.[51] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[51]
 A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[52] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[33] Amnon H. Eden described them as the ""rationalist paradigm"" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the ""technocratic paradigm"" (which might be found in engineering approaches, most prominently in software engineering), and the ""scientific paradigm"" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[53] identifiable in some branches of artificial intelligence).[54]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[55]
 As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.[56][57]
CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[58]—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.[56]
 Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
 According to Peter Denning, the fundamental question underlying computer science is, ""What can be automated?""[3] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
 The famous P = NP? problem, one of the Millennium Prize Problems,[59] is an open problem in the theory of computation.
 Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[60]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[61]
 Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
 Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
 Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[62] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
 Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
 Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
 Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[63] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[64]
 Social computing is an area that is concerned with the intersection of social behavior and computational systems. Human–computer interaction research develops theories, principles, and guidelines for user interface designers.
 Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.
 Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question ""Can computers think?"", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
 Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[65] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term ""architecture"" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959.
 Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[66] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[67] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[68]
 This branch of computer science aims to manage networks between computers worldwide.
 Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
 Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[69] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
 A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
 The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[70]
 Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
 Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[76]
 Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[77][78] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[79]
 Computer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students.[80] In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.[81]
 In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[82] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[83] According to a 2021 report, only 51% of high schools in the US offer computer science.[84]
 Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[85][86] and several others are following.[87]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['computational complexity theory', 'Lyle R. Johnson and Frederick P. Brooks Jr.', 'art of writing and deciphering secret messages', 'determining what can and cannot be automated', 'determining what can and cannot be automated'], 'answer_start': [], 'answer_end': []}"
"
 Software development is the process used to create software. Programming and maintaining the source code is the central step of this process, but it also includes conceiving the project, evaluating its feasibility, analyzing the business requirements, software design, testing, to release.  Software engineering, in addition to development, also includes project management, employee management, and other overhead functions.[1] Software development may be sequential, in which each step is complete before the next begins, but iterative development methods where multiple steps can be executed at once and earlier steps can be revisited have also been devised to improve flexibility, efficiency, and scheduling.
 Software development involves professionals from various fields, not just software programmers but also individuals specialized in testing, documentation writing, graphic design, user support, marketing, and fundraising. A number of tools and models are commonly used in software development, such as integrated development environment (IDE), version control, computer-aided software engineering, and software documentation.
 Each of the available methodologies are best suited to specific kinds of projects, based on various technical, organizational, project, and team considerations.[3]
 Another focus in many programming methodologies is the idea of trying to catch issues such as security vulnerabilities and bugs as early as possible (shift-left testing) to reduce the cost of tracking and fixing them.[13]
 In 2009, it was estimated that 32 percent of software projects were delivered on time and budget, and with the full functionality. An additional 44 percent were delivered, but missing at least one of these features. The remaining 24 percent were cancelled prior to release.[14]
 Software development life cycle refers to the systematic process of developing applications.[15]
 The sources of ideas for software products are plentiful. These ideas can come from market research including the demographics of potential new customers, existing customers, sales prospects who rejected the product, other internal software development staff, or a creative third party. Ideas for software products are usually first evaluated by marketing personnel for economic feasibility, fit with existing channels of distribution, possible effects on existing product lines, required features, and fit with the company's marketing objectives. In the marketing evaluation phase, the cost and time assumptions become evaluated.[16] The feasibility analysis estimates the project's return on investment, its development cost and timeframe. Based on this analysis, the company can make a business decision to invest in further development.[17] After deciding to develop the software, the company is focused on delivering the product at or below the estimated cost and time, and with a high standard of quality (i.e., lack of bugs) and the desired functionality. Nevertheless, most software projects run late and sometimes compromises are made in features or quality to meet a deadline.[18]
 Software analysis begins with a requirements analysis to capture the business needs of the software.[19] Challenges for the identification of needs are that current or potential users may have different and incompatible needs, may not understand their own needs, and change their needs during the process of software development.[20] Ultimately, the result of analysis is a detailed specification for the product that developers can work from. Software analysts often decompose the project into smaller objects, components that can be reused for increased cost-effectiveness, efficiency, and reliability.[19] Decomposing the project may enable a multi-threaded implementation that runs significantly faster on multiprocessor computers.[21]
 During the analysis and design phases of software development, structured analysis is often used to break down the customer's requirements into pieces that can be implemented by software programmers.[22] The underlying logic of the program may be represented in data-flow diagrams, data dictionaries, pseudocode, state transition diagrams, and/or entity relationship diagrams.[23] If the project incorporates a piece of legacy software that has not been modeled, this software may be modeled to help ensure it is correctly incorporated with the newer software.[24]
 Design involves choices about the implementation of the software, such as which programming languages and database software to use, or how the hardware and network communications will be organized. Design may be iterative with users consulted about their needs in a process of trial and error. Design often involves people expert in aspect such as database design, screen architecture, and the performance of servers and other hardware.[19] Designers often attempt to find patterns in the software's functionality to spin off distinct modules that can be reused with object-oriented programming. An example of this is the model–view–controller, an interface between a graphical user interface and the backend.[25]
 The central feature of software development is creating and understanding the software that implements the desired functionality.[26] There are various strategies for writing the code. Cohesive software has various components that are independent from each other.[19] Coupling is the interrelation of different software components, which is viewed as undesirable because it increases the difficulty of maintenance.[27] Often, software programmers do not follow industry best practices, resulting in code that is inefficient, difficult to understand, or lacking documentation on its functionality.[28] These standards are especially likely to break down in the presence of deadlines.[29] As a result, testing, debugging, and revising the code becomes much more difficult. Code refactoring, for example adding more comments to the code, is a solution to improve the understandibility of code.[30]
 Testing is the process of ensuring that the code executes correctly and without errors. Debugging is performed by each software developer on their own code to confirm that the code does what it is intended to. In particular, it is crucial that the software executes on all inputs, even if the result is incorrect.[31] Code reviews by other developers are often used to scrutinize new code added to the project, and according to some estimates dramatically reduce the number of bugs persisting after testing is complete.[32] Once the code has been submitted, quality assurance—a separate department of non-programmers for most large companies—test the accuracy of the entire software product. Acceptance tests derived from the original software requirements are a popular tool for this.[31] Quality testing also often includes stress and load checking (whether the software is robust to heavy levels of input or usage), integration testing (to ensure that the software is adequately integrated with other software), and compatibility testing (measuring the software's performance across different operating systems or browsers).[31] When tests are written before the code, this is called test-driven development.[33]
 Production is the phase in which software is deployed to the end user.[34] During production, the developer may create technical support resources for users[35][34] or a process for fixing bugs and errors that were not caught earlier. There might also be a return to earlier development phases if user needs changed or were misunderstood.[34]
 Software development is performed by software developers, usually working on a team. Efficient communications between team members is essential to success. This is more easily achieved if the team is small, used to working together, and located near each other.[36] Communications also help identify problems at an earlier state of development and avoid duplicated effort. Many development projects avoid the risk of losing essential knowledge held by only one employee by ensuring that multiple workers are familiar with each component.[37] Software development involves professionals from various fields, not just software programmers but also individuals specialized in testing, documentation writing, graphic design, user support, marketing, and fundraising. Although workers for proprietary software are paid, most contributors to open-source software are volunteers.[38] Alternately, they may be paid by companies whose business model does not involve selling the software, but something else—such as services and modifications to open source software.[39]
 Computer-aided software engineering (CASE) is tools for the partial automation of software development.[40] CASE enables designers to sketch out the logic of a program, whether one to be written, or an already existing one to help integrate it with new code or reverse engineer it (for example, to change the programming language).[41]
 Documentation comes in two forms that are usually kept separate—that intended for software developers, and that made available to the end user to help them use the software.[42][43] Most developer documentation is in the form of code comments for each file, class, and method that cover the application programming interface (API)—how the piece of software can be accessed by another—and often implementation details.[44] This documentation is helpful for new developers to understand the project when they begin working on it.[45] In agile development, the documentation is often written at the same time as the code.[46] User documentation is more frequently written by technical writers.[47]
 Accurate estimation is crucial at the feasibility stage and in delivering the product on time and within budget. The process of generating estimations is often delegated by the project manager.[48] Because the effort estimation is directly related to the size of the complete application, it is strongly influenced by addition of features in the requirements—the more requirements, the higher the development cost. Aspects not related to functionality, such as the experience of the software developers and code reusability, are also essential to consider in estimation.[49] As of 2019[update], most of the tools for estimating the amount of time and resources for software development were designed for conventional applications and are not applicable to web applications or mobile applications.[50]
 An integrated development environment (IDE) supports software development with enhanced features compared to a simple text editor.[51] IDEs often include automated compiling, syntax highlighting of errors,[52] debugging assistance,[53] integration with version control, and semi-automation of tests.[51]
 Version control is a popular way of managing changes made to the software. Whenever a new version is checked in, the software saves a backup of all modified files. If multiple programmers are working on the software simultaneously, it manages the merging of their code changes. The software highlights cases where there is a conflict between two sets of changes and allows programmers to fix the conflict.[54]
 A view model is a framework that provides the viewpoints on the system and its environment, to be used in the software development process. It is a graphical representation of the underlying semantics of a view.
 The purpose of viewpoints and views is to enable human engineers to comprehend very complex systems and to organize the elements of the problem around domains of expertise. In the engineering of physically intensive systems, viewpoints often correspond to capabilities and responsibilities within the engineering organization.[55]
 Intellectual property can be an issue when developers integrate open-source code or libraries into a proprietary product, because most open-source licenses used for software require that modifications be released under the same license. As an alternative, developers may choose a proprietary alternative or write their own software module.[56]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Programming and maintaining the source code', 'professionals from various fields, not just software programmers', 'experience of the software developers and code reusability', 'experience of the software developers and code reusability', 'implementation details'], 'answer_start': [], 'answer_end': []}"
"
 A programming language is a system of notation for writing computer programs.[1]
 Programming languages are described in terms of their syntax (form) and semantics (meaning), usually defined by a formal language. Languages usually provide features such as a type system, variables and mechanisms for error handling. An implementation of a programming language in the form of a compiler or interpreter allows programs to be executed, either directly or by producing what's known in programming as an executable.
 Computer architecture has strongly influenced the design of programming languages, with the most common type (imperative languages—which implement operations in a specified order) developed to perform well on the popular von Neumann architecture. While early programming languages were closely tied to the hardware, over time they have developed more abstraction to hide implementation details for greater simplicity. 
 Thousands of programming languages—often classified as imperative, functional, logic, or object-oriented—have been developed for a wide variety of uses. Many aspects of programming language design involve tradeoffs—for example, exception handling simplifies error handling, but at a performance cost. Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.
 There are a variety of criteria that may be considered when defining what constitutes a programming language.
 The term computer language is sometimes used interchangeably with programming language.[2] However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages.[3] Similarly, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.[4]
One way of classifying computer languages is by the computations they are capable of expressing, as described by the theory of computation. The majority of practical programming languages are Turing complete,[5] and all Turing complete languages can implement the same set of algorithms. ANSI/ISO SQL-92 and Charity are examples of languages that are not Turing complete, yet are often called programming languages.[6][7] However, some authors restrict the term ""programming language"" to Turing complete languages.[1][8]
 Another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.[9] John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.[10]
 In most practical contexts, a programming language involves a computer; consequently, programming languages are usually defined and studied this way.[11] Programming languages differ from natural languages in that natural languages are only used for interaction between people, while programming languages also allow humans to communicate instructions to machines.
 The domain of the language is also worth consideration. Markup languages like XML, HTML, or troff, which define structured data, are not usually considered programming languages.[12][13][14] Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete language entirely using XML syntax.[15][16][17] Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.[18][19]
 Programming languages usually contain abstractions for defining and manipulating data structures or controlling the flow of execution. The practical necessity that a programming language supports adequate abstractions is expressed by the abstraction principle.[20] This principle is sometimes formulated as a recommendation to the programmer to make proper use of such abstractions.[21]
 The first programmable computers were invented at the end of the 1940s, and with them, the first programming languages.[22] The earliest computers were programmed in first-generation programming languages (1GLs), machine language (simple instructions that could be directly executed by the processor). This code was very difficult to debug and was not portable between different computer systems.[23] In order to improve the ease of programming, assembly languages (or second-generation programming languages—2GLs) were invented, diverging from the machine language to make programs easier to understand for humans, although they did not increase portability.[24]
 Initially, hardware resources were scarce and expensive, while human resources were cheaper. Therefore, cumbersome languages that were time-consuming to use, but were closer to the hardware for higher efficiency were favored.[25] The introduction of high-level programming languages (third-generation programming languages—3GLs)—revolutionized programming. These languages abstracted away the details of the hardware, instead being designed to express algorithms that could be understood more easily by humans. For example, arithmetic expressions could now be written in symbolic notation and later translated into machine code that the hardware could execute.[24] In 1957, Fortran (FORmula TRANslation) was invented. Often considered the first compiled high-level programming language,[24][26] Fortran has remained in use into the twenty-first century.[27]
 Around 1960, the first mainframes—general purpose computers—were developed, although they could only be operated by professionals and the cost was extreme. The data and instructions were input by punch cards, meaning that no input could be added while the program was running. The languages developed at this time therefore are designed for minimal interaction.[29] After the invention of the microprocessor, computers in the 1970s became dramatically cheaper.[30] New computers also allowed more user interaction, which was supported by newer programming languages.[31]
 Lisp, implemented in 1958, was the first functional programming language.[32] Unlike Fortran, it supports recursion and conditional expressions,[33] and it also introduced dynamic memory management on a heap and automatic garbage collection.[34] For the next decades, Lisp dominated artificial intelligence applications.[35] In 1978, another functional language, ML, introduced inferred types and polymorphic parameters.[31][36]
 After ALGOL (ALGOrithmic Language) was released in 1958 and 1960,[37] it became the standard in computing literature for describing algorithms. Although its commercial success was limited, most popular imperative languages—including C, Pascal, Ada, C++, Java, and C#—are directly or indirectly descended from ALGOL 60.[38][27] Among its innovations adopted by later programming languages included greater portability and the first use of context-free, BNF grammar.[39] Simula, the first language to support object-oriented programming (including subtypes, dynamic dispatch, and inheritance), also descends from ALGOL and achieved commercial success.[40] C, another ALGOL descendant, has sustained popularity into the twenty-first century. C allows access to lower-level machine operations more than other contemporary languages. Its power and efficiency, generated in part with flexible pointer operations, comes at the cost of making it more difficult to write correct code.[31]
 Prolog, designed in 1972, was the first logic programming language, communicating with a computer using formal logic notation.[41][42] With logic programming, the programmer specifies a desired result and allows the interpreter to decide how to achieve it.[43][42]
 During the 1980s, the invention of the personal computer transformed the roles for which programming languages were used.[44] New languages introduced in the 1980s included C++, a superset of C that can compile C programs but also supports classes and inheritance.[45] Ada and other new languages introduced support for concurrency.[46] The Japanese government invested heavily into the so-called fifth-generation languages that added support for concurrency to logic programming constructs, but these languages were outperformed by other concurrency-supporting languages.[47][48]
 Due to the rapid growth of the Internet and the World Wide Web in the 1990s, new programming languages were introduced to support Web pages and networking.[49] Java, based on C++ and designed for increased portability across systems and security, enjoyed large-scale success because these features are essential for many Internet applications.[50][51] Another development was that of dynamically typed scripting languages—Python, JavaScript, PHP, and Ruby—designed to quickly produce small programs that coordinate existing applications. Due to their integration with HTML, they have also been used for building web pages hosted on servers.[52][53]
 During the 2000s, there was a slowdown in the development of new programming languages that achieved widespread popularity.[54] One innovation was service-oriented programming, designed to exploit distributed systems whose components are connected by a network. Services are similar to objects in object-oriented programming, but run on a separate process.[55] C# and F# cross-pollinated ideas between imperative and functional programming.[56] After 2010, several new languages—Rust, Go, Swift, Zig and Carbon —competed for the performance-critical software for which C had historically been used.[57] Most of the new programming languages uses static typing while a few numbers of new languages use dynamic typing like Ring and Julia.[58][59]
 Some of the new programming languages are classified as visual programming languages like Scratch, LabVIEW and PWCT. Also, some of these languages mix between textual and visual programming usage like Ballerina.[60][61][62][63] Also, this trend lead to developing projects that help in developing new VPLs like Blockly by Google.[64] Many game engines like Unreal and Unity added support for visual scripting too.[65][66]
 All programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.
 A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, some programming languages are more graphical in nature, using visual relationships between symbols to specify a program.
 The syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.
 The programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:
 This grammar specifies the following:
 The following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)).
 Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.
 Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:
 The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p >> 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer):
 If the type declaration on the first line were omitted, the program would trigger an error on the undefined variable p during compilation. However, the program would still be syntactically correct since type declarations provide only semantic information.
 The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars.[67] Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution.[68] In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.[69]
 The term semantics refers to the meaning of languages, as opposed to their form (syntax).
 Static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.[1][failed verification] For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct.[70] Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Programming languages such as Java and C# have definite assignment analysis, a form of data flow analysis, as part of their respective static semantics.
 Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research goes into formal semantics of programming languages, which allows execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.
 A type system defines how a programming language classifies values and expressions into types, how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have type loopholes, usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as type theory.
 A language is typed if the specification of every operation defines types of data to which the operation is applicable.[71] For example, the data represented by ""this text between the quotes"" is a string, and in many programming languages, dividing a number by a string has no meaning and will not be executed. The invalid operation may be detected when the program is compiled (""static"" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected while the program is running (""dynamic"" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to handle this exception and, for example, always return ""-1"" as the result.
 A special case of typed languages is the single-typed languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type[dubious  – discuss]–—most commonly character strings which are used for both symbolic and numeric data.
 In contrast, an untyped language, such as most assembly languages, allows any operation to be performed on any data, generally sequences of bits of various lengths.[71] High-level untyped languages include BCPL, Tcl, and some varieties of Forth.
 In practice, while few languages are considered typed from the type theory (verifying or rejecting all operations), most modern languages offer a degree of typing.[71] Many production languages provide means to bypass or subvert the type system, trading type safety for finer control over the program's execution (see casting).
 In static typing, all expressions have their types determined before a program executes, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string or stored in a variable that is defined to hold dates.[71]
 Statically-typed languages can be either manifestly typed or type-inferred. In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler infers the types of expressions and declarations based on context. Most mainstream statically-typed languages, such as C++, C#, and Java, are manifestly typed. Complete type inference has traditionally been associated with functional languages such as Haskell and ML.[72] However, many manifestly-typed languages support partial type inference; for example, C++, Java, and C# all infer types in certain limited cases.[73] Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float.
 Dynamic typing, also called latent typing, determines the type-safety of operations at run time; in other words, types are associated with run-time values rather than textual expressions.[71] As with type-inferred languages, dynamically-typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, Ruby, Ring and Julia are all examples of dynamically-typed languages.
 Weak typing allows a value of one type to be treated as another, for example treating a string as a number.[71] This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.
 Strong typing prevents these program faults. An attempt to perform an operation on the wrong type of value raises an error.[71] Strongly-typed languages are often termed type-safe or safe.
 An alternative definition for ""weakly typed"" refers to languages, such as Perl, Ring and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression 2 * x implicitly converts x to a number, and this conversion succeeds even if x is null, undefined, an Array, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors. Strong and static are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term strongly typed to mean strongly, statically typed, or, even more confusingly, to mean simply statically typed. Thus C has been called both strongly typed and weakly, statically typed.[74][75][76]
 It may seem odd to some professional programmers that C could be ""weakly, statically typed"". However, the use of the generic pointer, the void* pointer, does allow casting pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as (int) or (char).
 Most programming languages have an associated core library (sometimes known as the ""standard library"", especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.
 The line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language's core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the java.lang.String class; similarly, in Smalltalk, an anonymous function expression (a ""block"") constructs an instance of the library's BlockContext class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.
 In computing, multiple instructions can be executed simultaneously. Many programming languages support instruction-level and subprogram-level concurrency.[77] By the twenty-first century, additional processing power on computers was increasingly coming from the use of additional processors, which requires programmers to design software that makes use of multiple processors simultaneously to achieve improved performance.[78] Interpreted languages such as Python and Ruby do not support the concurrent use of multiple processors.[79] Other programming languages do support managing data shared between different threads by controlling the order of execution of key instructions via the use of semaphores, controlling access to shared data via monitor, or enabling message passing between threads.[80]
 Many programming languages include exception handlers, a section of code triggered by runtime errors that can deal with them in two main ways:[81]
 Some programming languages support dedicating a block of code to run regardless of whether an exception occurs before the code is reached; this is called finalization.[82]
 There is a tradeoff between increased ability to handle exceptions and reduced performance.[83] For example, even though array index errors are common[84] C does not check them for performance reasons.[83]  Although programmers can write code to catch user-defined exceptions, this can clutter a program. Standard libraries in some languages, such as C, use their return values to indicate an exception.[85] Some languages and their compilers have the option of turning on and off error handling capability, either temporarily or permanently.[86]
 Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing language families of related languages branching one from another.[87][88] But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety since it has a precise and finite definition.[89] By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.
 Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one ""universal"" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role.[90] The need for diverse programming languages arises from the diversity of contexts in which languages are used:
 One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.[91]
 
Natural-language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural-language programming as ""foolish"".[92] Alan Perlis was similarly dismissive of the idea.[93] Hybrid approaches have been taken in Structured English and SQL.
 A language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language specification and implementation.
 The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
 A programming language specification can take several forms, including the following:
 An implementation of a programming language is the conversion of a program into machine code that can be executed by the hardware. The machine code then can be executed with the help of the operating system.[97] The most common form of interpretation in production code is by a compiler, which translates the source code via an intermediate-level language into machine code, known as an executable. Once the program is compiled, it will run more quickly than with other implementation methods.[98] Some compilers are able to provide further optimization to reduce memory or computation usage when the executable runs, but increasing compilation time.[99]
 Another implementation method is to run the program with an interpreter, which translates each line of software into machine code just before it executes. Although it can make debugging easier, the downside of interpretation is that it runs 10 to 100 times slower than a compiled executable.[100] Hybrid interpretation methods provide some of the benefits of compilation and some of the benefits of interpretation via partial compilation. One form this takes is just-in-time compilation, in which the software is compiled ahead of time into an intermediate language, and then into machine code immediately before execution.[101]
 Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain-specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.[citation needed]
 Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language,[102] and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.[103]
 Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally Ericsson's internal programming language.[104]
 Open source programming languages are particularly helpful for open science applications, enhancing the capacity for replication and code sharing.[105]
 Thousands of different programming languages have been created, mainly in the computing field.[106]
Individual software projects commonly use five programming languages or more.[107]
 Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers ""do exactly what they are told to do"", and cannot ""understand"" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
 A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives).[108] Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
 Programs for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the ""commands"" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.[109]
 Determining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes;[110][111] Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time, and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
 Various methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:
 Combining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.[115]
 As of February 2024, the top five programming languages as measured by TIOBE index are Python, C, C++, Java and C#. TIOBE provide a list of top 100 programming languages according to popularity and update this list every month.[116]
 A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.
 Programming languages are often placed into four main categories: imperative, functional, logic, and object oriented.[117]
 Although markup languages are not programming languages, some have extensions that support limited programming. Additionally, there are special-purpose languages that are not easily compared to other programming languages.[121]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['textual syntax', 'Ring and Julia', 'dialect is created for use in a domain-specific language', 'illegitimate by implementors', 'artificial intelligence applications'], 'answer_start': [], 'answer_end': []}"
"
 An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.
 Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, peripherals, and other resources.
 For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer – from cellular phones and video game consoles to web servers and supercomputers.
 In the personal computer market, as of September 2023[update], Microsoft Windows holds a dominant market share of around 68%. macOS by Apple Inc. is in second place (20%), and the varieties of Linux, including ChromeOS, are collectively in third place (7%).[3] In the mobile sector (including smartphones and tablets), as of September 2023[update], Android's share is 68.92%, followed by Apple's iOS and iPadOS with 30.42%, and other operating systems with .66%.[4] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems),[5][6] such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.
 Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. USB stick).
 A single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running concurrently. This is achieved by time-sharing, where the available processor time is divided between multiple processes. These processes are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and cooperative types. In preemptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. Unix-like operating systems, such as Linux—as well as non-Unix-like, such as AmigaOS—support preemptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking; 32-bit versions of both Windows NT and Win9x used preemptive multi-tasking.
 Single-user operating systems have no facilities to distinguish users but may allow multiple programs to run in tandem.[7] A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users.
 A distributed operating system manages a group of distinct, networked computers and makes them appear to be a single computer, as all computations are distributed (divided amongst the constituent computers).[8]
 Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines with less autonomy (e.g. PDAs). They are very compact and extremely efficient by design and are able to operate with a limited amount of resources. Windows CE and Minix 3 are some examples of embedded operating systems.
 A real-time operating system is an operating system that guarantees to process events or data by a specific moment in time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. Such an event-driven system switches between tasks based on their priorities or external events, whereas time-sharing operating systems switch tasks based on clock interrupts.
 A library operating system is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with the application and configuration code to construct a unikernel: a specialized, single address space, machine image that can be deployed to cloud or embedded environments.[further explanation needed]
 Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s.[9] Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.
 In the 1940s, the earliest electronic digital systems had no operating systems. Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plugboards. These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards. After programmable general-purpose computers were invented, machine languages(consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981).[full citation needed]
 In the early 1950s, a computer could execute only one program at a time. Each user had sole use of the computer for a limited period and would arrive at a scheduled time with their program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the universal Turing machine.[9]
 Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and compiling (generating machine code from human-readable symbolic code). This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England, the job queue was at one time a washing line (clothesline) from which tapes were hung with different colored clothes-pegs to indicate job priority.[citation needed]
 By the late 1950s, programs that one would recognize as an operating system were beginning to appear. Often pointed to as the earliest recognizable example is GM-NAA I/O, released in 1956 on the IBM 704. The first known example that actually referred to itself was the SHARE Operating System, a development of GM-NAA I/O, released in 1959. In a May 1960 paper describing the system, George Ryckman noted:
 The development of computer operating systems have materially aided the problem of getting a program or series of programs on and off the computer efficiently.[10] One of the more famous examples that is often found in discussions of early systems is the Atlas Supervisor, running on the Atlas in 1962.[11] It was referred to as such in a December 1961 article describing the system, but the context of ""the Operating System"" is more along the lines of ""the system operates in the fashion"". The Atlas team itself used the term ""supervisor"",[12] which was widely used along with ""monitor"". Brinch Hansen described it as ""the most significant breakthrough in the history of operating systems.""[13]
 Through the 1950s, many major features were pioneered in the field of operating systems on mainframe computers, including batch processing, input/output interrupting, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications. In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094, which in turn influenced the later 7040-PR-150 (7040/7044) and 1410-PR-155 (1410/7010) operating systems.
 During the 1960s, IBM's OS/360 introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines. IBM's current mainframe operating systems are distant descendants of this original system and modern machines are backward compatible with applications written for OS/360.[citation needed]
 OS/360 also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and file locking during updates. When a process is terminated for any reason, all of these resources are re-claimed by the operating system.
 The alternative CP-67 system for the S/360-67 started a whole line of IBM operating systems focused on the concept of virtual machines. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: DOS/360[a] (Disk Operating System), TSS/360 (Time Sharing System), TOS/360 (Tape Operating System), BOS/360 (Basic Operating System), and ACP (Airline Control Program), as well as a few non-IBM systems: MTS (Michigan Terminal System), MUSIC (Multi-User System for Interactive Computing), and ORVYL (Stanford Timesharing System).
 Control Data Corporation developed the SCOPE operating system in the 1960s, for batch processing. In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.
 In 1961, Burroughs Corporation introduced the B5000 with the MCP (Master Control Program) operating system. The B5000 was a stack machine designed to exclusively support high-level languages with no assembler;[b] indeed, the MCP was the first OS to be written exclusively in a high-level language (ESPOL, a dialect of ALGOL). MCP also introduced many other ground-breaking innovations, such as being the first commercial implementation of virtual memory. MCP is still in use today in the Unisys company's MCP/ClearPath line of computers.
 UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems.[14][15][16] Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.
 General Electric developed General Electric Comprehensive Operating Supervisor (GECOS), which primarily supported batch processing. After its acquisition by Honeywell, it was renamed General Comprehensive Operating System (GCOS).
 Bell Labs,[c] General Electric and MIT developed Multiplexed Information and Computing Service (Multics), which introduced the concept of ringed security privilege levels.
 Digital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time-sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community. RT-11 was a single-user real-time OS for the PDP-11 class minicomputer, and RSX-11 was the corresponding multi-user OS.
 From the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/44, 360/75, 360/91, 360/95 and 360/195) were microprogrammed implementations.
 The enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:
 The first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minicomputers; minimalistic operating systems were developed, often loaded from ROM and known as monitors. One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft's MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM's version of it was called IBM DOS or PC DOS).
 In the 1980s, Apple Computer Inc. (now Apple Inc.) introduced the Apple Macintosh alongside its popular Apple II series of microcomputers. The Macintosh had an innovative graphical user interface (GUI) and a mouse; it ran an operating system later known as the (classic) Mac OS. 
 The introduction of the Intel 80386 CPU chip in October 1985,[17] with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier superminicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft's operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NeXTSTEP operating system. NeXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X (macOS after latest name change).
 The GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply ""Linux"" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD.
 Unix was originally written in assembly language.[18] Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History).
 The Unix-like family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name ""UNIX"" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. ""UNIX-like"" is commonly used to refer to the large set of operating systems which resemble the original UNIX.
 Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.
 Five operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, Sun Microsystems's Solaris can run on multiple types of hardware, including x86 and SPARC servers, and PCs. Apple's macOS, a replacement for Apple's earlier (non-Unix) classic Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD. IBM's z/OS UNIX System Services includes a shell and utilities based on Mortice Kerns' InterOpen products.
 Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.
 A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP.
 In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.
 Steve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web.
 Developers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. After two years of legal disputes, the BSD project spawned a number of free derivatives, such as NetBSD and FreeBSD (both in 1993), and OpenBSD (from NetBSD in 1995).
 macOS (formerly ""Mac OS X"" and later ""OS X"") is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. macOS is the successor to the original classic Mac OS, which had been Apple's primary operating system since 1984. Unlike its predecessor, macOS is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997.
The operating system was first released in 1999 as Mac OS X Server 1.0, followed in March 2001 by a client version (Mac OS X v10.0 ""Cheetah""). Since then, six more distinct ""client"" and ""server"" editions of macOS have been released, until the two were merged in OS X 10.7 ""Lion"".
 Prior to its merging with macOS, the server edition – macOS Server – was architecturally identical to its desktop counterpart and usually ran on Apple's line of Macintosh server hardware. macOS Server included work group management and administration software tools that provide simplified access to key network services, including a mail transfer agent, a Samba server, an LDAP server, a domain name server, and others. With Mac OS X v10.7 Lion, all server aspects of Mac OS X Server have been integrated into the client version and the product re-branded as ""OS X"" (dropping ""Mac"" from the name). The server tools are now offered as an application.[19]
 First introduced as the OpenEdition upgrade to MVS/ESA System Product Version 4 Release 3, announced[20] February 1993 with support for POSIX and other standards.[21][22][23] z/OS UNIX System Services is built on top of MVS services and cannot run independently. While IBM initially introduced OpenEdition to satisfy FIPS requirements, several z/OS component now require UNIX services, e.g., TCP/IP.
 The Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel.
 Linux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smartwatches. Although estimates suggest that Linux is used on only 2.81% of all ""desktop"" (or laptop) PCs,[3] it has been widely adopted for use in servers[28] and embedded systems[29] such as cell phones. 
 Linux has superseded Unix on many platforms and is used on most supercomputers, including all 500 most powerful supercomputers on the TOP500 list — having displaced all competitors by 2017.[30] Linux is also commonly used on other small energy-efficient computers, such as smartphones and smartwatches. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google's Android, ChromeOS, and ChromiumOS.
 Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to x86 architecture based computers. As of 2022[update], its worldwide market share on all platforms was approximately 30%,[31] and on the desktop/laptop platforms, its market share was approximately 75%.[32] The latest version is Windows 11.
 Microsoft Windows was first released in 1985, as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS[33][34] and 16-bit Windows 3.x[35] drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and Arm microprocessors.[36] In the past, Windows NT supported additional architectures.
 Server editions of Windows are widely used, however, Windows' usage on servers is not as widespread as on personal computers as Windows competes against Linux and BSD for server market share.[37][38]
 ReactOS is a Windows-alternative operating system, which is being developed on the principles of Windows – without using any of Microsoft's code.
 There have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; classic Mac OS, the non-Unix precursor to Apple's macOS; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications.
 The z/OS operating system for IBM z/Architecture mainframe computers is still being used and developed, and 
OpenVMS, formerly from DEC, is still under active development by VMS Software Inc. The IBM i operating system for IBM AS/400 and IBM Power Systems midrange computers is also still being used and developed.
 Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is MINIX, while for example Singularity is used purely for research. Another example is the Oberon System designed at ETH Zürich by Niklaus Wirth, Jürg Gutknecht and a group of students at the former Computer Systems Institute in the 1980s. It was used mainly for research, teaching, and daily work in Wirth's group.
 Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' Plan 9.
 The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.
 With the aid of firmware and device drivers, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc.
 The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program typically involves the creation of a process by the operating system kernel, which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program, which then interacts with the user and with hardware devices. However, in some systems an application can request that the operating system execute another application within the same process, either as a subroutine or in a separate thread, e.g., the LINK and ATTACH facilities of OS/360 and successors.
 An interrupt (also known as an abort, exception, fault, signal,[39] or trap)[40] provides an efficient way for most operating systems to react to the environment. Interrupts cause the central processing unit (CPU) to have a control flow change away from the currently running program to an interrupt handler, also known as an interrupt service routine (ISR).[41][42] An interrupt service routine may cause the central processing unit (CPU) to have a context switch.[43][d] The details of how a computer processes an interrupt vary from architecture to architecture, and the details of how interrupt service routines behave vary from operating system to operating system.[44] However, several interrupt functions are common.[44] The architecture and operating system must:[44]
 A software interrupt is a message to a process that an event has occurred.[39] This contrasts with a hardware interrupt — which is a message to the central processing unit (CPU) that an event has occurred.[45] Software interrupts are similar to hardware interrupts — there is a change away from the currently running process.[46] Similarly, both hardware and software interrupts execute an interrupt service routine.
 Software interrupts may be normally occurring events. It is expected that a time slice will occur, so the kernel will have to perform a context switch.[47] A computer program may set a timer to go off after a few seconds in case too much data causes an algorithm to take too long.[48]
 Software interrupts may be error conditions, such as a malformed machine instruction.[48] However, the most common error conditions are division by zero and accessing an invalid memory address.[48]
 Users can send messages to the kernel to modify the behavior of a currently running process.[48] For example, in the command-line environment, pressing the interrupt character (usually Control-C) might terminate the currently running process.[48]
 To generate software interrupts for x86 CPUs, the INT assembly language instruction is available.[49] The syntax is INT X, where X is the offset number (in hexadecimal format) to the interrupt vector table.
 To generate software interrupts in Unix-like operating systems, the kill(pid,signum) system call will send a signal to another process.[50] pid is the process identifier of the receiving process. signum is the signal number (in mnemonic format)[e] to be sent. (The abrasive name of kill was chosen because early implementations only terminated the process.)[51]
 In Unix-like operating systems, signals inform processes of the occurrence of asynchronous events.[50] To communicate asynchronously, interrupts are required.[52] One reason a process needs to asynchronously communicate to another process solves a variation of the classic reader/writer problem.[53] The writer receives a pipe from the shell for its output to be sent to the reader's input stream.[54] The command-line syntax is alpha | bravo. alpha will write to the pipe when its computation is ready and then sleep in the wait queue.[55] bravo will then be moved to the ready queue and soon will read from its input stream.[56] The kernel will generate software interrupts to coordinate the piping.[56]
 Signals may be classified into 7 categories.[50] The categories are:
 Input/Output (I/O) devices are slower than the CPU. Therefore, it would slow down the computer if the CPU had to wait for each I/O to finish. Instead, a computer may implement interrupts for I/O completion, avoiding the need for polling or busy waiting.[57]
 Some computers require an interrupt for each character or word, costing a significant amount of CPU time. Direct memory access (DMA) is an architecture feature to allow devices to bypass the CPU and access main memory directly.[58] (Separate from the architecture, a device may perform direct memory access[f] to and from main memory either directly or via a bus.)[59][g]
 When a computer user types a key on the keyboard, typically the character appears immediately on the screen. Likewise, when a user moves a mouse, the cursor immediately moves across the screen. Each keystroke and mouse movement generates an interrupt called Interrupt-driven I/O. An interrupt-driven I/O occurs when a process causes an interrupt for every character[59] or word[60] transmitted.
 Devices such as hard disk drives, solid state drives, and magnetic tape drives can transfer data at a rate high enough that interrupting the CPU for every byte or word transferred, and having the CPU transfer the byte or word between the device and memory, would require too much CPU time. Data is, instead, transferred between the device and memory independently of the CPU by hardware such as a channel or a direct memory access controller; an interrupt is delivered only when all the data is transferred.[61]
 If a computer program executes a system call to perform a block I/O write operation, then the system call might execute the following instructions:
 While the writing takes place, the operating system will context switch to other processes as normal. When the device finishes writing, the device will interrupt the currently running process by asserting an interrupt request. The device will also place an integer onto the data bus.[65] Upon accepting the interrupt request, the operating system will:
 When the writing process has its time slice expired, the operating system will:[66]
 With the program counter now reset, the interrupted process will resume its time slice.[44]
 Modern computers support multiple modes of operation. CPUs with this capability offer at least two modes: user mode and supervisor mode. In general terms, supervisor mode operation allows unrestricted access to all machine resources, including all MPU instructions. User mode operation sets limits on instruction use and typically disallows direct access to machine resources. CPUs might have other modes similar to user mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.
 At power-on or reset, the system begins in supervisor mode. Once an operating system kernel has been loaded and started, the boundary between user mode and supervisor mode (also known as kernel mode) can be established.
 Supervisor mode is used by the kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is accessed, and communicating with devices such as disk drives and video display devices. User mode, in contrast, is used for almost everything else. Application programs, such as word processors and database managers, operate within user mode, and can only access machine resources by turning control over to the kernel, a process which causes a switch to supervisor mode. Typically, the transfer of control to the kernel is achieved by executing a software interrupt instruction, such as the Motorola 68000 TRAP instruction. The software interrupt causes the processor to switch from user mode to supervisor mode and begin executing code that allows the kernel to take control.
 In user mode, programs usually have access to a restricted set of processor instructions, and generally cannot execute any instructions that could potentially cause disruption to the system's operation. In supervisor mode, instruction execution restrictions are typically removed, allowing the kernel unrestricted access to all machine resources.
 The term ""user mode resource"" generally refers to one or more CPU registers, which contain information that the running program is not allowed to alter. Attempts to alter these resources generally cause a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting; for example, by forcibly terminating (""killing"") the program.
 Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by the programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.
 Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.
 Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which does not exist in all computers.
 In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt, which causes the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.
 Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.
 The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.
 If a program tries to access memory that is not in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault.
 When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet.
 In modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.
 ""Virtual memory"" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.[67]
 Multitasking refers to the running of multiple independent computer programs on the same computer, giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.
 An operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch.
 An early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop.
 Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.
 The philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)
 On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. AmigaOS is an exception, having preemptive multitasking from its first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it did not reach the home user market until Windows XP (since Windows NT was targeted at professionals).
 Access to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use of the drive's available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree.
 Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.
 While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers.
 A connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices.
 When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.
 Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ReiserFS, Reiser4, ext3, ext4 and Btrfs in Linux. However, in practice, third party drivers are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software).
 Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on. It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system.
 A device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.
 The key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system's point of view.
 Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do.
 Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface.
 Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's IP address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.
 Many operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware.
 Security means protecting users from other users of the same computer, as well as from those who seeking remote access to it over a network.[68]  Operating systems security rests on achieving the CIA triad: confidentiality (unauthorized users cannot access data), integrity (unauthorized users cannot modify data), and availability (ensuring that the system remains available to authorized users, even in the event of a denial of service attack).[69] As with other computer systems, isolating security domains—in the case of operating systems, the kernel, processes, and virtual machines—is key to achieving security.[70] Other ways to increase security include simplicity to minimize the attack surface, locking access to resources by default, checking all requests for authorization, principle of least authority (granting the minimum privilege essential for performing a task), privilege separation, and reducing shared data.[71]
 Some operating system designs are more secure than others. Those with no isolation between the kernel and applications are least secure, while those with a monolithic kernel like most general-purpose operating systems are still vulnerable if any part of the kernel is compromised. A more secure design features microkernels that separate the kernel's privileges into many separate security domains and reduce the consequences of a single kernel breach.[72] Unikernels are another approach that improves security by minimizing the kernel and separating out other operating systems functionality by application.[72]
 Most operating systems are written in C or C++, which can cause vulnerabilities. Despite various attempts to protect against them, a substantial number of vulnerabilities are caused by buffer overflow attacks, which are enabled by the lack of bounds checking.[73]  Hardware vulnerabilities, some of them caused by CPU optimizations, can also be used to compromise the operating system.[74] Programmers coding the operating system may have deliberately implanted vulnerabilities, such as back doors.[75]
 Operating systems security is hampered by their increasing complexity and the resulting inevitability of bugs.[76] Because formal verification of operating systems may not be feasible, operating systems developers use hardening to reduce vulnerabilities,[77] such as address space layout randomization, control-flow integrity,[78] access restrictions,[79] and other techniques.[80] Anyone can contribute code to open source operating systems, which have transparent code histories and distributed governance structures.[81] Their developers work together to find and eliminate security vulnerabilities, using techniques such as code review and type checking to avoid malicious code.[82][83] Andrew S. Tanenbaum advises releasing the source code of all operating systems, arguing that it prevents the developer from falsely believing it is secret and relying on security by obscurity.[84]
 Every computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported. The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present.
 Most of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of the classic Mac OS, the GUI is integrated into the kernel.
 While technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of context switches required for the GUI to perform its output functions. Other operating systems are modular, separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and macOS are also built this way. Modern releases of Microsoft Windows such as Windows Vista implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between Windows NT 4.0 and Windows Server 2003 exist mostly in kernel space. Windows 9x had very little distinction between the interface and the kernel.
 Many computer operating systems allow the user to install or create any user interface they desire. The X Window System in conjunction with GNOME or KDE Plasma 5 is a commonly found setup on most Unix and Unix-like (BSD, Linux, Solaris) systems. A number of Windows shell replacements have been released for Microsoft Windows, which offer alternatives to the included Windows shell, but the shell itself cannot be separated from Windows.
 Numerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to COSE and CDE failed for various reasons, and were eventually eclipsed by the widespread adoption of GNOME and K Desktop Environment. Prior to free software-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).
 Graphical user interfaces evolve over time. For example, Windows has modified its user interface almost every time a new major version of Windows is released, and the Mac OS GUI changed dramatically with the introduction of Mac OS X in 1999.[85]
 A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.
 An early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System.
 Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase.[86] Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.
 Some embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing.
 A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.[citation needed]
 In some cases, hobby development is in support of a ""homebrew"" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is her/his own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
 Examples of a hobby operating system include Syllable and TempleOS.
 If an application is written for use on a specific operating system, and is ported to another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.
 This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.
 Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['page fault', 'Syllable and TempleOS', 'command-line interface', 'the hobbyist is her/his own developer', 'interacts with the user and with hardware devices'], 'answer_start': [], 'answer_end': []}"
"
 The Internet of things (IoT) describes devices with sensors, processing ability, software and other technologies that connect and exchange data with other devices and systems over the Internet or other communications networks.[1][2][3][4][5] The Internet of things encompasses electronics, communication, and computer science engineering. ""Internet of things"" has been considered a misnomer because devices do not need to be connected to the public internet; they only need to be connected to a network[6] and be individually addressable.[7][8]
 The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, and increasingly powerful embedded systems, as well as machine learning.[9] Older fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), independently and collectively enable the Internet of things.[10]  In the consumer market, IoT technology is most synonymous with ""smart home"" products, including devices and appliances (lighting fixtures, thermostats, home security systems, cameras, and other home appliances) that support one or more common ecosystems and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers. IoT is also used in healthcare systems.[11]
 There are a number of concerns about the risks in the growth of IoT technologies and products, especially in the areas of privacy and security, and consequently there have been industry and government moves to address these concerns, including the development of international and local standards, guidelines, and regulatory frameworks.[12] Because of their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer.[13]
 The main concept of a network of smart devices was discussed as early as 1982, with a modified Coca-Cola vending machine at Carnegie Mellon University becoming the first ARPANET-connected appliance,[14] able to report its inventory and whether newly loaded drinks were cold or not.[15] Mark Weiser's 1991 paper on ubiquitous computing, ""The Computer of the 21st Century"", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IOT.[16][17] In 1994, Reza Raji described the concept in IEEE Spectrum as ""[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories"".[18] Between 1993 and 1997, several companies proposed solutions like Microsoft's at Work or Novell's NEST. The field gained momentum when Bill Joy envisioned device-to-device communication as a part of his ""Six Webs"" framework, presented at the World Economic Forum at Davos in 1999.[19]
 The concept of the ""Internet of things"" and the term itself, first appeared in a speech by Peter T. Lewis, to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C., published in September 1985.[20] According to Lewis, ""The Internet of Things, or IoT, is the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices.""[21]
 The term ""Internet of things"" was coined independently by Kevin Ashton of Procter & Gamble, later of MIT's Auto-ID Center, in 1999,[22] though he prefers the phrase ""Internet for things"".[23] At that point, he viewed radio-frequency identification (RFID) as essential to the Internet of things,[24] which would allow computers to manage all individual things.[25][26][27] The main theme of the Internet of things is to embed short-range mobile transceivers in various gadgets and daily necessities to enable new forms of communication between people and things, and between things themselves.[28]
 In 2004 Cornelius ""Pete"" Peterson, CEO of NetSilicon, predicted that, ""The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations."" Peterson believed that medical devices and industrial controls would become dominant applications of the technology.[29]
 Defining the Internet of things as ""simply the point in time when more 'things or objects' were connected to the Internet than people"", Cisco Systems estimated that the IoT was ""born"" between 2008 and 2009, with the things/people ratio growing from 0.08 in 2003 to 1.84 in 2010.[30]
 The extensive set of applications for IoT devices[31] is often divided into consumer, commercial, industrial, and infrastructure spaces.[32][33]
 A growing portion of IoT devices is created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities.[34]
 IoT devices are a part of the larger concept of home automation, which can include lighting, heating and air conditioning, media and security systems and camera systems.[35][36] Long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage.[37]
 A smart home or automated home could be based on a platform or hubs that control smart devices and appliances.[38] For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch.[39][40] This could be a dedicated app or iOS native applications such as Siri.[41] This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge.[41] There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products. These include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub.[42] In addition to the commercial systems, there are many non-proprietary, open source ecosystems, including Home Assistant, OpenHAB and Domoticz.[43]
 One key application of a smart home is to assist the elderly and disabled. These home systems use assistive technology to accommodate an owner's specific disabilities.[44] Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by hearing-impaired users.[45] They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or seizures.[46] Smart home technology applied in this way can provide users with more freedom and a higher quality of life.[44]
 The term ""Enterprise IoT"" refers to devices used in business and corporate settings. By 2019, it is estimated that the EIoT will account for 9.1 billion devices.[32]
 The Internet of Medical Things (IoMT) is an application of the IoT for medical and health-related purposes, data collection and analysis for research, and monitoring.[47][48][49][50][51] The IoMT has been referenced as ""Smart Healthcare"",[52] as the technology for creating a digitized healthcare system, connecting available medical resources and healthcare services.[53][54]
 IoT devices can be used to enable remote health monitoring and emergency notification systems. These health monitoring devices can range from blood pressure and heart rate monitors to advanced devices capable of monitoring specialized implants, such as pacemakers, Fitbit electronic wristbands, or advanced hearing aids.[55] Some hospitals have begun implementing ""smart beds"" that can detect when they are occupied and when a patient is attempting to get up. It can also adjust itself to ensure appropriate pressure and support are applied to the patient without the manual interaction of nurses.[47] A 2015 Goldman Sachs report indicated that healthcare IoT devices ""can save the United States more than $300 billion in annual healthcare expenditures by increasing revenue and decreasing cost.""[56] Moreover, the use of mobile devices to support medical follow-up led to the creation of 'm-health', used analyzed health statistics.""[57]
 Specialized sensors can also be equipped within living spaces to monitor the health and general well-being of senior citizens, while also ensuring that proper treatment is being administered and assisting people to regain lost mobility via therapy as well.[58] These sensors create a network of intelligent sensors that are able to collect, process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems.[52] Other consumer devices to encourage healthy living, such as connected scales or wearable heart monitors, are also a possibility with the IoT.[59] End-to-end health monitoring IoT platforms are also available for antenatal and chronic patients, helping one manage health vitals and recurring medication requirements.[60]
 Advances in plastic and fabric electronics fabrication methods have enabled ultra-low cost, use-and-throw IoMT sensors. These sensors, along with the required RFID electronics, can be fabricated on paper or e-textiles for wireless powered disposable sensing devices.[61] Applications have been established for point-of-care medical diagnostics, where portability and low system-complexity is essential.[62]
 As of 2018[update] IoMT was not only being applied in the clinical laboratory industry,[49] but also in the healthcare and health insurance industries. IoMT in the healthcare industry is now permitting doctors, patients, and others, such as guardians of patients, nurses, families, and similar, to be part of a system, where patient records are saved in a database, allowing doctors and the rest of the medical staff to have access to patient information.[63] IoMT in the insurance industry provides access to better and new types of dynamic information. This includes sensor-based solutions such as biosensors, wearables, connected health devices, and mobile apps to track customer behavior. This can lead to more accurate underwriting and new pricing models.[64]
 The application of the IoT in healthcare plays a fundamental role in managing chronic diseases and in disease prevention and control. Remote monitoring is made possible through the connection of powerful wireless solutions. The connectivity enables health practitioners to capture patient's data and apply complex algorithms in health data analysis.[65]
 The IoT can assist in the integration of communications, control, and information processing across various transportation systems. Application of the IoT extends to all aspects of transportation systems (i.e., the vehicle,[66] the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication,[67] smart traffic control, smart parking, electronic toll collection systems, logistics and fleet management, vehicle control, safety, and road assistance.[55][68]
 In vehicular communication systems, vehicle-to-everything communication (V2X), consists of three main components: vehicle-to-vehicle communication (V2V), vehicle-to-infrastructure communication (V2I) and vehicle to pedestrian communications (V2P). V2X is the first step to autonomous driving and connected road infrastructure.[69]
 IoT devices can be used to monitor and control the mechanical, electrical and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential)[55] in home automation and building automation systems. In this context, three main areas are being covered in literature:[70]
 Also known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, operational technology (OT), locations, and people. Combined with operational technology (OT) monitoring devices, IIoT helps regulate and monitor industrial systems.[71] Also, the same implementation can be carried out for automated record updates of asset placement in industrial storage units as the size of the assets can vary from a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money.
 The IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities.[72] Network control and management of manufacturing equipment, asset and situation management, or manufacturing process control allow IoT to be used for industrial applications and smart manufacturing.[73] IoT intelligent systems enable rapid manufacturing and optimization of new products and rapid response to product demands.[55]
 Digital control systems to automate process controls, operator tools and service information systems to optimize plant safety and security are within the purview of the IIoT.[74] IoT can also be applied to asset management via predictive maintenance, statistical evaluation, and measurements to maximize reliability.[75] Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors.[55]
 In addition to general manufacturing, IoT is also used for processes in the industrialization of construction.[76]
 There are numerous IoT applications in farming[77] such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, take informed decisions to improve quality and quantity, minimize risk and waste, and reduce the effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar and even apply IoT-acquired data to precision fertilization programs.[78] The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs.
 In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide.[79] The FarmBeats project[80] from Microsoft Research that uses TV white space to connect farms is also a part of the Azure Marketplace now.[81]
 IoT devices are in use to monitor the environments and systems of boats and yachts.[82] Many pleasure boats are left unattended for days in summer, and months in winter so such devices provide valuable early alerts of boat flooding, fire, and deep discharge of batteries. The use of global internet data networks such as Sigfox, combined with long-life batteries, and microelectronics allows the engine rooms, bilge, and batteries to be constantly monitored and reported to connected Android & Apple applications for example.
 Monitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind farms is a key application of the IoT.[74] The IoT infrastructure can be used for monitoring any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost-saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and saving money in Real-Time Data Analytics. It can also be used for scheduling repair and maintenance activities efficiently, by coordinating tasks between different service providers and users of these facilities.[55] IoT devices can also be used to control critical infrastructure like bridges to provide access to ships. The usage of IoT devices for monitoring and operating infrastructure is likely to improve incident management and emergency response coordination, and quality of service, up-times and reduce costs of operation in all infrastructure-related areas.[83] Even areas such as waste management can benefit[84] from automation and optimization that could be brought in by the IoT.[citation needed]
 There are several planned or ongoing large-scale deployments of the IoT, to enable better management of cities and systems. For example, Songdo, South Korea, the first of its kind fully equipped and wired smart city, is gradually being built[when?], with approximately 70 percent of the business district completed as of June 2018[update]. Much of the city is planned to be wired and automated, with little or no human intervention.[85]
 Another application is currently[when?] undergoing a project in Santander, Spain. For this deployment, two approaches have been adopted. This city of 180,000 inhabitants has already seen 18,000 downloads of its city smartphone app. The app is connected to 10,000 sensors that enable services like parking search, and environmental monitoring. City context information is used in this deployment so as to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification.[86]
 Other examples of large-scale deployments underway include the Sino-Singapore Guangzhou Knowledge City;[87] work on improving air and water quality, reducing noise pollution, and increasing transportation efficiency in San Jose, California;[88] and smart traffic management in western Singapore.[89] Using its RPMA (Random Phase Multiple Access) technology, San Diego-based Ingenu has built a nationwide public network[90] for low-bandwidth data transmissions using the same unlicensed 2.4 gigahertz spectrum as Wi-Fi. Ingenu's ""Machine Network"" covers more than a third of the US population across 35 major cities including San Diego and Dallas.[91] French company, Sigfox, commenced building an Ultra Narrowband wireless data network in the San Francisco Bay Area in 2014, the first business to achieve such a deployment in the U.S.[92][93] It subsequently announced it would set up a total of 4000 base stations to cover a total of 30 cities in the U.S. by the end of 2016, making it the largest IoT network coverage provider in the country thus far.[94][95] Cisco also participates in smart cities projects. Cisco has deployed technologies for Smart Wi-Fi, Smart Safety & Security, Smart Lighting, Smart Parking, Smart Transports, Smart Bus Stops, Smart Kiosks, Remote Expert for Government Services (REGS) and Smart Education in the five km area in the city of Vijaywada, India.[96][97]
 Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live 24/7. The network was designed and engineered by Fluidmesh Networks, a Chicago-based company developing wireless networks for critical applications. The NYWW network is currently providing coverage on the Hudson River, East River, and Upper New York Bay. With the wireless network in place, NY Waterway is able to take control of its fleet and passengers in a way that was not previously possible. New applications can include security, energy and fleet management, digital signage, public Wi-Fi, paperless ticketing and others.[98]
 Significant numbers of energy-consuming devices (e.g. lamps, household appliances, motors, pumps, etc.) already integrate Internet connectivity, which can allow them to communicate with utilities not only to balance power generation but also helps optimize the energy consumption as a whole.[55] These devices allow for remote control by users, or central management via a cloud-based interface, and enable functions like scheduling (e.g., remotely powering on or off heating systems, controlling ovens, changing lighting conditions etc.).[55] The smart grid is a utility-side IoT application; systems gather and act on energy and power-related information to improve the efficiency of the production and distribution of electricity.[99] Using advanced metering infrastructure (AMI) Internet-connected devices, electric utilities not only collect data from end-users, but also manage distribution automation devices like transformers.[55]
 Environmental monitoring applications of the IoT typically use sensors to assist in environmental protection[100] by monitoring air or water quality,[101] atmospheric or soil conditions,[102] and can even include areas like monitoring the movements of wildlife and their habitats.[103] Development of resource-constrained devices connected to the Internet also means that other applications like earthquake or tsunami early-warning systems can also be used by emergency services to provide more effective aid. IoT devices in this application typically span a large geographic area and can also be mobile.[55] It has been argued that the standardization that IoT brings to wireless sensing will revolutionize this area.[104]
 Living Lab
 Another example of integrating the IoT is Living Lab which integrates and combines research and innovation processes, establishing within a public-private-people-partnership.[105] Between 2006 and January 2024, there were over 440 Living Labs (though not all are currently active)[106] that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. For companies to implement and develop IoT services for smart cities, they need to have incentives. The governments play key roles in smart city projects as changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the government provides tax incentives and cheap rent, improves public transports, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production process, and transaction costs.[105]
 The Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield.[107]
 One of the examples of IOT devices used in the military is Xaver 1000 system. The Xaver 1000 was developed by Israel's Camero Tech, which is the latest in the company's line of ""through wall imaging systems"". The Xaver line uses millimeter wave (MMW) radar, or radar in the range of 30-300 gigahertz. It is equipped with an AI-based life target tracking system as well as its own 3D 'sense-through-the-wall' technology.[108]
 The Internet of Battlefield Things (IoBT) is a project initiated and executed by the U.S. Army Research Laboratory (ARL) that focuses on the basic science related to the IoT that enhance the capabilities of Army soldiers.[109] In 2017, ARL launched the Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA), establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations.[110][111]
 The Ocean of Things project is a DARPA-led program designed to establish an Internet of things across large ocean areas for the purposes of collecting, monitoring, and analyzing environmental and vessel activity data. The project entails the deployment of about 50,000 floats that house a passive sensor suite that autonomously detect and track military and commercial vessels as part of a cloud-based network.[112]
 There are several applications of smart or active packaging in which a QR code or NFC tag is affixed on a product or its packaging. The tag itself is passive, however, it contains a unique identifier (typically a URL) which enables a user to access digital content about the product via a smartphone.[113] Strictly speaking, such passive items are not part of the Internet of things, but they can be seen as enablers of digital interactions.[114] The term ""Internet of Packaging"" has been coined to describe applications in which unique identifiers are used, to automate supply chains, and are scanned on large scale by consumers to access digital content.[115] Authentication of the unique identifiers, and thereby of the product itself, is possible via a copy-sensitive digital watermark or copy detection pattern for scanning when scanning a QR code,[116] while NFC tags can encrypt communication.[117]
 The IoT's major significant trend in recent years[when?] is the explosive growth of devices connected and controlled via the Internet.[118] The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.
 The IoT creates opportunities for more direct integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions.[119][120][121][122]
 The number of IoT devices increased 31% year-over-year to 8.4 billion in the year 2017[123] and it is estimated that there will be 30 billion devices by 2020.[118]
 Ambient intelligence and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as Intel) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT.[124] An approach in this context is deep reinforcement learning where most of IoT systems provide a dynamic and interactive environment.[125] Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as supervised learning. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn HVAC on or off) and learn through the maximizing accumulated rewards it receives in long term.
 IoT intelligence can be offered at three levels: IoT devices, Edge/Fog nodes, and cloud computing.[126] The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time obstacle detection to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder.[127][126]
 In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend,[128] clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments.[129]
 IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud.[130] Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as Modbus, Bluetooth, Zigbee, or proprietary protocols, to connect to an Edge Gateway.[130] The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing.[130] Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various database systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, PostgreSQL).[130] The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers.[131] Some experts classified the three-tiers in the IoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively.[132]
 Building on the Internet of things, the web of things is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices.[citation needed]
 The Internet of things requires huge scalability in the network space to handle the surge of devices.[133] IETF 6LoWPAN can be used to connect devices to IP networks. With billions of devices[134] being added to the Internet space, IPv6 will play a major role in handling the network layer scalability. IETF's Constrained Application Protocol, ZeroMQ, and MQTT can provide lightweight data transport. In practice many groups of IoT devices are hidden behind gateway nodes and may not have unique addresses. Also the vision of everything-interconnected is not needed for most applications as it is mainly the data which need interconnecting at a higher layer.[citation needed]
 Fog computing is a viable alternative to prevent such a large burst of data flow through the Internet.[135] The edge devices' computation power to analyze and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the internet to a server with sufficient processing power.[136]
 Decentralized Internet of things, or decentralized IoT, is a modified IoT which utilizes fog computing to handle and balance requests of connected IoT devices in order to reduce loading on the cloud servers and improve responsiveness for latency-sensitive IoT applications like vital signs monitoring of patients, vehicle-to-vehicle communication of autonomous driving, and critical failure detection of industrial devices.[137] Performance is improved, especially for huge IoT systems with millions of nodes.[138]
 Conventional IoT is connected via a mesh network and led by a major head node (centralized controller).[139] The head node decides how a data is created, stored, and transmitted.[140] In contrast, decentralized IoT attempts to divide IoT systems into smaller divisions.[141] The head node authorizes partial decision-making power to lower level sub-nodes under mutual agreed policy.[142]
 Some approached to decentralized IoT attempts to address the limited bandwidth and hashing capacity of battery powered or wireless IoT devices via blockchain.[143][144][145]
 In semi-open or closed loops (i.e., value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a complex system[146] due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality). 
As a practical approach, not all elements on the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability. For example, domestic robotics (domotics) running inside a smart home might only share data within and be available via a local network.[147] Managing and controlling a high dynamic ad hoc IoT things/devices network is a tough task with the traditional networks architecture, Software Defined Networking (SDN) provides the agile dynamic solution that can cope with the special requirements of the diversity of innovative IoT applications.[148][149]
 The exact scale of the Internet of things is unknown, with quotes of billions or trillions often quoted at the beginning of IoT articles. In 2015 there were 83 million smart devices in people's homes. This number is expected to grow to 193 million devices by 2020.[36][150]
 The figure of online capable devices grew 31% from 2016 to 2017 to reach 8.4 billion.[123]
 In the Internet of things, the precise geographic location of a thing—and also the precise geographic dimensions of a thing—can be critical.[151] Therefore, facts about a thing, such as its location in time and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things on the Internet of things will be sensors, and sensor location is usually important.[152]) The GeoWeb and Digital Earth are applications that become possible when things can become organized and connected by location. However, the challenges that remain include the constraints of variable spatial scales, the need to handle massive amounts of data, and an indexing for fast search and neighbour operations. On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role on the Internet and the Web, geo-spatial standards will play a key role on the Internet of things.[153][154]
 Many IoT devices have the potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note,[155] where he predicts that the most likely problem will be what he calls the ""basket of remotes"" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another.[155] For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, ""where collected data is used to predict and trigger actions on the specific devices"" while making them work together.[156]
 Social Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices.[157] SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services,[158] and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering.[159]
 IoT defines a device with an identity like a citizen in a community and connect them to the internet to provide services to its users.[160] SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human.[161]
 SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users.[162]
 IoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, navigates and groups with other IoT devices in the same or nearby network for useful service compositions in order to help its users proactively in every day's life especially during emergency.[163]
 There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:[170][171][172]
 The original idea of the Auto-ID Center is based on RFID-tags and distinct identification through the Electronic Product Code. This has evolved into objects having an IP address or URI.[173] An alternative view, from the world of the Semantic Web[174] focuses instead on making all things (not just those electronic, smart, or RFID-enabled) addressable by the existing naming protocols, such as URI. The objects themselves do not converse, but they may now be referred to by other agents, such as powerful centralised servers acting for their human owners.[175] Integration with the Internet implies that devices will use an IP address as a distinct identifier. Due to the limited address space of IPv4 (which allows for 4.3 billion different addresses), objects in the IoT will have to use the next generation of the Internet protocol (IPv6) to scale to the extremely large address space required.[176][177][178]
Internet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6,[179] as it reduces the configuration overhead on the hosts,[177] and the IETF 6LoWPAN header compression. To a large extent, the future of the Internet of things will not be possible without the support of IPv6; and consequently, the global adoption of IPv6 in the coming years will be critical for the successful development of the IoT in the future.[178]
 Different technologies have different roles in a protocol stack. Below is a simplified[notes 1] presentation of the roles of several popular communication technologies in IoT applications:
 This is a list of technical standards for the IoT, most of which are open standards, and the standards organizations that aspire to successfully setting them.[194][195]
 The GS1 digital link standard,[199] first released in August 2018, allows the use QR Codes, GS1 Datamatrix, RFID and NFC to enable various types of business-to-business, as well as business-to-consumers interactions.
 Some scholars and activists argue that the IoT can be used to create new models of civic engagement if device networks can be open to user control and inter-operable platforms. Philip N. Howard, a professor and author, writes that political life in both democracies and authoritarian regimes will be shaped by the way the IoT will be used for civic engagement. For that to happen, he argues that any connected device should be able to divulge a list of the ""ultimate beneficiaries"" of its sensor data and that individual citizens should be able to add new organisations to the beneficiary list. In addition, he argues that civil society groups need to start developing their IoT strategy for making use of data and engaging with the public.[201]
 One of the key drivers of the IoT is data. The success of the idea of connecting devices to make them more efficient is dependent upon access to and storage & processing of data. For this purpose, companies working on the IoT collect data from multiple sources and store it in their cloud network for further processing. This leaves the door wide open for privacy and security dangers and single point vulnerability of multiple systems.[202] The other issues pertain to consumer choice and ownership of data[203] and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop.[204][205][206] IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995.[207]
 Current regulatory environment:
 A report published by the Federal Trade Commission (FTC) in January 2015 made the following three recommendations:[208]
 However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights.[210]
 A resolution passed by the Senate in March 2015, is already being considered by the Congress.[211] This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the Federal Communications Commission to assess the need for more spectrum to connect IoT devices.
 Approved on 28 September 2018, California Senate Bill No. 327[212] goes into effect on 1 January 2020. The bill requires ""a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure,""
 Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the National Highway Traffic Safety Administration (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure.[213]
 A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT.[214] These include –
 In early December 2021, the U.K. government introduced the Product Security and Telecommunications Infrastructure bill (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards. The bill also seeks to improve the security credentials of consumer IoT devices.[215]
 The IoT suffers from platform fragmentation, lack of interoperability and common technical standards[216][217][218][219][220][221][222][excessive citations] a situation where the variety of IoT devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently between different inconsistent technology ecosystems hard.[1] For example, wireless connectivity for IoT devices can be done using Bluetooth, Wi-Fi, Wi-Fi HaLow, Zigbee, Z-Wave, LoRa, NB-IoT, Cat M1 as well as completely custom proprietary radios – each with its own advantages and disadvantages; and unique support ecosystem.[223]
 The IoT's amorphous computing nature is also a problem for security, since patches to bugs found in the core operating system often do not reach users of older and lower-price devices.[224][225][226] One set of researchers says that the failure of vendors to support older devices with patches and updates leaves more than 87% of active Android devices vulnerable.[227][228]
 Philip N. Howard, a professor and author, writes that the Internet of things offers immense potential for empowering citizens, making government transparent, and broadening information access. Howard cautions, however, that privacy threats are enormous, as is the potential for social control and political manipulation.[229]
 Concerns about privacy have led many to consider the possibility that big data infrastructures such as the Internet of things and data mining are inherently incompatible with privacy.[230] Key challenges of increased digitalization in the water, transport or energy sector are related to privacy and cybersecurity which necessitate an adequate response from research and policymakers alike.[231]
 Writer Adam Greenfield claims that IoT technologies are not only an invasion of public space but are also being used to perpetuate normative behavior, citing an instance of billboards with hidden cameras that tracked the demographics of passersby who stopped to read the advertisement.
 The Internet of Things Council compared the increased prevalence of digital surveillance due to the Internet of things to the concept of the panopticon described by Jeremy Bentham in the 18th century.[232] The assertion is supported by the works of French philosophers Michel Foucault and Gilles Deleuze. In Discipline and Punish: The Birth of the Prison, Foucault asserts that the panopticon was a central element of the discipline society developed during the Industrial Era.[233] Foucault also argued that the discipline systems established in factories and school reflected Bentham's vision of panopticism.[233] In his 1992 paper ""Postscripts on the Societies of Control"", Deleuze wrote that the discipline society had transitioned into a control society, with the computer replacing the panopticon as an instrument of discipline and control while still maintaining the qualities similar to that of panopticism.[234]
 Peter-Paul Verbeek, a professor of philosophy of technology at the University of Twente, Netherlands, writes that technology already influences our moral decision making, which in turn affects human agency, privacy and autonomy. He cautions against viewing technology merely as a human tool and advocates instead to consider it as an active agent.[235]
 Justin Brookman, of the Center for Democracy and Technology, expressed concern regarding the impact of the IoT on consumer privacy, saying that ""There are some people in the commercial space who say, 'Oh, big data – well, let's collect everything, keep it around forever, we'll pay for somebody to think about security later.' The question is whether we want to have some sort of policy framework in place to limit that.""[236]
 Tim O'Reilly believes that the way companies sell the IoT devices on consumers are misplaced, disputing the notion that the IoT is about gaining efficiency from putting all kinds of devices online and postulating that the ""IoT is really about human augmentation. The applications are profoundly different when you have sensors and data driving the decision-making.""[237]
 Editorials at WIRED have also expressed concern, one stating ""What you're about to lose is your privacy. Actually, it's worse than that. You aren't just going to lose your privacy, you're going to have to watch the very concept of privacy be rewritten under your nose.""[238]
 The American Civil Liberties Union (ACLU) expressed concern regarding the ability of IoT to erode people's control over their own lives. The ACLU wrote that ""There's simply no way to forecast how these immense powers – disproportionately accumulating in the hands of corporations seeking financial advantage and governments craving ever more control – will be used. Chances are big data and the Internet of Things will make it harder for us to control our own lives, as we grow increasingly transparent to powerful corporations and government institutions that are becoming more opaque to us.""[239]
 In response to rising concerns about privacy and smart technology, in 2007 the British Government stated it would follow formal Privacy by Design principles when implementing their smart metering program. The program would lead to replacement of traditional power meters with smart power meters, which could track and manage energy usage more accurately.[240] However the British Computer Society is doubtful these principles were ever actually implemented.[241] In 2009 the Dutch Parliament rejected a similar smart metering program, basing their decision on privacy concerns. The Dutch program later revised and passed in 2011.[241]
 A challenge for producers of IoT applications is to clean, process and interpret the vast amount of data which is gathered by the sensors. There is a solution proposed for the analytics of the information referred to as Wireless Sensor Networks.[242] These networks share data among sensor nodes that are sent to a distributed system for the analytics of the sensory data.[243]
 Another challenge is the storage of this bulk data. Depending on the application, there could be high data acquisition requirements, which in turn lead to high storage requirements. In 2013, the Internet was estimated to be responsible for consuming 5% of the total energy produced,[242] and a ""daunting challenge to power"" IoT devices to collect and even store data still remains.[244]
 Data silos, although a common challenge of legacy systems, still commonly occur with the implementation of IoT devices, particularly within manufacturing. As there are a lot of benefits to be gained from IoT and IIoT devices, the means in which the data is stored can present serious challenges without the principles of autonomy, transparency, and interoperability being considered.[245] The challenges do not occur by the device itself, but the means in which databases and data warehouses are set-up. These challenges were commonly identified in manufactures and enterprises which have begun upon digital transformation, and are part of the digital foundation, indicating that in order to receive the optimal benefits from IoT devices and for decision making, enterprises will have to first re-align their data storing methods. These challenges were identified by Keller (2021) when investigating the IT and application landscape of I4.0 implementation within German M&E manufactures.[245]
 Security is the biggest concern in adopting Internet of things technology,[246] with concerns that rapid development is happening without appropriate consideration of the profound security challenges involved[247] and the regulatory changes that might be necessary.[248][249] The rapid development of the Internet of Things (IoT) has allowed billions of devices to connect to the network. Due to too many connected devices and the limitation of communication security technology, various security issues gradually appear in the IoT.[250]
 Most of the technical security concerns are similar to those of conventional servers, workstations and smartphones.[251] These concerns include using weak authentication, forgetting to change default credentials, unencrypted messages sent between devices, SQL injections, man-in-the-middle attacks, and poor handling of security updates.[252][253] However, many IoT devices have severe operational limitations on the computational power available to them. These constraints often make them unable to directly use basic security measures such as implementing firewalls or using strong cryptosystems to encrypt their communications with other devices[254] - and the low price and consumer focus of many devices makes a robust security patching system uncommon.[255]
 Rather than conventional security vulnerabilities, fault injection attacks are on the rise and targeting IoT devices. A fault injection attack is a physical attack on a device to purposefully introduce faults in the system to change the intended behavior. Faults might happen unintentionally by environmental noises and electromagnetic fields. There are ideas stemmed from control-flow integrity (CFI) to prevent fault injection attacks and system recovery to a healthy state before the fault.[256]
 Internet of things devices also have access to new areas of data, and can often control physical devices,[257] so that even by 2014 it was possible to say that many Internet-connected appliances could already ""spy on people in their own homes"" including televisions, kitchen appliances,[258] cameras, and thermostats.[259] Computer-controlled devices in automobiles such as brakes, engine, locks, hood and trunk releases, horn, heat, and dashboard have been shown to be vulnerable to attackers who have access to the on-board network. In some cases, vehicle computer systems are Internet-connected, allowing them to be exploited remotely.[260] By 2008 security researchers had shown the ability to remotely control pacemakers without authority. Later hackers demonstrated remote control of insulin pumps[261] and implantable cardioverter defibrillators.[262]
 Poorly secured Internet-accessible IoT devices can also be subverted to attack others. In 2016, a distributed denial of service attack powered by Internet of things devices running the Mirai malware took down a DNS provider and major web sites.[263] The Mirai Botnet had infected roughly 65,000 IoT devices within the first 20 hours.[264] Eventually the infections increased to around 200,000 to 300,000 infections.[264] Brazil, Colombia and Vietnam made up of 41.5% of the infections.[264] The Mirai Botnet had singled out specific IoT devices that consisted of DVRs, IP cameras, routers and printers.[264] Top vendors that contained the most infected devices were identified as Dahua, Huawei, ZTE, Cisco, ZyXEL and MikroTik.[264] In May 2017, Junade Ali, a Computer Scientist at Cloudflare noted that native DDoS vulnerabilities exist in IoT devices due to a poor implementation of the Publish–subscribe pattern.[265][266] These sorts of attacks have caused security experts to view IoT as a real threat to Internet services.[267]
 The U.S. National Intelligence Council in an unclassified report maintains that it would be hard to deny ""access to networks of sensors and remotely-controlled objects by enemies of the United States, criminals, and mischief makers... An open market for aggregated sensor data could serve the interests of commerce and security no less than it helps criminals and spies identify vulnerable targets. Thus, massively parallel sensor fusion may undermine social cohesion, if it proves to be fundamentally incompatible with Fourth-Amendment guarantees against unreasonable search.""[268] In general, the intelligence community views the Internet of things as a rich source of data.[269]
 On 31 January 2019, the Washington Post wrote an article regarding the security and ethical challenges that can occur with IoT doorbells and cameras: ""Last month, Ring got caught allowing its team in Ukraine to view and annotate certain user videos; the company says it only looks at publicly shared videos and those from Ring owners who provide consent. Just last week, a California family's Nest camera let a hacker take over and broadcast fake audio warnings about a missile attack, not to mention peer in on them, when they used a weak password.""[270]
 There have been a range of responses to concerns over security. The Internet of Things Security Foundation (IoTSF) was launched on 23 September 2015 with a mission to secure the Internet of things by promoting knowledge and best practice. Its founding board is made from technology providers and telecommunications companies. In addition, large IT companies are continually developing innovative solutions to ensure the security of IoT devices. In 2017, Mozilla launched Project Things, which allows to route IoT devices through a safe Web of Things gateway.[271] As per the estimates from KBV Research,[272] the overall IoT security market[273] would grow at 27.9% rate during 2016–2022 as a result of growing infrastructural concerns and diversified usage of Internet of things.[274][275]
 Governmental regulation is argued by some to be necessary to secure IoT devices and the wider Internet – as market incentives to secure IoT devices is insufficient.[276][248][249] It was found that due to the nature of most of the IoT development boards, they generate predictable and weak keys which make it easy to be utilized by man-in-the-middle attack. However, various hardening approaches were proposed by many researchers to resolve the issue of SSH weak implementation and weak keys.[277]
 IoT security within the field of manufacturing presents different challenges, and varying perspectives. Within the EU and Germany, data protection is constantly referenced throughout manufacturing and digital policy particularly that of I4.0. However, the attitude towards data security differs from the enterprise perspective whereas there is an emphasis on less data protection in the form of GDPR as the data being collected from IoT devices in the manufacturing sector does not display personal details.[245] Yet, research has indicated that manufacturing experts are concerned about ""data security for protecting machine technology from international competitors with the ever-greater push for interconnectivity"".[245]
 IoT systems are typically controlled by event-driven smart apps that take as input either sensed data, user inputs, or other external triggers (from the Internet) and command one or more actuators towards providing different forms of automation.[278] Examples of sensors include smoke detectors, motion sensors, and contact sensors. Examples of actuators include smart locks, smart power outlets, and door controls. Popular control platforms on which third-party developers can build smart apps that interact wirelessly with these sensors and actuators include Samsung's SmartThings,[279] Apple's HomeKit,[280] and Amazon's Alexa,[281] among others.
 A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states, e.g., ""unlock the entrance door when no one is at home"" or ""turn off the heater when the temperature is below 0 degrees Celsius and people are sleeping at night"".[278] Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. Recently, researchers from the University of California Riverside have proposed IotSan, a novel practical system that uses model checking as a building block to reveal ""interaction-level"" flaws by identifying events that can lead the system to unsafe states.[278] They have evaluated IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities (i.e., violations of safe physical states/properties).
 Given widespread recognition of the evolving nature of the design and management of the Internet of things, sustainable and secure deployment of IoT solutions must design for ""anarchic scalability"".[282] Application of the concept of anarchic scalability can be extended to physical systems (i.e. controlled real-world objects), by virtue of those systems being designed to account for uncertain management futures. This hard anarchic scalability thus provides a pathway forward to fully realize the potential of Internet-of-things solutions by selectively constraining physical systems to allow for all management regimes without risking physical failure.[282]
 Brown University computer scientist Michael Littman has argued that successful execution of the Internet of things requires consideration of the interface's usability as well as the technology itself. These interfaces need to be not only more user-friendly but also better integrated: ""If users need to learn different interfaces for their vacuums, their locks, their sprinklers, their lights, and their coffeemakers, it's tough to say that their lives have been made any easier.""[283]
 A concern regarding Internet-of-things technologies pertains to the environmental impacts of the manufacture, use, and eventual disposal of all these semiconductor-rich devices.[284] Modern electronics are replete with a wide variety of heavy metals and rare-earth metals, as well as highly toxic synthetic chemicals. This makes them extremely difficult to properly recycle. Electronic components are often incinerated or placed in regular landfills. Furthermore, the human and environmental cost of mining the rare-earth metals that are integral to modern electronic components continues to grow. This leads to societal questions concerning the environmental impacts of IoT devices over their lifetime.[285]
 The Electronic Frontier Foundation has raised concerns that companies can use the technologies necessary to support connected devices to intentionally disable or ""brick"" their customers' devices via a remote software update or by disabling a service necessary to the operation of the device. In one example, home automation devices sold with the promise of a ""Lifetime Subscription"" were rendered useless after Nest Labs acquired Revolv and made the decision to shut down the central servers the Revolv devices had used to operate.[286] As Nest is a company owned by Alphabet (Google's parent company), the EFF argues this sets a ""terrible precedent for a company with ambitions to sell self-driving cars, medical devices, and other high-end gadgets that may be essential to a person's livelihood or physical safety.""[287]
 Owners should be free to point their devices to a different server or collaborate on improved software. But such action violates the United States DMCA section 1201, which only has an exemption for ""local use"". This forces tinkerers who want to keep using their own equipment into a legal grey area. EFF thinks buyers should refuse electronics and software that prioritize the manufacturer's wishes above their own.[287]
 Examples of post-sale manipulations include Google Nest Revolv, disabled privacy settings on Android, Sony disabling Linux on PlayStation 3, enforced EULA on Wii U.[287]
 Kevin Lonergan at Information Age, a business technology magazine, has referred to the terms surrounding the IoT as a ""terminology zoo"".[288] The lack of clear terminology is not ""useful from a practical point of view"" and a ""source of confusion for the end user"".[288] A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics.[288] According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we know them today existed, and there is a long list of terms with varying degrees of overlap and technological convergence: Internet of things, Internet of everything (IoE), Internet of goods (supply chain), industrial Internet, pervasive computing, pervasive sensing, ubiquitous computing, cyber-physical systems (CPS), wireless sensor networks (WSN), smart objects, digital twin, cyberobjects or avatars,[146] cooperating objects, machine to machine (M2M), ambient intelligence (AmI), Operational technology (OT), and information technology (IT).[288] Regarding IIoT, an industrial sub-field of IoT, the Industrial Internet Consortium's Vocabulary Task Group has created a ""common and reusable vocabulary of terms""[289] to ensure ""consistent terminology""[289][290] across publications issued by the Industrial Internet Consortium. IoT One has created an IoT Terms Database including a New Term Alert[291] to be notified when a new term is published. As of March 2020[update], this database aggregates 807 IoT-related terms, while keeping material ""transparent and comprehensive"".[292][293]
 Despite a shared belief in the potential of the IoT, industry leaders and consumers are facing barriers to adopt IoT technology more widely. Mike Farley argued in Forbes that while IoT solutions appeal to early adopters, they either lack interoperability or a clear use case for end-users.[294] A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle ""to pinpoint exactly where the value of IoT lies for them"".[295]
 As for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the ""things"" around the user can cooperate to provide better services that fulfill personal preference.[296] When the collected information which describes a user in detail travels through multiple hops in a network, due to a diverse integration of services, devices and network, the information stored on a device is vulnerable to privacy violation by compromising nodes existing in an IoT network.[297]
 For example, on 21 October 2016, a multiple distributed denial of service (DDoS) attacks systems operated by domain name system provider Dyn, which caused the inaccessibility of several websites, such as GitHub, Twitter, and others. This attack is executed through a botnet consisting of a large number of IoT devices including IP cameras, gateways, and even baby monitors.[298]
 Fundamentally there are 4 security objectives that the IoT system requires: (1) data confidentiality: unauthorised parties cannot have access to the transmitted and stored data; (2) data integrity: intentional and unintentional corruption of transmitted and stored data must be detected; (3) non-repudiation: the sender cannot deny having sent a given message; (4) data availability: the transmitted and stored data should be available to authorised parties even with the denial-of-service (DOS) attacks.[299]
 Information privacy regulations also require organisations to practice ""reasonable security"". California's SB-327 Information privacy: connected devices ""would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorised access, destruction, use, modification, or disclosure, as specified"".[300] As each organisation's environment is unique, it can prove challenging to demonstrate what ""reasonable security"" is and what potential risks could be involved for the business. Oregon's HB2395 also ""requires [a] person that manufactures, sells or offers to sell connected device] manufacturer to equip connected device with reasonable security features that protect connected device and information that connected device collects, contains, stores or transmits] stores from access, destruction, modification, use or disclosure that consumer does not authorise.""[301]
 According to antivirus provider Kaspersky, there were 639 million data breaches of IoT devices in 2020 and 1.5 billion breaches in the first six months of 2021.[215]
 A study issued by Ericsson regarding the adoption of Internet of things among Danish companies identified a ""clash between IoT and companies' traditional governance structures, as IoT still presents both uncertainties and a lack of historical precedence.""[295] Among the respondents interviewed, 60 percent stated that they ""do not believe they have the organizational capabilities, and three of four do not believe they have the processes needed, to capture the IoT opportunity.""[295] This has led to a need to understand organizational culture in order to facilitate organizational design processes and to test new innovation management practices. A lack of digital leadership in the age of digital transformation has also stifled innovation and IoT adoption to a degree that many companies, in the face of uncertainty, ""were waiting for the market dynamics to play out"",[295] or further action in regards to IoT ""was pending competitor moves, customer pull, or regulatory requirements"".[295] Some of these companies risk being ""kodaked"" – ""Kodak was a market leader until digital disruption eclipsed film photography with digital photos"" – failing to ""see the disruptive forces affecting their industry""[302] and ""to truly embrace the new business models the disruptive change opens up"".[302] Scott Anthony has written in Harvard Business Review that Kodak ""created a digital camera, invested in the technology, and even understood that photos would be shared online""[302] but ultimately failed to realize that ""online photo sharing was the new business, not just a way to expand the printing business.""[302]
 According to 2018 study, 70–75% of IoT deployments were stuck in the pilot or prototype stage, unable to reach scale due in part to a lack of business planning.[303][page needed][304]
 Even though scientists, engineers, and managers across the world are continuously working to create and exploit the benefits of IoT products, there are some flaws in the governance, management and implementation of such projects. Despite tremendous forward momentum in the field of information and other underlying technologies, IoT still remains a complex area and the problem of how IoT projects are managed still needs to be addressed. IoT projects must be run differently than simple and traditional IT, manufacturing or construction projects. Because IoT projects have longer project timelines, a lack of skilled resources and several security/legal issues, there is a need for new and specifically designed project processes. The following management techniques should improve the success rate of IoT projects:[305]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['governance', 'scientists, engineers, and managers', 'whether we want to have some sort of policy framework in place to limit that', 'increased digitalization in the water, transport or energy sector', 'when things can become organized and connected by location'], 'answer_start': [], 'answer_end': []}"
"
 Cloud computing[1] is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user.[2] Large clouds often have functions distributed over multiple locations, each of which is a data center. Cloud computing relies on sharing of resources to achieve coherence and typically uses a pay-as-you-go model, which can help in reducing capital expenses but may also lead to unexpected operating expenses for users.[3]
 A European Commission communication issued in 2012 argued that the breadth of scope offered by cloud computing made a general definition ""elusive"",[4] although the United States National Institute of Standards and Technology's 2011 definition of cloud computing identified ""five essential characteristics"":
 Cloud computing has a rich history that extends back to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The ""data center"" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This was a time of exploration and experimentation with ways to make large-scale computing power available to more users through time-sharing, optimizing the infrastructure, platform, and applications, and increasing efficiency for end users.[6]
 The ""cloud"" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of ""places"" that mobile agents in the Telescript environment could ""go"". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom.[7] The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with ""cloud computing-enabled applications"". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers.[8]
 In the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 the beta version of Google Docs was released, Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2), in 2008 NASA's development of the first open-source software for deploying private and hybrid clouds.[9][10]
 The following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities.[11][12]
 Since the global pandemic of 2020, cloud technology has surged in popularity due to the level of data security it offers and the flexibility of working options it provides for all employees, notably remote workers.[13]
 Advocates of public and hybrid clouds claim that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand,[14][15][16] providing burst computing capability: high computing power at certain periods of peak demand.[17]
 Additional value propositions of cloud computing include:
 One of the main challenges of cloud computing, in comparison to more traditional on-premises computing, is data security and privacy. Cloud users entrust their sensitive data to third-party providers, who may not have adequate measures to protect it from unauthorized access, breaches, or leaks. Cloud users also face compliance risks if they have to adhere to certain regulations or standards regarding data protection, such as GDPR or HIPAA.[35]
 Another challenge of cloud computing is reduced visibility and control. Cloud users may not have full insight into how their cloud resources are managed, configured, or optimized by their providers. They may also have limited ability to customize or modify their cloud services according to their specific needs or preferences.[35] Complete understanding of all technology may be impossible, especially given the scale, complexity, and deliberate opacity of contemporary systems; however, there is a need for understanding complex technologies and their interconnections to have power and agency within them.[36] The metaphor of the cloud can be seen as problematic as cloud computing retains the aura of something noumenal and numinous; it is something experienced without precisely understanding what it is or how it works.[37]
 In addition, cloud migration is a significant issue. Cloud migration is the process of moving data, applications, or workloads from one cloud environment to another or from on-premises to the cloud. Cloud migration can be complex, time-consuming, and costly, especially if there are incompatibility issues between different cloud platforms or architectures. Cloud migration can also cause downtime, performance degradation, or data loss if not planned and executed properly.[38]
 Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information.[39] Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored.[39] Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.[39] Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity.[40] The systems work by creating and describing identities, recording activities, and getting rid of unused identities.
 According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. ""There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into"". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called ""hyperjacking"". Some examples of this include the Dropbox security breach, and iCloud 2014 leak.[41] Dropbox had been breached in October 2014, having over seven million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[41]
 There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.[42] Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.[43] Some small businesses that do not have expertise in IT security could find that it is more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes do not read the many pages of the terms of service agreement, and just click ""Accept"" without reading). This is important now that cloud computing is common and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Assistant). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[44]
 The attacks that can be made on cloud computing systems include man-in-the middle attacks, phishing attacks, authentication attacks, and malware attacks. One of the largest threats is considered to be malware attacks, such as Trojan horses. Recent research conducted in 2022 has revealed that the Trojan horse injection method is a serious problem with harmful impacts on cloud computing systems.[45]
 The service-oriented architecture (SOA) promotes the idea of ""Everything as a Service"" (EaaS or XaaS, or simply aAsS).[46] This concept is operationalized in cloud computing through several service models as defined by the National Institute of Standards and Technology (NIST). The three standard service models are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).[5] They are commonly depicted as layers in a stack, providing different levels of abstraction. However, these layers are not necessarily interdependent. For instance, SaaS can be delivered on bare metal, bypassing PaaS and IaaS, and a program can run directly on IaaS without being packaged as SaaS.
 Infrastructure as a service (IaaS) refers to online services that provide high-level APIs used to abstract various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A hypervisor runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. The use of containers offers higher performance than virtualization because there is no hypervisor overhead. IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.[47]
 The NIST's definition of cloud computing describes IaaS as ""where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).""[5]
 IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the number of resources allocated and consumed.[48]
 The NIST's definition of cloud computing defines Platform as a Service as:[5]
 The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment. PaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including an operating system, programming-language execution environment, database, and the web server. Application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers. With some PaaS, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually.[49][need quotation to verify]
 Some integration and data management providers also use specialized applications of PaaS as delivery models for data. Examples include iPaaS (Integration Platform as a Service) and dPaaS (Data Platform as a Service). iPaaS enables customers to develop, execute and govern integration flows.[50] Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.[51] dPaaS delivers integration—and data-management—products as a fully managed service.[52] Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of programs by building data applications for the customer. dPaaS users access data through data-visualization tools.[53]
 The NIST's definition of cloud computing defines Software as a Service as:[5]
 The capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings. In the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as ""on-demand software"" and is usually priced on a pay-per-use basis or using a subscription fee.[54] In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability—which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.[55] Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.
 The pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[56] so prices become scalable and adjustable if users are added or removed at any point. It may also be free.[57] Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result,[citation needed] there could be unauthorized access to the data.[58] Examples of applications offered as SaaS are games and productivity software like Google Docs and Office Online. SaaS applications may be integrated with cloud storage or File hosting services, which is the case with Google Docs being integrated with Google Drive, and Office Online being integrated with OneDrive.[59]
 In the mobile ""backend"" as a service (m) model, also known as ""backend as a service"" (BaaS), web app and mobile app developers are provided with a way to link their applications to cloud storage and cloud computing services with application programming interfaces (APIs) exposed to their applications and custom software development kits (SDKs). Services include user management, push notifications, integration with social networking services[60] and more. This is a relatively recent model in cloud computing,[61] with most BaaS startups dating from 2011 or later[62][63][64] but trends indicate that these services are gaining significant mainstream traction with enterprise consumers.[65]
 Serverless computing is a cloud computing code execution model in which the cloud provider fully manages starting and stopping virtual machines as necessary to serve requests. Requests are billed by an abstract measure of the resources required to satisfy the request, rather than per virtual machine per hour.[66] Despite the name, serverless computing does not actually involve running code without servers.[66] The business or person using the system does not have to purchase, rent or provide servers or virtual machines for the back-end code to run on.
 Function as a service (FaaS) is a service-hosted remote procedure call that utilizes serverless computing to enable deploying individual functions in the cloud to run in response to events.[67] Some consider FaaS to fall under the umbrella of serverless computing, while others use the terms interchangeably.[68]
 Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally.[5] Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers[69] are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users ""still have to buy, build, and manage them"" and thus do not benefit from less hands-on management,[70] essentially ""[lacking] the economic model that makes cloud computing such an intriguing concept"".[71][72]
 Cloud services are considered ""public"" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge.[73] Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications.[19][74]
 Several factors like the functionality of the solutions, cost, integrational and organizational aspects as well as safety & security are influencing the decision of enterprises and organizations to choose a public cloud or on-premises solution.[75]
 Hybrid cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources,[76][77] that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.[5] Gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.[78] A hybrid cloud service crosses isolation and provider boundaries so that it cannot be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.
 Varied use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.[79] This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.[80]
 Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.[81] This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.[5] Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and ""bursts"" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed.[82] Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.[83]
 Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.[5]
 A cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.
 Multicloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).[85][86][87]
 Poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more that could be done with a single provider.[88]
 The issues of transferring large amounts of data to the cloud as well as data security once the data is in the cloud initially hampered adoption of cloud for big data, but now that much data originates in the cloud and with the advent of bare-metal servers, the cloud has become[89] a solution for use cases including business analytics and geospatial analysis.[90]
 HPC cloud refers to the use of cloud computing services and infrastructure to execute high-performance computing (HPC) applications.[91] These applications consume a considerable amount of computing power and memory and are traditionally executed on clusters of computers. In 2016 a handful of companies, including R-HPC, Amazon Web Services, Univa, Silicon Graphics International, Sabalcore, Gomput, and Penguin Computing offered a high-performance computing cloud. The Penguin On Demand (POD) cloud was one of the first non-virtualized remote HPC services offered on a pay-as-you-go basis.[92][93] Penguin Computing launched its HPC cloud in 2016 as an alternative to Amazon's EC2 Elastic Compute Cloud, which uses virtualized computing nodes.[94][95]
 Cloud architecture,[96] the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.
 Cloud engineering is the application of engineering disciplines of cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization and governance in conceiving, developing, operating and maintaining cloud computing systems. It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information technology engineering, security, platform, risk, and quality engineering.
 According to International Data Corporation (IDC), global spending on cloud computing services has reached $706 billion and expected to reach $1.3 trillion by 2025.[97] While Gartner estimated that global public cloud services end-user spending would reach $600 billion by 2023.[98] As per a McKinsey & Company report, cloud cost-optimization levers and value-oriented business use cases foresee more than $1 trillion in run-rate EBITDA across Fortune 500 companies as up for grabs in 2030.[99] In 2022, more than $1.3 trillion in enterprise IT spending was at stake from the shift to the cloud, growing to almost $1.8 trillion in 2025, according to Gartner.[100]
 The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles.[101] The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more ""virtual"" devices, each of which can be easily used and managed to perform computing tasks. With operating system–level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.[101]
 Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.[101]
 Cloud computing shares characteristics with:
 
 Media related to Cloud computing at Wikimedia Commons
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['data storage (cloud storage) and computing power', 'public-resource computing and volunteer cloud', 'the 1960s', 'Media related to Cloud computing at Wikimedia Commons', 'data security and privacy'], 'answer_start': [], 'answer_end': []}"
"
 Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.[2] Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.[3]
 Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity.[4] The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.[5]
 Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.""[6]
Analysis of data sets can find new correlations to ""spot business trends, prevent diseases, combat crime and so on"".[7] Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics,[8] connectomics, complex physics simulations, biology, and environmental research.[9]
 The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.[10][11] The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;[12] as of 2012[update], every day 2.5 exabytes (2.17×260 bytes) of data are generated.[13] Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data.[14] According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021.[15][16] While Statista report, the global big data market is forecasted to grow to $103 billion by 2027.[17] In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year.[18] In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data.[18] And users of services enabled by personal-location data could capture $600 billion in consumer surplus.[18] One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.[19]
 Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require ""massively parallel software running on tens, hundreds, or even thousands of servers"".[20] What qualifies as ""big data"" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. ""For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.""[21]
 The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.[22][23]  Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.[24][page needed]  Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data.[25] Big data ""size"" is a constantly moving target; as of 2012[update] ranging from a few dozen terabytes to many zettabytes of data.[26]  Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.[27]
 ""Variety"", ""veracity"", and various other ""Vs"" are added by some organizations to describe it, a revision challenged by some industry authorities.[28] The Vs of big data were often referred to as the ""three Vs"", ""four Vs"", and ""five Vs"". They represented the qualities of big data in volume, variety, velocity, veracity, and value.[4] Variability is often included as an additional quality of big data.
 A 2018 definition states ""Big data is where parallel computing tools are needed to handle data"", and notes, ""This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model.""[29]
 In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases.[30] For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait.[31] Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.
 The growing maturity of the concept more starkly delineates the difference between ""big data"" and ""business intelligence"":[32]
 Big data can be described by the following characteristics:
 Other possible characteristics of big data are:[41]
 Big data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s. For many years, WinterCorp published the largest database report.[42][promotional source?]
 Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017[update], there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added unstructured data types including XML, JSON, and Avro.
 In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc.[43] and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008.[44] In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.
 CERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current ""big data"" movement.
 In 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the ""map"" step). The results are then gathered and delivered (the ""reduce"" step). The framework was very successful,[45] so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named ""Hadoop"".[46] Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds in-memory processing and the ability to set up many operations (not just map followed by reducing).
 MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled ""Big Data Solution Offering"".[47] The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.[48]
 Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.[49]
 The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.[50][51]
 A 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:[52]
 Multidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.
Additional technologies being applied to big data include efficient tensor-based computation,[53] such as multilinear subspace learning,[54] massively parallel-processing (MPP) databases, search-based applications, data mining,[55] distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources),[56] and the Internet.[citation needed] Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.[57]
 Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.[58][promotional source?]
 DARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called ""Ayasdi"".[59][third-party source needed]
 The practitioners of big data analytics processes are generally hostile to slower shared storage,[60] preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures—storage area network (SAN) and network-attached storage (NAS)— is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.
 Real or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good—data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.
 Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year, about twice as fast as the software business as a whole.[7]
 Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet.[7] Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007[12] and predictions put the amount of internet traffic at 667 exabytes annually by 2014.[7] According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data,[61] which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).
 While many vendors offer off-the-shelf products for big data, experts promote the development of in-house custom-tailored systems if the company has sufficient technical capabilities.[62]
 The use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation,[63] but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.
 Civil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.
 Research on the effective usage of information and communication technologies for development (also known as ""ICT4D"") suggests that big data technology can make important contributions but also present unique challenges to international development.[64][65] Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management.[66][page needed][67][68] Additionally, user-generated data offers new opportunities to give the unheard a voice.[69] However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.[66][page needed]  The challenge of ""big data for development""[66][page needed] is currently evolving toward the application of this data through machine learning, known as ""artificial intelligence for development (AI4D).[70]
 A major practical application of big data for development has been ""fighting poverty with data"".[71] In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata [72] and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty.[73] Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues [74][75] argue that digital trace data has several benefits such as:
 At the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:
 Big Data is being rapidly adopted in Finance to 1) speed up processing and 2) deliver better, more informed inferences, both internally and to the clients of the financial institutions.[77] The financial applications of Big Data range from investing decisions and trading (processing volumes of available price data, limit order books, economic data and more, all at the same time), portfolio management (optimizing over an increasingly large array of financial instruments, potentially selected from different asset classes), risk management (credit rating based on extended information), and any other aspect where the data inputs are large.[78]
 Big data analytics has been used in healthcare in providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries.[79][80][81][82] Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality.[83] ""Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth."" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed.[84] While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use.[85] The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.[86]
 Big data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research.[87] Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.
 A related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.[88][page needed]  For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily.[89] Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.[90]  These are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.[91]
 A McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers[52] and a number of universities[92][better source needed] including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly.[93] In the specific field of marketing, one of the problems stressed by Wedel and Kannan[94] is that marketing has several sub domains (e.g., advertising, promotions,
product development, branding) that all use different types of data.
 To understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.[95]
 Channel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.[97]
 Health insurance providers are collecting data on social ""determinants of health"" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.[98]
 Big data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical,[99] manufacturing[100] and transportation[101] contexts.
 Kevin Ashton, the digital innovation expert who is credited with coining the term,[102] defines the Internet of things in this quote: ""If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.""
 Especially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA).[103] By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them.[103] ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data.
 Compared to survey-based data collection, big data has low cost per data point, applies analysis techniques via machine learning and data mining, and includes diverse and new data sources, e.g., registers, social media, apps, and other forms digital data. Since 2018, survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality. To date, there have been three Big Data Meets Survey Science (BigSurv) conferences in 2018, 2020 (virtual), 2023, and as of 2023[update] one conference forthcoming in 2025,[104] a special issue in the Social Science Computer Review,[105] a special issue in Journal of the Royal Statistical Society,[106] and a special issue in EP J Data Science,[107] and a book called Big Data Meets Social Sciences[108] edited by Craig Hill and five other Fellows of the American Statistical Association. In 2021, the founding members of BigSurv received the Warren J. Mitofsky Innovators Award from the American Association for Public Opinion Research.[109]
 Big data is notable in marketing due to the constant “datafication”[110] of everyday consumers of the internet, in which all forms of data are tracked. The datafication of consumers can be defined as  quantifying many of or all human behaviors for the purpose of marketing.[111] The increasingly digital world of rapid datafication makes this idea relevant to marketing because the amount of data constantly grows exponentially. It is predicted to increase from 44 to 163 zettabytes within the span of five years.[112] The size of big data can often be difficult to navigate for marketers.[113] As a result, adopters of big data may find themselves at a disadvantage. Algorithmic findings can be difficult to achieve with such large datasets.[114] Big data in marketing is a highly lucrative tool that can be used for large corporations, its value being as a result of the possibility of predicting significant trends, interests, or statistical outcomes in a consumer-based manner.[115]
 There are three significant factors in the use of big data in marketing:
 Examples of uses of big data in public services:
 Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.[159]
Future performance of players could be predicted as well.[160] Thus, players' value and salary is determined by data collected throughout the season.[161]
 In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.[162]
Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.[163]
 During the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.[169]
 Governments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.[170][171][172]
 Encrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.[173]
 In March 2012, The White House announced a national ""Big Data Initiative"" that consisted of six federal departments and agencies committing more than $200 million to big data research projects.[174]
 The initiative included a National Science Foundation ""Expeditions in Computing"" grant of $10 million over five years to the AMPLab[175] at the University of California, Berkeley.[176] The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion[177] to fighting cancer.[178]
 The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,[179] led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.
 The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.[180] The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.[181]
 The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.[182]
 The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.[183]
 At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.[184]
 Computational social sciences – Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences.[185] Often these APIs are provided for free.[185] Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators.[186][187][188] The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the ""future orientation index"".[189] They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.
 Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.[190] Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports,[191] suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.[192][193][194][195][196][197][198]
 Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.[199]
 A research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient. Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.
 Critiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done.[200] One approach to this criticism is the field of critical data studies.
 ""A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data.""[24][page needed]  In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory:[201] focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.[202] Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by ""big judgment"", according to an article in the Harvard Business Review.[203]
 Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably ""informed by the world as it was in the past, or, at best, as it currently is"".[66][page needed] Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past.[204] If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory.[204] As a response to this critique Alemany Oliver and Vayre suggest to use ""abductive reasoning as a first step in the research process in order to bring context to consumers' digital traces and make new theories emerge"".[205]  Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models[66][page needed] and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.[206][207] Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (e.g. contingency tables) typically employed with smaller data sets.
 In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.[208]  A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.[209][210] In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.[211] The search logic is reversed and the limits of induction (""Glory of Science and Philosophy scandal"", C. D. Broad, 1926) are to be considered.[citation needed]
 Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy.[212] The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.[213]
 Barocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.[214]
 The ""V"" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:[215]
 Large data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial ""big data"". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.
 Ulf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a ""fad"" in scientific research.[185] Researcher Danah Boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data.[216] This approach may lead to results that have a bias in one way or another.[217] Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.[218]
In the provocative article ""Critical Questions for Big Data"",[219] the authors title big data a part of mythology: ""large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy"". Users of big data are often ""lost in the sheer volume of numbers"", and ""working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth"".[219] Recent developments in BI domain, such as pro-active reporting especially target improvements in the usability of big data, through automated filtering of non-useful data and correlations.[220] Big structures are full of spurious correlations[221] either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness[222] (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers ""speak for themselves"" and revolutionize scientific method, is questioned.[223] Catherine Tucker has pointed to ""hype"" around big data, writing ""By itself, big data is unlikely to be valuable."" The article explains: ""The many contexts where data is cheap relative to the cost of retaining talent to process it, suggests that processing skills are more important than data itself in creating value for a firm.""[224]
 Big data analysis is often shallow compared to analysis of smaller data sets.[225] In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.[225]
 Big data is a buzzword and a ""vague term"",[226][227] but at the same time an ""obsession""[227] with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.
Big data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.
On the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.
Ioannidis argued that ""most published research findings are false""[228] due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a ""significant"" result being false grows fast – even more so, when only positive results are published.
Furthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. presidential election[229] with varying degrees of success.
 Big data has been used in policing and surveillance by institutions like law enforcement and corporations.[230] Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing,[231] big data policing can reproduce existing societal inequalities in three ways:
 If these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['presidential election', 'law enforcement and corporations', 'the likelihood of a ""significant"" result being false grows fast', 'entrepreneurs, consultants, scientists, and the media', 'investing decisions and trading'], 'answer_start': [], 'answer_end': []}"
"
 Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.[2]
 Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]
 Data science is ""a concept to unify statistics, data analysis, informatics, and their related methods"" to ""understand and analyze actual phenomena"" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge.[7][8]
 A  data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.[9]
 Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[11][12] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[13][14] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[15]
 Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics.[16] Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data.[17] Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[18] Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.[19]
 Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[20]
 In 1962, John Tukey described a field he called ""data analysis"", which resembles modern data science.[20] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term ""data science"" for the first time as an alternative name for statistics.[21] Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[22][23]
 The term ""data science"" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.[6] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[6] However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[24] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[23]
 During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included ""knowledge discovery"" and ""data mining"".[6][25]
 In 2012, technologists Thomas H. Davenport and DJ Patil declared ""Data Scientist: The Sexiest Job of the 21st Century"",[26] a catchphrase that was picked up even by major-city newspapers like the New York Times[27] and the Boston Globe.[28] A decade later, they reaffirmed it, stating that ""the job is more in demand than ever with employers"".[29]
 The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[30] In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name.[25] ""Data science"" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science.[25] In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[31]
 The professional title of ""data scientist"" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[32] Though it was used by the National Science Board in their 2005 report ""Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century"", it referred broadly to any key role in managing a digital data collection.[33]
 There is still no consensus on the definition of data science, and it is considered by some to be a buzzword.[34] Big data is a related marketing term.[35] Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.[36]
 Data science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.[37][38]
 Data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.[37]
 Data science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.[38][39]
 While data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.[38]
 Despite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.[37][38]
 In summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.
 Cloud computing can offer access to large amounts of computational power and storage.[40] In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.[41]
 Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.[42]
 Data science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts [43][44]
 Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.[45][46]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Ethical concerns', 'DJ Patil and Jeff Hammerbacher', 'ascendant popularity of data science', 'potential privacy violations, bias perpetuation, and negative societal impacts', 'various origins and forms'], 'answer_start': [], 'answer_end': []}"
"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]
 ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.
 The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.[7][8]
 From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.
 The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[9][10] The synonym self-teaching computers was also used in this time period.[11][12]
 Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[13] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[14] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[13] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[13]
 By the early 1960s an experimental ""learning machine"" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively ""trained"" by a human operator/teacher to recognize patterns and equipped with a ""goof"" button to cause it to re-evaluate incorrect decisions.[15] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[16] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[17] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[18]
 Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.""[19] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".[20]
 Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[21]
 As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488 
 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[24]: 25 
 Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[25]
 There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for ""general intelligence"".[26][27][28]
 An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[29]
 According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.
 Examples of AI-powered audio/video compression software include VP9, NVIDIA Maxine, AIVC, AccMPEG.[30] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[31]
 In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[32]
 Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[33]
 Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
 Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).[35]
 The difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.
 Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[36] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[37] He also suggested the term data science as a placeholder to call the overall field.[37]
 Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[38]
 Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[39] wherein ""algorithmic model"" means more or less the machine learning algorithms like Random Forest.
 Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[40]
 Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks.[41] Statistical physics is thus finding applications in the area of medical diagnostics.[42]
 A core objective of a learner is to generalize from its experience.[6][43] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
 The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
 For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[44]
 In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.
 
Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the ""signal"" or ""feedback"" available to the learning system:
 Although each algorithm has advantages and limitations, no single algorithm works for all problems.[45][46][47]
 Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[48] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[49] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[19]
 Types of supervised-learning algorithms include active learning, classification and regression.[50] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.
 Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.
 Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[8] and density estimation.[51] Unsupervised learning algorithms also streamlined the process of identifying large indel based haplotypes of a gene of interest from pan-genome.[52]
 Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.
 Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.
 In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[54]
 Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.
 Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[56] In other words, it is a process of reducing the dimension of the feature set, also called the ""number of features"". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). This results in a smaller dimension of data (2D instead of 3D), while keeping all original variables in the model without changing the data.[57]
The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.
 Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.[58]
 Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[59] It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[60]
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 
 It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[61]
 Several learning algorithms aim at discovering better representations of the inputs provided during training.[62] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
 Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[63] and various forms of clustering.[64][65][66]
 Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[67] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[68]
 Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
 Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[69] A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[70]
 In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[71] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[72]
 In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[73]
 Three broad categories of anomaly detection techniques exist.[74] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.
 Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[75][76] and finally meta-learning (e.g. MAML).
 Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of ""interestingness"".[77]
 Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves ""rules"" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[78] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
 Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[79] For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}

 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
 Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[80]
 Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
 Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[81][82][83] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[84] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.
 A machine learning model is a type of mathematical model which, after being ""trained"" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions.[85] By extension the term model can refer to several level of specifity, from a general class of models and their associated learning algorithms, to a fully trained model with all its internal parameters tuned.[86]
 Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.
 Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
 An ANN is a model based on a collection of connected units or nodes called ""artificial neurons"", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a ""signal"", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called ""edges"". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
 The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
 Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[87]
 Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.
 Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[88]  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
 Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[89]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.
 A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
 A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.
 Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.
 Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.
 A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[91][92] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[93]
 The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach[clarification needed] would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and Uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[3][5][10] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead a much higher computation time when compared to other machine learning approaches.
 Typically, machine learning models require a high quantity of reliable data in order for the models to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Bias models may result in detrimental outcomes thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably be integrated within machine learning engineering teams.
 Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[94]
 There are many applications for machine learning, including:
 In 2006, the media-services provider Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[97] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (""everything is a recommendation"") and they changed their recommendation engine accordingly.[98] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[99] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[100] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.[101] In 2019 Springer Nature published the first research book created using machine learning.[102] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[103] Machine learning was recently applied to predict the pro-environmental behavior of travelers.[104] Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.[105][106][107] When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[108]
 Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[109]
 Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[110][111][112] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[113]
 The ""black box theory"" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[114] The House of Lords Select Committee, which claimed that such an “intelligence system” that could have a “substantial impact on an individual’s life” would not be considered acceptable unless it provided “a full and satisfactory explanation for the decisions” it makes.[114]
 In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[115] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[116][117] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[118]
 Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[119]
 Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[120]
 Language models learned from data have been shown to contain human-like biases.[121][122] In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged ""black defendants high risk twice as often as white defendants.""[123] In 2015, Google Photos would often tag black people as gorillas,[123] and in 2018, this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data and thus was not able to recognize real gorillas at all.[124] Similar issues with recognizing non-white people have been found in many other systems.[125] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[126]
 Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[127] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that ""[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.""[128]
 Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[129] It contrasts with the ""black box"" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[130] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.
 Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.[131]
 Learners can also disappoint by ""learning the wrong lesson"". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[132] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in ""adversarial"" images that the system misclassifies.[133][134]
 Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[135] Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.[136]
 Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories ""spam"" and well-visible ""not spam"" of posts) machine learning models which are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[137][138][139]
 Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[140]
 In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).[141]
 Machine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[142] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to be either women or had non-European sounding names.[120] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[143][144] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in “disproportionately high levels of over-policing in low-income and minority communities” after being trained with historical crime data.[123]
 While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[145] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, “female faculty merely make up 16.1%” of all faculty members who focus on AI among several universities around the world.[146] Furthermore, among the group of “new U.S. resident AI PhD graduates,” 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[146]
 AI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning.[147] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[148][149]
 Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines.[150] This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[151]
 Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units.[152] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[153] OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[154][155]
 A physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. ""Physical"" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.[156][157]
 Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers.[158][159][160] Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration,[161][162] using approximate computing,[163] optimization of machine learning models and many more.[164][165] Pruning, Quantization, Knowledge Distillation, Low-Rank Factorization, Network Architecture Search (NAS) & Parameter Sharing are few of the techniques used for optimization of machine learning models.
 Software suites containing a variety of machine learning algorithms include the following:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['artificial intelligence', 'Fei-Fei Li', 'decades of human desire and effort to study human cognitive processes', 'clustering, dimensionality reduction,[8] and density estimation', 'computer vision and speech recognition'], 'answer_start': [], 'answer_end': []}"
"Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of ""understanding""[citation needed] the contents of documents, including the contextual nuances of the language within them. To this end, natural language processing often borrows ideas from theoretical linguistics. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
 Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.
 Natural language processing has its roots in the 1940s.[1] Already in 1940, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
 The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
 Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[8]
 In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]
 In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]
 Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.
 Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
 Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. 
 Before that they were commonly used:
 In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]
 The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.
Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.
 A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach was replaced by the neural networks approach, using word embeddings to capture semantic properties of words. 
 Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) have not been needed anymore. 
 Neural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.
 The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
 Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
 Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[45]
 Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
 Cognition refers to ""the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.""[46] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[47] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[48] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.
 As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[49] with two defining aspects:
 Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[52] functional grammar,[53] construction grammar,[54] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[55] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of ""cognitive AI"".[56] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[57] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[58] and new directions in artificial general intelligence based on the free energy principle[59] by British neuroscientist and theoretician at University College London Karl J. Friston.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['giving computers the ability to support and manipulate human language', 'British neuroscientist and theoretician', 'giving computers the ability to support and manipulate human language', 'developments in artificial intelligence', 'direct real-world applications'], 'answer_start': [], 'answer_end': []}"
"Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a ""Human-computer Interface (HCI)"".
 As a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human–Computer Interaction. The first known use was in 1975 by Carlisle.[1] The term is intended to convey that, unlike other tools with specific and limited uses, computers have many uses which often involve an open-ended dialogue between the user and the computer. The notion of dialogue likens human–computer interaction to human-to-human interaction: an analogy that is crucial to theoretical considerations in the field.[2][3]
 Humans interact with computers in many ways, and the interface between the two is crucial to facilitating this interaction. HCI is also sometimes termed human–machine interaction (HMI), man-machine interaction (MMI) or computer-human interaction (CHI). Desktop applications, internet browsers, handheld computers, and computer kiosks make use of the prevalent graphical user interfaces (GUI) of today.[4] Voice user interfaces (VUI) are used for speech recognition and synthesizing systems, and the emerging multi-modal and Graphical user interfaces (GUI) allow humans to engage with embodied character agents in a way that cannot be achieved with other interface paradigms. The growth in human–computer interaction field has led to an increase in the quality of interaction, and resulted in many new areas of research beyond. Instead of designing regular interfaces, the different research branches focus on the concepts of multimodality[citation needed] over unimodality, intelligent adaptive interfaces over command/action based ones, and active interfaces over passive interfaces.[5]
 The Association for Computing Machinery (ACM) defines human–computer interaction as ""a discipline that is concerned with the design, evaluation, and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them"".[4] A key aspect of HCI is user satisfaction, also referred to as End-User Computing Satisfaction. It goes on to say:
 ""Because human–computer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant. On the human side, communication theory, graphic and industrial design disciplines, linguistics, social sciences, cognitive psychology, social psychology, and human factors such as computer user satisfaction are relevant. And, of course, engineering and design methods are relevant.""[4]
 Due to the multidisciplinary nature of HCI, people with different backgrounds contribute to its success.
 Poorly designed human-machine interfaces can lead to many unexpected problems. A classic example is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster.[6][7][8] Similarly, accidents in aviation have resulted from manufacturers' decisions to use non-standard flight instruments or throttle quadrant layouts: even though the new designs were proposed to be superior in basic human-machine interaction, pilots had already ingrained the ""standard"" layout. Thus, the conceptually good idea had unintended results.
 The human–computer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the loop of interaction. The loop of interaction has several aspects to it, including:
 Human–computer interaction studies the ways in which humans make—or do not make—use of computational artifacts, systems, and infrastructures. Much of the research in this field seeks to improve the human–computer interaction by improving the usability of computer interfaces.[9] How usability is to be precisely understood, how it relates to other social and cultural values, and when it is, and when it may not be a desirable property of computer interfaces is increasingly debated.[10][11]
 Much of the research in the field of human–computer interaction takes an interest in:
 Visions of what researchers in the field seek to achieve might vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may seek to align computer interfaces with existing social practices or existing sociocultural values.
 Researchers in HCI are interested in developing design methodologies, experimenting with devices, prototyping software, and hardware systems, exploring interaction paradigms, and developing models and theories of interaction.
 The following experimental design principles are considered, when evaluating a current user interface, or designing a new user interface:
 The iterative design process is repeated until a sensible, user-friendly interface is created.[14]
 Various strategies delineating methods for human–PC interaction design have developed since the conception of the field during the 1980s. Most plan philosophies come from a model for how clients, originators, and specialized frameworks interface. Early techniques treated clients' psychological procedures as unsurprising and quantifiable and urged plan specialists to look at subjective science to establish zones, (for example, memory and consideration) when structuring UIs. Present-day models, in general, center around a steady input and discussion between clients, creators, and specialists and push for specialized frameworks to be folded with the sorts of encounters clients need to have, as opposed to wrapping user experience around a finished framework.
 Displays are human-made artifacts designed to support the perception of relevant system variables and facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g., navigating, controlling, decision making, learning, entertaining, etc.). A user or operator must be able to process whatever information a system generates and displays; therefore, the information must be displayed according to principles to support perception, situation awareness, and understanding.
 Christopher Wickens et al. defined 13 principles of display design in their book An Introduction to Human Factors Engineering.[18]
 These human perception and information processing principles can be utilized to create an effective display design. A reduction in errors, a reduction in required training time, an increase in efficiency, and an increase in user satisfaction are a few of the many potential benefits that can be achieved by utilizing these principles.
 Certain principles may not apply to different displays or situations. Some principles may also appear to be conflicting, and there is no simple solution to say that one principle is more important than another. The principles may be tailored to a specific design or situation. Striking a functional balance among the principles is critical for an effective design.[19]
 1.	Make displays legible (or audible). A display's legibility is critical and necessary for designing a usable display. If the characters or objects being displayed cannot be discernible, the operator cannot effectively use them.
 2.	Avoid absolute judgment limits. Do not ask the user to determine the level of a variable based on a single sensory variable (e.g., color, size, loudness). These sensory variables can contain many possible levels.
 3.	Top-down processing. Signals are likely perceived and interpreted by what is expected based on a user's experience. If a signal is presented contrary to the user's expectation, more physical evidence of that signal may need to be presented to assure that it is understood correctly.
 4.	Redundancy gain. If a signal is presented more than once, it is more likely to be understood correctly. This can be done by presenting the signal in alternative physical forms (e.g., color and shape, voice and print, etc.), as redundancy does not imply repetition. A traffic light is a good example of redundancy, as color and position are redundant.
 5.	Similarity causes confusion: Use distinguishable elements. Signals that appear to be similar will likely be confused. The ratio of similar features to different features causes signals to be similar. For example, A423B9 is more similar to A423B8 than 92 is to 93. Unnecessarily similar features should be removed, and dissimilar features should be highlighted.
 6.	Principle of pictorial realism. A display should look like the variable that it represents (e.g., the high temperature on a thermometer shown as a higher vertical level). If there are multiple elements, they can be configured in a manner that looks like they would in the represented environment.
 7.	Principle of the moving part. Moving elements should move in a pattern and direction compatible with the user's mental model of how it actually moves in the system. For example, the moving element on an altimeter should move upward with increasing altitude.
 8.	Minimizing information access cost or interaction cost. When the user's attention is diverted from one location to another to access necessary information, there is an associated cost in time or effort. A display design should minimize this cost by allowing frequently accessed sources to be located at the nearest possible position. However, adequate legibility should not be sacrificed to reduce this cost.
 9.	Proximity compatibility principle. Divided attention between two information sources may be necessary for the completion of one task. These sources must be mentally integrated and are defined to have close mental proximity. Information access costs should be low, which can be achieved in many ways (e.g., proximity, linkage by common colors, patterns, shapes, etc.). However, close display proximity can be harmful by causing too much clutter.
 10.	Principle of multiple resources. A user can more easily process information across different resources. For example, visual and auditory information can be presented simultaneously rather than presenting all visual or all auditory information.
 11.	Replace memory with visual information: knowledge in the world. A user should not need to retain important information solely in working memory or retrieve it from long-term memory. A menu, checklist, or another display can aid the user by easing the use of their memory. However, memory use may sometimes benefit the user by eliminating the need to reference some knowledge globally (e.g., an expert computer operator would rather use direct commands from memory than refer to a manual). The use of knowledge in a user's head and knowledge in the world must be balanced for an effective design.
 12.	Principle of predictive aiding. Proactive actions are usually more effective than reactive actions. A display should eliminate resource-demanding cognitive tasks and replace them with simpler perceptual tasks to reduce the user's mental resources. This will allow the user to focus on current conditions and to consider possible future conditions. An example of a predictive aid is a road sign displaying the distance to a certain destination.
 13.	Principle of consistency. Old habits from other displays will easily transfer to support the processing of new displays if they are designed consistently. A user's long-term memory will trigger actions that are expected to be appropriate. A design must accept this fact and utilize consistency among different displays.
 Topics in human–computer interaction include the following:
 Social computing is an interactive and collaborative behavior considered between technology and people. In recent years, there has been an explosion of social science research focusing on interactions as the unit of analysis, as there are a lot of social computing technologies that include blogs, emails,  social networking, quick messaging, and various others. Much of this research draws from psychology, social psychology, and sociology. For example, one study found out that people expected a computer with a man's name to cost more than a machine with a woman's name.[20] Other research finds that individuals perceive their interactions with computers more negatively than humans, despite behaving the same way towards these machines.[21]
 In human and computer interactions, a semantic gap usually exists between human and computer's understandings towards mutual behaviors. Ontology, as a formal representation of domain-specific knowledge, can be used to address this problem by solving the semantic ambiguities between the two parties.[22]
 In the interaction of humans and computers, research has studied how computers can detect, process, and react to human emotions to develop emotionally intelligent information systems. Researchers have suggested several 'affect-detection channels'. The potential of telling human emotions in an automated and digital fashion lies in improvements to the effectiveness of human–computer interaction. The influence of emotions in human–computer interaction has been studied in fields such as financial decision-making using ECG and organizational knowledge sharing using eye-tracking and face readers as affect-detection channels. In these fields, it has been shown that affect-detection channels have the potential to detect human emotions and those information systems can incorporate the data obtained from affect-detection channels to improve decision models.
 A brain–computer interface (BCI), is a direct communication pathway between an enhanced or wired brain and an external device. BCI differs from neuromodulation in that it allows for bidirectional information flow. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions.[23]
 Security interactions are the study of interaction between humans and computers specifically as it pertains to information security.  Its aim, in plain terms, is to improve the usability of security features in end user applications.
 Unlike HCI, which has roots in the early days of Xerox PARC during the 1970s, HCISec is a nascent field of study by comparison. Interest in this topic tracks with that of Internet security, which has become an area of broad public concern only in very recent years.
 When security features exhibit poor usability, the following are common reasons:
 Traditionally, computer use was modeled as a human–computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human more reflective of the multidimensional nature of everyday communication. Because of potential issues, human–computer interaction shifted focus beyond the interface to respond to observations as articulated by D. Engelbart: ""If ease of use were the only valid criterion, people would stick to tricycles and never try bicycles.""[24]
 How humans interact with computers continues to evolve rapidly. Human–computer interaction is affected by developments in computing. These forces include:
 As of 2010[update] the future for HCI is expected[25] to include the following characteristics:
 One of the main conferences for new research in human–computer interaction is the annually held Association for Computing Machinery's (ACM) Conference on Human Factors in Computing Systems, usually referred to by its short name CHI (pronounced kai, or Khai). CHI is organized by ACM Special Interest Group on Computer-Human Interaction (SIGCHI). CHI is a large conference, with thousands of attendants, and is quite broad in scope. It is attended by academics, practitioners, and industry people, with company sponsors such as Google, Microsoft, and PayPal.
 There are also dozens of other smaller, regional, or specialized HCI-related conferences held around the world each year, including:[26]
 
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Internet security', 'Stuart K. Card, Allen Newell, and Thomas P. Moran', 'It is attended by academics, practitioners, and industry people', 'new research in human–computer interaction', 'broad in scope'], 'answer_start': [], 'answer_end': []}"
"
 Virtual reality (VR) is a simulated experience that employs 3D near-eye displays and pose tracking to give the user an immersive feel of a virtual world. Applications of virtual reality include entertainment (particularly video games), education (such as medical, safety or military training) and business (such as virtual meetings). VR is one of the key technologies in the reality-virtuality continuum. As such, it is different from other digital visualization solutions, such as augmented virtuality and augmented reality.[2]
 Currently, standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate some realistic images, sounds and other sensations that simulate a user's physical presence in a virtual environment. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes, but can also be created through specially designed rooms with multiple large screens. Virtual reality typically incorporates auditory and video feedback, but may also allow other types of sensory and force feedback through haptic technology.
 ""Virtual"" has had the meaning of ""being something in essence or effect, though not actually or in fact"" since the mid-1400s.[3] The term ""virtual"" has been used in the computer sense of ""not physically existing but made to appear by software"" since 1959.[3]
 In 1938, French avant-garde playwright Antonin Artaud described the illusory nature of characters and objects in the theatre as ""la réalité virtuelle"" in a collection of essays, Le Théâtre et son double. The English translation of this book, published in 1958 as The Theater and its Double,[4] is the earliest published use of the term ""virtual reality"". The term ""artificial reality"", coined by Myron Krueger, has been in use since the 1970s. The term ""virtual reality"" was first used in a science fiction context in The Judas Mandala, a 1982 novel by Damien Broderick.
 Widespread adoption of the term ""virtual reality"" in the popular media is attributed to Jaron Lanier, who in the late 1980s designed some of the first business-grade virtual reality hardware under his firm VPL Research, and the 1992 film Lawnmower Man, which features use of virtual reality systems.[5]
 One method by which virtual reality can be realized is simulation-based virtual reality. Driving simulators, for example, give the driver on board the impression of actually driving a vehicle by predicting vehicular motion caused by driver input and feeding back corresponding visual, motion and audio cues to the driver.
 With avatar image-based virtual reality, people can join the virtual environment in the form of real video as well as an avatar. One can participate in the 3D distributed virtual environment in the form of either a conventional avatar or a real video. Users can select their own type of participation based on the system capability.
 In projector-based virtual reality, modeling of the real environment plays a vital role in various virtual reality applications, including robot navigation, construction modeling, and airplane simulation. Image-based virtual reality systems have been gaining popularity in computer graphics and computer vision communities. In generating realistic models, it is essential to accurately register acquired 3D data; usually, a camera is used for modeling small objects at a short distance.
 Desktop-based virtual reality involves displaying a 3D virtual world on a regular desktop display without use of any specialized VR positional tracking equipment. Many modern first-person video games can be used as an example, using various triggers, responsive characters, and other such interactive devices to make the user feel as though they are in a virtual world. A common criticism of this form of immersion is that there is no sense of peripheral vision, limiting the user's ability to know what is happening around them.
 A head-mounted display (HMD) more fully immerses the user in a virtual world. A virtual reality headset typically includes two small high resolution OLED or LCD monitors which provide separate images for each eye for stereoscopic graphics rendering a 3D virtual world, a binaural audio system, positional and rotational real-time head tracking for six degrees of movement. Options include motion controls with haptic feedback for physically interacting within the virtual world in an intuitive way with little to no abstraction and an omnidirectional treadmill for more freedom of physical movement allowing the user to perform locomotive motion in any direction.
 Augmented reality (AR) is a type of virtual reality technology that blends what the user sees in their real surroundings with digital content generated by computer software. The additional software-generated images with the virtual scene typically enhance how the real surroundings look in some way. AR systems layer virtual information over a camera live feed into a headset or smartglasses or through a mobile device giving the user the ability to view three-dimensional images.
 Mixed reality (MR) is the merging of the real world and virtual worlds to produce new environments and visualizations where physical and digital objects co-exist and interact in real time.
 A cyberspace is sometimes defined as a networked virtual reality.[6]
 Simulated reality is a hypothetical virtual reality as truly immersive as the actual reality, enabling an advanced lifelike experience or even virtual eternity.
 The development of perspective in Renaissance European art and the stereoscope invented by Sir Charles Wheatstone were both precursors to virtual reality.[7][8][9] The first references to the more modern-day concept of virtual reality came from science fiction.
 Morton Heilig wrote in the 1950s of an ""Experience Theatre"" that could encompass all the senses in an effective manner, thus drawing the viewer into the onscreen activity. He built a prototype of his vision dubbed the Sensorama in 1962, along with five short films to be displayed in it while engaging multiple senses (sight, sound, smell, and touch). Predating digital computing, the Sensorama was a mechanical device. Heilig also developed what he referred to as the ""Telesphere Mask"" (patented in 1960). The patent application described the device as ""a telescopic television apparatus for individual use... The spectator is given a complete sensation of reality, i.e. moving three dimensional images which may be in colour, with 100% peripheral vision, binaural sound, scents and air breezes.""[10]
 In 1968, Ivan Sutherland, with the help of his students including Bob Sproull, created what was widely considered to be the first head-mounted display system for use in immersive simulation applications, called The Sword of Damocles. It was primitive both in terms of user interface and visual realism, and the HMD to be worn by the user was so heavy that it had to be suspended from the ceiling, which gave the device a formidable appearance and inspired its name.[11] Technically, the device was an augmented reality device due to optical passthrough. The graphics comprising the virtual environment were simple wire-frame model rooms.
 The virtual reality industry mainly provided VR devices for medical, flight simulation, automobile industry design, and military training purposes from 1970 to 1990.[12]
 David Em became the first artist to produce navigable virtual worlds at NASA's Jet Propulsion Laboratory (JPL) from 1977 to 1984.[13] The Aspen Movie Map, a crude virtual tour in which users could wander the streets of Aspen in one of the three modes (summer, winter, and polygons), was created at MIT in 1978.
 In 1979, Eric Howlett developed the Large Expanse, Extra Perspective (LEEP) optical system. The combined system created a stereoscopic image with a field-of-view wide enough to create a convincing sense of space. The users of the system have been impressed by the sensation of depth (field of view) in the scene and the corresponding realism. The original LEEP system was redesigned for NASA's Ames Research Center in 1985 for their first virtual reality installation, the VIEW (Virtual Interactive Environment Workstation)[14] by Scott Fisher. The LEEP system provides the basis for most of the modern virtual reality headsets.[15]
 By the late 1980s, the term ""virtual reality"" was popularized by Jaron Lanier, one of the modern pioneers of the field. Lanier had founded the company VPL Research in 1984. VPL Research has developed several VR devices like the DataGlove, the EyePhone, the Reality Built For Two (RB2), and the AudioSphere. VPL licensed the DataGlove technology to Mattel, which used it to make the Power Glove, an early affordable VR device.
 Atari, Inc. founded a research lab for virtual reality in 1982, but the lab was closed after two years due to the video game crash of 1983. However, its hired employees, such as Thomas G. Zimmerman,[16] Scott Fisher, Jaron Lanier, Michael Naimark, and Brenda Laurel, kept their research and development on VR-related technologies.
 In 1988, the Cyberspace Project at Autodesk was the first to implement VR on a low-cost personal computer.[17][18]  The project leader Eric Gullichsen left in 1990 to found Sense8 Corporation and develop the WorldToolKit virtual reality SDK,[19] which offered the first real time graphics with Texture mapping on a PC, and was widely used throughout industry and academia.[20][21]
 The 1990s saw the first widespread commercial releases of consumer headsets. In 1992, for instance, Computer Gaming World predicted ""affordable VR by 1994"".[22]
 In 1991, Sega announced the Sega VR headset for the Mega Drive home console. It used LCD screens in the visor, stereo headphones, and inertial sensors that allowed the system to track and react to the movements of the user's head.[23] In the same year, Virtuality launched and went on to become the first mass-produced, networked, multiplayer VR entertainment system that was released in many countries, including a dedicated VR arcade at Embarcadero Center. Costing up to $73,000 per multi-pod Virtuality system, they featured headsets and exoskeleton gloves that gave one of the first ""immersive"" VR experiences.[24]
 That same year, Carolina Cruz-Neira, Daniel J. Sandin and Thomas A. DeFanti from the Electronic Visualization Laboratory created the first cubic immersive room, the Cave automatic virtual environment (CAVE). Developed as Cruz-Neira's PhD thesis, it involved a multi-projected environment, similar to the holodeck, allowing people to see their own bodies in relation to others in the room.[25][26] Antonio Medina, a MIT graduate and NASA scientist, designed a virtual reality system to ""drive"" Mars rovers from Earth in apparent real time despite the substantial delay of Mars-Earth-Mars signals.[27]
 In 1992, Nicole Stenger created Angels, the first real-time interactive immersive movie where the interaction was facilitated with a dataglove and high-resolution goggles. That same year, Louis Rosenberg created the virtual fixtures system at the U.S. Air Force's Armstrong Labs using a full upper-body exoskeleton, enabling a physically realistic mixed reality in 3D. The system enabled the overlay of physically real 3D virtual objects registered with a user's direct view of the real world, producing the first true augmented reality experience enabling sight, sound, and touch.[28][29]
 By July 1994, Sega had released the VR-1 motion simulator ride attraction in Joypolis indoor theme parks,[30] as well as the Dennou Senki Net Merc arcade game. Both used an advanced head-mounted display dubbed the ""Mega Visor Display"" developed in conjunction with Virtuality;[31][32] it was able to track head movement in a 360-degree stereoscopic 3D environment, and in its Net Merc incarnation was powered by the Sega Model 1 arcade system board.[33] Apple released QuickTime VR, which, despite using the term ""VR"", was unable to represent virtual reality, and instead displayed 360-degree interactive panoramas.
 Nintendo's Virtual Boy console was released in 1995.[34] A group in Seattle created public demonstrations of a ""CAVE-like"" 270 degree immersive projection room called the Virtual Environment Theater, produced by entrepreneurs Chet Dagit and Bob Jacobson.[35] Forte released the VFX1, a PC-powered virtual reality headset that same year.
 In 1999, entrepreneur Philip Rosedale formed Linden Lab with an initial focus on the development of VR hardware. In its earliest form, the company struggled to produce a commercial version of ""The Rig"", which was realized in prototype form as a clunky steel contraption with several computer monitors that users could wear on their shoulders. The concept was later adapted into the personal computer-based, 3D virtual world program Second Life.[36]
 The 2000s were a period of relative public and investment indifference to commercially available VR technologies.
 In 2001, SAS Cube (SAS3) became the first PC-based cubic room, developed by Z-A Production (Maurice Benayoun, David Nahon), Barco, and Clarté. It was installed in Laval, France. The SAS library gave birth to Virtools VRPack. In 2007, Google introduced Street View, a service that shows panoramic views of an increasing number of worldwide positions such as roads, indoor buildings and rural areas. It also features a stereoscopic 3D mode, introduced in 2010.[37]
 In 2010, Palmer Luckey designed the first prototype of the Oculus Rift. This prototype, built on a shell of another virtual reality headset, was only capable of rotational tracking. However, it boasted a 90-degree field of vision that was previously unseen in the consumer market at the time. Luckey eliminated distortion issues arising from the type of lens used to create the wide field of vision using software that pre-distorted the rendered image in real-time. This initial design would later serve as a basis from which the later designs came.[38] In 2012, the Rift is presented for the first time at the E3 video game trade show by John Carmack.[39][40] In 2014, Facebook purchased Oculus VR for what at the time was stated as $2 billion[41] but later revealed that the more accurate figure was $3 billion.[40] This purchase occurred after the first development kits ordered through Oculus' 2012 Kickstarter had shipped in 2013 but before the shipping of their second development kits in 2014.[42] ZeniMax, Carmack's former employer, sued Oculus and Facebook for taking company secrets to Facebook;[40] the verdict was in favour of ZeniMax, settled out of court later.[43]
 In 2013, Valve discovered and freely shared the breakthrough of low-persistence displays which make lag-free and smear-free display of VR content possible.[44] This was adopted by Oculus and was used in all their future headsets. In early 2014, Valve showed off their SteamSight prototype, the precursor to both consumer headsets released in 2016. It shared major features with the consumer headsets including separate 1K displays per eye, low persistence, positional tracking over a large area, and Fresnel lenses.[45][46] HTC and Valve announced the virtual reality headset HTC Vive and controllers in 2015. The set included tracking technology called Lighthouse, which utilized wall-mounted ""base stations"" for positional tracking using infrared light.[47][48][49]
 In 2014, Sony announced Project Morpheus (its code name for the PlayStation VR), a virtual reality headset for the PlayStation 4 video game console.[50] In 2015, Google announced Cardboard, a do-it-yourself stereoscopic viewer: the user places their smartphone in the cardboard holder, which they wear on their head. Michael Naimark was appointed Google's first-ever 'resident artist' in their new VR division. The Kickstarter campaign for Gloveone, a pair of gloves providing motion tracking and haptic feedback, was successfully funded, with over $150,000 in contributions.[51] Also in 2015, Razer unveiled its open source project OSVR.
 By 2016, there were at least 230 companies developing VR-related products. Amazon, Apple, Facebook, Google, Microsoft, Sony and Samsung all had dedicated AR and VR groups. Dynamic binaural audio was common to most headsets released that year. However, haptic interfaces were not well developed, and most hardware packages incorporated button-operated handsets for touch-based interactivity. Visually, displays were still of a low-enough resolution and frame rate that images were still identifiable as virtual.[52]
 In 2016, HTC shipped its first units of the HTC Vive SteamVR headset.[53] This marked the first major commercial release of sensor-based tracking, allowing for free movement of users within a defined space.[54] A patent filed by Sony in 2017 showed they were developing a similar location tracking technology to the Vive for PlayStation VR, with the potential for the development of a wireless headset.[55]
 In 2019, Oculus released the Oculus Rift S and a standalone headset, the Oculus Quest. These headsets utilized inside-out tracking compared to external outside-in tracking seen in previous generations of headsets.[56]
 Later in 2019, Valve released the Valve Index. Notable features include a 130° field of view, off-ear headphones for immersion and comfort, open-handed controllers which allow for individual finger tracking, front facing cameras, and a front expansion slot meant for extensibility.[57]
 In 2020, Oculus released the Oculus Quest 2. Some new features include a sharper screen, reduced price, and increased performance. Facebook (which became Meta a year later) initially required users to log in with a Facebook account in order to use the new headset.[58] In 2021 the Oculus Quest 2 accounted for 80% of all VR headsets sold.[59]
 In 2021, EASA approved the first Virtual Reality based Flight Simulation Training Device. The device, for rotorcraft pilots, enhances safety by opening up the possibility of practicing risky maneuvers in a virtual environment. This addresses a key risk area in rotorcraft operations,[61] where statistics show that around 20% of accidents occur during training flights.
 In 2023, Sony released the PlayStation VR2, a follow-up to their 2016 headset. PlayStation VR2 comes with inside-out tracking, higher-resolution displays, controllers with adaptive triggers and haptic feedback, and a wider field-of-view.[62]
 In June 2023, Apple announced the Apple Vision Pro. This marks their first venture into the VR headset market. The device uses a mix of AR and VR to produce visuals and is one of the only mainstream headsets to purely use hand-tracking and nothing else for controllers. With the potential integration of ChatGPT users of Apple Vision can benefit from technology like real-time translation. This will also allow users to be able to see information about items they are looking at in real time.  [63]
 Modern virtual reality headset displays are based on technology developed for smartphones including: gyroscopes and motion sensors for tracking head, body, and hand positions; small HD screens for stereoscopic displays; and small, lightweight and fast computer processors. These components led to relative affordability for independent VR developers, and led to the 2012 Oculus Rift Kickstarter offering the first independently developed VR headset.[52]
 Independent production of VR images and video has increased alongside the development of affordable omnidirectional cameras, also known as 360-degree cameras or VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video.[64] In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications.[65][66]
 To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective image to the right eye. A distinction is made between active (e.g. shutter glasses) and passive technologies (e.g. polarizing filters or Infitec).[67]
 In order to improve the feeling of immersion, wearable multi-string cables offer haptics to complex geometries in virtual reality. These strings offer fine control of each finger joint to simulate the haptics involved in touching these virtual geometries.[68]
 Special input devices are required for interaction with the virtual world. Some of the most common input devices are motion controllers and optical tracking sensors. In some cases, wired gloves are used. Controllers typically use optical tracking systems (primarily infrared cameras) for location and navigation, so that the user can move freely without wiring. Some input devices provide the user with force feedback to the hands or other parts of the body, so that the user can orientate themselves in the three-dimensional world through haptics and sensor technology as a further sensory sensation and carry out realistic simulations. This allows for the viewer to have a sense of direction in the artificial landscape. Additional haptic feedback can be obtained from omnidirectional treadmills (with which walking in virtual space is controlled by real walking movements) and vibration gloves and suits.
 Virtual reality cameras can be used to create VR photography using 360-degree panorama videos. 360-degree camera shots can be mixed with virtual elements to merge reality and fiction through special effects. Virtual reality cameras can be used as a steppingstone to make realistic holographic displays these cameras can be used to cover every angle of the needed experience.[69] VR cameras are available in various formats, with varying numbers of lenses installed in the camera.[70]
 The Virtual Reality Modelling Language (VRML), first introduced in 1994, was intended for the development of ""virtual worlds"" without dependency on headsets.[71] The Web3D consortium was subsequently founded in 1997 for the development of industry standards for web-based 3D graphics. The consortium subsequently developed X3D from the VRML framework as an archival, open-source standard for web-based distribution of VR content.[72] WebVR is an experimental JavaScript application programming interface (API) that provides support for various virtual reality devices, such as the HTC Vive, Oculus Rift, Google Cardboard or OSVR, in a web browser.[73]
 Minimal Angle of Resolution (MAR) refers to the minimum distance between two display pixels. At the distance, viewer can clearly distinguish the independent pixels. Often measured in arc-seconds, MAR between two pixels has to do with the viewing distance. For the general public, resolution is about 30-65 arc-seconds, which is referred to as the spatial resolution when combined with distance. Given the viewing distance of 1m and 2m respectively, regular viewers won't be able to perceive two pixels as separate if they are less than 0.29mm apart at 1m and less than 0.58mm apart at 2m.[74]
 Most small-size displays have a refresh rate of 60 Hz, which adds about 15ms of additional latency. The number is reduced to less than 7ms if the refresh rate is increased to 120 Hz or even 240 Hz and more.[75] Participants generally feel that the experience is more immersive with higher refresh rates as a result. However, higher refresh rates require a more powerful graphics processing unit.
 In assessing the achieved immersion by a VR device, we need to consider our field of view (FOV) in addition to image quality. Our eyes have a horizontal FOV from about 107 or 110 degrees to the temporal side to about 60 or 70 degrees toward the nose, and a vertical FOV from about 95 degrees downward to 85 degrees upward,[76] and eye movements are estimated as roughly 30 deg to either side horizontally and 20 vertically. Binocular vision is limited to the 120 or 140 degrees where the right and the left visual fields overlap. With eye movements, we have a FOV of roughly 300 degrees x 175 degrees with two eyes, i.e., approximately one third of the full 360-deg sphere.
 Virtual reality is most commonly used in entertainment applications such as video games, 3D cinema, amusement park rides including dark rides and social virtual worlds. Consumer virtual reality headsets were first released by video game companies in the early-mid 1990s. Beginning in the 2010s, next-generation commercial tethered headsets were released by Oculus (Rift), HTC (Vive) and Sony (PlayStation VR), setting off a new wave of application development.[77] 3D cinema has been used for sporting events, pornography, fine art, music videos and short films. Since 2015, roller coasters and theme parks have incorporated virtual reality to match visual effects with haptic feedback.[52] VR not only fits the trend of the digital industry but also enhances the film's visual effect. The film gives the audience more ways to interact through VR technology.[78]
 In social sciences and psychology, virtual reality offers a cost-effective tool to study and replicate interactions in a controlled environment.[79] It can be used as a form of therapeutic intervention.[80] For instance, there is the case of the virtual reality exposure therapy (VRET), a form of exposure therapy for treating anxiety disorders such as post traumatic stress disorder (PTSD) and phobias.[81][82][83]
 A VR therapy has been designed to help people with psychosis and agoraphobia manage their avoidance of outside environments. In the therapy, the user wears a headset and a virtual character provides psychological advice and guides them as they explore simulated environments (such as a cafe or a busy street). NICE is assessing the therapy to see if it should be recommended on the NHS.[84][85]
 During the COVID-19 pandemic, social VR has also been used as a mental-health tool in a form of self-administered, non-traditional cognitive behavioural therapy.[86]
 Virtual reality programs are being used in the rehabilitation processes with elderly individuals that have been diagnosed with Alzheimer's disease. This gives these elderly patients the opportunity to simulate real experiences that they would not otherwise be able to experience due to their current state. 17 recent studies with randomized controlled trials have shown that virtual reality applications are effective in treating cognitive deficits with neurological diagnoses.[87] Loss of mobility in elderly patients can lead to a sense of loneliness and depression. Virtual reality is able to assist in making aging in place a lifeline to an outside world that they cannot easily navigate. Virtual reality allows exposure therapy to take place in a safe environment.[88]
 In medicine, simulated VR surgical environments were first developed in the 1990s.[89][90][91]  Under the supervision of experts, VR can provide effective and repeatable training[92] at a low cost, allowing trainees to recognize and amend errors as they occur.[93]
 Virtual reality has been used in physical rehabilitation since the 2000s. Despite numerous studies conducted, good quality evidence of its efficacy compared to other rehabilitation methods without sophisticated and expensive equipment is lacking for the treatment of Parkinson's disease.[94] A 2018 review on the effectiveness of mirror therapy by virtual reality and robotics for any type of pathology concluded in a similar way.[95] Another study was conducted that showed the potential for VR to promote mimicry and revealed the difference between neurotypical and autism spectrum disorder individuals in their response to a two-dimensional avatar.[96][97]
 Immersive virtual reality technology with myoelectric and motion tracking control may represent a possible therapy option for treatment-resistant phantom limb pain. Pain scale measurements were taken into account and an interactive 3-D kitchen environment was developed based on the principles of mirror therapy to allow for control of virtual hands while wearing a motion-tracked VR headset.[98] A systematic search in Pubmed and Embase was performed to determine results that were pooled in two meta-analysis. Meta-analysis showed a significant result in favor of VRT for balance.[99]
 
In the fast-paced and globalised business world, meetings in VR are used to create an environment in which interactions with other people (e.g. colleagues, customers, partners) can feel more natural than a phone call or video chat. In the customisable meeting rooms all parties can join using the VR headset and interact as if they are in the same physical room. Presentations, videos or 3D models (of e.g. products or prototypes) can be uploaded and interacted with.[100] Compared to traditional text-based CMC, Avatar-based interactions in 3D virtual environment lead to higher levels of consensus, satisfaction, and cohesion among group members.[101] VR can simulate real workspaces for workplace occupational safety and health purposes, educational purposes, and training purposes. It can be used to provide learners with a virtual environment where they can develop their skills without the real-world consequences of failing. It has been used and studied in primary education,[102] anatomy teaching,[103][104] military,[105][106] astronaut training,[107][108][109] flight simulators,[110] miner training,[111] medical education,[112] geography education,[113] architectural design,[citation needed] driver training[114] and bridge inspection.[115] Immersive VR engineering systems enable engineers to see virtual prototypes prior to the availability of any physical prototypes.[116] Supplementing training with virtual training environments has been claimed to offer avenues of realism in military[117] and healthcare[118] training while minimizing cost.[119] It also has been claimed to reduce military training costs by minimizing the amounts of ammunition expended during training periods.[117] VR can be used for the healthcare training and education for medical practitioners.[120][121] Further, several application have been developed for multiple types of safety training.[122][123] The latest results indicates that virtual reality safety training is more effective than traditional training in terms of knowledge acquisition and knowledge retention.[124]
 In the engineering field, VR has proved very useful for both engineering educators and the students. A previously expensive cost in the educational department now being much more accessible due to lowered overall costs, has proven to be a very useful tool in educating future engineers. The most significant element lies in the ability for the students to be able to interact with 3-D models that accurately respond based on real world possibilities. This added tool of education provides many the immersion needed to grasp complex topics and be able to apply them.[125] As noted, the future architects and engineers benefit greatly by being able to form understandings between spatial relationships and providing solutions based on real-world future applications.[126]
 The first fine art virtual world was created in the 1970s.[127] As the technology developed, more artistic programs were produced throughout the 1990s, including feature films. When commercially available technology became more widespread, VR festivals began to emerge in the mid-2010s. The first uses of VR in museum settings began in the 1990s, seeing a significant increase in the mid-2010s. Additionally, museums have begun making some of their content virtual reality accessible.[128][129]
 Virtual reality's growing market presents an opportunity and an alternative channel for digital marketing.[130] It is also seen as a new platform for e-commerce, particularly in the bid to challenge traditional ""brick and mortar"" retailers. However, a 2018 study revealed that the majority of goods are still purchased in physical stores.[131]
 In the case of education, the uses of virtual reality have demonstrated being capable of promoting higher order thinking,[132] promoting the interest and commitment of students, the acquisition of knowledge, promoting mental habits and understanding that are generally useful within an academic context.[133]
 A case has also been made for including virtual reality technology in the context of public libraries. This would give library users access to cutting-edge technology and unique educational experiences.[134] This could include giving users access to virtual, interactive copies of rare texts and artifacts and to tours of famous landmarks and archeological digs (as in the case with the Virtual Ganjali Khan Project).[135]
 Starting in the early 2020s, virtual reality has also been discussed as a technological setting that may support people's grieving process, based on digital recreations of deceased individuals. In 2021, this practice received substantial media attention following a South Korean TV documentary, which invited a grieving mother to interact with a virtual replica of her deceased daughter.[136] Subsequently, scientists have summarized several potential implications of such endeavours, including its potential to facilitate adaptive mourning, but also many ethical challenges.[137][138]
 Growing interest in the metaverse has resulted in organizational efforts to incorporate the many diverse applications of virtual reality into ecosystems like VIVERSE, reportedly offering connectivity between platforms for a wide range of uses.[139]
 In June of 2020, Jean Michel Jarre performed in VRChat.[140] In July, Brendan Bradley released the free FutureStages web-based virtual reality venue for live events and concerts throughout the 2020 shutdown,[141] Justin Bieber performed on November 18, 2021 in WaveXR.[142] On December 2, 2021, non-player characters performed at the Mugar Omni Theater with audiences interacting with a live performer in both virtual reality and projected on the IMAX dome screen.[143][144] Meta's Foo Fighters Super Bowl VR concert performed on Venues.[145] Post Malone performed in Venues starting July 15, 2022.[146] Megan Thee Stallion performed on AmazeVR at AMC Theaters throughout 2022.[147]
 On October 24, 2021, Billie Eilish performed on Oculus Venues. Pop group Imagine Dragons performed on June 15, 2022.
 There are many health and safety considerations of virtual reality. A number of unwanted symptoms have been caused by prolonged use of virtual reality,[148] and these may have slowed proliferation of the technology. Most virtual reality systems come with consumer warnings, including: seizures; developmental issues in children; trip-and-fall and collision warnings; discomfort; repetitive stress injury; and interference with medical devices.[149] Some users may experience twitches, seizures or blackouts while using VR headsets, even if they do not have a history of epilepsy and have never had blackouts or seizures before. One in 4,000 people, or .025%, may experience these symptoms. Motion sickness, eyestrain, headaches, and discomfort are the most prevalent short-term adverse effects. In addition, because of the virtual reality headsets' heavy weight, discomfort may be more likely among children. Therefore, children are advised against using VR headsets.[150] Other problems may occur in physical interactions with one's environment. While wearing VR headsets, people quickly lose awareness of their real-world surroundings and may injure themselves by tripping over, or colliding with real-world objects.[151]
 VR headsets may regularly cause eye fatigue, as does all screened technology, because people tend to blink less when watching screens, causing their eyes to become more dried out.[152] There have been some concerns about VR headsets contributing to myopia, but although VR headsets sit close to the eyes, they may not necessarily contribute to nearsightedness if the focal length of the image being displayed is sufficiently far away.[153]
 Virtual reality sickness (also known as cybersickness) occurs when a person's exposure to a virtual environment causes symptoms that are similar to motion sickness symptoms.[154] Women are significantly more affected than men by headset-induced symptoms, at rates of around 77% and 33% respectively.[155][156] The most common symptoms are general discomfort, headache, stomach awareness, nausea, vomiting, pallor, sweating, fatigue, drowsiness, disorientation, and apathy.[157] For example, Nintendo's Virtual Boy received much criticism for its negative physical effects, including ""dizziness, nausea, and headaches"".[158] These motion sickness symptoms are caused by a disconnect between what is being seen and what the rest of the body perceives. When the vestibular system, the body's internal balancing system, does not experience the motion that it expects from visual input through the eyes, the user may experience VR sickness. This can also happen if the VR system does not have a high enough frame rate, or if there is a lag between the body's movement and the onscreen visual reaction to it.[159] Because approximately 25–40% of people experience some kind of VR sickness when using VR machines, companies are actively looking for ways to reduce VR sickness.[160]
 Vergence-accommodation conflict (VAC) is one of the main causes of virtual reality sickness.[161]
 In January 2022 The Wall Street Journal found that VR usage could lead to physical injuries including leg, hand, arm and shoulder injuries.[162] VR usage has also been tied to incidents that resulted in neck injuries (especially injures to the cervical vertebrae).[163]
 Children are becoming increasingly aware of VR, with the number in the USA having never heard of it dropping by half from Autumn 2016 (40%) to Spring 2017 (19%).[164]
 A 2022 research report by Piper Sandler revealed that only 26% of U.S. teens own a VR device, 5% use it daily, while 48% of teen headset owners ""seldom"" use it. Of the teens who don't own a VR headset, 9% plan to buy one. 50% of surveyed teens are unsure about the metaverse or don't have any interest, and don't have any plans to purchase a VR headset.[165]
 Studies show that young children, compared to adults, may respond cognitively and behaviorally to immersive VR in ways that differ from adults. VR places users directly into the media content, potentially making the experience very vivid and real for children. For example, children of 6–18 years of age reported higher levels of presence and ""realness"" of a virtual environment compared with adults 19–65 years of age.[166]
 Studies on VR consumer behavior or its effect on children and a code of ethical conduct involving underage users are especially needed, given the availability of VR porn and violent content. Related research on violence in video games suggests that exposure to media violence may affect attitudes, behavior, and even self-concept. Self-concept is a key indicator of core attitudes and coping abilities, particularly in adolescents.[167] Early studies conducted on observing versus participating in violent VR games suggest that physiological arousal and aggressive thoughts, but not hostile feelings, are higher for participants than for observers of the virtual reality game.[168]
 Experiencing VR by children may further involve simultaneously holding the idea of the virtual world in mind while experiencing the physical world. Excessive usage of immersive technology that has very salient sensory features may compromise children's ability to maintain the rules of the physical world, particularly when wearing a VR headset that blocks out the location of objects in the physical world. Immersive VR can provide users with multisensory experiences that replicate reality or create scenarios that are impossible or dangerous in the physical world. Observations of 10 children experiencing VR for the first time suggested that 8-12-years-old kids were more confident to explore VR content when it was in a familiar situation, e.g. the children enjoyed playing in the kitchen context of Job Simulator, and enjoyed breaking rules by engaging in activities they are not allowed to do in reality, such as setting things on fire.[164]
 Digital privacy concerns have been associated with VR platforms;[169][170] the persistent tracking required by all VR systems makes the technology particularly useful for, and vulnerable to, mass surveillance, including information gathering of personal actions, movements and responses.[52] Data from eye tracking sensors, which are projected to become a standard feature in virtual reality headsets,[171][172] may indirectly reveal information about a user's ethnicity, personality traits, fears, emotions, interests, skills, and physical and mental health conditions.[173]
 The nature of VR technology means that it can gather a wide range of data about its users. This can include obvious information such as usernames and account information, but also extends to more personal data like physical movements, interaction habits, and responses to virtual environments. In addition, advanced VR systems can capture biometric data like voice patterns, eye movements, and physiological responses to VR experiences.[174][175] Virtual reality technology has grown substantially since its inception, moving from a niche technology to a mainstream consumer product. As the user base has grown, so too has the amount of personal data collected by these systems.[176] This data can be used to improve VR systems, to provide personalized experiences, or to collect demographic information for marketing purposes. However, it also raises significant privacy concerns, especially when this data is stored, shared, or sold without the user's explicit consent.[177]
 Existing data protection and privacy laws like the General Data Protection Regulation (GDPR) in the EU, and the California Consumer Privacy Act (CCPA) in the United States, can be applied to VR. These regulations require companies to disclose how they collect and use data, and give users a degree of control over their personal information.[178] Despite these regulations, enforcing privacy laws in VR can be challenging due to the global nature of the technology and the vast amounts of data collected.[179]
 Due to its history of privacy issues, the involvement of Meta Platforms (formerly Facebook, Inc.) in the VR market has led to privacy concerns specific to its platforms. In August 2020, Facebook announced that Oculus products would become subject to the terms of use and privacy policy of the Facebook social network, and that a Facebook account would be required to use future Oculus headset models, and all existing models (via deprecation of the separate Oculus account system) beginning January 2023. The announcement was criticized for the mandatory integration of Oculus headsets with Facebook data collection and policies (including the Facebook real-name policy), and preventing use of the hardware if the user's account is suspended.[180][181][182] The following month, Facebook halted the sale of Oculus products in Germany due to concerns from regulators that the new policy was a violation of GDPR.[183] In 2022, the company would later establish a separate ""Meta account"" system.[184]
 In 2024, researchers from the University of Chicago were able to exploit a vulnerability in Meta Platforms' Quest VR system to obtain users' login credentials and inject false details during online banking sessions. In another study using Beat Saber, the majority of the participants did not suspect anything when their VR headsets were attacked by the researchers. This hack should be difficult to execute outside research settings but would make its target vulnerable to many risks such as phishing, Internet fraud, and grooming.[185]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['education', 'Jaron Lanier, Michael Naimark, and Brenda Laurel', 'privacy issues', 'Motion sickness, eyestrain, headaches, and discomfort', 'real-world consequences of failing'], 'answer_start': [], 'answer_end': []}"
"
 Augmented reality (AR) is an interactive experience that combines the real world and computer-generated 3D content. The content can span multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory.[1] AR can be defined as a system that incorporates three basic features: a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects.[2] The overlaid sensory information can be constructive (i.e. additive to the natural environment), or destructive (i.e. masking of the natural environment).[3] As such, it is one of the key technologies in the reality-virtuality continuum.[4]
 This experience is seamlessly interwoven with the physical world such that it is perceived as an immersive aspect of the real environment.[3] In this way, augmented reality alters one's ongoing perception of a real-world environment, whereas virtual reality completely replaces the user's real-world environment with a simulated one.[5][6]
 Augmented reality is largely synonymous with mixed reality. There is also overlap in terminology with extended reality and computer-mediated reality.
 The primary value of augmented reality is the manner in which components of the digital world blend into a person's perception of the real world, not as a simple display of data, but through the integration of immersive sensations, which are perceived as natural parts of an environment. The earliest functional AR systems that provided immersive mixed reality experiences for users were invented in the early 1990s, starting with the Virtual Fixtures system developed at the U.S. Air Force's Armstrong Laboratory in 1992.[3][7][8] Commercial augmented reality experiences were first introduced in entertainment and gaming businesses.[9] Subsequently, augmented reality applications have spanned commercial industries such as education, communications, medicine, and entertainment. In education, content may be accessed by scanning or viewing an image with a mobile device or by using markerless AR techniques.[10][11][12]
 Augmented reality can be used to enhance natural environments or situations and offers perceptually enriched experiences. With the help of advanced AR technologies (e.g. adding computer vision, incorporating AR cameras into smartphone applications, and object recognition) the information about the surrounding real world of the user becomes interactive and digitally manipulated.[13] Information about the environment and its objects is overlaid on the real world. This information can be virtual. Augmented Reality is any experience which is artificial and which adds to the already existing reality.[14][15][16][17][18] or real, e.g. seeing other real sensed or measured information such as electromagnetic radio waves overlaid in exact alignment with where they actually are in space.[19][20][21] Augmented reality also has a lot of potential in the gathering and sharing of tacit knowledge. Augmentation techniques are typically performed in real-time and in semantic contexts with environmental elements. Immersive perceptual information is sometimes combined with supplemental information like scores over a live video feed of a sporting event. This combines the benefits of both augmented reality technology and heads up display technology (HUD).
 In virtual reality (VR), the users' perception of reality is completely based on virtual information. In augmented reality (AR) the user is provided with additional computer- generated information within the data collected from real life that enhances their perception of reality.[22][23] For example, in architecture, VR can be used to create a walk-through simulation of the inside of a new building; and AR can be used to show a building's structures and systems super-imposed on a real-life view. Another example is through the use of utility applications. Some AR applications, such as Augment, enable users to apply digital objects into real environments, allowing businesses to use augmented reality devices as a way to preview their products in the real world.[24] Similarly, it can also be used to demo what products may look like in an environment for customers, as demonstrated by companies such as Mountain Equipment Co-op or Lowe's who use augmented reality to allow customers to preview what their products might look like at home through the use of 3D models.[25]
 Augmented reality (AR) differs from virtual reality (VR) in the sense that in AR part of the surrounding environment is 'real' and AR is just adding layers of virtual objects to the real environment. On the other hand, in VR the surrounding environment is completely virtual and computer generated. A demonstration of how AR layers objects onto the real world can be seen with augmented reality games. WallaMe is an augmented reality game application that allows users to hide messages in real environments, utilizing geolocation technology in order to enable users to hide messages wherever they may wish in the world.[26] Such applications have many uses in the world, including in activism and artistic expression.[27]
 Augmented reality requires hardware components including a processor, display, sensors, and input devices. Modern mobile computing devices like smartphones and tablet computers contain these elements, which often include a camera and microelectromechanical systems (MEMS) sensors such as an accelerometer, GPS, and solid state compass, making them suitable AR platforms.[66][67]
 Various technologies can be used to display augmented reality, including optical projection systems, monitors, and handheld devices. Two of the display technologies used in augmented reality are diffractive waveguides and reflective waveguides.
 A head-mounted display (HMD) is a display device worn on the forehead, such as a harness or helmet-mounted. HMDs place images of both the physical world and virtual objects over the user's field of view. Modern HMDs often employ sensors for six degrees of freedom monitoring that allow the system to align virtual information to the physical world and adjust accordingly with the user's head movements.[68][69][70] HMDs can provide VR users with mobile and collaborative experiences.[71] Specific providers, such as uSens and Gestigon, include gesture controls for full virtual immersion.[72][73]
 Vuzix is a company that has produced a number of head-worn optical see through displays marketed for augmented reality.[74][75][76]
 AR displays can be rendered on devices resembling eyeglasses. Versions include eyewear that employs cameras to intercept the real world view and re-display its augmented view through the eyepieces[77] and devices in which the AR imagery is projected through or reflected off the surfaces of the eyewear lens pieces.[78][79][80]
 The EyeTap (also known as Generation-2 Glass[81]) captures rays of light that would otherwise pass through the center of the lens of the wearer's eye, and substitutes synthetic computer-controlled light for each ray of real light. The Generation-4 Glass[81] (Laser EyeTap) is similar to the VRD (i.e. it uses a computer-controlled laser light source) except that it also has infinite depth of focus and causes the eye itself to, in effect, function as both a camera and a display by way of exact alignment with the eye and resynthesis (in laser light) of rays of light entering the eye.[82]
 A head-up display (HUD) is a transparent display that presents data without requiring users to look away from their usual viewpoints. A precursor technology to augmented reality, heads-up displays were first developed for pilots in the 1950s, projecting simple flight data into their line of sight, thereby enabling them to keep their ""heads up"" and not look down at the instruments. Near-eye augmented reality devices can be used as portable head-up displays as they can show data, information, and images while the user views the real world. Many definitions of augmented reality only define it as overlaying the information.[83][84] This is basically what a head-up display does; however, practically speaking, augmented reality is expected to include registration and tracking between the superimposed perceptions, sensations, information, data, and images and some portion of the real world.[85]
 Contact lenses that display AR imaging are in development. These bionic contact lenses might contain the elements for display embedded into the lens including integrated circuitry, LEDs and an antenna for wireless communication. The first contact lens display was patented in 1999 by Steve Mann and was intended to work in combination with AR spectacles, but the project was abandoned,[86][87] then 11 years later in 2010–2011.[88][89][90][91] Another version of contact lenses, in development for the U.S. military, is designed to function with AR spectacles, allowing soldiers to focus on close-to-the-eye AR images on the spectacles and distant real world objects at the same time.[92][93]
 At CES 2013, a company called Innovega also unveiled similar contact lenses that required being combined with AR glasses to work.[94]
 Many scientists have been working on contact lenses capable of different technological feats. A patent filed by Samsung describes an AR contact lens, that, when finished, will include a built-in camera on the lens itself.[95] The design is intended to control its interface by blinking an eye. It is also intended to be linked with the user's smartphone to review footage, and control it separately. When successful, the lens would feature a camera, or sensor inside of it. It is said that it could be anything from a light sensor, to a temperature sensor.
 The first publicly unveiled working prototype of an AR contact lens not requiring the use of glasses in conjunction was developed by Mojo Vision and announced and shown off at CES 2020.[96][97][98]
 A virtual retinal display (VRD) is a personal display device under development at the University of Washington's Human Interface Technology Laboratory under Dr. Thomas A. Furness III.[99] With this technology, a display is scanned directly onto the retina of a viewer's eye. This results in bright images with high resolution and high contrast. The viewer sees what appears to be a conventional display floating in space.[100]
 Several of tests were done to analyze the safety of the VRD.[99] In one test, patients with partial loss of vision—having either macular degeneration (a disease that degenerates the retina) or keratoconus—were selected to view images using the technology. In the macular degeneration group, five out of eight subjects preferred the VRD images to the cathode-ray tube (CRT) or paper images and thought they were better and brighter and were able to see equal or better resolution levels. The Keratoconus patients could all resolve smaller lines in several line tests using the VRD as opposed to their own correction. They also found the VRD images to be easier to view and sharper. As a result of these several tests, virtual retinal display is considered safe technology.
 Virtual retinal display creates images that can be seen in ambient daylight and ambient room light. The VRD is considered a preferred candidate to use in a surgical display due to its combination of high resolution and high contrast and brightness. Additional tests show high potential for VRD to be used as a display technology for patients that have low vision.
 A Handheld display employs a small display that fits in a user's hand. All handheld AR solutions to date opt for video see-through. Initially handheld AR employed fiducial markers,[101] and later GPS units and MEMS sensors such as digital compasses and six degrees of freedom accelerometer–gyroscope. Today simultaneous localization and mapping (SLAM) markerless trackers such as PTAM (parallel tracking and mapping) are starting to come into use. Handheld display AR promises to be the first commercial success for AR technologies. The two main advantages of handheld AR are the portable nature of handheld devices and the ubiquitous nature of camera phones. The disadvantages are the physical constraints of the user having to hold the handheld device out in front of them at all times, as well as the distorting effect of classically wide-angled mobile phone cameras when compared to the real world as viewed through the eye.[102]
 Projection mapping augments real-world objects and scenes without the use of special displays such as monitors, head-mounted displays or hand-held devices. Projection mapping makes use of digital projectors to display graphical information onto physical objects. The key difference in projection mapping is that the display is separated from the users of the system. Since the displays are not associated with each user, projection mapping scales naturally up to groups of users, allowing for collocated collaboration between users.
 Examples include shader lamps, mobile projectors, virtual tables, and smart projectors. Shader lamps mimic and augment reality by projecting imagery onto neutral objects. This provides the opportunity to enhance the object's appearance with materials of a simple unit—a projector, camera, and sensor.
 Other applications include table and wall projections. Virtual showcases, which employ beam splitter mirrors together with multiple graphics displays, provide an interactive means of simultaneously engaging with the virtual and the real.
 A projection mapping system can display on any number of surfaces in an indoor setting at once. Projection mapping supports both a graphical visualization and passive haptic sensation for the end users. Users are able to touch physical objects in a process that provides passive haptic sensation.[18][43][103][104]
 Modern mobile augmented-reality systems use one or more of the following motion tracking technologies: digital cameras and/or other optical sensors, accelerometers, GPS, gyroscopes, solid state compasses, radio-frequency identification (RFID). These technologies offer varying levels of accuracy and precision. These technologies are implemented in the ARKit API by Apple and ARCore API by Google to allow tracking for their respective mobile device platforms.
 Techniques include speech recognition systems that translate a user's spoken words into computer instructions, and gesture recognition systems that interpret a user's body movements by visual detection or from sensors embedded in a peripheral device such as a wand, stylus, pointer, glove or other body wear.[105][106][107][108] Products which are trying to serve as a controller of AR headsets include Wave by Seebright Inc. and Nimble by Intugine Technologies.
 Computers are responsible for graphics in augmented reality. A computer analyzes the sensed visual and other data to synthesize and position virtual objects. With the improvement of technology and computers, augmented reality is going to lead to a drastic change on ones perspective of the real world.[109]
 Computers are improving at a very fast rate, leading to new ways to improve other technology. The more that computers progress, augmented reality will become more flexible and more common in society. Computers are the core of augmented reality.
[110] The computer receives data from the sensors which determine the relative position of an objects' surface. This translates to an input to the computer which then outputs to the users by adding something that would otherwise not be there. The computer comprises memory and a processor.[111] The computer takes the scanned environment then generates images or a video and puts it on the receiver for the observer to see. The fixed marks on an object's surface are stored in the memory of a computer. The computer also withdraws from its memory to present images realistically to the onlooker.
 Projectors can also be used to display AR contents. The projector can throw a virtual object on a projection screen and the viewer can interact with this virtual object. Projection surfaces can be many objects such as walls or glass panes.[112]
 Mobile augmented reality applications are gaining popularity because of the wide adoption of mobile and especially wearable devices. However, they often rely on computationally intensive computer vision algorithms with extreme latency requirements. To compensate for the lack of computing power, offloading data processing to a distant machine is often desired. Computation offloading introduces new constraints in applications, especially in terms of latency and bandwidth. Although there are a plethora of real-time multimedia transport protocols, there is a need for support from network infrastructure as well.[113]
 A key measure of AR systems is how realistically they integrate augmentations with the real world. The software must derive real world coordinates, independent of camera, and camera images. That process is called image registration, and uses different methods of computer vision, mostly related to video tracking.[114][115] Many computer vision methods of augmented reality are inherited from visual odometry.
 Usually those methods consist of two parts. The first stage is to detect interest points, fiducial markers or optical flow in the camera images. This step can use feature detection methods like corner detection, blob detection, edge detection or thresholding, and other image processing methods.[116][117] The second stage restores a real world coordinate system from the data obtained in the first stage. Some methods assume objects with known geometry (or fiducial markers) are present in the scene. In some of those cases the scene 3D structure should be calculated beforehand. If part of the scene is unknown simultaneous localization and mapping (SLAM) can map relative positions. If no information about scene geometry is available, structure from motion methods like bundle adjustment are used. Mathematical methods used in the second stage include: projective (epipolar) geometry, geometric algebra, rotation representation with exponential map, kalman and particle filters, nonlinear optimization, robust statistics.[citation needed]
 In augmented reality, the distinction is made between two distinct modes of tracking, known as marker and markerless. Markers are visual cues which trigger the display of the virtual information.[118] A piece of paper with some distinct geometries can be used. The camera recognizes the geometries by identifying specific points in the drawing. Markerless tracking, also called instant tracking, does not use markers. Instead, the user positions the object in the camera view preferably in a horizontal plane. It uses sensors in mobile devices to accurately detect the real-world environment, such as the locations of walls and points of intersection.[119]
 Augmented Reality Markup Language (ARML) is a data standard developed within the Open Geospatial Consortium (OGC),[120] which consists of Extensible Markup Language (XML) grammar to describe the location and appearance of virtual objects in the scene, as well as ECMAScript bindings to allow dynamic access to properties of virtual objects.
 
To enable rapid development of augmented reality applications, software development applications have emerged, including Lens Studio from Snapchat and Spark AR from Facebook. Augmented reality Software Development Kits (SDKs) have been launched by Apple and Google.[121][122]
 The implementation of augmented reality in consumer products requires considering the design of the applications and the related constraints of the technology platform. Since AR systems rely heavily on the immersion of the user and the interaction between the user and the system, design can facilitate the adoption of virtuality. For most augmented reality systems, a similar design guideline can be followed. The following lists some considerations for designing augmented reality applications:
 Context Design focuses on the end-user's physical surrounding, spatial space, and accessibility that may play a role when using the AR system. Designers should be aware of the possible physical scenarios the end-user may be in such as:
 By evaluating each physical scenario, potential safety hazards can be avoided and changes can be made to greater improve the end-user's immersion. UX designers will have to define user journeys for the relevant physical scenarios and define how the interface reacts to each.
 Another aspect of context design involves the design of the system's functionality and its ability to accommodate user preferences.[124][125] While accessibility tools are common in basic application design, some consideration should be made when designing time-limited prompts (to prevent unintentional operations), audio cues and overall engagement time. It is important to note that in some situations, the application's functionality may hinder the user's ability. For example, applications that is used for driving should reduce the amount of user interaction and use audio cues instead.
 Interaction design in augmented reality technology centers on the user's engagement with the end product to improve the overall user experience and enjoyment. The purpose of interaction design is to avoid alienating or confusing the user by organizing the information presented. Since user interaction relies on the user's input, designers must make system controls easier to understand and accessible. A common technique to improve usability for augmented reality applications is by discovering the frequently accessed areas in the device's touch display and design the application to match those areas of control.[126] It is also important to structure the user journey maps and the flow of information presented which reduce the system's overall cognitive load and greatly improves the learning curve of the application.[127]
 In interaction design, it is important for developers to utilize augmented reality technology that complement the system's function or purpose.[128] For instance, the utilization of exciting AR filters and the design of the unique sharing platform in Snapchat enables users to augment their in-app social interactions. In other applications that require users to understand the focus and intent, designers can employ a reticle or raycast from the device.[124]
 In general, graphic design is the appearance of the developing application that engages the user. To improve the graphic interface elements and user interaction, developers may use visual cues to inform the user what elements of UI are designed to interact with and how to interact with them. Since navigating in an AR application may appear difficult and seem frustrating, visual cue design can make interactions seem more natural.[123]
 In some augmented reality applications that use a 2D device as an interactive surface, the 2D control environment does not translate well in 3D space making users hesitant to explore their surroundings. To solve this issue, designers should apply visual cues to assist and encourage users to explore their surroundings.
 It is important to note the two main objects in AR when developing VR applications: 3D volumetric objects that are manipulated and realistically interact with light and shadow; and animated media imagery such as images and videos which are mostly traditional 2D media rendered in a new context for augmented reality.[123] When virtual objects are projected onto a real environment, it is challenging for augmented reality application designers to ensure a perfectly seamless integration relative to the real-world environment, especially with 2D objects. As such, designers can add weight to objects, use depths maps, and choose different material properties that highlight the object's presence in the real world. Another visual design that can be applied is using different lighting techniques or casting shadows to improve overall depth judgment. For instance, a common lighting technique is simply placing a light source overhead at the 12 o’clock position, to create shadows on virtual objects.[123]
 Augmented reality has been explored for many uses, from gaming and entertainment to medicine, education and business.[129] Example application areas described below include archaeology, architecture, commerce and education. Some of the earliest cited examples include augmented reality used to support surgery by providing virtual overlays to guide medical practitioners, to AR content for astronomy and welding.[8][130]
 AR has been used to aid archaeological research. By augmenting archaeological features onto the modern landscape, AR allows archaeologists to formulate possible site configurations from extant structures.[131] Computer generated models of ruins, buildings, landscapes or even ancient people have been recycled into early archaeological AR applications.[132][133][134] For example, implementing a system like VITA (Visual Interaction Tool for Archaeology) will allow users to imagine and investigate instant excavation results without leaving their home. Each user can collaborate by mutually ""navigating, searching, and viewing data"". Hrvoje Benko, a researcher in the computer science department at Columbia University, points out that these particular systems and others like them can provide ""3D panoramic images and 3D models of the site itself at different excavation stages"" all the while organizing much of the data in a collaborative way that is easy to use. Collaborative AR systems supply multimodal interactions that combine the real world with virtual images of both environments.[135]
 AR can aid in visualizing building projects. Computer-generated images of a structure can be superimposed onto a real-life local view of a property before the physical building is constructed there; this was demonstrated publicly by Trimble Navigation in 2004. AR can also be employed within an architect's workspace, rendering animated 3D visualizations of their 2D drawings. Architecture sight-seeing can be enhanced with AR applications, allowing users viewing a building's exterior to virtually see through its walls, viewing its interior objects and layout.[136][137][51]
 With continual improvements to GPS accuracy, businesses are able to use augmented reality to visualize georeferenced models of construction sites, underground structures, cables and pipes using mobile devices.[138] Augmented reality is applied to present new projects, to solve on-site construction challenges, and to enhance promotional materials.[139] Examples include the Daqri Smart Helmet, an Android-powered hard hat used to create augmented reality for the industrial worker, including visual instructions, real-time alerts, and 3D mapping.
 Following the Christchurch earthquake, the University of Canterbury released CityViewAR,[140] which enabled city planners and engineers to visualize buildings that had been destroyed.[141] This not only provided planners with tools to reference the previous cityscape, but it also served as a reminder of the magnitude of the resulting devastation, as entire buildings had been demolished.
 In educational settings, AR has been used to complement a standard curriculum. Text, graphics, video, and audio may be superimposed into a student's real-time environment. Textbooks, flashcards and other educational reading material may contain embedded ""markers"" or triggers that, when scanned by an AR device, produced supplementary information to the student rendered in a multimedia format.[142][143][144] The 2015 Virtual, Augmented and Mixed Reality: 7th International Conference mentioned Google Glass as an example of augmented reality that can replace the physical classroom.[145] First, AR technologies help learners engage in authentic exploration in the real world, and virtual objects such as texts, videos, and pictures are supplementary elements for learners to conduct investigations of the real-world surroundings.[146]
 As AR evolves, students can participate interactively and interact with knowledge more authentically. Instead of remaining passive recipients, students can become active learners, able to interact with their learning environment. Computer-generated simulations of historical events allow students to explore and learning details of each significant area of the event site.[147]
 In higher education, Construct3D, a Studierstube system, allows students to learn mechanical engineering concepts, math or geometry.[148] Chemistry AR apps allow students to visualize and interact with the spatial structure of a molecule using a marker object held in the hand.[149] Others have used HP Reveal, a free app, to create AR notecards for studying organic chemistry mechanisms or to create virtual demonstrations of how to use laboratory instrumentation.[150] Anatomy students can visualize different systems of the human body in three dimensions.[151] Using AR as a tool to learn anatomical structures has been shown to increase the learner knowledge and provide intrinsic benefits, such as increased engagement and learner immersion.[152][153]
 AR has been used to develop different safety training application for several types of disasters such as, earthquakes and building fire.[154][155] Further, several AR solutions have been proposed and tested to navigate building evacuees towards safe places in both large scale and small scale disasters.[156][157] Further, AR applications can have several overlapping with many other digital technologies, such as BIM, internet of things and artificial intelligence, to generate smarter safety training and navigation solutions.[158]
 AR is used to substitute paper manuals with digital instructions which are overlaid on the manufacturing operator's field of view, reducing mental effort required to operate.[159] AR makes machine maintenance efficient because it gives operators direct access to a machine's maintenance history.[160] Virtual manuals help manufacturers adapt to rapidly-changing product designs, as digital instructions are more easily edited and distributed compared to physical manuals.[159]
 Digital instructions increase operator safety by removing the need for operators to look at a screen or manual away from the working area, which can be hazardous. Instead, the instructions are overlaid on the working area.[161][162] The use of AR can increase operators' feeling of safety when working near high-load industrial machinery by giving operators additional information on a machine's status and safety functions, as well as hazardous areas of the workspace.[161][163]
 AR is used to integrate print and video marketing. Printed marketing material can be designed with certain ""trigger"" images that, when scanned by an AR-enabled device using image recognition, activate a video version of the promotional material. A major difference between augmented reality and straightforward image recognition is that one can overlay multiple media at the same time in the view screen, such as social media share buttons, the in-page video even audio and 3D objects. Traditional print-only publications are using augmented reality to connect different types of media.[164][165][166][167][168]
 AR can enhance product previews such as allowing a customer to view what's inside a product's packaging without opening it.[169] AR can also be used as an aid in selecting products from a catalog or through a kiosk. Scanned images of products can activate views of additional content such as customization options and additional images of the product in its use.[170]
 By 2010, virtual dressing rooms had been developed for e-commerce.[171]
 In 2012, a mint used AR techniques to market a commemorative coin for Aruba. The coin itself was used as an AR trigger, and when held in front of an AR-enabled device it revealed additional objects and layers of information that were not visible without the device.[172][173]
 In 2018, Apple announced Universal Scene Description (USDZ) AR file support for iPhones and iPads with iOS 12. Apple has created an AR QuickLook Gallery that allows masses to experience augmented reality on their own Apple device.[174]
 In 2018, Shopify, the Canadian e-commerce company, announced AR Quick Look integration. Their merchants will be able to upload 3D models of their products and their users will be able to tap on the models inside the Safari browser on their iOS devices to view them in their real-world environments.[175]
 In 2018, Twinkl released a free AR classroom application. Pupils can see how York looked over 1,900 years ago.[176] Twinkl launched the first ever multi-player AR game, Little Red[177] and has over 100 free AR educational models.[178]
 Augmented reality is becoming more frequently used for online advertising. Retailers offer the ability to upload a picture on their website and ""try on"" various clothes which are overlaid on the picture. Even further, companies such as Bodymetrics install dressing booths in department stores that offer full-body scanning. These booths render a 3-D model of the user, allowing the consumers to view different outfits on themselves without the need of physically changing clothes.[179] For example, JC Penney and Bloomingdale's use ""virtual dressing rooms"" that allow customers to see themselves in clothes without trying them on.[180] Another store that uses AR to market clothing to its customers is Neiman Marcus.[181] Neiman Marcus offers consumers the ability to see their outfits in a 360-degree view with their ""memory mirror"".[181] Makeup stores like L'Oreal, Sephora, Charlotte Tilbury, and Rimmel also have apps that utilize AR.[182] These apps allow consumers to see how the makeup will look on them.[182] According to Greg Jones, director of AR and VR at Google, augmented reality is going to ""reconnect physical and digital retail"".[182]
 AR technology is also used by furniture retailers such as IKEA, Houzz, and Wayfair.[182][180] These retailers offer apps that allow consumers to view their products in their home prior to purchasing anything.[182] [183]
In 2017, Ikea announced the Ikea Place app. It contains a catalogue of over 2,000 products—nearly the company's full collection of sofas, armchairs, coffee tables, and storage units which one can place anywhere in a room with their phone.[184] The app made it possible to have 3D and true-to-scale models of furniture in the customer's living space. IKEA realized that their customers are not shopping in stores as often or making direct purchases anymore.[185][186] Shopify's acquisition of Primer, an AR app aims to push small and medium-sized sellers towards interactive AR shopping with easy to use AR integration and user experience for both merchants and consumers.[187] AR helps the retail industry reduce operating costs. Merchants upload product information to the AR system, and consumers can use mobile terminals to search and generate 3D maps.[188]
 The first description of AR as it is known today was in Virtual Light, the 1994 novel by William Gibson. In 2011, AR was blended with poetry by ni ka from Sekai Camera in Tokyo, Japan. The prose of these AR poems come from Paul Celan, Die Niemandsrose, expressing the aftermath of the 2011 Tōhoku earthquake and tsunami.[189]
 AR applied in the visual arts allows objects or places to trigger artistic multidimensional experiences and interpretations of reality.
 The Australian new media artist Jeffrey Shaw pioneered Augmented Reality in three artworks: Viewpoint in 1975, Virtual Sculptures in 1987 and The Golden Calf in 1993.[191][192] He continues to explore new permutations of AR in numerous recent works.
 Augmented reality can aid in the progression of visual art in museums by allowing museum visitors to view artwork in galleries in a multidimensional way through their phone screens.[193] The Museum of Modern Art in New York has created an exhibit in their art museum showcasing AR features that viewers can see using an app on their smartphone.[194] The museum has developed their personal app, called MoMAR Gallery, that museum guests can download and use in the augmented reality specialized gallery in order to view the museum's paintings in a different way.[195] This allows individuals to see hidden aspects and information about the paintings, and to be able to have an interactive technological experience with artwork as well.
 AR technology was used in Nancy Baker Cahill's ""Margin of Error"" and ""Revolutions,""[196] the two public art pieces she created for the 2019 Desert X exhibition.[197]
 AR technology aided the development of eye tracking technology to translate a disabled person's eye movements into drawings on a screen.[198]
 A Danish artist, Olafur Eliasson, has placed objects like burning suns, extraterrestrial rocks, and rare animals, into the user's environment.[199] Martin & Muñoz started using Augmented Reality (AR) technology in 2020 to create and place virtual works, based on their snow globes, in their exhibitions and in user's environments. Their first AR work was presented at the Cervantes Institute in New York in early 2022.[200]
 AR hardware and software for use in fitness includes smart glasses made for biking and running, with performance analytics and map navigation projected onto the user's field of vision,[201] and boxing, martial arts, and tennis, where users remain aware of their physical environment for safety.[202] Fitness-related games and software include Pokémon Go and Jurassic World Alive.[203]
 Human–computer interaction (HCI) is an interdisciplinary area of computing that deals with design and implementation of systems that interact with people. Researchers in HCI come from a number of disciplines, including computer science, engineering, design, human factor, and social science, with a shared goal to solve problems in the design and the use of technology so that it can be used more easily, effectively, efficiently, safely, and with satisfaction.[204]
 According to a 2017 Time article, in about 15 to 20 years it is predicted that augmented reality and virtual reality are going to become the primary use for computer interactions.[205]
 Primary school children learn easily from interactive experiences. As an example, astronomical constellations and the movements of objects in the solar system were oriented in 3D and overlaid in the direction the device was held, and expanded with supplemental video information. Paper-based science book illustrations could seem to come alive as video without requiring the child to navigate to web-based materials.
 In 2013, a project was launched on Kickstarter to teach about electronics with an educational toy that allowed children to scan their circuit with an iPad and see the electric current flowing around.[206] While some educational apps were available for AR by 2016, it was not broadly used. Apps that leverage augmented reality to aid learning included SkyView for studying astronomy,[207] AR Circuits for building simple electric circuits,[208] and SketchAr for drawing.[209]
 AR would also be a way for parents and teachers to achieve their goals for modern education, which might include providing more individualized and flexible learning, making closer connections between what is taught at school and the real world, and helping students to become more engaged in their own learning.
 Augmented reality systems are used in public safety situations, from super storms to suspects at large.
 As early as 2009, two articles from Emergency Management discussed AR technology for emergency management. The first was ""Augmented Reality—Emerging Technology for Emergency Management"", by Gerald Baron.[210] According to Adam Crow,: ""Technologies like augmented reality (ex: Google Glass) and the growing expectation of the public will continue to force professional emergency managers to radically shift when, where, and how technology is deployed before, during, and after disasters.""[211]
 Another early example was a search aircraft looking for a lost hiker in rugged mountain terrain. Augmented reality systems provided aerial camera operators with a geographic awareness of forest road names and locations blended with the camera video. The camera operator was better able to search for the hiker knowing the geographic context of the camera image. Once located, the operator could more efficiently direct rescuers to the hiker's location because the geographic position and reference landmarks were clearly labeled.[212]
 AR can be used to facilitate social interaction. An augmented reality social network framework called Talk2Me enables people to disseminate information and view others' advertised information in an augmented reality way. The timely and dynamic information sharing and viewing functionalities of Talk2Me help initiate conversations and make friends for users with people in physical proximity.[213] However, use of an AR headset can inhibit the quality of an interaction between two people if one isn't wearing one if the headset becomes a distraction.[214]
 Augmented reality also gives users the ability to practice different forms of social interactions with other people in a safe, risk-free environment. Hannes Kauffman, Associate Professor for virtual reality at TU Vienna, says: ""In collaborative augmented reality multiple users may access a shared space populated by virtual objects, while remaining grounded in the real world. This technique is particularly powerful for educational purposes when users are collocated and can use natural means of communication (speech, gestures, etc.), but can also be mixed successfully with immersive VR or remote collaboration.""[This quote needs a citation] Hannes cites education as a potential use of this technology.
 The gaming industry embraced AR technology. A number of games were developed for prepared indoor environments, such as AR air hockey, Titans of Space, collaborative combat against virtual enemies, and AR-enhanced pool table games.[215][216][217]
 In 2010, Ogmento became the first AR gaming startup to receive VC Funding.  The company went on to produce early location-based AR games for titles like Paranormal Activity: Sanctuary, NBA: King of the Court, and Halo: King of the Hill. The companies computer vision technology was eventually repackaged and sold to Apple, became a major contribution to ARKit.[218]
 Augmented reality allows video game players to experience digital game play in a real-world environment. Niantic released the augmented reality mobile game Pokémon Go.[219] Disney has partnered with Lenovo to create the augmented reality game Star Wars: Jedi Challenges that works with a Lenovo Mirage AR headset, a tracking sensor and a Lightsaber controller, scheduled to launch in December 2017.[220]
 AR allows industrial designers to experience a product's design and operation before completion. Volkswagen has used AR for comparing calculated and actual crash test imagery.[221] AR has been used to visualize and modify car body structure and engine layout. It has also been used to compare digital mock-ups with physical mock-ups to find discrepancies between them.[222][223]
 One of the first applications of augmented reality was in healthcare, particularly to support the planning, practice, and training of surgical procedures. As far back as 1992, enhancing human performance during surgery was a formally stated objective when building the first augmented reality systems at U.S. Air Force laboratories.[3] Since 2005, a device called a near-infrared vein finder that films subcutaneous veins, processes and projects the image of the veins onto the skin has been used to locate veins.[224][225] AR provides surgeons with patient monitoring data in the style of a fighter pilot's heads-up display, and allows patient imaging records, including functional videos, to be accessed and overlaid. Examples include a virtual X-ray view based on prior tomography or on real-time images from ultrasound and confocal microscopy probes,[226] visualizing the position of a tumor in the video of an endoscope,[227] or radiation exposure risks from X-ray imaging devices.[228][229] AR can enhance viewing a fetus inside a mother's womb.[230] Siemens, Karl Storz and IRCAD have developed a system for laparoscopic liver surgery that uses AR to view sub-surface tumors and vessels.[231]
AR has been used for cockroach phobia treatment[232] and to reduce the fear of spiders.[233] Patients wearing augmented reality glasses can be reminded to take medications.[234] Augmented reality can be very helpful in the medical field.[235] It could be used to provide crucial information to a doctor or surgeon without having them take their eyes off the patient. On 30 April 2015 Microsoft announced the Microsoft HoloLens, their first attempt at augmented reality. The HoloLens has advanced through the years and is capable of projecting holograms for near infrared fluorescence based image guided surgery.[236] As augmented reality advances, it finds increasing applications in healthcare. Augmented reality and similar computer based-utilities are being used to train medical professionals.[237][238] In healthcare, AR can be used to provide guidance during diagnostic and therapeutic interventions e.g. during surgery. Magee et al.,[239] for instance, describe the use of augmented reality for medical training in simulating ultrasound-guided needle placement. A very recent study by Akçayır, Akçayır, Pektaş, and Ocak (2016) revealed that AR technology both improves university students' laboratory skills and helps them to build positive attitudes relating to physics laboratory work.[240] Recently, augmented reality began seeing adoption in neurosurgery, a field that requires heavy amounts of imaging before procedures.[241]
 Augmented reality applications, running on handheld devices utilized as virtual reality headsets, can also digitize human presence in space and provide a computer generated model of them, in a virtual space where they can interact and perform various actions. Such capabilities are demonstrated by Project Anywhere, developed by a postgraduate student at ETH Zurich, which was dubbed as an ""out-of-body experience"".[242][243][244]
 Building on decades of perceptual-motor research in experimental psychology, researchers at the Aviation Research Laboratory of the University of Illinois at Urbana–Champaign used augmented reality in the form of a flight path in the sky to teach flight students how to land an airplane using a flight simulator. An adaptive augmented schedule in which students were shown the augmentation only when they departed from the flight path proved to be a more effective training intervention than a constant schedule.[30][245] Flight students taught to land in the simulator with the adaptive augmentation learned to land a light aircraft more quickly than students with the same amount of landing training in the simulator but with constant augmentation or without any augmentation.[30]
 An interesting early application of AR occurred when Rockwell International created video map overlays of satellite and orbital debris tracks to aid in space observations at Air Force Maui Optical System. In their 1993 paper ""Debris Correlation Using the Rockwell WorldView System"" the authors describe the use of map overlays applied to video from space surveillance telescopes. The map overlays indicated the trajectories of various objects in geographic coordinates. This allowed telescope operators to identify satellites, and also to identify and catalog potentially dangerous space debris.[39]
 Starting in 2003 the US Army integrated the SmartCam3D augmented reality system into the Shadow Unmanned Aerial System to aid sensor operators using telescopic cameras to locate people or points of interest. The system combined fixed geographic information including street names, points of interest, airports, and railroads with live video from the camera system. The system offered a ""picture in picture"" mode that allows it to show a synthetic view of the area surrounding the camera's field of view. This helps solve a problem in which the field of view is so narrow that it excludes important context, as if ""looking through a soda straw"". The system displays real-time friend/foe/neutral location markers blended with live video, providing the operator with improved situational awareness.
 Researchers at USAF Research Lab (Calhoun, Draper et al.) found an approximately two-fold increase in the speed at which UAV sensor operators found points of interest using this technology.[246] This ability to maintain geographic awareness quantitatively enhances mission efficiency. The system is in use on the US Army RQ-7 Shadow and the MQ-1C Gray Eagle Unmanned Aerial Systems.
 In combat, AR can serve as a networked communication system that renders useful battlefield data onto a soldier's goggles in real time. From the soldier's viewpoint, people and various objects can be marked with special indicators to warn of potential dangers. Virtual maps and 360° view camera imaging can also be rendered to aid a soldier's navigation and battlefield perspective, and this can be transmitted to military leaders at a remote command center.[247] The combination of 360° view cameras visualization and AR can be used on board combat vehicles and tanks as circular review system.
 AR can be an effective tool for virtually mapping out the 3D topologies of munition storages in the terrain, with the choice of the munitions combination in stacks and distances between them with a visualization of risk areas.[248][unreliable source?] The scope of AR applications also includes visualization of data from embedded munitions monitoring sensors.[248]
 The NASA X-38 was flown using a hybrid synthetic vision system that overlaid map data on video to provide enhanced navigation for the spacecraft during flight tests from 1998 to 2002. It used the LandForm software which was useful for times of limited visibility, including an instance when the video camera window frosted over leaving astronauts to rely on the map overlays.[44] The LandForm software was also test flown at the Army Yuma Proving Ground in 1999. In the photo at right one can see the map markers indicating runways, air traffic control tower, taxiways, and hangars overlaid on the video.[45]
 AR can augment the effectiveness of navigation devices. Information can be displayed on an automobile's windshield indicating destination directions and meter, weather, terrain, road conditions and traffic information as well as alerts to potential hazards in their path.[249][250][251] Since 2012, a Swiss-based company WayRay has been developing holographic AR navigation systems that use holographic optical elements for projecting all route-related information including directions, important notifications, and points of interest right into the drivers' line of sight and far ahead of the vehicle.[252][253] Aboard maritime vessels, AR can allow bridge watch-standers to continuously monitor important information such as a ship's heading and speed while moving throughout the bridge or performing other tasks.[254]
 Augmented reality may have a positive impact on work collaboration as people may be inclined to interact more actively with their learning environment. It may also encourage tacit knowledge renewal which makes firms more competitive. AR was used to facilitate collaboration among distributed team members via conferences with local and virtual participants. AR tasks included brainstorming and discussion meetings utilizing common visualization via touch screen tables, interactive digital whiteboards, shared design spaces and distributed control rooms.[255][256][257]
 In industrial environments, augmented reality is proving to have a substantial impact with more and more use cases emerging across all aspect of the product lifecycle, starting from product design and new product introduction (NPI) to manufacturing to service and maintenance, to material handling and distribution. For example, labels were displayed on parts of a system to clarify operating instructions for a mechanic performing maintenance on a system.[258][259] Assembly lines benefited from the usage of AR. In addition to Boeing, BMW and Volkswagen were known for incorporating this technology into assembly lines for monitoring process improvements.[260][261][262] Big machines are difficult to maintain because of their multiple layers or structures. AR permits people to look through the machine as if with an x-ray, pointing them to the problem right away.[263]
 As AR technology has evolved and second and third generation AR devices come to market, the impact of AR in enterprise continues to flourish. In the Harvard Business Review, Magid Abraham and Marco Annunziata discuss how AR devices are now being used to ""boost workers' productivity on an array of tasks the first time they're used, even without prior training"".[264] They contend that ""these technologies increase productivity by making workers more skilled and efficient, and thus have the potential to yield both more economic growth and better jobs"".[264]
 Weather visualizations were the first application of augmented reality in television. It has now become common in weather casting to display full motion video of images captured in real-time from multiple cameras and other imaging devices. Coupled with 3D graphics symbols and mapped to a common virtual geospatial model, these animated visualizations constitute the first true application of AR to TV.
 AR has become common in sports telecasting. Sports and entertainment venues are provided with see-through and overlay augmentation through tracked camera feeds for enhanced viewing by the audience. Examples include the yellow ""first down"" line seen in television broadcasts of American football games showing the line the offensive team must cross to receive a first down. AR is also used in association with football and other sporting events to show commercial advertisements overlaid onto the view of the playing area. Sections of rugby fields and cricket pitches also display sponsored images. Swimming telecasts often add a line across the lanes to indicate the position of the current record holder as a race proceeds to allow viewers to compare the current race to the best performance. Other examples include hockey puck tracking and annotations of racing car performance[265] and snooker ball trajectories.[114][266]
 AR has been used to enhance concert and theater performances. For example, artists allow listeners to augment their listening experience by adding their performance to that of other bands/groups of users.[267][268][269]
 Travelers may use AR to access real-time informational displays regarding a location, its features, and comments or content provided by previous visitors. Advanced AR applications include simulations of historical events, places, and objects rendered into the landscape.[270][271][272]
 AR applications linked to geographic locations present location information by audio, announcing features of interest at a particular site as they become visible to the user.[273][274][275]
 AR systems such as Word Lens can interpret the foreign text on signs and menus and, in a user's augmented view, re-display the text in the user's language. Spoken words of a foreign language can be translated and displayed in a user's view as printed subtitles.[276][277][278]
 It has been suggested that augmented reality may be used in new methods of music production, mixing, control and visualization.[279][280][281][282]
 In a proof-of-concept project Ian Sterling, an interaction design student at California College of the Arts, and software engineer Swaroop Pal demonstrated a HoloLens app whose primary purpose is to provide a 3D spatial UI for cross-platform devices—the Android Music Player app and Arduino-controlled Fan and Light—and also allow interaction using gaze and gesture control.[283][284][285][286]
 Research by members of the CRIStAL at the University of Lille makes use of augmented reality to enrich musical performance. The ControllAR project allows musicians to augment their MIDI control surfaces with the remixed graphical user interfaces of music software.[287] The Rouages project proposes to augment digital musical instruments to reveal their mechanisms to the audience and thus improve the perceived liveness.[288] Reflets is a novel augmented reality display dedicated to musical performances where the audience acts as a 3D display by revealing virtual content on stage, which can also be used for 3D musical interaction and collaboration.[289]
 Snapchat users have access to augmented reality in the app through use of camera filters. In September 2017, Snapchat updated its app to include a camera filter that allowed users to render an animated, cartoon version of themselves called ""Bitmoji"". These animated avatars would be projected in the real world through the camera, and can be photographed or video recorded.[290] In the same month, Snapchat also announced a new feature called ""Sky Filters"" that will be available on its app. This new feature makes use of augmented reality to alter the look of a picture taken of the sky, much like how users can apply the app's filters to other pictures. Users can choose from sky filters such as starry night, stormy clouds, beautiful sunsets, and rainbow.[291]
 In a paper titled ""Death by Pokémon GO"", researchers at Purdue University's Krannert School of Management claim the game caused ""a disproportionate increase in vehicular crashes and associated vehicular damage, personal injuries, and fatalities in the vicinity of locations, called PokéStops, where users can play the game while driving.""[292] Using data from one municipality, the paper extrapolates what that might mean nationwide and concluded ""the increase in crashes attributable to the introduction of Pokémon GO is 145,632 with an associated increase in the number of injuries of 29,370 and an associated increase in the number of fatalities of 256 over the period of 6 July 2016, through 30 November 2016."" The authors extrapolated the cost of those crashes and fatalities at between $2bn and $7.3 billion for the same period. Furthermore, more than one in three surveyed advanced Internet users would like to edit out disturbing elements around them, such as garbage or graffiti.[293] They would like to even modify their surroundings by erasing street signs, billboard ads, and uninteresting shopping windows. So it seems that AR is as much a threat to companies as it is an opportunity. Although, this could be a nightmare to numerous brands that do not manage to capture consumer imaginations it also creates the risk that the wearers of augmented reality glasses may become unaware of surrounding dangers. Consumers want to use augmented reality glasses to change their surroundings into something that reflects their own personal opinions. Around two in five want to change the way their surroundings look and even how people appear to them. [citation needed]
 Next, to the possible privacy issues that are described below, overload and over-reliance issues are the biggest danger of AR. For the development of new AR-related products, this implies that the user-interface should follow certain guidelines as not to overload the user with information while also preventing the user from over-relying on the AR system such that important cues from the environment are missed.[18] This is called the virtually-augmented key.[18] Once the key is ignored, people might not desire the real world anymore.
 The concept of modern augmented reality depends on the ability of the device to record and analyze the environment in real time. Because of this, there are potential legal concerns over privacy. While the First Amendment to the United States Constitution allows for such recording in the name of public interest, the constant recording of an AR device makes it difficult to do so without also recording outside of the public domain. Legal complications would be found in areas where a right to a certain amount of privacy is expected or where copyrighted media are displayed.
 In terms of individual privacy, there exists the ease of access to information that one should not readily possess about a given person. This is accomplished through facial recognition technology. Assuming that AR automatically passes information about persons that the user sees, there could be anything seen from social media, criminal record, and marital status.[294]
 The Code of Ethics on Human Augmentation, which was originally introduced by Steve Mann in 2004 and further refined with Ray Kurzweil and Marvin Minsky in 2013, was ultimately ratified at the virtual reality Toronto conference on 25 June 2017.[295][296][297][298]
 The interaction of location-bound augmented reality with property law is largely undefined.[299][300] Several models have been analysed for how this interaction may be resolved in a common law context: an extension of real property rights to also cover augmentations on or near the property with a strong notion of trespassing, forbidding augmentations unless allowed by the owner; an 'open range' system, where augmentations are allowed unless forbidden by the owner; and a 'freedom to roam' system, where real property owners have no control over non-disruptive augmentations.[301]
 One issue experienced during the Pokémon Go craze was the game's players disturbing owners of private property while visiting nearby location-bound augmentations, which may have been on the properties or the properties may have been en route. The terms of service of Pokémon Go explicitly disclaim responsibility for players' actions, which may limit (but may not totally extinguish) the liability of its producer, Niantic, in the event of a player trespassing while playing the game: by Niantic's argument, the player is the one committing the trespass, while Niantic has merely engaged in permissible free speech. A theory advanced in lawsuits brought against Niantic is that their placement of game elements in places that will lead to trespass or an exceptionally large flux of visitors can constitute nuisance, despite each individual trespass or visit only being tenuously caused by Niantic.[302][303][304]
 Another claim raised against Niantic is that the placement of profitable game elements on land without permission of the land's owners is unjust enrichment.[305] More hypothetically, a property may be augmented with advertising or disagreeable content against its owner's wishes.[306] Under American law, these situations are unlikely to be seen as a violation of real property rights by courts without an expansion of those rights to include augmented reality (similarly to how English common law came to recognise air rights).[305]
 An article in the Michigan Telecommunications and Technology Law Review argues that there are three bases for this extension, starting with various understanding of property. The personality theory of property, outlined by Margaret Radin, is claimed to support extending property rights due to the intimate connection between personhood and ownership of property; however, her viewpoint is not universally shared by legal theorists.[307] Under the utilitarian theory of property, the benefits from avoiding the harms to real property owners caused by augmentations and the tragedy of the commons, and the reduction in transaction costs by making discovery of ownership easy, were assessed as justifying recognising real property rights as covering location-bound augmentations, though there does remain the possibility of a tragedy of the anticommons from having to negotiate with property owners slowing innovation.[308] Finally, following the 'property as the law of things' identification as supported by Thomas Merrill and Henry E Smith, location-based augmentation is naturally identified as a 'thing', and, while the non-rivalrous and ephemeral nature of digital objects presents difficulties to the excludeability prong of the definition, the article argues that this is not insurmountable.[309]
 Some attempts at legislative regulation have been made in the United States. Milwaukee County, Wisconsin attempted to regulate augmented reality games played in its parks, requiring prior issuance of a permit,[310] but this was criticised on free speech grounds by a federal judge;[311] and Illinois considered mandating a notice and take down procedure for location-bound augmentations.[312]
 An article for the Iowa Law Review observed that dealing with many local permitting processes would be arduous for a large-scale service,[313] and, while the proposed Illinois mechanism could be made workable,[314] it was reactive and required property owners to potentially continually deal with new augmented reality services; instead, a national-level geofencing registry, analogous to a do-not-call list, was proposed as the most desirable form of regulation to efficiently balance the interests of both providers of augmented reality services and real property owners.[315] An article in the Vanderbilt Journal of Entertainment and Technology Law, however, analyses a monolithic do-not-locate registry as an insufficiently flexible tool, either permitting unwanted augmentations or foreclosing useful applications of augmented reality.[316] Instead, it argues that an 'open range' model, where augmentations are permitted by default but property owners may restrict them on a case-by-case basis (and with noncompliance treated as a form of trespass), will produce the socially-best outcome.[317]
 The futuristic short film Sight[321] features contact lens-like augmented reality devices.[322][323]
  Media related to Augmented reality at Wikimedia Commons
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the real world', 'Thomas Merrill and Henry E Smith', 'it is one of the key technologies in the reality-virtuality continuum', 'Augmented reality', 'latency and bandwidth'], 'answer_start': [], 'answer_end': []}"
"
 A blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes.[1][2][3][4] Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (compare linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are irreversible in that, once they are recorded, the data in any given block cannot be altered retroactively without altering all subsequent blocks.
 Blockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.[5]
 A blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer.[6] The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need for a trusted authority or central server. The bitcoin design has inspired other applications[3][2] and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail.[7]
 Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model ""snake oil"";[8] however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.[4][9]
 Cryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation ""Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups.""[10] Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta.[4][11] They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees into the design, which improved its efficiency by allowing several document certificates to be collected into one block.[4][12] Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995.[13]
 The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain.[4] The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.[3]
 In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes).[14] In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size. The ledger size had exceeded 200 GB by early 2020.[15]
 The words block and chain were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, blockchain, by 2016.[16]
 According to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters' phase.[17] Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.
 In May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ""planning or [looking at] active experimentation with blockchain"".[18] For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.[19]
 A blockchain is a decentralized, distributed, and often public, digital ledger consisting of records called blocks that are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks.[3][20] This allows the participants to verify and audit transactions independently and relatively inexpensively.[21] A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests.[22] Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double-spending. A blockchain has been described as a value-exchange protocol.[23] A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance.[citation needed]
 Logically, a blockchain can be seen as consisting of several layers:[24]
 Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree.[3] Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain.[3] This iterative process confirms the integrity of the previous block, all the way back to the initial block, which is known as the genesis block (Block 0).[26][27] To assure the integrity of a block and the data contained in it, the block is usually digitally signed.[28]
 Sometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks.[27] Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially[29] as more blocks are built on top of it, eventually becoming very low.[3][30]: ch. 08 [31] For example, bitcoin uses a proof-of-work system, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.[32]
 The block time is the average time it takes for the network to generate one extra block in the blockchain. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes.[33]
 A hard fork is a change to the blockchain protocol that is not backward compatible and requires all users to upgrade their software in order to continue participating in the network. In a hard fork, the network splits into two separate versions: one that follows the new rules and one that follows the old rules.
 For example, Ethereum was hard forked in 2016 to ""make whole"" the investors in The DAO, which had been hacked by exploiting a vulnerability in its code. In this case, the fork resulted in a split creating Ethereum and Ethereum Classic chains. In 2014 the Nxt community was asked to consider a hard fork that would have led to a rollback of the blockchain records to mitigate the effects of a theft of 50 million NXT from a major cryptocurrency exchange. The hard fork proposal was rejected, and some of the funds were recovered after negotiations and ransom payment. Alternatively, to prevent a permanent split, a majority of nodes using the new software may return to the old rules, as was the case of bitcoin split on 12 March 2013.[34]
 By storing data across its peer-to-peer network, the blockchain eliminates some risks that come with data being held centrally.[3] The decentralized blockchain may use ad hoc message passing and distributed networking.[37]
 In a so-called ""51% attack"" a central entity gains control of more than half of a network and can then manipulate that specific blockchain record at will, allowing double-spending.[38]
 Blockchain security methods include the use of public-key cryptography.[39]: 5  A public key (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A private key is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.[3]
 Every node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication[40] and computational trust. No centralized ""official"" copy exists and no user is ""trusted"" more than any other.[39] Transactions are broadcast to the network using the software. Messages are delivered on a best-effort basis. Early blockchains rely on energy-intensive mining nodes to validate transactions,[27] add them to the block they are building, and then broadcast the completed block to other nodes.[30]: ch. 08  Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes.[41] Later consensus methods include proof of stake.[27] The growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive.[42]
 Finality is the level of confidence that the well-formed block recently appended to the blockchain will not be revoked in the future (is ""finalized"") and thus can be trusted. Most distributed blockchain protocols, whether proof of work or proof of stake, cannot guarantee the finality of a freshly committed block, and instead rely on ""probabilistic finality"": as the block goes deeper into a blockchain, it is less likely to be altered or reverted by a newly found consensus.[43]
 Byzantine fault tolerance-based proof-of-stake protocols purport to provide so called ""absolute finality"": a randomly chosen validator proposes a block, the rest of validators vote on it, and, if a supermajority decision approves it, the block is irreversibly committed into the blockchain.[43] A modification of this method, an ""economic finality"", is used in practical protocols, like the Casper protocol used in Ethereum: validators which sign two different blocks at the same position in the blockchain are subject to ""slashing"", where their leveraged stake is forfeited.[43]
 Open blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain.[44][45][46][47][48] Proponents of permissioned or private chains argue that the term ""blockchain"" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases.[49] Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain.[50]: 30–31  Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision.[44][46] Nikolai Hampton of Computerworld said that ""many in-house blockchain solutions will be nothing more than cumbersome databases,"" and ""without a clear security model, proprietary blockchains should be eyed with suspicion.""[8][51]
 An advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed.[29] This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer.[29]
 Bitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper ""Pricing via Processing or Combatting Junk Mail"".
 In 2016, venture capital investment for blockchain-related projects was weakening in the USA but increasing in China.[52] Bitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018[update], bitcoin has the highest market capitalization.
 Permissioned blockchains use an access control layer to govern who has access to the network.[53] It has been argued that permissioned blockchains can guarantee a certain level of decentralization, if carefully designed, as opposed to permissionless blockchains, which are often centralized in practice.[9]
 Nikolai Hampton argued in Computerworld that ""There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.""[8] This has a set of particularly profound adverse implications during a financial crisis or debt crisis like the financial crisis of 2007–08, where politically powerful actors may make decisions that favor some groups at the expense of others,[54] and ""the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power — it's time-consuming and expensive.""[8] He also said, ""Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.""[8]
 The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies.[55] A blockchain, if it is public, provides anyone who wants access to observe and analyse the chain data, given one has the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks.[56][57] The reason for this is accusations of blockchain-enabled cryptocurrencies enabling illicit dark market trade of drugs, weapons, money laundering, etc.[58] A common belief has been that cryptocurrency is private and untraceable, thus leading many actors to use it for illegal purposes. This is changing and now specialised tech companies provide blockchain tracking services, making crypto exchanges, law-enforcement and banks more aware of what is happening with crypto funds and fiat-crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as Monero.[59][60][61]
 In April 2016, Standards Australia submitted a proposal to the International Organization for Standardization to consider developing standards to support blockchain technology. This proposal resulted in the creation of ISO Technical Committee 307, Blockchain and Distributed Ledger Technologies.[62] The technical committee has working groups relating to blockchain terminology, reference architecture, security and privacy, identity, smart contracts, governance and interoperability for blockchain and DLT, as well as standards specific to industry sectors and generic government requirements.[63][non-primary source needed] More than 50 countries are participating in the standardization process together with external liaisons such as the Society for Worldwide Interbank Financial Telecommunication (SWIFT), the European Commission, the International Federation of Surveyors, the International Telecommunication Union (ITU) and the United Nations Economic Commission for Europe (UNECE).[63]
 Many other national standards bodies and open standards bodies are also working on blockchain standards.[64] These include the National Institute of Standards and Technology[65] (NIST), the European Committee for Electrotechnical Standardization[66] (CENELEC), the Institute of Electrical and Electronics Engineers[67] (IEEE), the Organization for the Advancement of Structured Information Standards (OASIS), and some individual participants in the Internet Engineering Task Force[68] (IETF).
 Although most of blockchain implementation are decentralized and distributed, Oracle launched a centralized blockchain table feature in Oracle 21c database. The Blockchain Table in Oracle 21c database is a centralized blockchain which provide immutable feature. Compared to decentralized blockchains, centralized blockchains normally can provide a higher throughput and lower latency of transactions than consensus-based distributed blockchains.[69][70]
 Currently, there are at least four types of blockchain networks — public blockchains, private blockchains, consortium blockchains and hybrid blockchains.
 A public blockchain has absolutely no access restrictions. Anyone with an Internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol).[71][self-published source?] Usually, such networks offer economic incentives for those who secure them and utilize some type of a proof-of-stake or proof-of-work algorithm.
 Some of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain.
 A private blockchain is permissioned.[53] One cannot join it unless invited by the network administrators. Participant and validator access is restricted. To distinguish between open blockchains and other peer-to-peer decentralized database applications that are not open ad-hoc compute clusters, the terminology Distributed Ledger (DLT) is normally used for private blockchains.
 A hybrid blockchain has a combination of centralized and decentralized features.[72] The exact workings of the chain can vary based on which portions of centralization and decentralization are used.
 A sidechain is a designation for a blockchain ledger that runs in parallel to a primary blockchain.[73][74] Entries from the primary blockchain (where said entries typically represent digital assets) can be linked to and from the sidechain; this allows the sidechain to otherwise operate independently of the primary blockchain (e.g., by using an alternate means of record keeping, alternate consensus algorithm, etc.).[75][better source needed]
 A consortium blockchain is a type of blockchain that combines elements of both public and private blockchains. In a consortium blockchain, a group of organizations come together to create and operate the blockchain, rather than a single entity. The consortium members jointly manage the blockchain network and are responsible for validating transactions. Consortium blockchains are permissioned, meaning that only certain individuals or organizations are allowed to participate in the network. This allows for greater control over who can access the blockchain and helps to ensure that sensitive information is kept confidential.
 Consortium blockchains are commonly used in industries where multiple organizations need to collaborate on a common goal, such as supply chain management or financial services. One advantage of consortium blockchains is that they can be more efficient and scalable than public blockchains, as the number of nodes required to validate transactions is typically smaller. Additionally, consortium blockchains can provide greater security and reliability than private blockchains, as the consortium members work together to maintain the network. Some examples of consortium blockchains include Quorum and Hyperledger.[76]
 Blockchain technology can be integrated into multiple areas. The primary use of blockchains is as a distributed ledger for cryptocurrencies such as bitcoin; there were also a few other operational products that had matured from proof of concept by late 2016.[52] As of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their back office.[77]
 In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp has estimated that corporate investment into blockchain technology will reach $12.4 billion by 2022.[78] Furthermore, According to PricewaterhouseCoopers (PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicates a significant demand and interest in blockchain technology.[79]
 In 2019, the BBC World Service radio and podcast series Fifty Things That Made the Modern Economy identified blockchain as a technology that would have far-reaching consequences for economics and society. The economist and Financial Times journalist and broadcaster Tim Harford discussed why the underlying technology might have much wider applications and the challenges that needed to be overcome.[80] His first broadcast was on June 29, 2019.
 The number of blockchain wallets quadrupled to 40 million between 2016 and 2020.[81]
 A paper published in 2022 discussed the potential use of blockchain technology in sustainable management.[82]
 Most cryptocurrencies use blockchain technology to record transactions. For example, the bitcoin network and Ethereum network are both based on blockchain.
 The criminal enterprise Silk Road, which operated on Tor, utilized cryptocurrency for payments, some of which the US federal government has seized through research on the blockchain and forfeiture.[83]
 Governments have mixed policies on the legality of their citizens or banks owning cryptocurrencies. China implements blockchain technology in several industries including a national digital currency which launched in 2020.[84] To strengthen their respective currencies, Western governments including the European Union and the United States have initiated similar projects.[85]
 Blockchain-based smart contracts are contracts that can be partially or fully executed or enforced without human interaction.[86] One of the main objectives of a smart contract is automated escrow. A key feature of smart contracts is that they do not need a trusted third party (such as a trustee) to act as an intermediary between contracting entities — the blockchain network executes the contract on its own. This may reduce friction between entities when transferring value and could subsequently open the door to a higher level of transaction automation.[87] An IMF staff discussion from 2018 reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general. But ""no viable smart contract systems have yet emerged."" Due to the lack of widespread use, their legal status was unclear.[88][89]
 According to Reason, many banks have expressed interest in implementing distributed ledgers for use in banking and are cooperating with companies creating private blockchains,[90][91][92] and according to a September 2016 IBM study, this is occurring faster than expected.[93]
 Banks are interested in this technology not least because it has the potential to speed up back office settlement systems.[94] Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails.[95]
 Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.[96][97]
 Berenberg, a German bank, believes that blockchain is an ""overhyped technology"" that has had a large number of ""proofs of concept"", but still has major challenges, and very few success stories.[98]
 The blockchain has also given rise to initial coin offerings (ICOs) as well as a new category of digital asset called security token offerings (STOs), also sometimes referred to as digital security offerings (DSOs).[99] STO/DSOs may be conducted privately or on public, regulated stock exchange and are used to tokenize traditional assets such as company shares as well as more innovative ones like intellectual property, real estate,[100] art, or individual products. A number of companies are active in this space providing services for compliant tokenization, private STOs, and public STOs.
 Blockchain technology, such as cryptocurrencies and non-fungible tokens (NFTs), has been used in video games for monetization. Many live-service games offer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling, and has led to gray market issues such as skin gambling, and thus publishers typically have shied away from allowing players to earn real-world funds from games.[101] Blockchain games typically allow players to trade these in-game items for cryptocurrency, which can then be exchanged for money.[102]
 The first known game to use blockchain technologies was CryptoKitties, launched in November 2017, where the player would purchase NFTs with Ethereum cryptocurrency, each NFT consisting of a virtual pet that the player could breed with others to create offspring with combined traits as new NFTs.[103][102] The game made headlines in December 2017 when one virtual pet sold for more than US$100,000.[104] CryptoKitties also illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network in early 2018 with approximately 30% of all Ethereum transactions[clarification needed] being for the game.[105][106]
 By the early 2020s, there had not been a breakout success in video games using blockchain, as these games tend to focus on using blockchain for speculation instead of more traditional forms of gameplay, which offers limited appeal to most players. Such games also represent a high risk to investors as their revenues can be difficult to predict.[102] However, limited successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021.[107] Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future.[108]
 In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely used for personal computer gaming, claiming that this was an extension of their policy banning games that offered in-game items with real-world value. Valve's prior history with gambling, specifically skin gambling, was speculated to be a factor in the decision to ban blockchain games.[109] Journalists and players responded positively to Valve's decision as blockchain and NFT games have a reputation for scams and fraud among most PC gamers,[101][109] and Epic Games, which runs the Epic Games Store in competition to Steam, said that they would be open to accepted blockchain games in the wake of Valve's refusal.[110]
 There have been several different efforts to employ blockchains in supply chain management.
 There are several different efforts to offer domain name services via the blockchain. These domain names can be controlled by the use of a private key, which purports to allow for uncensorable websites. This would also bypass a registrar's ability to suppress domains used for fraud, abuse, or illegal content.[118]
 Namecoin is a cryptocurrency that supports the "".bit"" top-level domain (TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by ICANN, instead requiring an alternative DNS root.[118] As of 2015, .bit was used by 28 websites, out of 120,000 registered names.[119] Namecoin was dropped by OpenNIC in 2019, due to malware and potential other legal issues.[120] Other blockchain alternatives to ICANN include The Handshake Network,[119] EmerDNS, and Unstoppable Domains.[118]
 Specific TLDs include "".eth"", "".luxe"", and "".kred"", which are associated with the Ethereum blockchain through the Ethereum Name Service (ENS). The .kred TLD also acts as an alternative to conventional cryptocurrency wallet addresses as a convenience for transferring cryptocurrency.[121]
 
Blockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users[122] or musicians.[123] The Gartner 2019 CIO Survey reported 2% of higher education respondents had launched blockchain projects and another 18% were planning academic projects in the next 24 months.[124] In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution.[125] Imogen Heap's Mycelia service has also been proposed as a blockchain-based alternative ""that gives artists more control over how their songs and associated data circulate among fans and other musicians.""[126][127]
 New distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain.[128][129] The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers.[130] The use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services.[131]
 Other blockchain designs include Hyperledger, a collaborative effort from the Linux Foundation to support blockchain-based distributed ledgers, with projects under this initiative including Hyperledger Burrow (by Monax) and Hyperledger Fabric (spearheaded by IBM).[132][133][134] Another is Quorum, a permissioned private blockchain by JPMorgan Chase with private storage, used for contract applications.[135]
 Oracle introduced a blockchain table feature in its Oracle 21c database.[69][70]
 Blockchain is also being used in peer-to-peer energy trading.[136][137][138]
 Lightweight blockchains, or simplified blockchains, are more suitable for internet of things (IoT) applications than conventional blockchains.[139] One experiment suggested that a lightweight blockchain-based network could accommodate up to 1.34 million authentication processes every second, which could be sufficient for resource-constrained IoT networks.[140]
 Blockchain could be used in detecting counterfeits by associating unique identifiers to products, documents and shipments, and storing records associated with transactions that cannot be forged or altered.[141][142] It is however argued that blockchain technology needs to be supplemented with technologies that provide a strong binding between physical objects and blockchain systems,[143] as well as provisions for content creator verification ala KYC standards.[144] The EUIPO established an Anti-Counterfeiting Blockathon Forum, with the objective of ""defining, piloting and implementing"" an anti-counterfeiting infrastructure at the European level.[145][146] The Dutch Standardisation organisation NEN uses blockchain together with QR Codes to authenticate certificates.[147]
 Beijing and Shanghai are among the cities designated by China to trial blockchain applications as January 30, 2022.[148] In Chinese legal proceedings, blockchain technology was first accepted as a method for authenticating internet evidence by the Hangzhou Internet Court in 2019 and has since been accepted by other Chinese courts.[149]: 123–125 
 With the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner[150] stated that ""interoperability is the ability of two or more software components to cooperate despite differences in language, interface, and execution platform"". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences.
 There are already several blockchain interoperability solutions available.[151] They can be classified into three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors.
 Several individual IETF participants produced the draft of a blockchain interoperability architecture.[152]
 Some cryptocurrencies use blockchain mining — the peer-to-peer computer computations by which transactions are validated and verified. This requires a large amount of energy. In June 2018, the Bank for International Settlements criticized the use of public proof-of-work blockchains for their high energy consumption.[153][154][155]
 Early concern over the high energy consumption was a factor in later blockchains such as Cardano (2017), Solana (2020) and Polkadot (2020) adopting the less energy-intensive proof-of-stake model. Researchers have estimated that Bitcoin consumes 100,000 times as much energy as proof-of-stake networks.[156][157]
 In 2021, a study by Cambridge University determined that Bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh).[158] According to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in 24 days.[159]
 In February 2021, U.S. Treasury secretary Janet Yellen called Bitcoin ""an extremely inefficient way to conduct transactions"", saying ""the amount of energy consumed in processing those transactions is staggering"".[160] In March 2021, Bill Gates stated that ""Bitcoin uses more electricity per transaction than any other method known to mankind"", adding ""It's not a great climate thing.""[161]
 Nicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate.[162][163] The 31TWh-45TWh of electricity used for bitcoin in 2018 produced 17-23 million tonnes of CO2.[164][165] By 2022, the University of Cambridge and Digiconomist estimated that the two largest proof-of-work blockchains, Bitcoin and Ethereum, together used twice as much electricity in one year as the whole of Sweden, leading to the release of up to 120 million tonnes of CO2 each year.[166]
 Some cryptocurrency developers are considering moving from the proof-of-work model to the proof-of-stake model.[167]
 In October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.[168] Many universities have founded departments focusing on crypto and blockchain, including MIT, in 2017. In the same year, Edinburgh became ""one of the first big European universities to launch a blockchain course"", according to the Financial Times.[169]
 Motivations for adopting blockchain technology (an aspect of innovation adoptation) have been investigated by researchers. For example, Janssen, et al. provided a framework for analysis,[170] and Koens & Poll pointed out that adoption could be heavily driven by non-technical factors.[171] Based on behavioral models, Li[172] has discussed the differences between adoption at the individual level and organizational levels.
 Scholars in business and management have started studying the role of blockchains to support collaboration.[173][174] It has been argued that blockchains can foster both cooperation (i.e., prevention of opportunistic behavior) and coordination (i.e., communication and information sharing). Thanks to reliability, transparency, traceability of records, and information immutability, blockchains facilitate collaboration in a way that differs both from the traditional use of contracts and from relational norms. Contrary to contracts, blockchains do not directly rely on the legal system to enforce agreements.[175] In addition, contrary to the use of relational norms, blockchains do not require a trust or direct connections between collaborators.
 The need for internal audits to provide effective oversight of organizational efficiency will require a change in the way that information is accessed in new formats.[177] Blockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The Institute of Internal Auditors has identified the need for internal auditors to address this transformational technology. New methods are required to develop audit plans that identify threats and risks. The Internal Audit Foundation study, Blockchain and Internal Audit, assesses these factors.[178] The American Institute of Certified Public Accountants has outlined new roles for auditors as a result of blockchain.[179]
 In September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, Ledger, was announced. The inaugural issue was published in December 2016.[180] The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies.[181][182] The journal encourages authors to digitally sign a file hash of submitted papers, which are then timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address on the first page of their papers for non-repudiation purposes.[183]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['cryptocurrencies', 'Stuart Haber and W. Scott Stornetta', 'blockchain interoperability', 'increasing number of blockchain systems appearing', 'applications can be added to the network without the approval or trust of others'], 'answer_start': [], 'answer_end': []}"
"
 A cryptocurrency, crypto-currency, or crypto[a] is a digital currency designed to work as a medium of exchange through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it.[2]
 Individual coin ownership records are stored in a digital ledger, which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership.[3][4][5] Despite the term that has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice.[6][7][8] Some crypto schemes use validators to maintain the cryptocurrency. In a proof-of-stake model, owners put up their tokens as collateral. In return, they get authority over the token in proportion to the amount they stake. Generally, these token stakers get additional ownership in the token over time via network fees, newly minted tokens, or other such reward mechanisms.[9]
 Cryptocurrency does not exist in physical form (like paper money) and is typically not issued by a central authority. Cryptocurrencies typically use decentralized control as opposed to a central bank digital currency (CBDC). [10] When a cryptocurrency is minted, created prior to issuance, or issued by a single issuer, it is generally considered centralized. When implemented with decentralized control, each cryptocurrency works through distributed ledger technology, typically a blockchain, that serves as a public financial transaction database.[11]
 The first cryptocurrency was Bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion.[12]
 In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash.[13][14] Later, in 1995, he implemented it through Digicash,[15] an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before they could be sent to a recipient. This allowed the digital currency to be untraceable by a third party.
 In 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list[16] and later in 1997 in The American Law Review.[17]
 In 1998, Wei Dai described ""b-money,"" an anonymous, distributed electronic cash system. [18] Shortly thereafter, Nick Szabo described bit gold.[19] Like Bitcoin and other cryptocurrencies that would follow it, BitGold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system that required users to complete a proof of work function with solutions being cryptographically put together and published.
 In January 2009, Bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme.[20][21] In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin, was released which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake.[22]
 Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013–2014/15, 2017–2018 and 2021–2023.[23][24]
 On 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies, and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered.[25] Its final report was published in 2018,[26] and it issued a consultation on cryptoassets and stablecoins in January 2021.[27]
 In June 2021, El Salvador became the first country to accept Bitcoin as legal tender, after the Legislative Assembly had voted 62–22 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.[28]
 In August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as Bitcoin.[29]
 In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.[30]
 On 15 September 2022, the world's second largest cryptocurrency at that time, Ethereum transitioned its consensus mechanism from proof-of-work (PoW) to proof-of-stake (PoS) in an upgrade process known as ""the Merge"".  According to the Ethereum Founder, the upgrade can cut both Ethereum's energy use and carbon-dioxide emissions by 99.9%.[31]
 On 11 November 2022, FTX Trading Ltd., a cryptocurrency exchange, which also operated a crypto hedge fund, and had been valued at $18 billion,[32] filed for bankruptcy.[33] The financial impact of the collapse extended beyond the immediate FTX customer base, as reported,[34] while, at a Reuters conference, financial industry executives said that ""regulators must step in to protect crypto investors.""[35] Technology analyst Avivah Litan commented on the cryptocurrency ecosystem that ""everything...needs to improve dramatically in terms of user experience, controls, safety, customer service.""[36]
 According to Jan Lansky, a cryptocurrency is a system that meets six conditions:[37]
 In March 2018, the word cryptocurrency was added to the Merriam-Webster Dictionary.[38]
 After the early innovation of Bitcoin in 2008, and the early network effect gained by Bitcoin, tokens, cryptocurrencies, and other digital assets that were not Bitcoin became collectively known during the 2010s as alternative cryptocurrencies,[39][40][41] or ""altcoins.""[42] 
Sometimes the term ""alt coins"" was used,[43][44] or disparagingly, ""shitcoins"".[45] Paul Vigna of The Wall Street Journal described altcoins in 2020 as ""alternative versions of Bitcoin""[46] given its role as the model protocol for cryptocurrency designers.  A Polytechnic University of Catalonia thesis in 2021 used a broader description including not only alternative versions of Bitcoin, but every cryptocurrency other than bitcoin. ""As of early 2020, there were more than 5,000 cryptocurrencies. Altcoin is the combination of two words ""alt"" and ""coin"" and includes all alternatives to Bitcoin.""[42]: 14 
  Altcoins often have underlying differences when compared to Bitcoin. For example, Litecoin aims to process a block every 2.5 minutes, rather than Bitcoin's 10 minutes, which allows Litecoin to confirm transactions faster than Bitcoin.[47] Another example is Ethereum, which has smart contract functionality that allows decentralized applications to be run on its blockchain.[48] Ethereum was the most used blockchain in 2020, according to Bloomberg News.[49] In 2016, it had the largest ""following"" of any altcoin, according to the New York Times.[50]
 Significant market price rallies across multiple altcoin markets are often referred to as an ""altseason"".[51][52]
 Stablecoins are cryptocurrencies designed to maintain a stable level of purchasing power.[53] Notably, these designs are not foolproof, as a number of stablecoins have crashed or lost their peg. For example, on 11 May 2022, Terra's stablecoin UST fell from $1 to 26 cents.[54][55] The subsequent failure of Terraform Labs resulted in the loss of nearly $40B invested in the Terra and Luna coins.[56] In September 2022, South Korean prosecutors requested the issuance of an Interpol Red Notice against the company's founder, Do Kwon.[57] In Hong Kong, the expected regulatory framework for stablecoins in 2023/24 is being shaped and includes a few considerations.[58]
 Cryptocurrency is produced by an entire cryptocurrency system collectively, at a rate which is defined when the system is created and which is publicly stated. In centralized banking and economic systems such as the US Federal Reserve System, corporate boards or governments control the supply of currency.[citation needed] In the case of cryptocurrency, companies or governments cannot produce new units, and have not so far provided backing for other firms, banks or corporate entities which hold asset value measured in it. The underlying technical system upon which cryptocurrencies are based was created by Satoshi Nakamoto.[59]
 Within a proof-of-work system such as Bitcoin, the safety, integrity and balance of ledgers is maintained by a community of mutually distrustful parties referred to as miners. Miners use their computers to help validate and timestamp transactions, adding them to the ledger in accordance with a particular timestamping scheme.[20] In a proof-of-stake blockchain, transactions are validated by holders of the associated cryptocurrency, sometimes grouped together in stake pools.
 Most cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation.[60] Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.[3]
 The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography.[59][61] Each block typically contains a hash pointer as a link to a previous block,[61] a timestamp and transaction data.[62] By design, blockchains are inherently resistant to modification of the data. It is ""an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way"".[63] For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.
 Blockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain.[64]
 A node is a computer that connects to a cryptocurrency network. The node supports the cryptocurrency's network through either relaying transactions, validation, or hosting a copy of the blockchain. In terms of relaying transactions, each network computer (node) has a copy of the blockchain of the cryptocurrency it supports. When a transaction is made, the node creating the transaction broadcasts details of the transaction using encryption to other nodes throughout the node network so that the transaction (and every other transaction) is known.
 Node owners are either volunteers, those hosted by the organization or body responsible for developing the cryptocurrency blockchain network technology, or those who are enticed to host a node to receive rewards from hosting the node network.[65]
 Cryptocurrencies use various timestamping schemes to ""prove"" the validity of transactions added to the blockchain ledger without the need for a trusted third party.
 The first timestamping scheme invented was the proof-of-work scheme. The most widely used proof-of-work schemes are based on SHA-256 and scrypt.[22]
 Some other hashing algorithms that are used for proof-of-work include CryptoNote, Blake, SHA-3, and X11.
 Another method is called the proof-of-stake scheme. Proof-of-stake is a method of securing a cryptocurrency network and achieving distributed consensus through requesting users to show ownership of a certain amount of currency. It is different from proof-of-work systems that run difficult hashing algorithms to validate electronic transactions. The scheme is largely dependent on the coin, and there is currently no standard form of it. Some cryptocurrencies use a combined proof-of-work and proof-of-stake scheme.[22]
 On a blockchain, mining is the validation of transactions. For this effort, successful miners obtain new cryptocurrency as a reward. The reward decreases transaction fees by creating a complementary incentive to contribute to the processing power of the network. The rate of generating hashes, which validate any transaction, has been increased by the use of specialized machines such as FPGAs and ASICs running complex hashing algorithms like SHA-256 and scrypt.[66] This arms race for cheaper-yet-efficient machines has existed since Bitcoin was introduced in 2009.[66] Mining is measured by hash rate typically in TH/s.[67]
 With more people entering the world of virtual currency, generating hashes for validation has become more complex over time, forcing miners to invest increasingly large sums of money to improve computing performance. Consequently, the reward for finding a hash has diminished and often does not justify the investment in equipment and cooling facilities (to mitigate the heat the equipment produces), and the electricity required to run them.[68] Popular regions for mining include those with inexpensive electricity, a cold climate, and jurisdictions with clear and conducive regulations. By July 2019, Bitcoin's electricity consumption was estimated to be approximately 7 gigawatts, around 0.2% of the global total, or equivalent to the energy consumed nationally by Switzerland.[69]
 Some miners pool resources, sharing their processing power over a network to split the reward equally, according to the amount of work they contributed to the probability of finding a block. A ""share"" is awarded to members of the mining pool who present a valid partial proof-of-work.
 As of February 2018[update], the Chinese Government has halted trading of virtual currency, banned initial coin offerings and shut down mining. Many Chinese miners have since relocated to Canada[70] and Texas.[71] One company is operating data centers for mining operations at Canadian oil and gas field sites, due to low gas prices.[72] In June 2018, Hydro Quebec proposed to the provincial government to allocate 500 megawatts of power to crypto companies for mining.[73] According to a February 2018 report from Fortune, Iceland has become a haven for cryptocurrency miners in part because of its cheap electricity.[74]
 In March 2018, the city of Plattsburgh, New York put an 18-month moratorium on all cryptocurrency mining in an effort to preserve natural resources and the ""character and direction"" of the city.[75] In 2021, Kazakhstan became the second-biggest crypto-currency mining country, producing 18.1% of the global exahash rate. The country built a compound containing 50,000 computers near Ekibastuz.[76]
 An increase in cryptocurrency mining increased the demand for graphics cards (GPU) in 2017.[77] The computing power of GPUs makes them well-suited to generating hashes. Popular favorites of cryptocurrency miners such as Nvidia's GTX 1060 and GTX 1070 graphics cards, as well as AMD's RX 570 and RX 580 GPUs, doubled or tripled in price – or were out of stock.[78] A GTX 1070 Ti which was released at a price of $450 sold for as much as $1,100. Another popular card, the GTX 1060 (6 GB model) was released at an MSRP of $250, and sold for almost $500. RX 570 and RX 580 cards from AMD were out of stock for almost a year. Miners regularly buy up the entire stock of new GPU's as soon as they are available.[79]
 Nvidia has asked retailers to do what they can when it comes to selling GPUs to gamers instead of miners. Boris Böhles, PR manager for Nvidia in the German region, said: ""Gamers come first for Nvidia.""[80]
 Numerous companies developed dedicated crypto-mining accelerator chips, capable of price-performance far higher than that of CPU or GPU mining. At one point Intel marketed its own brand of crypto accelerator chip, named Blockscale.[81]
 A cryptocurrency wallet is a means of storing the public and private ""keys"" (address) or seed which can be used to receive or spend the cryptocurrency.[82] With the private key, it is possible to write in the public ledger, effectively spending the associated cryptocurrency. With the public key, it is possible for others to send currency to the wallet.
 There exist multiple methods of storing keys or seed in a wallet. These methods range from using paper wallets (which are public, private or seed keys written on paper), to using hardware wallets (which are hardware to store your wallet information), to a digital wallet (which is a computer with a software hosting your wallet information), to hosting your wallet using an exchange where cryptocurrency is traded, or by storing your wallet information on a digital medium such as plaintext.[83]
 Bitcoin is pseudonymous, rather than anonymous; the cryptocurrency in a wallet is not tied to a person, but rather to one or more specific keys (or ""addresses"").[84] Thereby, Bitcoin owners are not immediately identifiable, but all transactions are publicly available in the blockchain.[85] Still, cryptocurrency exchanges are often required by law to collect the personal information of their users.[86]
 Some cryptocurrencies, such as Monero, Zerocoin, Zerocash, and CryptoNote, implement additional measures to increase privacy, such as by using zero-knowledge proofs.[87][88]
 A recent 2020 study presented different attacks on privacy in cryptocurrencies. The attacks demonstrated how the anonymity techniques are not sufficient safeguards. In order to improve privacy, researchers suggested several different ideas including new cryptographic schemes and mechanisms for hiding the IP address of the source.[89]
 Cryptocurrencies are used primarily outside banking and governmental institutions and are exchanged over the Internet.
 Proof-of-work cryptocurrencies, such as Bitcoin, offer block rewards incentives for miners. There has been an implicit belief that whether miners are paid by block rewards or transaction fees does not affect the security of the blockchain, but a study suggests that this may not be the case under certain circumstances.[90]
 The rewards paid to miners increase the supply of the cryptocurrency. By making sure that verifying transactions is a costly business, the integrity of the network can be preserved as long as benevolent nodes control a majority of computing power. The verification algorithm requires a lot of processing power, and thus electricity in order to make verification costly enough to accurately validate public blockchain. Not only do miners have to factor in the costs associated with expensive equipment necessary to stand a chance of solving a hash problem, they must further consider the significant amount of electrical power in search of the solution. Generally, the block rewards outweigh electricity and equipment costs, but this may not always be the case.[91]
 The current value, not the long-term value, of the cryptocurrency supports the reward scheme to incentivize miners to engage in costly mining activities.[92] In 2018, Bitcoin's design caused a 1.4% welfare loss compared to an efficient cash system, while a cash system with 2% money growth has a minor 0.003% welfare cost. The main source for this inefficiency is the large mining cost, which is estimated to be US$360 million per year. This translates into users being willing to accept a cash system with an inflation rate of 230% before being better off using Bitcoin as a means of payment. However, the efficiency of the Bitcoin system can be significantly improved by optimizing the rate of coin creation and minimizing transaction fees. Another potential improvement is to eliminate inefficient mining activities by changing the consensus protocol altogether.[93]
 Transaction fees for cryptocurrency depend mainly on the supply of network capacity at the time, versus the demand from the currency holder for a faster transaction.[citation needed] The currency holder can choose a specific transaction fee, while network entities process transactions in order of highest offered fee to lowest.[citation needed] Cryptocurrency exchanges can simplify the process for currency holders by offering priority alternatives and thereby determine which fee will likely cause the transaction to be processed in the requested time.[citation needed]
 For Ethereum, transaction fees differ by computational complexity, bandwidth use, and storage needs, while Bitcoin transaction fees differ by transaction size and whether the transaction uses SegWit. In February 2023, the median transaction fee for Ether corresponded to $2.2845,[94] while for Bitcoin it corresponded to $0.659.[95]
 Some cryptocurrencies have no transaction fees, and instead rely on client-side proof-of-work as the transaction prioritization and anti-spam mechanism.[96][97][98]
 Cryptocurrency exchanges allow customers to trade cryptocurrencies[99] for other assets, such as conventional fiat money, or to trade between different digital currencies.
 Crypto marketplaces do not guarantee that an investor is completing a purchase or trade at the optimal price. As a result, as of 2020 it was possible to arbitrage to find the difference in price across several markets.[100]
 Atomic swaps are a mechanism where one cryptocurrency can be exchanged directly for another cryptocurrency, without the need for a trusted third party such as an exchange.[101]
 Jordan Kelley, founder of Robocoin, launched the first Bitcoin ATM in the United States on 20 February 2014. The kiosk installed in Austin, Texas, is similar to bank ATMs but has scanners to read government-issued identification such as a driver's license or a passport to confirm users' identities.[102]
 An initial coin offering (ICO) is a controversial means of raising funds for a new cryptocurrency venture. An ICO may be used by startups with the intention of avoiding regulation. However, securities regulators in many jurisdictions, including in the U.S., and Canada, have indicated that if a coin or token is an ""investment contract"" (e.g., under the Howey test, i.e., an investment of money with a reasonable expectation of profit based significantly on the entrepreneurial or managerial efforts of others), it is a security and is subject to securities regulation. In an ICO campaign, a percentage of the cryptocurrency (usually in the form of ""tokens"") is sold to early backers of the project in exchange for legal tender or other cryptocurrencies, often Bitcoin or Ether.[103][104][105]
 According to PricewaterhouseCoopers, four of the 10 biggest proposed initial coin offerings have used Switzerland as a base, where they are frequently registered as non-profit foundations. The Swiss regulatory agency FINMA stated that it would take a ""balanced approach"" to ICO projects and would allow ""legitimate innovators to navigate the regulatory landscape and so launch their projects in a way consistent with national laws protecting investors and the integrity of the financial system."" In response to numerous requests by industry representatives, a legislative ICO working group began to issue legal guidelines in 2018, which are intended to remove uncertainty from cryptocurrency offerings and to establish sustainable business practices.[106]
 The market capitalization of a cryptocurrency is calculated by multiplying the price by the number of coins in circulation. The total cryptocurrency market cap has historically been dominated by Bitcoin accounting for at least 50% of the market cap value where altcoins have increased and decreased in market cap value in relation to Bitcoin. Bitcoin's value is largely determined by speculation among other technological limiting factors known as blockchain rewards coded into the architecture technology of Bitcoin itself. The cryptocurrency market cap follows a trend known as the ""halving"", which is when the block rewards received from Bitcoin are halved due to technological mandated limited factors instilled into Bitcoin which in turn limits the supply of Bitcoin. As the date reaches near of a halving (twice thus far historically) the cryptocurrency market cap increases, followed by a downtrend.[107]
 By June 2021, cryptocurrency had begun to be offered by some wealth managers in the US for 401(k)s.[108][109][110]
 Cryptocurrency prices are much more volatile than established financial assets such as stocks. For example, over one week in May 2022, Bitcoin lost 20% of its value and Ethereum lost 26%, while Solana and Cardano lost 41% and 35% respectively. The falls were attributed to warnings about inflation. By comparison, in the same week, the Nasdaq tech stock index fell 7.6 per cent and the FTSE 100 was 3.6 per cent down.[111]
 In the longer term, of the 10 leading cryptocurrencies identified by the total value of coins in circulation in January 2018, only four (Bitcoin, Ethereum, Cardano and Ripple (XRP)) were still in that position in early 2022.[112] The total value of all cryptocurrencies was  $2 trillion at the end of 2021, but had halved nine months later.[113][114] The Wall Street Journal has commented that the crypto sector has become ""intertwined"" with the rest of the capital markets and ""sensitive to the same forces that drive tech stocks and other risk assets"", such as inflation forecasts.[115]
 There are also centralized databases, outside of blockchains, that store crypto market data. Compared to the blockchain, databases perform fast as there is no verification process. Four of the most popular cryptocurrency market databases are CoinMarketCap, CoinGecko, BraveNewCoin, and Cryptocompare.[116]
 According to Alan Feuer of The New York Times, libertarians and anarcho-capitalists were attracted to the philosophical idea behind Bitcoin. Early Bitcoin supporter Roger Ver said: ""At first, almost everyone who got involved did so for philosophical reasons. We saw Bitcoin as a great idea, as a way to separate money from the state.""[117] Economist Paul Krugman argues that cryptocurrencies like Bitcoin are ""something of a cult"" based in ""paranoid fantasies"" of government power.[118]
 David Golumbia says that the ideas influencing Bitcoin advocates emerge from right-wing extremist movements such as the Liberty Lobby and the John Birch Society and their anti-Central Bank rhetoric, or, more recently, Ron Paul and Tea Party-style libertarianism.[119] Steve Bannon, who owns a ""good stake"" in Bitcoin, sees cryptocurrency as a form of disruptive populism, taking control back from central authorities.[120]
 Bitcoin's founder, Satoshi Nakamoto, has supported the idea that cryptocurrencies go well with libertarianism. ""It's very attractive to the libertarian viewpoint if we can explain it properly,"" Nakamoto said in 2008.[121]
 According to the European Central Bank, the decentralization of money offered by Bitcoin has its theoretical roots in the Austrian school of economics, especially with Friedrich von Hayek in his book Denationalisation of Money: The Argument Refined,[122] in which Hayek advocates a complete free market in the production, distribution and management of money to end the monopoly of central banks.[123]
 The rise in the popularity of cryptocurrencies and their adoption by financial institutions has led some governments to assess whether regulation is needed to protect users. The Financial Action Task Force (FATF) has defined cryptocurrency-related services as ""virtual asset service providers"" (VASPs) and recommended that they be regulated with the same money laundering (AML) and know your customer (KYC) requirements as financial institutions.[124]
 In May 2020, the Joint Working Group on interVASP Messaging Standards published ""IVMS 101"", a universal common language for communication of required originator and beneficiary information between VASPs. The FATF and financial regulators were informed as the data model was developed.[125]
 In June 2020, FATF updated its guidance to include the ""Travel Rule"" for cryptocurrencies, a measure which mandates that VASPs obtain, hold, and exchange information about the originators and beneficiaries of virtual asset transfers.[126] Subsequent standardized protocol specifications recommended using JSON for relaying data between VASPs and identity services. As of December 2020, the IVMS 101 data model has yet to be finalized and ratified by the three global standard setting bodies that created it.[127]
 The European Commission published a digital finance strategy in September 2020. This included a draft regulation on Markets in Crypto-Assets (MiCA), which aimed to provide a comprehensive regulatory framework for digital assets in the EU.[128][129]
 On 10 June 2021, the Basel Committee on Banking Supervision proposed that banks that held cryptocurrency assets must set aside capital to cover all potential losses. For instance, if a bank were to hold Bitcoin worth $2 billion, it would be required to set aside enough capital to cover the entire $2 billion. This is a more extreme standard than banks are usually held to when it comes to other assets. However, this is a proposal and not a regulation.
 The IMF is seeking a coordinated, consistent and comprehensive approach to supervising cryptocurrencies. Tobias Adrian, the IMF's financial counsellor and head of its monetary and capital markets department said in a January 2022 interview that ""Agreeing global regulations is never quick. But if we start now, we can achieve the goal of maintaining financial stability while also enjoying the benefits which the underlying technological innovations bring,""[130]
 In May 2024, 15 years after the advent of the first blockchain, Bitcoin, the US Congress advanced a bill to the full House of Representatives to provide regulatory clarity for digital assets. The Financial Innovation and Technology for the 21st Century Act, which defines responsibilities between various US agencies, notably between the Commodity Futures Trading Commission (CFTC) for decentralized blockchains and the Securities and Exchange Commission (SEC) for blockchains that are functional but not decentralized.  Stablecoins are excluded from both CFTC and SEC regulation in this bill, ""except for fraud and certain activities by registered firms.""[131]
 In September 2017, China banned ICOs to cause abnormal return from cryptocurrency decreasing during announcement window. The liquidity changes by banning ICOs in China was temporarily negative while the liquidity effect became positive after news.[132]
 On 18 May 2021, China banned financial institutions and payment companies from being able to provide cryptocurrency transaction related services.[133] This led to a sharp fall in the price of the biggest proof of work cryptocurrencies. For instance, Bitcoin fell 31%, Ethereum fell 44%, Binance Coin fell 32% and Dogecoin fell 30%.[134] Proof of work mining was the next focus, with regulators in popular mining regions citing the use of electricity generated from highly polluting sources such as coal to create Bitcoin and Ethereum.[135]
 In September 2021, the Chinese government declared all cryptocurrency transactions of any kind illegal, completing its crackdown on cryptocurrency.[30]
 In April 2024, TVNZ's 1 News reported that the Cook Islands government was proposing legislation that would allow ""recovery agents"" to use various means including hacking to investigate or find cryptocurrency that may have been used for illegal means or is the ""proceeds of crime."" The Tainted Cryptocurrency Recovery Bill was drafted by two lawyers hired by US-based debt collection company Drumcliffe. The proposed legislation was criticised by Cook Islands Crown Law's deputy solicitor general David Greig, who described it as ""flawed"" and said that some provisions were ""clearly unconstitutional"". The Cook Islands Financial Services Development Authority described Drumcliffe's involvement as a conflict of interest.[136]
 Similar criticism was echoed by Auckland University of Technology cryptocurrency specialist and senior lecturer Jeff Nijsse and University of Otago political scientist Professor Robert Patman, who described it as government overreach and described it as inconsistent with international law. Since the Cook Islands is an associated state that is part of the Realm of New Zealand, Patman said that the law would have ""implications for New Zealand's governance arrangements."" A spokesperson for New Zealand Foreign Minister Winston Peters confirmed that New Zealand officials were discussing the legislation with their Cook Islands counterparts.  Cook Islands Prime Minister Mark Brown defended the legislation as part of the territory's fight against international cybercrime.[136]
 On 9 June 2021, El Salvador announced that it will adopt Bitcoin as legal tender, becoming the first country to do so.[137]
 At present, India neither prohibits nor allows investment in the cryptocurrency market. In 2020, the Supreme Court of India had lifted the ban on cryptocurrency, which was imposed by the Reserve Bank of India.[138][139][140][141] Since then, an investment in cryptocurrency is considered legitimate, though there is still ambiguity about the issues regarding the extent and payment of tax on the income accrued thereupon and also its regulatory regime. But it is being contemplated that the Indian Parliament will soon pass a specific law to either ban or regulate the cryptocurrency market in India.[142] Expressing his public policy opinion on the Indian cryptocurrency market to a well-known online publication, a leading public policy lawyer and Vice President of SAARCLAW (South Asian Association for Regional Co-operation in Law) Hemant Batra has said that the ""cryptocurrency market has now become very big with involvement of billions of dollars in the market hence, it is now unattainable and irreconcilable for the government to completely ban all sorts of cryptocurrency and its trading and investment"".[143] He mooted regulating the cryptocurrency market rather than completely banning it. He favoured following IMF and FATF guidelines in this regard.
 South Africa, which has seen a large number of scams related to cryptocurrency, is said to be putting a regulatory timeline in place that will produce a regulatory framework.[144] The largest scam occurred in April 2021, where the two founders of an African-based cryptocurrency exchange called Africrypt, Raees Cajee and Ameer Cajee, disappeared with $3.8 billion worth of Bitcoin.[145] Additionally, Mirror Trading International disappeared with $170 million worth of cryptocurrency in January 2021.[145]
 In March 2021, South Korea implemented new legislation to strengthen their oversight of digital assets. This legislation requires all digital asset managers, providers and exchanges to be registered with the Korea Financial Intelligence Unit in order to operate in South Korea.[146] Registering with this unit requires that all exchanges are certified by the Information Security Management System and that they ensure all customers have real name bank accounts. It also requires that the CEO and board members of the exchanges have not been convicted of any crimes and that the exchange holds sufficient levels of deposit insurance to cover losses arising from hacks.[146]
 Switzerland was one of the first countries to implement the FATF's Travel Rule. FINMA, the Swiss regulator, issued its own guidance to VASPs in 2019. The guidance followed the FATF's Recommendation 16, however with stricter requirements. According to FINMA's[147] requirements, VASPs need to verify the identity of the beneficiary of the transfer.
 On 30 April 2021, the Central Bank of the Republic of Turkey banned the use of cryptocurrencies and cryptoassets for making purchases on the grounds that the use of cryptocurrencies for such payments poses significant transaction risks.[148]
 In the United Kingdom, as of 10 January 2021, all cryptocurrency firms, such as exchanges, advisors and professionals that have either a presence, market product or provide services within the UK market must register with the Financial Conduct Authority. Additionally, on 27 June 2021, the financial watchdog demanded that Binance, the world's largest cryptocurrency exchange,[149] cease all regulated activities in the UK.[150]
 In 2021, 17 states passed laws and resolutions concerning cryptocurrency regulation.[151] The U.S. Securities and Exchange Commission (SEC) is considering what steps to take. On 8 July 2021, Senator Elizabeth Warren, part of the Senate Banking Committee, wrote to the chairman of the SEC and demanded answers on cryptocurrency regulation due to the increase in cryptocurrency exchange use and the danger this posed to consumers. On 5 August 2021, SEC Chairman Gary Gensler responded to Senator Elizabeth Warren's letter regarding cryptocurrency regulation and called for legislation focused on ""crypto trading, lending and DeFi platforms,"" because of how vulnerable the investors could be when they traded on crypto trading platforms without a broker. He also argued that many tokens in the crypto market may be unregistered securities without required disclosures or market oversight. Additionally, Gensler did not hold back in his criticism of stablecoins. These tokens, which are pegged to the value of fiat currencies, may allow individuals to bypass important public policy goals related to traditional banking and financial systems, such as anti-money laundering, tax compliance, and sanctions.[152]
 On 19 October 2021, the first bitcoin-linked exchange-traded fund (ETF) from ProShares started trading on the NYSE under the ticker ""BITO."" ProShares CEO Michael L. Sapir said the ETF would expose Bitcoin to a wider range of investors without the hassle of setting up accounts with cryptocurrency providers. Ian Balina, the CEO of Token Metrics, stated that the approval of the ""BITO"" ETF by the SEC was a significant endorsement for the crypto industry because many regulators globally were not in favor of crypto as well as the hesitance to accept crypto from retail investors. This event would eventually open more opportunities for new capital and new people in this space.[153]
 The United States Department of the Treasury, on 20 May 2021, announced that it would require any transfer worth $10,000 or more to be reported to the Internal Revenue Service since cryptocurrency already posed a problem where illegal activity like tax evasion was facilitated broadly. This release from the IRS was a part of efforts to promote better compliance and consider more severe penalties for tax evaders.[154]
 On 17 February 2022, the Justice department named Eun Young Choi as the first director of a National Cryptocurrency Enforcement Team to aid in identification of and dealing with misuse of cryptocurrencies and other digital assets.[155]
 The Biden administration faced a dilemma as it tried to develop regulations for the cryptocurrency industry. On one hand, officials were hesitant to restrict the growing and profitable industry. On the other hand, they were committed to preventing illegal cryptocurrency transactions. To reconcile these conflicting goals, on 9 March 2022, President Biden issued an executive order.[156] Followed by the executive order, on 16 September 2022, the Comprehensive Framework for Responsible Development of Digital Assets document was released [157] to support development of cryptocurrencies and restrict their illegal use. The executive order included all digital assets, but cryptocurrencies posed both the greatest security risks and potential economic benefits. Though this might not address all of the challenges in crypto industry, it was a significant milestone in the U.S. cryptocurrency regulation history.[158]
 In February 2023, the Securities and Exchange Commission (SEC) ruled that cryptocurrency exchange Kraken's estimated $42 billion in staked assets globally operated as an illegal securities seller. The company agreed to a $30 million settlement with the SEC and to cease selling its staking service in the U.S. The case would impact other major crypto exchanges operating staking programs.[159]
 On 23 March 2023, the U.S. Securities and Exchange Commission (SEC) issued an alert to investors stating that firms offering crypto asset securities may not be complying with U.S. laws. The SEC stated that unregistered offerings of crypto asset securities may not include important information.[160]
 The legal status of cryptocurrencies varies substantially from country to country and is still undefined or changing in many of them. At least one study has shown that broad generalizations about the use of Bitcoin in illicit finance are significantly overstated and that blockchain analysis is an effective crime fighting and intelligence gathering tool.[161] While some countries have explicitly allowed their use and trade,[162] others have banned or restricted it. According to the Library of Congress in 2021,
an ""absolute ban"" on trading or using cryptocurrencies applies in 9 countries:
Algeria, Bangladesh, Bolivia, China, Egypt, Iraq, Morocco, Nepal, and the United Arab Emirates. An ""implicit ban"" applies in another 39 countries or regions, which include: Bahrain, Benin, Burkina Faso, Burundi, Cameroon, Chad, Cote d’Ivoire, the Dominican Republic, Ecuador, Gabon, Georgia, Guyana, Indonesia, Iran, Jordan, Kazakhstan, Kuwait, Lebanon, Lesotho, Macau, Maldives, Mali, Moldova, Namibia, Niger, Nigeria, Oman, Pakistan, Palau, Republic of Congo, Saudi Arabia, Sengeal, Tajikistan, Tanzania, Togo, Turkey, Turkmenistan, Qatar and Vietnam.[163] In the United States and Canada, state and provincial securities regulators, coordinated through the North American Securities Administrators Association, are investigating ""Bitcoin scams"" and ICOs in 40 jurisdictions.[164]
 Various government agencies, departments, and courts have classified Bitcoin differently. China Central Bank banned the handling of Bitcoins by financial institutions in China in early 2014.
 In Russia, though owning cryptocurrency is legal, its residents are only allowed to purchase goods from other residents using the Russian ruble while nonresidents are allowed to use foreign currency.[165] Regulations and bans that apply to Bitcoin probably extend to similar cryptocurrency systems.[166]
 In August 2018, the Bank of Thailand announced its plans to create its own cryptocurrency, the Central Bank Digital Currency (CBDC).[167]
 Cryptocurrency advertisements have been banned on the following platforms:
 On 25 March 2014, the United States Internal Revenue Service (IRS) ruled that Bitcoin will be treated as property for tax purposes. Therefore, virtual currencies are considered commodities subject to capital gains tax.[175]
 As the popularity and demand for online currencies has increased since the inception of Bitcoin in 2009,[176] so have concerns that such an unregulated person to person global economy that cryptocurrencies offer may become a threat to society. Concerns abound that altcoins may become tools for anonymous web criminals.[177]
 Cryptocurrency networks display a lack of regulation that has been criticized as enabling criminals who seek to evade taxes and launder money. Money laundering issues are also present in regular bank transfers, however with bank-to-bank wire transfers for instance, the account holder must at least provide a proven identity.
 Transactions that occur through the use and exchange of these altcoins are independent from formal banking systems, and therefore can make tax evasion simpler for individuals. Since charting taxable income is based upon what a recipient reports to the revenue service, it becomes extremely difficult to account for transactions made using existing cryptocurrencies, a mode of exchange that is complex and difficult to track.[177]
 Systems of anonymity that most cryptocurrencies offer can also serve as a simpler means to launder money. Rather than laundering money through an intricate net of financial actors and offshore bank accounts, laundering money through altcoins can be achieved through anonymous transactions.[177]
 Cryptocurrency makes legal enforcement against extremist groups more complicated, which consequently strengthens them.[178] White supremacist Richard Spencer went as far as to declare Bitcoin the ""currency of the alt-right"".[179]
 In February 2014, the world's largest Bitcoin exchange, Mt. Gox, declared bankruptcy. Likely due to theft, the company claimed that it had lost nearly 750,000 Bitcoins belonging to their clients. This added up to approximately 7% of all Bitcoins in existence, worth a total of $473 million. Mt. Gox blamed hackers, who had exploited the transaction malleability problems in the network. The price of a Bitcoin fell from a high of about $1,160 in December to under $400 in February.[180]
 On 21 November 2017, Tether announced that it had been hacked, losing $31 million in USDT from its core treasury wallet.[181]
 On 7 December 2017, Slovenian cryptocurrency exchange Nicehash reported that hackers had stolen over $70M using a hijacked company computer.[182]
 On 19 December 2017, Yapian, the owner of South Korean exchange Youbit, filed for bankruptcy after suffering two hacks that year.[183][184] Customers were still granted access to 75% of their assets.
 In May 2018, Bitcoin Gold had its transactions hijacked and abused by unknown hackers.[185] Exchanges lost an estimated $18m and Bitcoin Gold was delisted from Bittrex after it refused to pay its share of the damages.
 On 13 September 2018, Homero Josh Garza was sentenced to 21 months of imprisonment, followed by three years of supervised release.[186] Garza had founded the cryptocurrency startups GAW Miners and ZenMiner in 2014, acknowledged in a plea agreement that the companies were part of a pyramid scheme, and pleaded guilty to wire fraud in 2015. The U.S. Securities and Exchange Commission separately brought a civil enforcement action against Garza, who was eventually ordered to pay a judgment of $9.1 million plus $700,000 in interest. The SEC's complaint stated that Garza, through his companies, had fraudulently sold ""investment contracts representing shares in the profits they claimed would be generated"" from mining.[187]
 In January 2018, Japanese exchange Coincheck reported that hackers had stolen $530M worth of cryptocurrencies.[188]
 In June 2018, South Korean exchange Coinrail was hacked, losing over $37M worth of cryptos.[189] The hack worsened an already ongoing cryptocurrency selloff by an additional $42 billion.[190]
 On 9 July 2018, the exchange Bancor, whose code and fundraising had been subjects of controversy, had $23.5 million in cryptocurrency stolen.[191]
 A 2020 EU report found that users had lost crypto-assets worth hundreds of millions of US dollars in security breaches at exchanges and storage providers. Between 2011 and 2019, reported breaches ranged from four to twelve a year. In 2019, more than a billion dollars worth of cryptoassets was reported stolen. Stolen assets ""typically find their way to illegal markets and are used to fund further criminal activity"".[192]
 According to a 2020 report produced by the United States Attorney General's Cyber-Digital Task Force, the following three categories make up the majority of illicit cryptocurrency uses: ""(1) financial transactions associated with the commission of crimes; (2) money laundering and the shielding of legitimate activity from tax, reporting, or other legal requirements; or (3) crimes, such as theft, directly implicating the cryptocurrency marketplace itself."" The report concludes that ""for cryptocurrency to realize its truly transformative potential, it is imperative that these risks be addressed"" and that ""the government has legal and regulatory tools available at its disposal to confront the threats posed by cryptocurrency's illicit uses"".[193][194]
 According to the UK 2020 national risk assessment—a comprehensive assessment of money laundering and terrorist financing risk in the UK—the risk of using cryptoassets such as Bitcoin for money laundering and terrorism financing is assessed as ""medium"" (from ""low"" in the previous 2017 report).[195] Legal scholars suggested that the money laundering opportunities may be more perceived than real.[196] Blockchain analysis company Chainalysis concluded that illicit activities like cybercrime, money laundering and terrorism financing made up only 0.15% of all crypto transactions conducted in 2021, representing a total of $14 billion.[197][198][199]
 In December 2021, Monkey Kingdom, a NFT project based in Hong Kong, lost US$1.3 million worth of cryptocurrencies via a phishing link used by the hacker.[200]
 On November 2, 2023, Sam Bankman-Fried was pronounced guilty on seven counts of fraud related to FTX.[201] Federal criminal court sentencing experts speculated on the potential amount of prison time likely to be meted out.[202][203][204] On March 28, 2024, the court sentenced Bankman-Fried to 25 years in prison.[205]
 According to blockchain data company Chainalysis, criminals laundered US$8,600,000,000 worth of cryptocurrency in 2021, up by 30% from the previous year.[206] The data suggests that rather than managing numerous illicit havens, cybercriminals make use of a small group of purpose built centralized exchanges for sending and receiving illicit cryptocurrency. In 2021, those exchanges received 47% of funds sent by crime linked addresses.[207] Almost $2.2bn worth of cryptocurrencies was embezzled from DeFi protocols in 2021, which represents 72% of all cryptocurrency theft in 2021.
 According to Bloomberg and the New York Times, Federation Tower, a two skyscraper complex in the heart of Moscow City, is home to many cryptocurrency businesses under suspicion of facilitating extensive money laundering, including accepting illicit cryptocurrency funds obtained through scams, darknet markets, and ransomware.[208] Notable businesses include Garantex,[209] Eggchange, Cashbank, Buy-Bitcoin, Tetchange, Bitzlato, and Suex, which was sanctioned by the U.S. in 2021. Bitzlato founder and owner Anatoly Legkodymov was arrested following money-laundering charges by the United States Department of Justice.[210]
 Dark money has also been flowing into Russia through a dark web marketplace called Hydra, which is powered by cryptocurrency, and enjoyed more than $1 billion in sales in 2020, according to Chainalysis.[211] The platform demands that sellers liquidate cryptocurrency only through certain regional exchanges, which has made it difficult for investigators to trace the money.
 Almost 74% of ransomware revenue in 2021 — over $400 million worth of cryptocurrency — went to software strains likely affiliated with Russia, where oversight is notoriously limited.[208] However, Russians are also leaders in the benign adoption of cryptocurrencies, as the ruble is unreliable, and President Putin favours the idea of ""overcoming the excessive domination of the limited number of reserve currencies.""[212]
 In 2022, RenBridge - an unregulated alternative to exchanges for transferring value between blockchains - was found to be responsible for the laundering of at least $540 million since 2020. It is especially popular with people attempting to launder money from theft. This includes a cyberattack on Japanese crypto exchange Liquid that has been linked to North Korea.[213]
 Properties of cryptocurrencies gave them popularity in applications such as a safe haven in banking crises and means of payment, which also led to the cryptocurrency use in controversial settings in the form of online black markets, such as Silk Road.[177] The original Silk Road was shut down in October 2013 and there have been two more versions in use since then. In the year following the initial shutdown of Silk Road, the number of prominent dark markets increased from four to twelve, while the amount of drug listings increased from 18,000 to 32,000.[177]
 Darknet markets present challenges in regard to legality. Cryptocurrency used in dark markets are not clearly or legally classified in almost all parts of the world. In the U.S., Bitcoins are labelled as ""virtual assets"".[citation needed] This type of ambiguous classification puts pressure on law enforcement agencies around the world to adapt to the shifting drug trade of dark markets.[214][unreliable source?]
 Various studies have found that crypto-trading is rife with wash trading. Wash trading is a process, illegal in some jurisdictions, involving buyers and sellers being the same person or group, and may be used to manipulate the price of a cryptocurrency or inflate volume artificially. Exchanges with higher volumes can demand higher premiums from token issuers.[215] A study from 2019 concluded that up to 80% of trades on unregulated cryptocurrency exchanges could be wash trades.[215] A 2019 report by Bitwise Asset Management claimed that 95% of all Bitcoin trading volume reported on major website CoinMarketCap had been artificially generated, and of 81 exchanges studied, only 10 provided legitimate volume figures.[216]
 In 2022, cryptocurrencies attracted attention when Western nations imposed severe economic sanctions on Russia in the aftermath of its invasion of Ukraine in February. However, American sources warned in March that some crypto-transactions could potentially be used to evade economic sanctions against Russia and Belarus.[217]
 In April 2022, the computer programmer Virgil Griffith received a five-year prison sentence in the US for attending a Pyongyang cryptocurrency conference, where he gave a presentation on blockchains which might be used for sanctions evasion.[218]
 The Bank for International Settlements summarized several criticisms of cryptocurrencies in Chapter V of their 2018 annual report. The criticisms include the lack of stability in their price, the high energy consumption, high and variable transactions costs, the poor security and fraud at cryptocurrency exchanges, vulnerability to debasement (from forking), and the influence of miners.[219][220][221]
 Cryptocurrencies have been compared to Ponzi schemes, pyramid schemes[222] and economic bubbles,[223] such as housing market bubbles.[224] Howard Marks of Oaktree Capital Management stated in 2017 that digital currencies were ""nothing but an unfounded fad (or perhaps even a pyramid scheme), based on a willingness to ascribe value to something that has little or none beyond what people will pay for it"", and compared them to the tulip mania (1637), South Sea Bubble (1720), and dot-com bubble (1999), which all experienced profound price booms and busts.[225]
 Regulators in several countries have warned against cryptocurrency and some have taken measures to dissuade users.[226] However, research in 2021 by the UK's financial regulator suggests such warnings either went unheard, or were ignored. Fewer than one in 10 potential cryptocurrency buyers were aware of consumer warnings on the FCA website, and 12% of crypto users were not aware that their holdings were not protected by statutory compensation.[227][228] Of 1,000 respondents between the ages of eighteen and forty, almost 70% wrongly assumed cryptocurrencies were regulated, 75% of younger crypto investors claimed to be driven by competition with friends and family, 58% said that social media enticed them to make high risk investments.[229] The FCA recommends making use of its warning list, which flags unauthorized financial firms.[230]
 Many banks do not offer virtual currency services themselves and can refuse to do business with virtual currency companies.[231] In 2014, Gareth Murphy, a senior banking officer, suggested that the widespread adoption of cryptocurrencies may lead to too much money being obfuscated, blinding economists who would use such information to better steer the economy.[232] While traditional financial products have strong consumer protections in place, there is no intermediary with the power to limit consumer losses if Bitcoins are lost or stolen. One of the features cryptocurrency lacks in comparison to credit cards, for example, is consumer protection against fraud, such as chargebacks.
 The French regulator Autorité des marchés financiers (AMF) lists 16 websites of companies that solicit investment in cryptocurrency without being authorized to do so in France.[233]
 An October 2021 paper by the National Bureau of Economic Research found that Bitcoin suffers from systemic risk as the top 10,000 addresses control about one-third of all Bitcoin in circulation.[234] It is even worse for Bitcoin miners, with 0.01% controlling 50% of the capacity. According to researcher Flipside Crypto, less than 2% of anonymous accounts control 95% of all available Bitcoin supply.[235] This is considered risky as a great deal of the market is in the hands of a few entities.
 A paper by John Griffin, a finance professor at the University of Texas, and Amin Shams, a graduate student found that in 2017 the price of Bitcoin had been substantially inflated using another cryptocurrency, Tether.[236]
 Roger Lowenstein, author of ""Bank of America: The Epic Struggle to Create the Federal Reserve,"" says in a New York Times story that FTX will face over $8 billion in claims.[237]
 Non-fungible tokens (NFTs) are digital assets that represent art, collectibles, gaming, etc. Like crypto, their data is stored on the blockchain. NFTs are bought and traded using cryptocurrency. The Ethereum blockchain was the first place where NFTs were implemented, but now many other blockchains have created their own versions of NFTs.
 As the first big Wall Street bank to embrace cryptocurrencies, Morgan Stanley announced on 17 March 2021 that they will be offering access to Bitcoin funds for their wealthy clients through three funds which enable Bitcoin ownership for investors with an aggressive risk tolerance.[238] BNY Mellon on 11 February 2021 announced that it would begin offering cryptocurrency services to its clients.[239]
 On 20 April 2021,[240] Venmo added support to its platform to enable customers to buy, hold and sell cryptocurrencies.[241]
 In October 2021, financial services company Mastercard announced it is working with digital asset manager Bakkt on a platform that would allow any bank or merchant on the Mastercard network to offer cryptocurrency services.[242]
 Mining for proof-of-work cryptocurrencies requires enormous amounts of electricity and consequently comes with a large carbon footprint due to causing greenhouse gas emissions.[243] Proof-of-work blockchains such as Bitcoin, Ethereum, Litecoin, and Monero were estimated to have added between 3 million and 15 million tons of carbon dioxide (CO2) to the atmosphere in the period from 1 January 2016 to 30 June 2017.[244] By November 2018, Bitcoin was estimated to have an annual energy consumption of 45.8TWh, generating 22.0 to 22.9 million tons of CO2, rivalling nations like Jordan and Sri Lanka.[245] By the end of 2021, Bitcoin was estimated to produce 65.4 million tons of CO2, as much as Greece,[246] and consume between 91 and 177 terawatt-hours annually.[247][248]
 Critics have also identified a large electronic waste problem in disposing of mining rigs.[249] Mining hardware is improving at a fast rate, quickly resulting in older generations of hardware.[250]
 Bitcoin is the least energy-efficient cryptocurrency, using 707.6 kilowatt-hours of electricity per transaction.[251]
 Before June 2021, China was the primary location for Bitcoin mining. However, due to concerns over power usage and other factors, China forced out Bitcoin operations, at least temporarily. As a result, the United States promptly emerged as the top global leader in the industry. An example of a gross amount of electronic waste associated with Bitcoin mining operations in the US is a facility that located in Dalton, Georgia which is consuming nearly the same amount of electricity as the combined power usage of 97,000 households in its vicinity. Another example is that Riot Platforms operates a Bitcoin mining facility in Rockdale, Texas, which consumes approximately as much electricity as the nearby 300,000 households. This makes it the most energy-intensive Bitcoin mining operation in the United States.[252]
 The world's second-largest cryptocurrency, Ethereum, uses 62.56 kilowatt-hours of electricity per transaction.[253] XRP is the world's most energy efficient cryptocurrency, using 0.0079 kilowatt-hours of electricity per transaction.[254]
 Although the biggest PoW blockchains consume energy on the scale of medium-sized countries, the annual power demand from proof-of-stake (PoS) blockchains is on a scale equivalent to a housing estate. The Times identified six ""environmentally friendly"" cryptocurrencies: Chia, IOTA, Cardano, Nano, Solarcoin and Bitgreen.[255] Academics and researchers have used various methods for estimating the energy use and energy efficiency of blockchains. A study of the six largest proof-of-stake networks in May 2021 concluded:
 In terms of annual consumption (kWh/yr), the figures were: Polkadot (70,237), Tezos (113,249), Avalanche (489,311), Algorand (512,671), Cardano (598,755) and Solana (1,967,930). This equates to Polkadot consuming 7 times the electricity of an average U.S. home, Cardano 57 homes and Solana 200 times as much. The research concluded that PoS networks consumed 0.001% the electricity of the Bitcoin network.[256] University College London researchers reached a similar conclusion.[257]
 Variable renewable energy power stations could invest in Bitcoin mining to reduce curtailment, hedge electricity price risk, stabilize the grid, increase the profitability of renewable energy power stations and therefore accelerate transition to sustainable energy.[258][259][260][261][262]
 There are also purely technical elements to consider. For example, technological advancement in cryptocurrencies such as Bitcoin result in high up-front costs to miners in the form of specialized hardware and software.[263] Cryptocurrency transactions are normally irreversible after a number of blocks confirm the transaction. Additionally, cryptocurrency private keys can be permanently lost from local storage due to malware, data loss or the destruction of the physical media. This precludes the cryptocurrency from being spent, resulting in its effective removal from the markets.[264]
 In September 2015, the establishment of the peer-reviewed academic journal Ledger (ISSN 2379-5980) was announced. It covers studies of cryptocurrencies and related technologies, and is published by the University of Pittsburgh.[265]
 The journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the Bitcoin blockchain. Authors are also asked to include a personal Bitcoin address in the first page of their papers.[266][267]
 A number of aid agencies have started accepting donations in cryptocurrencies, including UNICEF.[268] Christopher Fabian, principal adviser at UNICEF Innovation, said the children's fund would uphold donor protocols, meaning that people making donations online would have to pass checks before they were allowed to deposit funds.[269][270]
 However, in 2021, there was a backlash against donations in Bitcoin because of the environmental emissions it caused. Some agencies stopped accepting Bitcoin and others turned to ""greener"" cryptocurrencies.[271] The U.S. arm of Greenpeace stopped accepting bitcoin donations after seven years. It said: ""As the amount of energy needed to run Bitcoin became clearer, this policy became no longer tenable.""[272]
 In 2022, the Ukrainian government raised over US$10,000,000 worth of aid through cryptocurrency following the 2022 Russian invasion of Ukraine.[273]
 Bitcoin has been characterized as a speculative bubble by eight winners of the Nobel Memorial Prize in Economic Sciences: Paul Krugman,[274] Robert J. Shiller,[275] Joseph Stiglitz,[276] Richard Thaler,[277] James Heckman,[278] Thomas Sargent,[278] Angus Deaton,[278] and Oliver Hart;[278] and by central bank officials including Alan Greenspan,[279] Agustín Carstens,[280] Vítor Constâncio,[281] and Nout Wellink.[282]
 Investors Warren Buffett and George Soros have respectively characterized it as a ""mirage""[283] and a ""bubble"";[284] while business executives Jack Ma and JP Morgan Chase CEO Jamie Dimon have called it a ""bubble""[285] and a ""fraud"",[286] respectively, although Jamie Dimon later said he regretted dubbing Bitcoin a fraud.[287] BlackRock CEO Laurence D. Fink called Bitcoin an ""index of money laundering"".[288]
 In June 2022, business magnate Bill Gates said that cryptocurrencies are ""100% based on greater fool theory"".[289]
 Legal scholars criticize the lack of regulation, which hinders conflict resolution when crypto assets are at the center of a legal dispute, for example a divorce or an inheritance. In Switzerland, jurists generally deny that cryptocurrencies are objects that fall under property law, as cryptocurrencies do not belong to any class of legally defined objects (Typenzwang, the legal numerus clausus). Therefore, it is debated whether anybody could even be sued for embezzlement of cryptocurrency if he/she had access to someone's wallet. However, in the law of obligations and contract law, any kind of object would be legally valid, but the object would have to be tied to an identified counterparty. However, as the more popular cryptocurrencies can be freely and quickly exchanged into legal tender, they are financial assets and have to be taxed and accounted for as such.[290][291]
 In 2018, an increase in crypto-related suicides was noticed after the cryptocurrency market crashed in August. The situation was particularly critical in Korea as crypto traders were on ""suicide watch"". A cryptocurrency forum on Reddit even started providing suicide prevention support to affected investors.[292][293] The May 2022 collapse of the Luna currency operated by Terra also led to reports of suicidal investors in crypto-related subreddits.[294]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the potential amount of prison time likely to be meted out', 'Warren Buffett and George Soros', 'U.S. cryptocurrency regulation history', 'cybercrime, money laundering and terrorism financing', 'varying legal treatments'], 'answer_start': [], 'answer_end': []}"
"
 Fintech, a clipped compound of ""financial technology"", refers to firms using new technology to compete with traditional financial methods in the delivery of financial services.[3][4][5] The use of smartphones for mobile banking, investing, borrowing services,[6] and cryptocurrency are examples of technologies designed to make financial services more accessible to the general public. Fintech companies consist of both startups and established financial institutions and technology companies trying to replace or enhance the usage of financial services provided by existing financial companies.
 Financial technology has been used to automate investments, insurance, trading, banking services and risk management.[7]
 Robo-advisers are a class of automated financial adviser that provide financial advice or investment management online with moderate to very little human intervention.[8] They provide digital financial advice based on mathematical rules or algorithms and can even create and manage automated investment portfolios. This can provide a lower-cost alternative to a human adviser and also has the ability to avoid human error and bias.[9]
 Global investment in financial technology increased more than 12,000% from $930 million in 2008 to $121.6 billion in 2020.[10]  2019 saw a record high with the total global investment in financial technology being $215.3 billion, of which Q3 alone accounted for $144.7 billion in investment.[11]
 In H1 2021, fintech deal volume hit 2,456 deals accounting for $98 billion in investment. Global VC investment was higher than $52 billion in H1’21, close to the annual record of $54 billion seen in 2018.[11]
 H1’21 saw $21 billion in corporate-affiliated VC investment. CVC deal volume reached a high of 284 in Q1’21, and then grew further to 312 in Q2’21.[12]
 The Americas saw about $51.4 billion of fintech investment in H1’21, with the US alone accounting for $42.1 billion. In the EMEA region, investment in fintech was very robust at $39.1 billion. In Asia-Pacific, fintech investment grew between H2’20 and H1’21—rising from $4.5 billion to $7.5 billion, although it was subdued in comparison with previous record highs.[13]
 The nascent financial technology industry in London has seen rapid growth over the last few years, according to the office of the Mayor of London. Forty percent of the City of London's workforce is employed in financial and technology services.[14] As of April 2019, about 76,500 people form the UK-wide FinTech workforce, and this number is projected to rise to 105,500 by 2030. Of the current fintech workforce in the UK, 42% of workers are from overseas.[15]
 In Europe, $1.5 billion was invested in financial technology companies in 2014, with London-based companies receiving $539 million, Amsterdam-based companies $306 million, and Stockholm-based companies receiving $266 million in investment. After London, Stockholm is the second highest funded city in Europe in the past 10 years. Europe's fintech deals reached a five-quarter high, rising from 37 in Q4 2015 to 47 in Q1 2016.[16][17] Lithuania is starting to become a northern European hub for financial technology companies since the news in 2016 about the exit of Britain from the European Union. Lithuania has issued 51 fintech licenses since 2016, 32 of those in 2017.[18]
 Fintech companies in the United States raised $12.4 billion in 2018, a 43% increase over 2017 figures.[19]
 In the Asia Pacific region, the growth will see a new financial technology hub to be opened in Sydney, in April 2015.[20] According to KPMG, Sydney's financial services sector in 2017 creates 9 per cent of national GDP and is bigger than the financial services sector in either Hong Kong or Singapore.[21] A financial technology innovation lab was launched in Hong Kong in 2015.[22]  In 2015, the Monetary Authority of Singapore launched an initiative named Fintech and Information Group to draw in start-ups from around the world. It pledged to spend $225 million in the fintech sector over the next five years.[23]
 While Singapore has been one of the central fintech hubs in Asia, start ups in the sector from Vietnam and Indonesia have been attracting more venture capital investments in recent years. Since 2014, Southeast Asian fintech companies have increased VC funding from $35 million to $679 million in 2018 and $1.14 billion in 2019.[7]
 Africa's overall fintech sector has expanded quickly. There were more than 1000 active businesses as of April 2022, up from 450 in 2020.[24] The venture capital sector, which saw deal value rise from $485 million in 2020 to $3.23 billion in 2021, was mostly responsible for the increase in investment in Africa. About half of this investment was made in fintech with Nigeria, Kenya and South Africa remaining key markets in the West, East, and South respectively.[25][26][27]
 Financial magazine Forbes created a list of the leading disruptors in financial technology for its Forbes 2021 Global Fintech 50.[28]
 A report published in February 2016 by EY commissioned by the UK Treasury compared seven leading fintech hubs: the United Kingdom, California, New York City, Singapore, Germany, Australia and Hong Kong. It ranked California first for 'talent' and 'capital', the United Kingdom first for 'government policy', and New York City first for 'demand'.[29]
 Finance is seen as one of the industries most vulnerable to disruption by software because financial services, much like publishing, are made of information rather than concrete goods. In particular blockchains have the potential to reduce the cost of transacting in a financial system.[30] However, aggressive enforcement of the Bank Secrecy Act and money transmission regulations represents an ongoing threat to fintech companies.[31] In response, the International Monetary Fund (IMF) and the World Bank jointly presented Bali Fintech Agenda on October 11, 2018[32] which consists of 12 policy elements acting as a guidelines for various governments and central banking institutions to adopt and deploy ""rapid advances in financial technology"".[33]
 The New York Venture Capital Association (NYVCA) hosts annual summits to educate those interested in learning more about fintech.[34] In 2018 alone, fintech was responsible for over 1,700 deals worth over 40 billion dollars.[35] In 2021, one in every five dollars invested by venture capital has gone into fintech.[36]
 In addition to established competitors, fintech companies often face doubts from financial regulators like issuing banks and national governments.[37][38] In July 2018, the Trump Administration in the United States issued a policy statement that allowed fintech companies to apply for special purpose national bank charters from the federal Office of the Comptroller of the Currency.[39] Federal preemption applies to state law regarding federally chartered banks.[40]
 Data security is another issue regulators are concerned about because of the threat of hacking as well as the need to protect sensitive consumer and corporate financial data.[41][42] Leading global fintech companies are proactively turning to cloud technology to meet increasingly stringent compliance regulations.[43]
 The Federal Trade Commission provides free resources for corporations of all sizes to meet their legal obligations of protecting sensitive data.[44] Several private initiatives suggest that multiple layers of defense can help isolate and secure financial data.[45]
 In the European Union, fintech companies must adhere to data protection laws, such as GDPR. Companies need to proactively protect users and companies data or face fines of 20 million euros, or in the case of an undertaking, up to 4% of their total global turnover.[46] In addition to GDPR, European financial institutions including fintech firms have to update their regulatory affairs departments with the Payment Services Directive (PSD2), meaning they must organise their revenue structure around a central goal of privacy.[47]
 The online financial sector is also an increasing target of distributed denial of service extortion attacks.[48][49] This security challenge is also faced by historical bank companies since they do offer Internet-connected customer services.[50]
 European Regulation on Fintechs refers to the body of laws, directives, and guidelines that govern the operation, development, and innovation of financial technology (fintech) companies in the European Union (EU). As the fintech sector has rapidly evolved in recent years, the European Commission and European Parliament have introduced various regulatory frameworks to maintain financial stability, promote innovation, and protect consumer interests. These frameworks aim to create a single market for fintech services, while also addressing the emerging risks and challenges associated with new financial technology.[51][52]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['emerging risks and challenges associated with new financial technology', 'historical bank companies', 'they do offer Internet-connected customer services', 'distributed denial of service extortion attacks', 'emerging risks and challenges associated with new financial technology'], 'answer_start': [], 'answer_end': []}"
"
 
E-commerce (electronic commerce) is the activity of electronically buying or selling products on online services or over the Internet. E-commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. E-commerce is the largest sector of the electronics industry and is in turn driven by the technological advances of the semiconductor industry.
 The term was coined and first employed by Robert Jacobson, Principal Consultant to the California State Assembly's Utilities & Commerce Committee, in the title and text of California's Electronic Commerce Act, carried by the late Committee Chairwoman Gwen Moore (D-L.A.) and enacted in 1984.
 E-commerce typically uses the web for at least a part of a transaction's life cycle although it may also use other technologies such as e-mail. Typical e-commerce transactions include the purchase of products (such as books from Amazon) or services (such as music downloads in the form of digital distribution such as the iTunes Store).[1] There are three areas of e-commerce: online retailing, electronic markets, and online auctions. E-commerce is supported by electronic business.[2] The existence value of e-commerce is to allow consumers to shop online and pay online through the Internet, saving the time and space of customers and enterprises, greatly improving transaction efficiency, especially for busy office workers, and also saving a lot of valuable time.[3]
 E-commerce businesses may also employ some or all of the following:
 There are five essential categories of E-commerce:[6]
 Contemporary electronic commerce can be classified into two categories. The first category is business based on types of goods sold (involves everything from ordering ""digital"" content for immediate online consumption, to ordering conventional goods and services, to ""meta"" services to facilitate other types of electronic commerce). The second category is based on the nature of the participant (B2B, B2C, C2B and C2C).[7]
 On the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are pressing issues for electronic commerce.
 Aside from traditional e-commerce, the terms m-Commerce (mobile commerce) as well (around 2013) t-Commerce[8] have also been used.
 In the United States, California's Electronic Commerce Act (1984), enacted by the Legislature, the more recent California Privacy Rights Act (2020), enacted through a popular election proposition and to control specifically how electronic commerce may be conducted in California. In the US in its entirety, electronic commerce activities are regulated more broadly by the Federal Trade Commission (FTC). These activities include the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive.[9] Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information.[10] As a result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.
 The Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.[11]
 Conflict of laws in cyberspace is a major hurdle for harmonization of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996).[12]
 Internationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.
 There is also Asia Pacific Economic Cooperation. APEC was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.
 In Australia, trade is covered under Australian Treasury Guidelines for electronic commerce and the Australian Competition & Consumer Commission[13] regulates and offers advice on how to deal with businesses online,[14] and offers specific advice on what happens if things go wrong.[15]
 The European Union undertook an extensive enquiry into e-commerce in 2015-16 which observed significant growth in the development of e-commerce, along with some developments which raised concerns, such as increased use of selective distribution systems, which allow manufacturers to control routes to market, and ""increased use of contractual restrictions to better control product distribution"". The European Commission felt that some emerging practices might be justified if they could improve the quality of product distribution, but ""others may unduly prevent consumers from benefiting from greater product choice and lower prices in e-commerce and therefore warrant Commission action"" in order to promote compliance with EU competition rules.[16]
 In the United Kingdom, the Financial Services Authority (FSA)[17] was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority.[18] The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD requires the European Commission to report on the implementation and impact of the PSD by 1 November 2012.[19]
 In India, the Information Technology Act 2000 governs the basic applicability of e-commerce.
 In China, the Telecommunications Regulations of the People's Republic of China (promulgated on 25 September 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce.[20] On the same day, the Administrative Measures on Internet Information Services were released, the first administrative regulations to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China.[21] On 28 August 2004, the eleventh session of the tenth NPC Standing Committee adopted an Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China's e-commerce legislation. It was a milestone in the course of improving China's electronic commerce legislation, and also marks the entering of China's rapid development stage for electronic commerce legislation.[22]
 E-commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.[23][24]
 Cross-border e-Commerce is also an essential field for e-Commerce businesses.  It has responded to the trend of globalization. It shows that numerous firms have opened up new businesses, expanded new markets, and overcome trade barriers; more and more enterprises have started exploring the cross-border cooperation field. In addition, compared with traditional cross-border trade, the information on cross-border e-commerce is more concealed. In the era of globalization, cross-border e-commerce for inter-firm companies means the activities, interactions, or social relations of two or more e-commerce enterprises. However, the success of cross-border e-commerce promotes the development of small and medium-sized firms, and it has finally become a new transaction mode. It has helped the companies solve financial problems and realize the reasonable allocation of resources field. SMEs ( small and medium enterprises) can also precisely match the demand and supply in the market, having the industrial chain majorization and creating more revenues for companies.[25]
 In 2012, e-commerce sales topped $1 trillion for the first time in history.[26]
 Mobile devices are playing an increasing role in the mix of e-commerce, this is also commonly called mobile commerce, or m-commerce. In 2014, one estimate saw purchases made on mobile devices making up 25% of the market by 2017.[27]
 For traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested an enormous volume of investment in mobile applications. The DeLone and McLean Model stated that three perspectives contribute to a successful e-business: information system quality, service quality and users' satisfaction.[28] There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in the company.[29]
 Modern 3D graphics technologies, such as Facebook 3D Posts, are considered by some social media marketers and advertisers as a preferable way to promote consumer goods than static photos, and some brands like Sony are already paving the way for augmented reality commerce. Wayfair now lets you inspect a 3D version of its furniture in a home setting before buying.[30]
 Among emerging economies, China's e-commerce presence continues to expand every year. With 668 million Internet users, China's online shopping sales reached $253 billion in the first half of 2015, accounting for 10% of total Chinese consumer retail sales in that period.[31] The Chinese retailers have been able to help consumers feel more comfortable shopping online.[32] e-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade.[33] In 2013, Alibaba had an e-commerce market share of 80% in China.[34] In 2014, Alibaba still dominated the B2B marketplace in China with a market share of 44.82%, followed by several other companies including Made-in-China.com at 3.21%, and GlobalSources.com at 2.98%, with the total transaction value of China's B2B market exceeding 4.5 billion yuan.[35] In 2014, there were 600 million Internet users in China (twice as many as in the US), making it the world's biggest online market.[36]
 China is also the largest e-commerce market in the world by value of sales, with an estimated US$899 billion in 2016.[37] It accounted for 42.4% of worldwide retail e-commerce in that year, the most of any country.[38]: 110  Research shows that Chinese consumer motivations are different enough from Western audiences to require unique e-commerce app designs instead of simply porting Western apps into the Chinese market.[39]
 The expansion of e-commerce in China has resulted in the development of Taobao villages, clusters of e-commerce businesses operating in rural areas.[38]: 112  Because Taobao villages have increased the incomes or rural people and entrepreneurship in rural China, Taobao villages have become a component of rural revitalization strategies.[40]: 278 
 In 2019, the city of Hangzhou established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to e-commerce and internet-related intellectual property claims.[41]: 124 
 In 2010, the United Kingdom had the highest per capita e-commerce spending in the world.[42] As of 2013, the Czech Republic was the European country where e-commerce delivers the biggest contribution to the enterprises' total revenue. Almost a quarter (24%) of the country's total turnover is generated via the online channel.[43]
 The rate of growth of the number of internet users in the Arab countries has been rapid – 13.1% in 2015. A significant portion of the e-commerce market in the Middle East comprises people in the 30–34 year age group. Egypt has the largest number of internet users in the region, followed by Saudi Arabia and Morocco; these constitute 3/4th of the region's share. Yet, internet penetration is low: 35% in Egypt and 65% in Saudi Arabia.[44]
 The Gulf Cooperation Council countries have a rapidly growing market and are characterized by a population that becomes wealthier (Yuldashev). As such, retailers have launched Arabic-language websites as a means to target this population. Secondly, there are predictions of increased mobile purchases and an expanding internet audience (Yuldashev). The growth and development of the two aspects make the GCC countries become larger players in the electronic commerce market with time progress. Specifically, research shows that the e-commerce market is expected to grow to over $20 billion by 2020 among these GCC countries (Yuldashev). The e-commerce market has also gained much popularity among western countries, and in particular Europe and the U.S. These countries have been highly characterized by consumer-packaged goods (CPG) (Geisler, 34). However, trends show that there are future signs of a reverse. Similar to the GCC countries, there has been increased purchase of goods and services in online channels rather than offline channels. Activist investors are trying hard to consolidate and slash their overall cost and the governments in western countries continue to impose more regulation on CPG manufacturers (Geisler, 36). In these senses, CPG investors are being forced to adapt to e-commerce as it is effective as well as a means for them to thrive.
 The future trends in the GCC countries will be similar to that of the western countries. Despite the forces that push business to adapt e-commerce as a means to sell goods and products, the manner in which customers make purchases is similar in countries from these two regions. For instance, there has been an increased usage of smartphones which comes in conjunction with an increase in the overall internet audience from the regions. Yuldashev writes that consumers are scaling up to more modern technology that allows for mobile marketing.
However, the percentage of smartphone and internet users who make online purchases is expected to vary in the first few years. It will be independent on the willingness of the people to adopt this new trend (The Statistics Portal). For example, UAE has the greatest smartphone penetration of 73.8 per cent and has 91.9 per cent of its population has access to the internet. On the other hand, smartphone penetration in Europe has been reported to be at 64.7 per cent (The Statistics Portal). Regardless, the disparity in percentage between these regions is expected to level out in future because e-commerce technology is expected to grow to allow for more users.
 The e-commerce business within these two regions will result in competition. Government bodies at the country level will enhance their measures and strategies to ensure sustainability and consumer protection (Krings, et al.). These increased measures will raise the environmental and social standards in the countries, factors that will determine the success of the e-commerce market in these countries. For example, an adoption of tough sanctions will make it difficult for companies to enter the e-commerce market while lenient sanctions will allow ease of companies. As such, the future trends between GCC countries and the Western countries will be independent of these sanctions (Krings, et al.). These countries need to make rational conclusions in coming up with effective sanctions.
 India has an Internet user base of about 460 million as of December 2017.[45] Despite being the third largest user base in the world, the penetration of the Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around six million new entrants every month.[citation needed] In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities.[46][citation needed] The India retail market is expected to rise from 2.5% in 2016 to 5% in 2020.[47]
 In 2013, Brazil's e-commerce was growing quickly with retail e-commerce sales expected to grow at a double-digit pace through 2014. By 2016, eMarketer expected retail e-commerce sales in Brazil to reach $17.3 billion.[48]
 Logistics in e-commerce mainly concerns fulfillment. Online markets and retailers have to find the best possible way to fill orders and deliver products. Small companies usually control their own logistic operation because they do not have the ability to hire an outside company. Most large companies hire a fulfillment service that takes care of a company's logistic needs.[49] The optimization of logistics processes that contains long-term investment in an efficient storage infrastructure system and adoption of inventory management strategies is crucial to prioritize customer satisfaction throughout the entire process, from order placement to final delivery. [50]
 E-commerce markets are growing at noticeable rates. The online market is expected to grow by 56% in 2015–2020. In 2017, retail e-commerce sales worldwide amounted to 2.3 trillion US dollars and e-retail revenues are projected to grow to 4.891 trillion US dollars in 2021.[51] Traditional markets are only expected 2% growth during the same time. Brick and mortar retailers are struggling because of online retailer's ability to offer lower prices and higher efficiency. Many larger retailers are able to maintain a presence offline and online by linking physical and online offerings.[52]
 E-commerce allows customers to overcome geographical barriers and allows them to purchase products anytime and from anywhere. Online and traditional markets have different strategies for conducting business. Traditional retailers offer fewer assortment of products because of shelf space where, online retailers often hold no inventory but send customer orders directly to the manufacturer. The pricing strategies are also different for traditional and online retailers. Traditional retailers base their prices on store traffic and the cost to keep inventory. Online retailers base prices on the speed of delivery.
 There are two ways for marketers to conduct business through e-commerce: fully online or online along with a brick and mortar store. Online marketers can offer lower prices, greater product selection, and high efficiency rates. Many customers prefer online markets if the products can be delivered quickly at relatively low price. However, online retailers cannot offer the physical experience that traditional retailers can. It can be difficult to judge the quality of a product without the physical experience, which may cause customers to experience product or seller uncertainty. Another issue regarding the online market is concerns about the security of online transactions. Many customers remain loyal to well-known retailers because of this issue.[53]
 Security is a primary problem for e-commerce in developed and developing countries. E-commerce security is protecting businesses' websites and customers from unauthorized access, use, alteration, or destruction. The type of threats include: malicious codes, unwanted programs (ad ware, spyware), phishing, hacking, and cyber vandalism. E-commerce websites use different tools to avert security threats. These tools include firewalls, encryption software, digital certificates, and passwords.[citation needed]
 For a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.[54]
 E-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimized the capacity of information processing than companies used to have, and for the financial flows, e-commerce allows companies to have more efficient payment and settlement solutions.[54]
 In addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems, like SAP ERP, Xero, or Megaventory, have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.[54]
 E-commerce helps create new job opportunities due to information related services, software app and digital products. It also causes job losses. The areas with the greatest predicted job-loss are retail, postal, and travel agencies. The development of e-commerce will create jobs that require highly skilled workers to manage large amounts of information, customer demands, and production processes. In contrast, people with poor technical skills cannot enjoy the wages welfare. On the other hand, because e-commerce requires sufficient stocks that could be delivered to customers in time, the warehouse becomes an important element. Warehouse needs more staff to manage, supervise and organize, thus the condition of warehouse environment will be concerned by employees.[55]
 E-commerce brings convenience for customers as they do not have to leave home and only need to browse websites online, especially for buying products which are not sold in nearby shops. It could help customers buy a wider range of products and save customers' time. Consumers also gain power through online shopping. They are able to research products and compare prices among retailers. Thanks to the practice of user-generated ratings and reviews from companies like Bazaarvoice, Trustpilot, and Yelp, customers can also see what other people think of a product, and decide before buying if they want to spend money on it.[56][57] Also, online shopping often provides sales promotion or discounts code, thus it is more price effective for customers. Moreover, e-commerce provides products' detailed information; even the in-store staff cannot offer such detailed explanation. Customers can also review and track the order history online. 
 E-commerce technologies cut transaction costs by allowing both manufactures and consumers to skip through the intermediaries. This is achieved through by extending the search area best price deals and by group purchase. The success of e-commerce in urban and regional levels depend on how the local firms and consumers have adopted to e-commerce.[58]
 However, e-commerce lacks human interaction for customers, especially who prefer face-to-face connection. Customers are also concerned with the security of online transactions and tend to remain loyal to well-known retailers. In recent years, clothing retailers such as Tommy Hilfiger have started adding Virtual Fit platforms to their e-commerce sites to reduce the risk of customers buying the wrong sized clothes, although these vary greatly in their fit for purpose.[59] When the customer regret the purchase of a product, it involves returning goods and refunding process. This process is inconvenient as customers need to pack and post the goods. If the products are expensive, large or fragile, it refers to safety issues.[52]
 In 2018, E-commerce generated 1.3 million short tons (1.2 megatonnes) of container cardboard in North America, an increase from 1.1 million (1.00)) in 2017. Only 35 percent of North American cardboard manufacturing capacity is from recycled content. The recycling rate in Europe is 80 percent and Asia is 93 percent. Amazon, the largest user of boxes, has a strategy to cut back on packing material and has reduced packaging material used by 19 percent by weight since 2016. Amazon is requiring retailers to manufacture their product packaging in a way that does not require additional shipping packaging. Amazon also has an 85-person team researching ways to reduce and improve their packaging and shipping materials.[60]
 Accelerated movement of packages around the world includes accelerated movement of living things, with all its attendant risks.[61] Weeds, pests, and diseases all sometimes travel in packages of seeds.[61] Some of these packages are part of brushing manipulation of e-commerce reviews.[61]
 E-commerce has been cited as a major force for the failure of major U.S. retailers in a trend frequently referred to as a ""retail apocalypse.""[62] The rise of e-commerce outlets like Amazon has made it harder for traditional retailers to attract customers to their stores and forced companies to change their sales strategies. Many companies have turned to sales promotions and increased digital efforts to lure shoppers while shutting down brick-and-mortar locations.[63] The trend has forced some traditional retailers to shutter its brick and mortar operations.[64]
 In March 2020, global retail website traffic hit 14.3 billion visits[65] signifying an unprecedented growth of e-commerce during the lockdown of 2020. Later studies show that online sales increased by 25% and online grocery shopping increased by over 100% during the crisis in the United States.[66] Meanwhile, as many as 29% of surveyed shoppers state that they will never go back to shopping in person again; in the UK, 43% of consumers state that they expect to keep on shopping the same way even after the lockdown is over.[67]
 Retail sales of e-commerce shows that COVID-19 has a significant impact on e-commerce and its sales are expected to reach $6.5 trillion by 2023.[68]
 Some common applications related to electronic commerce are:
 A timeline for the development of e-commerce:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['e-commerce', 'Saudi Arabia and Morocco', 'A timeline for the development of e-commerce', 'phishing, hacking, and cyber vandalism', 'A timeline for the development of e-commerce'], 'answer_start': [], 'answer_end': []}"
"
 Digital marketing is the component of marketing that uses the Internet and online-based digital technologies such as desktop computers, mobile phones, and other digital media and platforms to promote products and services.[2][3] Its development during the 1990s and 2000s changed the way brands and businesses use technology for marketing. As digital platforms became increasingly incorporated into marketing plans and everyday life,[4] and as people increasingly used digital devices instead of visiting physical shops,[5][6] digital marketing campaigns have become prevalent, employing combinations of search engine optimization (SEO), search engine marketing (SEM), content marketing, influencer marketing, content automation, campaign marketing, data-driven marketing, e-commerce marketing, social media marketing, social media optimization, e-mail direct marketing, display advertising, e-books, and optical disks and games have become commonplace. Digital marketing extends to non-Internet channels that provide digital media, such as television, mobile phones (SMS and MMS), callbacks, and on-hold mobile ringtones.[7] The extension to non-Internet channels differentiates digital marketing from online marketing.[8]
 Digital marketing effectively began in 1990 when the Archie search engine was created as an index for FTP sites. In the 1980s, the storage capacity of computers was already large enough to store huge volumes of customer information. Companies started choosing online techniques, such as database marketing, rather than limited list brokers.[9] Databases allowed companies to track customers' information more effectively, transforming the relationship between buyer and seller.
 In the 1990s, the term digital marketing was coined.[10] With the development of server/client architecture and the popularity of personal computers, Customer Relationship Management (CRM) applications became a significant factor in marketing technology.[11] Fierce competition forced vendors to include more services in their software, for example, marketing, sales, and service applications. Marketers were also able to own online customer data through eCRM software after the Internet was born. This led to the first clickable banner ad going live in 1994, which was the ""You Will"" campaign by AT&T, and over the first four months of it going live, 44% of all people who saw it clicked on the ad.[12][13]
 In the 2000s, with increasing numbers of Internet users and the birth of the iPhone, customers began searching for products and making decisions about their needs online first, instead of consulting a salesperson, which created a new problem for the marketing department of a company.[14] In addition, a survey in 2000 in the United Kingdom found that most retailers still needed to register their own domain address.[15] These problems encouraged marketers to find new ways to integrate digital technology into market development.
 In 2007, marketing automation was developed as a response to the ever-evolving marketing climate. Marketing automation is the process by which software is used to automate conventional marketing processes.[16] Marketing automation helped companies segment customers, launch multichannel marketing campaigns, and provide personalized information for customers.,[16] based on their specific activities. In this way, users' activity (or lack thereof) triggers a personal message that is customized to the user in their preferred platform. However, despite the benefits of marketing automation many companies are struggling to adopt it to their everyday uses correctly.[17][page needed]
 Digital marketing became more sophisticated in the 2000s and the 2010s, 
when[18][19] the proliferation of devices' capable of accessing digital media led to sudden growth.[20] Statistics produced in 2012 and 2013 showed that digital marketing was still growing.[21][22]
With the development of social media in the 2000s, such as LinkedIn, Facebook, YouTube and Twitter, consumers became highly dependent on digital electronics in daily lives. Therefore, they expected a seamless user experience across different channels for searching product's information. The change of customer behavior improved the diversification of marketing technology.[23]
 The term ""Digital Marketing"" was coined in the 1990s. Digital marketing was formally known as and referred to as 'online marketing', 'internet marketing' or 'web marketing'.  Worldwide digital marketing has become the most common used term and took off in the business industry, especially after the year 2013. But in other countries like Italy, digital marketing is still known as web marketing.[24]
 Digital media growth was estimated at 4.5 trillion online ads served annually with digital media spend at 48% growth in 2010.[25] An increasing portion of advertising stems from businesses employing Online Behavioural Advertising (OBA) to tailor advertising for internet users, but OBA raises concern of consumer privacy and data protection.[20]
 Nonlinear marketing, a type of interactive marketing, is a long-term marketing approach which builds on businesses collecting information about an Internet user's online activities and trying to be visible in multiple areas.[26]
 Unlike traditional marketing techniques, which involve direct, one-way messaging to consumers (via print, television, and radio advertising), nonlinear digital marketing strategies are centered on reaching prospective customers across multiple online channels.[27]
 Combined with higher consumer knowledge and the demand for more sophisticated consumer offerings, this change has forced many businesses to rethink their outreach strategy and adopt or incorporate omnichannel, nonlinear marketing techniques to maintain sufficient brand exposure, engagement, and reach.[28]
 Nonlinear marketing strategies involve efforts to adapt the advertising to different platforms,[29] and to tailor the advertising to different individual buyers rather than a large coherent audience.[26]
 Tactics may include:
 Some studies indicate that consumer responses to traditional marketing approaches are becoming less predictable for businesses.[30]  According to a 2018 study, nearly 90% of online consumers in the United States researched products and brands online before visiting the store or making a purchase.[31] The Global Web Index estimated that in 2018, a little more than 50% of consumers researched products on social media.[32] Businesses often rely on individuals portraying their products in a positive light on social media, and may adapt their marketing strategy to target people with large social media followings in order to generate such comments.[33] In this manner, businesses can use consumers to advertise their products or services, decreasing the cost for the company.[34]
 One of the key objectives of modern digital marketing is to raise brand awareness, the extent to which customers and the general public are familiar with and recognize a particular brand.
 Enhancing brand awareness is important in digital marketing, and marketing in general, because of its impact on brand perception and consumer decision-making. According to the 2015 essay, ""Impact of Brand on Consumer Behavior"":
 ""Brand awareness, as one of the fundamental dimensions of brand equity, is often considered to be a prerequisite of consumers’ buying decision, as it represents the main factor for including a brand in the consideration set. Brand awareness can also influence consumers’ perceived risk assessment and their confidence in the purchase decision, due to familiarity with the brand and its characteristics.""[35]
 Recent trends show that businesses and digital marketers are prioritizing brand awareness, focusing more on their digital marketing efforts on cultivating brand recognition and recall than in previous years. This is evidenced by a 2019 Content Marketing Institute study, which found that 81% of digital marketers have worked on enhancing brand recognition over the past year.[36]
 Another Content Marketing Institute survey revealed 89% of B2B marketers now believe improving brand awareness to be more important than efforts directed at increasing sales.[37]
 Increasing brand awareness is a focus of digital marketing strategy for a number of reasons:
 Digital marketing strategies may include the use of one or more online channels and techniques (omnichannel) to increase brand awareness among consumers.
 Building brand awareness may involve such methods/tools as:
 Search engine optimization techniques may be used to improve the visibility of business websites and brand-related content for common industry-related search queries.
 The importance of SEO to increase brand awareness is said to correlate with the growing influence of search results and search features like featured snippets, knowledge panels, and local SEO on customer behavior.[45]
 SEM, also known as PPC advertising, involves the purchase of ad space in prominent, visible positions atop search results pages and websites. Search ads have been shown to have a positive impact on brand recognition, awareness and conversions.[46]
 33% of searchers who click on paid ads do so because they directly respond to their particular search query.[47]
 Social media marketing has the characteristics of being in the marketing state and interacting with consumers all the time, emphasizing content and interaction skills. The marketing process needs to be monitored, analyzed, summarized and managed in real-time, and the marketing target needs to be adjusted according to the real-time feedback from the market and consumers.[48] 70% of marketers list increasing brand awareness as their number one goal for marketing on social media platforms. Facebook, Instagram, Twitter, and YouTube are listed as the top platforms currently used by social media marketing teams.[citation needed] As of 2021, LinkedIn has been added as one of the most-used social media platforms by business leaders for its professional networking capabilities.[49]
 56% of marketers believe personalization content – brand-centered blogs, articles, social updates, videos, landing pages – improves brand recall and engagement.[50]
 One of the major changes that occurred in traditional marketing was the ""emergence of digital marketing"", this led to the reinvention of marketing strategies in order to adapt to this major change in traditional marketing.
 As digital marketing is dependent on technology which is ever-evolving and fast-changing, the same features should be expected from digital marketing developments and strategies. This portion is an attempt to qualify or segregate the notable highlights existing and being used as of press time.[when?]
 To summarize, Pull digital marketing is characterized by consumers actively seeking marketing content while Push digital marketing occurs when marketers send messages without that content being actively sought by the recipients.
 An important consideration today while deciding on a strategy is that the digital tools have democratized the promotional landscape.
 Six principles for building online brand content:[56]
 The new digital era has enabled brands to selectively target their customers that may potentially be interested in their brand or based on previous browsing interests. Businesses can now use social media to select the age range, location, gender, and interests of whom they would like their targeted post to be seen. Furthermore, based on a customer's recent search history they can be ‘followed’ on the internet so they see advertisements from similar brands, products, and services,[57] This allows businesses to target the specific customers that they know and feel will most benefit from their product or service, something that had limited capabilities up until the digital era.
 Digital marketing activity is still growing across the world according to the headline global marketing index. A study published in September 2018, found that global outlays on digital marketing tactics are approaching $100 billion.[59] Digital media continues to rapidly grow. While the marketing budgets are expanding, traditional media is declining.[60] Digital media helps brands reach consumers to engage with their product or service in a personalized way. Five areas, which are outlined as current industry practices that are often ineffective are prioritizing clicks, balancing search and display, understanding mobiles, targeting, viewability, brand safety and invalid traffic, and cross-platform measurement.[61] Why these practices are ineffective and some ways around making these aspects effective are discussed surrounding the following points.
 Prioritizing clicks refers to display click ads, although advantageous by being ‘simple, fast and inexpensive’ rates for display ads in 2016 is only 0.10 percent in the United States. This means one in a thousand click ads is relevant therefore having little effect. This displays that marketing companies should not just use click ads to evaluate the effectiveness of display advertisements.[61]
 Balancing search and display for digital display ads is important. marketers tend to look at the last search and attribute all of the effectiveness of this. This, in turn, disregards other marketing efforts, which establish brand value within the consumer's mind. ComScore determined through drawing on data online, produced by over one hundred multichannel retailers that digital display marketing poses strengths when compared with or positioned alongside, paid search.[61] This is why it is advised that when someone clicks on a display ad the company opens a landing page, not its home page. A landing page typically has something to draw the customer in to search beyond this page. Commonly marketers see increased sales among people exposed to a search ad. But the fact of how many people you can reach with a display campaign compared to a search campaign should be considered.  Multichannel retailers have an increased reach if the display is considered in synergy with search campaigns. Overall, both search and display aspects are valued as display campaigns build awareness for the brand so that more people are likely to click on these digital ads when running a search campaign.[61]
 Understanding mobile devices is a significant aspect of digital marketing because smartphones and tablets are now responsible for 64% of the time US consumers are online.[61] Apps provide a big opportunity as well as challenge for the marketers because firstly the app needs to be downloaded and secondly the person needs to actually use it. This may be difficult as ‘half the time spent on smartphone apps occurs on the individuals single most used app, and almost 85% of their time on the top four rated apps’.[61] Mobile advertising can assist in achieving a variety of commercial objectives and it is effective due to taking over the entire screen, and voice or status is likely to be considered highly. However, the message must not be seen or thought of as intrusive.[61]  Disadvantages of digital media used on mobile devices also include limited creative capabilities, and reach.  Although there are many positive aspects including the user's entitlement to select product information, digital media creating a flexible message platform and there is potential for direct selling.[62]
 The number of marketing channels continues to expand, as measurement practices are growing in complexity. A cross-platform view must be used to unify audience measurement and media planning. Market researchers need to understand how the Omni-channel affects consumer's behavior, although when advertisements are on a consumer's device this does not get measured. Significant aspects to cross-platform measurement involve deduplication and understanding that you have reached an incremental level with another platform, rather than delivering more impressions against people that have previously been reached.[61] An example is ‘ESPN and comScore partnered on Project Blueprint discovering the sports broadcaster achieved a 21% increase in unduplicated daily reach thanks to digital advertising’.[61] Television and radio industries are the electronic media, which competes with digital and other technological advertising. Yet television advertising is not directly competing with online digital advertising due to being able to cross platform with digital technology. Radio also gains power through cross platforms, in online streaming content. Television and radio continue to persuade and affect the audience, across multiple platforms.[63]
 Targeting, viewability, brand safety, and invalid traffic all are aspects used by marketers to help advocate digital advertising. Cookies are a form of digital advertising, which are tracking tools within desktop devices, causing difficulty, with shortcomings including deletion by web browsers, the inability to sort between multiple users of a device, inaccurate estimates for unique visitors, overstating reach, understanding frequency, problems with ad servers, which cannot distinguish between when cookies have been deleted and when consumers have not previously been exposed to an ad. Due to the inaccuracies influenced by cookies, demographics in the target market are low and vary.[61] Another element, which is affected by digital marketing, is ‘viewability’ or whether the ad was actually seen by the consumer. Many ads are not seen by a consumer and may never reach the right demographic segment. Brand safety is another issue of whether or not the ad was produced in the context of being unethical or having offensive content. Recognizing fraud when an ad is exposed is another challenge marketers face. This relates to invalid traffic as premium sites are more effective at detecting fraudulent traffic, although non-premium sites are more so the problem.[61]
 Digital Marketing Channels are systems based on the Internet that can create, accelerate, and transmit product value from producer to a consumer terminal, through digital networks.[64][65] Digital marketing is facilitated by multiple Digital Marketing channels, as an advertiser one's core objective is to find channels which result in maximum two-way communication and a better overall ROI for the brand. There are multiple digital marketing channels available namely:[66]
 It is important for a firm to reach out to consumers and create a two-way communication model, as digital marketing allows consumers to give back feedback to the firm on a community-based site or straight directly to the firm via email.[78] Firms should seek this long-term communication relationship by using multiple forms of channels and using promotional strategies related to their target consumer as well as word-of-mouth marketing.[78]
 Possible benefits of digital marketing include:
 The ICC Code has integrated rules that apply to marketing communications using digital interactive media throughout the guidelines. There is also an entirely updated section dealing with issues specific to digital interactive media techniques and platforms. Code self-regulation on the use of digital interactive media includes:
 Digital marketing planning is a term used in marketing management.  It describes the first stage of forming a digital marketing strategy for the wider digital marketing system. The difference between digital and traditional marketing planning is that it uses digitally based communication tools and technology such as Social, Web, Mobile, Scannable Surface.[84][85] Nevertheless, both are aligned with the vision, the mission of the company and the overarching business strategy.[86]
 Using Dr. Dave Chaffey's approach, the digital marketing planning (DMP) has three main stages: Opportunity, Strategy, and Action. He suggests that any business looking to implement a successful digital marketing strategy must structure their plan by looking at opportunity, strategy and action. This generic strategic approach often has phases of situation review, goal setting, strategy formulation, resource allocation and monitoring.[86]
 To create an effective DMP, a business first needs to review the marketplace and set 'SMART' (Specific, Measurable, Actionable, Relevant, and Time-Bound) objectives.[87] They can set SMART objectives by reviewing the current benchmarks and key performance indicators (KPIs) of the company and competitors. It is pertinent that the analytics used for the KPIs be customized to the type, objectives, mission, and vision of the company.[88][89]
 Companies can scan for marketing and sales opportunities by reviewing their own outreach as well as influencer outreach. This means they have competitive advantage because they are able to analyse their co-marketers influence and brand associations.[90]
 To seize the opportunity, the firm should summarize its current customers' personas and purchase journey from this they are able to deduce their digital marketing capability. This means they need to form a clear picture of where they are currently and how many resources, they can allocate for their digital marketing strategy i.e., labor, time, etc. By summarizing the purchase journey, they can also recognize gaps and growth for future marketing opportunities that will either meet objectives or propose new objectives and increase profit.
 To create a planned digital strategy, the company must review their digital proposition (what you are offering to consumers) and communicate it using digital customer targeting techniques. So, they must define online value proposition (OVP), this means the company must express clearly what they are offering customers online e.g., brand positioning.
 The company should also (re)select target market segments and personas and define digital targeting approaches.
 After doing this effectively, it is important to review the marketing mix for online options. The marketing mix comprises the 4Ps – Product, Price, Promotion, and Place.[91][92] Some academics have added three additional elements to the traditional 4Ps of marketing Process, Place, and Physical appearance making it 7Ps of marketing.[93]
 The third and final stage requires the firm to set a budget and management systems. These must be measurable touchpoints, such as the audience reached across all digital platforms. Furthermore, marketers must ensure the budget and management systems are integrating the paid, owned, and earned media of the company.[94] The Action and final stage of planning also requires the company to set in place measurable content creation e.g. oral, visual or written online media.[95]
 After confirming the digital marketing plan, a scheduled format of digital communications (e.g. Gantt Chart) should be encoded throughout the internal operations of the company. This ensures that all platforms used fall in line and complement each other for the succeeding stages of digital marketing strategy.
 One way marketers can reach out to consumers and understand their thought process is through what is called an empathy map. An empathy map is a four-step process. The first step is through asking questions that the consumer would be thinking in their demographic. The second step is to describe the feelings that the consumer may be having. The third step is to think about what the consumer would say in their situation. The final step is to imagine what the consumer will try to do based on the other three steps. This map is so marketing teams can put themselves in their target demographics shoes.[96] Web Analytics are also a very important way to understand consumers. They show the habits that people have online for each website.[97] One particular form of these analytics is predictive analytics which helps marketers figure out what route consumers are on. This uses the information gathered from other analytics and then creates different predictions of what people will do so that companies can strategize on what to do next, according to the people's trends.[98]
 The ""sharing economy"" refers to an economic pattern that aims to obtain a resource that is not fully used.[101] Nowadays, the sharing economy has had an unimagined effect on many traditional elements including labor, industry, and distribution system.[101] This effect is not negligible that some industries are obviously under threat.[101][102] The sharing economy is influencing the traditional marketing channels by changing the nature of some specific concept including ownership, assets, and recruitment.[102]
 Digital marketing channels and traditional marketing channels are similar in function that the value of the product or service is passed from the original producer to the end user by a kind of supply chain.[103] Digital Marketing channels, however, consist of internet systems that create, promote, and deliver products or services from producer to consumer through digital networks.[104] Increasing changes to marketing channels has been a significant contributor to the expansion and growth of the sharing economy.[104] Such changes to marketing channels has prompted unprecedented and historic growth.[104] In addition to this typical approach, the built-in control, efficiency and low cost of digital marketing channels is an essential features in the application of sharing economy.[103]
 Digital marketing channels within the sharing economy are typically divided into three domains including, e-mail, social media, and search engine marketing or SEM.[104]
 Other emerging digital marketing channels, particularly branded mobile apps, have excelled in the sharing economy.[104] Branded mobile apps are created specifically to initiate engagement between customers and the company. This engagement is typically facilitated through entertainment, information, or market transaction.[104]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Digital marketing', 'Product, Price, Promotion, and Place', 'the sharing economy has had an unimagined effect on many traditional elements', 'Targeting, viewability, brand safety, and invalid traffic', 'Targeting, viewability, brand safety, and invalid traffic'], 'answer_start': [], 'answer_end': []}"
"
 Social media are interactive technologies that facilitate the creation, sharing and aggregation of content, ideas, interests, and other forms of expression through virtual communities and networks.[1][2] Social media refer to new forms of media that involve interactive participation. While challenges to the definition of social media arise[3][4] there are common features:[2]
 The term social in regard to media suggests platforms are user-centric and enable communal activity. As such, social media can be viewed as online facilitators, or enhancers of human networks—webs of individuals who enhance social connectivity.[8] Users usually access social media through web-based apps on desktops or services that offer social media functionality to their mobile devices. As users engage with these services, they create highly interactive platforms in which individuals, communities, and organizations can share, co-create, discuss, participate, and modify user-generated or self-curated content posted online.[9][7][1] Social media are used to document memories, learn and form friendships.[10] They may be used to promote people, companies and ideas.[10] Social media can be used to read or share news, whether it is true or false. Since the expansion of the Internet, digital media or digital rhetoric can be used to represent or identify a culture. Studying rhetoric that exists in the digital environment has become a crucial new process for many scholars.
 The change in relationship between humans and technology is the focus of the emerging field of technoself studies.[11] Some of the most popular social media platforms, with more than 100 million registered users, include Twitter, Facebook, WeChat, ShareChat, Instagram, QZone, Weibo, VK, Tumblr, Baidu Tieba, and LinkedIn. Depending on interpretation, other popular platforms that are sometimes referred to as social media services include YouTube, Letterboxd, QQ, Quora, Telegram, WhatsApp, Signal, LINE, Snapchat, Pinterest, Viber, Reddit, Discord, TikTok, Microsoft Teams. Wikis are examples of collaborative content creation.
 Social media outlets differ from old media (e.g. newspapers, TV, and radio broadcasting) in many ways, including quality,[12] reach, frequency, usability, relevancy, and permanence.[13] Social media outlets operate in a dialogic transmission system (many sources to many receivers) while traditional media operate under a monologic transmission model (one source to many receivers). For instance, a newspaper is delivered to many subscribers, and a radio station broadcasts the same programs to a city.[14]
 Observers have noted a range of positive and negative impacts when it comes to social media. Social media can help to improve an individual's sense of connectedness with real or online communities and be an effective communication (or marketing) tool for corporations, entrepreneurs, non-profit organizations, advocacy groups, political parties, and governments. There has been a rise in social movements using social media for communicating and organizing during political unrest.
 The PLATO system was launched in 1960 after being developed at the University of Illinois and subsequently commercially marketed by Control Data Corporation. It offered early forms of social media features with 1973-era innovations such as Notes, PLATO's message-forum application; TERM-talk, its instant-messaging feature; Talkomatic, perhaps the first online chat room; News Report, a crowdsourced online newspaper, and blog and Access Lists, enabling the owner of a note file or other application to limit access to a certain set of users, for example, only friends, classmates, or co-workers.
 ARPANET, which first came online in 1967, had by the late 1970s developed a rich cultural exchange of non-government/business ideas and communication, as evidenced by the network etiquette (or ""netiquette"") described in a 1982 handbook on computing at MIT's Artificial Intelligence Laboratory.[15] ARPANET evolved into the Internet following the publication of the first Transmission Control Protocol (TCP) specification, RFC 675 (Specification of Internet Transmission Control Program), written by Vint Cerf, Yogen Dalal, and Carl Sunshine in 1974.[16] This became the foundation of Usenet, conceived by Tom Truscott and Jim Ellis in 1979 at the University of North Carolina at Chapel Hill and Duke University, and established in 1980.
 A precursor of the electronic bulletin board system (BBS), known as Community Memory, appeared by 1973. True electronic BBSs arrived with the Computer Bulletin Board System in Chicago, which first came online on February 16, 1978. Before long, most major cities had more than one BBS running on TRS-80, Apple II, Atari, IBM PC, Commodore 64, Sinclair, and similar personal computers. The IBM PC was introduced in 1981, and subsequent models of both Mac computers and PCs were used throughout the 1980s. Multiple modems, followed by specialized telecommunication hardware, allowed many users to be online simultaneously. CompuServe, Prodigy, and AOL were three of the largest BBS companies and were the first to migrate to the Internet in the 1990s. Between the mid-1980s and the mid-1990s, BBSes numbered in the tens of thousands in North America alone.[17] Message forums (a specific structure of social media) arose with the BBS phenomenon throughout the 1980s and early 1990s. When the World Wide Web (WWW, or ""the web"") was added to the Internet in the mid-1990s, message forums migrated to the web, becoming Internet forums, primarily due to cheaper per-person access as well as the ability to handle far more people simultaneously than telco modem banks.
 Digital imaging and semiconductor image sensor technology facilitated the development and rise of social media.[18] Advances in metal–oxide–semiconductor (MOS) semiconductor device fabrication, reaching smaller micron and then sub-micron levels during the 1980s–1990s, led to the development of the NMOS (n-type MOS) active-pixel sensor (APS) at Olympus in 1985,[19][20] and then the complementary MOS (CMOS) active-pixel sensor (CMOS sensor) at NASA's Jet Propulsion Laboratory (JPL) in 1993.[19][21] CMOS sensors enabled the mass proliferation of digital cameras and camera phones, which bolstered the rise of social media.[18]
 In 1991, when Tim Berners-Lee integrated hypertext software with the Internet, he created the World Wide Web, marking the beginning of the modern era of networked communication. This breakthrough facilitated the formation of online communities and enabled support for offline groups through the use of weblogs, list servers, and email services. The evolution of online services progressed from serving as channels for networked communication to becoming interactive platforms for networked social interaction with the advent of Web 2.0.[8]
 Social media started in the mid-1990s with the invention of platforms like GeoCities, Classmates.com, and SixDegrees.com.[22] While instant messaging and chat clients existed at the time, SixDegrees was unique as it was the first online service designed for real people to connect using their actual names. It boasted features like profiles, friends lists, and school affiliations, making it ""the very first social networking site"" according to CBS News.[22][23] The platform's name was inspired by the ""six degrees of separation"" concept, which suggests that every person on the planet is just six connections away from everyone else.[24]
 In the early 2000s, social media platforms gained widespread popularity with the likes of Friendster and Myspace, followed by Facebook, YouTube, and Twitter, among others.[25]
 Research from 2015 shows that the world spent 22% of their online time on social networks,[26] thus suggesting the popularity of social media platforms, likely fueled by the widespread adoption of smartphones.[27] There are as many as 4.76 billion social media users in the world[28] which, as of January 2023[update], equates to 59.4% of the total global population.
 The idea that social media are defined simply by their ability to bring people together has been seen as too broad, as this would suggest that fundamentally different technologies like the telegraph and telephone are also social media.[29] The terminology is unclear, with some early researchers referring to social media as social networks or social networking services in the mid-2000s.[7] A more recent paper from 2015 reviewed the prominent literature in the area and identified four common features unique to the then-current social media services:[2]
 In 2019, Merriam-Webster defined social media as ""forms of electronic communication (such as websites for social networking and microblogging) through which users create online communities to share information, ideas, personal messages, and other content (such as videos).""[30]
 While the variety of evolving stand-alone and built-in social media services makes it challenging to define them,[2] marketing and social media experts broadly agree that social media includes the following 13 types:[31]
 Some services of other social media subtypes (such as Twitter and YouTube) also allow users to create a social network, and so are sometimes also included in the social network subtype.[7]
 Mobile social media refers to the use of social media on mobile devices such as smartphones and tablet computers. Mobile social media are useful applications of mobile marketing because the creation, exchange, and circulation of user-generated content can assist companies with marketing research, communication, and relationship development.[32] Mobile social media differ from others because they incorporate the current location of the user (location-sensitivity) or the time delay between sending and receiving messages.
 Social media promotes users to share content with others and display content in order to enhance a particular brand or product.[33] Social media allows people to be creative and share interesting ideas with their followers or fans. Certain social media applications such as Twitter, Facebook, and Instagram are places where users share specific political or sports content. Many reporters and journalists produce updates and information on sports and political news. It can truly give users pertinent and necessary information to stay up to date on relevant news stories and topics. However, there is a downside to it. Users are advised to exercise due diligence when they are using social media platforms.
 According to Andreas Kaplan, mobile social media applications can be differentiated among four types:[32]
 Social media sites are powerful tools for sharing content across networks. Certain content has the potential to spread virally, an analogy for the way viral infections spread from individual to individual. When content or websites go viral, users are more likely to share them with their social network, which leads to even more sharing.
 Viral marketing campaigns are particularly attractive to businesses because they can achieve widespread advertising coverage at a fraction of the cost of traditional marketing campaigns. Nonprofit organizations and activists may also use social media to post content with the aim of it going viral.
 Many social media sites provide specific functionality to help users re-share content, such as Twitter's ""retweet"" button or Facebook's ""share"" option. This feature is especially popular on Twitter, allowing users to keep up with important events and stay connected with their peers.[34] When certain posts become popular, they start to get retweeted over and over again, becoming viral. Hashtags can also be used in tweets to take count of how many people have used that hashtag.
 However, not all content has the potential to go viral, and it is difficult to predict what content will take off. Despite this, viral marketing campaigns can still be a cost-effective and powerful tool for promoting a message or product.
 Bots are automated programs that operate on the internet,[35] which have become increasingly popular due to their ability to automate many communication tasks. This has led to the creation of a new industry of bot providers.[36]
 Chatbots and social bots are programmed to mimic natural human interactions such as liking, commenting, following, and unfollowing on social media platforms.[37] As companies aim for greater market shares and increased audiences, internet bots have also been developed to facilitate social media marketing.[38] With the existence of social bots and chatbots, however, the marketing industry has also met an analytical crisis, as these bots make it difficult to differentiate between human interactions and automated bot interactions.[39] For instance, marketing data has been negatively affected by some bots, causing ""digital cannibalism"" in social media marketing. Additionally, some bots violate the terms of use on many social media platforms such as Instagram, which can result in profiles being taken down and banned.[40]
 'Cyborgs'—either bot-assisted humans or human-assisted bots[41]—are used for a number of different purposes both legitimate and illegitimate, from spreading fake news to creating marketing buzz.[42][43][44] A common legitimate use includes using automated programs to post on social media at a specific time.[45] In these cases, often, the human writes the post content and the bot schedules the time of posting. In other cases, the cyborgs are more nefarious, e.g., contributing to the spread of fake news and misinformation.[41] Often these accounts blend human and bot activity in a strategic way, so that when an automated account is publicly identified, the human half of the cyborg is able to take over and could protest that the account has been used manually all along. In many cases, these accounts that are being used in a more illegitimate fashion try to pose as real people; in particular, the number of their friends or followers resemble that of a real person.[41] Cyborgs are also related to sock puppet accounts, where one human pretends to be someone else, but can also include one human operating multiple cyborg accounts.
 There has been rapid growth in the number of United States patent applications that cover new technologies that are related to social media, and the number of them that are published has been growing rapidly over the past five years.[citation needed] As of 2020[update], there are over 5000 published patent applications in the United States.[46] As many as 7000 applications may be currently on file including those that have not been published yet; however, only slightly over 100 of these applications have issued as patents, largely due to the multi-year backlog in examination of business method patents, i.e., patents that outline and claim new methods of doing business.[47]
 
As an instance of technological convergence, various social media platforms of different kinds adapted functionality beyond their original scope, increasingly overlapping with each other over time, albeit usually not implemented as completely as on dedicated platforms.
 Examples are the social hub site Facebook launching an integrated video platform in May 2007,[48] and Instagram, whose original scope was low-resolution photo sharing, introducing the ability to share quarter-minute 640×640 pixel videos in 2013[49] (later extended to a minute with increased resolution), acting like a minimal video platform without video seek bar. Instagram later implemented stories (short videos self-destructing after 24 hours), a concept popularized by Snapchat, as well as IGTV, for seekable videos of up to ten minutes or one hour depending on account status.[50] Stories have been later adapted by the dedicated video platform YouTube in 2018, although access is restricted to the mobile apps, excluding mobile and desktop websites.[51]
 Twitter, whose original scope was text-based microblogging, later adapted photo sharing functionality (deprecating third-party services such as TwitPic),[52] later video sharing with 140-second time limit and view counter but no manual quality selection or subtitles like on dedicated video platforms, and originally only available to mobile app users but later implemented in their website front ends.[53][54] Then a media studio feature for business users, which resembles YouTube's Creator Studio.[55]
 The discussion platform Reddit added an integrated image hoster in June 2016 after Reddit users commonly relied on the external standalone image sharing platform Imgur,[56] and an internal video hosting service around a year later.[57] In July 2020, the ability to share multiple images in a single post (image galleries), a feature known from Imgur, was implemented.[58] Imgur itself implemented sharing videos of up to 30 seconds in May 2018, later extended to one minute.[59][60]
 Starting in 2018, the dedicated video platform YouTube rolled out a Community feature accessible through a channel tab (which usurps the previous Discussion channel tab), where text-only posts, as well as polls can be shared. To be enabled, channels have to pass a subscriber count threshold which has been lowered over time.[61]
 According to Statista, it is estimated that, in 2022, there are around 3.96 billion people who are using social media around the globe. This number is up from 3.6 billion in 2020 and is expected to increase to 4.41 billion in 2025.[62]
 The following is a list of the most popular social networking services based on the number of active users as of January 2024[update] per Statista.[63]
 A study from 2009 suggests that there may be individual differences that help explain who uses social media and who does not: extraversion and openness have a positive relationship with social media, while emotional stability has a negative sloping relationship with social media.[65] A separate study from 2015 found that people with a higher social comparison orientation appear to use social media more heavily than people with low social comparison orientation.[66]
 Data from Common Sense Media has suggested that children under the age of 13 in the United States use social networking services despite the fact that many social media sites have policies that state one must be at least 13 years old or older to join.[67] In 2017, Common Sense Media conducted a nationally representative survey of parents of children from birth to age 8 and found that 4% of children at this age used social media sites such as Instagram, Snapchat, or (now-defunct) Musical.ly ""often"" or ""sometimes"".[68] A different nationally representative survey by Common Sense in 2019 surveyed young Americans ages 8–16 and found that about 31% of children ages 8–12 ever use social media such as Snapchat, Instagram, or Facebook.[69] In that same survey, when American teens ages 16–18 were asked when they started using social media, 28% said they started to use it before they were 13 years old. However, the median age of starting to use social media was 14 years old.
 Social media plays a role in communication during COVID-19 pandemic.[70] In June 2020, during the COVID-19 pandemic, a nationally representative survey by Cartoon Network and the Cyberbullying Research Center surveyed Americans tweens (ages 9–12) found that the most popular overall application in the past year was YouTube (67%).[71] (In general, as age increased, the tweens were more likely to have used major social media apps and games.) Similarly, a nationally representative survey by Common Sense Media conducted in 2020 of Americans ages 13–18 found that YouTube was also the most popular social media service (used by 86% of 13- to 18-year-old Americans in the past year).[72] As children grow older, they utilize certain social media services on a frequent basis and often use the application YouTube to consume content. The use of social media certainly increases as people grow older and it has become a customary thing to have an Instagram and Twitter account.
 
While adults were already using social media before the COVID-19 pandemic, more started using it to stay socially connected and to get updates on the pandemic.  ""Social media have become popularly use to seek for medical information and have fascinated the general public to collect information regarding corona virus pandemics in various perspectives. During these days, people are forced to stay at home and the social media have connected and supported awareness and pandemic updates.""[73] This also made healthcare workers and systems more aware of social media as a place people were getting health information about the pandemic: ""During the COVID-19 pandemic, social media use has accelerated to the point of becoming a ubiquitous part of modern healthcare systems.""[74] Though this also led to the spread of disinformation, indeed, on December 11, 2020, the CDC put out a ""Call to Action: Managing the Infodemic"".[75]
Some healthcare organizations even used hashtags as interventions and published articles on their Twitter data:[76]  ""Promotion of the joint usage of #PedsICU and #COVID19 throughout the international pediatric critical care community in tweets relevant to the coronavirus disease 2019 pandemic and pediatric critical care.""[76]  However others in the medical community were concerned about social media addiction, due to it as an increasingly important context and therefore ""source of social validation and reinforcement"" and are unsure if increased social media use is a coping mechanism or harmful.[77]
 Governments may use social media to (for example):[78]
 Social media has been used extensively in civil and criminal investigations.[80] It has also been used to assist in searches for missing persons.[81] Police departments often make use of official social media accounts to engage with the public, publicize police activity, and burnish law enforcement's image;[82][83] conversely, video footage of citizen-documented police brutality and other misconduct has sometimes been posted to social media.[83]
 In the United States, U.S. Immigration and Customs Enforcement identifies and track individuals via social media, and also has apprehended some people via social media based sting operations.[84] U.S. Customs and Border Protection (also known as CPB) and the United States Department of Homeland Security use social media data as influencing factors during the visa process, and continue to monitor individuals after they have entered the country.[85] CPB officers have also been documented performing searches of electronics and social media behavior at the border, searching both citizens and non-citizens without first obtaining a warrant.[85]
 As social media gained momentum among the younger generations, governments began using it to improve their image, especially among the youth. In January 2021, Egyptian authorities were found to be using Instagram influencers as part of its media ambassadors program. The program was designed to revamp Egypt's image and to counter the bad press Egypt had received because of the country's human rights record. Saudi Arabia and the United Arab Emirates participated in similar programs.[86] Similarly, Dubai has also extensively relied on social media and influencers to promote tourism. However, the restrictive laws of Dubai have always kept these influencers within the limits to not offend the authorities, or to criticize the city, politics or religion. The content of these foreign influencers is controlled to make sure that nothing portrays Dubai in a negative light.[87]
 Businesses can use social media tools for marketing research, communication, sales promotions/discounts, informal employee-learning/organizational development, relationship development/loyalty programs,[32] and e-Commerce. Companies are increasingly using social-media monitoring tools to monitor, track, and analyze online conversations on the Web about their brand, products or related topics of interest. This can prove useful in public relations management and advertising-campaign tracking, allowing analysts to measure return on investment for their social media ad spending, competitor-auditing, and for public engagement. Tools range from free, basic applications to subscription-based, more in-depth tools. Often social media can become a good source of information and explanation of industry trends for a business to embrace change. Within the financial industry, companies can utilize the power of social media as a tool for analyzing the sentiment of financial markets. These range from the marketing of financial products, gaining insights into market sentiment, future market predictions, and as a tool to identify insider trading.[88]
 To properly take advantage of these benefits, businesses need to have a set of guidelines that they can use on different social media platforms.[5] Social media can enhance a brand through a process called ""building social authority"".[89] However, this process can be difficult, because one of the foundational concepts in social media is that one cannot completely control one's message through social media but rather one can simply begin to participate in the ""conversation"" expecting that one can achieve a significant influence in that conversation.[90] Because of the wide use of social media by consumers and their own employees, companies use social media[91] on a customer-organizational level; and an intra-organizational level. Social media, by connecting individuals to new ties via the social network can increase entrepreneurship and innovation, especially for those individuals who lack conventional information channels due to their lower socioeconomic background.[92]
 Social media marketing is the use of social media platforms and websites to promote a product or service and also to establish a connection with its customers. Social media marketing has increased due to the growing active user rates on social media sites. Though these numbers are not exponential. For example, as of 2018[update] Facebook had 2.2 billion users, Twitter had 330 million active users and Instagram had 800 million users.[citation needed] Then in 2021 Facebook had 2.89 billion users[citation needed] and Twitter had 206 million users.[citation needed] Similar to traditional advertising, all of social media marketing can be divided into three types: (1) paid media, (2) earned media, and (3) owned media.[93] Paid social media is when a firm directly buys advertising on a social media platform. Earned social media is when the firms does something that impresses its consumers or other stakeholders and they spontaneously post their own content about it on social media. Owned social media is when the firm itself owns the social media channel and creates content for its followers.[94]
 One of the main uses of social media marketing is to create brand awareness of a company or organization, creating a customer engagement by directly interacting with customers (e.g., customers can provide feedback on the firm's products) and providing support for customer service.[95] However, since social media allows consumers to spread opinions and share experiences in a peer-to-peer fashion, this has shifted some of the power from the organization to consumers, since these messages can be transparent and honest and the company can not control the content of the messages posted by consumers.[96]
 Social media personalities, often referred to as ""influencers"", are internet celebrities who have been employed or sponsored by marketers to promote products online. Research shows that digital endorsements seem to be successfully attracting social media users,[97] especially younger consumers who have grown up in the digital age.[98] In 2013, the United Kingdom Advertising Standards Authority (ASA) began to advise celebrities and sports stars to make it clear if they had been paid to tweet about a product or service by using the hashtag #spon or #ad in tweets containing endorsements, and the US Federal Trade Commission has issued similar guidelines.[99] The practice of harnessing social media personalities to market or promote a product or service to their following is commonly referred to as influencer marketing.
 Social media can also be used to directly advertise. Placing an advertisement on Facebook's Newsfeed, for example, can provide exposure of the brand to a large number of people. Social media platforms also enable targeting specific audiences with advertising. Users of social media are then able to like, share, and comment on the advertisement; this turns the passive advertising consumers into active advertising producers since they can pass the advertisement's message on to their friends.[100] Companies using social media marketing have to keep up with the different social media platforms and stay on top of ongoing trends. Since the different platforms and trends attract different audiences, firms must be strategic about their use of social media to attract the right audience.[5] Moreover, the tone of the content can affect the efficacy of social media marketing. Companies such as fast food franchise Wendy's have used humor (such as shitposting) to advertise their products by poking fun at competitors such as McDonald's and Burger King.[101] This particular example spawned a lot of fanart of the Wendy's mascot which circulated widely online, (particularly on sites like DeviantArt)[102] increasing the effect of the marketing campaign. Other companies such as Juul have used hashtags (such as #ejuice and #eliquid) to promote themselves and their products.[103]
 Marketing efforts can also take advantage of the peer effects in social media. Consumers tend to treat content on social media differently from traditional advertising (such as print ads), but these messages may be part of an interactive marketing strategy involving modeling, reinforcement, and social interaction mechanisms. A 2012 study focused on this communication described how communication between peers through social media can affect purchase intentions: a direct impact through conformity, and an indirect impact by stressing product engagement. This study indicated that social media communication between peers about a product had a positive relationship with product engagement.[104]
 Social media have a range of uses in politics.[105] Politicians use social media to spread their messages and influence voters. Social media's role in democratizing media participation, which proponents heralded as ushering in a new era of participatory democracy, falls short of those ideals, given many follow like-minded individuals.[106] Online-media audience-members can be largely passive consumers, while content creation is often dominated by a small number who post comments and write new content.[107]: 78  Online engagement does not always translate into real-world action; Howard, Busch and Sheets have argued that there is a digital divide in North America because of the continent's history, culture, and geography.[108]
 Political campaigns target people online via social-media posts in they hope that they will increase their political engagement.[109] Social media was influential in the Arab Spring of revolutionary outbreaks in the Middle East and North Africa during 2011.[110][111][112][113]
However, debate persists about the extent to which social media facilitated this kind of political change.[114] 
Due to the abuse of human rights in Bahrain, activists have used social media to report violence and injustice. They publicized the brutality of government authorities and police, who were detaining, torturing and threatening individuals. On the other hand, Bahrain's government used social media to track and target activists who were critical of the authorities. The government stripped citizenship from over 1,000 activists as punishment.[115]
 Social-media footprints of candidates for political office have grown—the 2016 United States presidential election provided good examples. Dounoucos et al. noted that Twitter use by candidates was unprecedented during that election.[116][117] The public has increased their reliance on social-media sites for political information.[116] In the European Union, social media have amplified political messages.[118] Foreign-originated social-media campaigns have sought to influence political opinion in another country.[119][120][121]
 Militant groups see social media as a major organizing and recruiting tool.[122] Islamic State (also known as ISIS) has used social media to promote its cause. In 2014, #AllEyesonISIS went viral on Arabic Twitter.[123][124] State-sponsored cyber-groups have weaponized social-media platforms to attack governments in the United States, the European Union, and the Middle East.[citation needed] Although phishing attacks via email are the most commonly used tactic to breach government networks, phishing attacks on social media rose 500% in 2016.[125]
 Some employers examine job applicants' social media profiles as part of the hiring assessment. This issue raises many ethical questions that some consider an employer's right and others consider discrimination. Many Western-European countries have already implemented laws that restrict the regulation of social media in the workplace. States including Arkansas, California, Colorado, Illinois, Maryland, Michigan, Nevada, New Jersey, New Mexico, Utah, Washington, and Wisconsin have passed legislation that protects potential employees and current employees from employers that demand that they provide their usernames and passwords for any social media accounts.[citation needed] Use of social media by young people has caused significant problems for some applicants who are active on social media when they try to enter the job market. A survey of 17,000 young people in six countries in 2013 found that one in ten people aged 16 to 34 have been rejected for a job because of online comments they made on social media websites.[126]
 For potential employees, Social media services such as LinkedIn have shown to affect deception in resumes. While these services do not affect how often deception happens, they affect the types of deception that occur. LinkedIn resumes are less deceptive about prior work experience but more deceptive about interests and hobbies.[127]
 The use of social media in science communications offers extensive opportunities for exchanging scientific information, ideas, opinions and publications. Scientists use social media to share their scientific knowledge and new findings on platforms such as ResearchGate, LinkedIn, Facebook, Twitter and Academia.edu.[128] Among these the most common type of social media that scientists use is Twitter and blogs. It has been found that Twitter increased the scientific impact in the community. The use of social media has improved and elevated the interaction between scientists, reporters, and the general public. [citation needed] Over 495,000 opinions were shared on Twitter related to science in one year (between September 1, 2010, and August 31, 2011), which was an increase compared with past years.[129] Science related blogs motivate public interest in learning, following, and discussing science. Blogs use textual depth and graphical videos that provide the reader with a dynamic way to interact with scientific information. Both Twitter and blogs can be written quickly and allow the reader to interact in real time with the authors. However, the popularity of social media platforms changes quickly and scientists need to keep pace with changes in social media.[130] In terms of organized uses of scientific social media, one study in the context of climate change has shown that climate scientist and scientific institutions played a minimal role in online debate, while nongovernmental organizations played a larger role.[131]
 Signals from social media are used to assess academic publications,[132] as well as for different scientific approaches, such as gaining better understanding of the public sentiment concerning relevant topics,[133] identifying influencer accounts shaping the public opinion in specific domains,[134] or crowdsourcing for new ideas or solutions.[135] Another study found that most of the health science students acquiring academic materials from others through social media.[136]
 It is not only an issue in the workplace but an issue in post-secondary school admissions as well. There have been situations where students have been forced to give up their social media passwords to school administrators.[137] There are inadequate laws to protect a student's social media privacy, and organizations such as the ACLU are pushing for more privacy protection, as it is an invasion. They urge students who are pressured to give up their account information to tell the administrators to contact a parent or lawyer before they take the matter any further. Although they are students, they still have the right to keep their password-protected information private.[138]
 
According to a 2007 journal, before social media[139] admissions officials in the United States used SAT and other standardized test scores, extra-curricular activities, letters of recommendation, and high school report cards to determine whether to accept or deny an applicant. In the 2010s, while colleges and universities still used these traditional methods to evaluate applicants, these institutions were increasingly accessing applicants' social media profiles to learn about their character and activities. According to Kaplan, Inc, a corporation that provides higher education preparation, in 2012 27% of admissions officers used Google to learn more about an applicant, with 26% checking Facebook.[140] Students whose social media pages include offensive jokes or photos, racist or homophobic comments, photos depicting the applicant engaging in illegal drug use or drunkenness, and so on, may be screened out from admission processes. ""One survey in July 2017, by the American Association of College Registrars and Admissions Officers, found that 11 percent of respondents said they had refused to admit an applicant based on social media content. This includes 8 percent of public institutions, where the First Amendment applies. The survey found that 30 percent of institutions acknowledged reviewing the personal social media accounts of applicants at least some of the time.""[141] Social media comments and images are being used in a range of court cases including employment law, child custody/child support and insurance disability claims. After an Apple employee criticized his employer on Facebook, he was fired. When the former employee sued Apple for unfair dismissal, the court, after seeing the man's Facebook posts, found in favor of Apple, as the man's social media comments breached Apple's policies.[142] After a heterosexual couple broke up, the man posted ""violent rap lyrics from a song that talked about fantasies of killing the rapper's ex-wife"" and made threats against him. The court found him guilty and he was sentenced to jail.[142] In a disability claims case, a woman who fell at work claimed that she was permanently injured; the employer used the social media posts of her travels and activities to counter her claims.[142]
 Courts do not always admit social media evidence, in part, because screenshots can be faked or tampered with.[143] Judges are taking emojis into account to assess statements made on social media; in one Michigan case where a person alleged that another person had defamed them in an online comment, the judge disagreed, noting that there was an emoji after the comment which indicated that it was a joke.[143] In a 2014 case in Ontario against a police officer regarding alleged assault of a protester during the G20 summit, the court rejected the Crown's application to use a digital photo of the protest that was anonymously posted online, because there was no metadata proving when the photo was taken and it could have been digitally altered.[143]
 As of March 2010[update], in the United States, 81% of users look online for news of the weather, first and foremost, with the percentage seeking national news at 73%, 52% for sports news, and 41% for entertainment or celebrity news. According to CNN, in 2010 75% of people got their news forwarded through e-mail or social media posts, whereas 37% of people shared a news item via Facebook or Twitter.[144] Facebook and Twitter make news a more participatory experience than before as people share news articles and comment on other people's posts. Rainie and Wellman (2012) have argued that media making now has become a participation work,[145] which changes communication systems. However, 27% of respondents worry about the accuracy of a story on a blog.[107] From a 2019 poll, Pew Research Center found that Americans are wary about the ways that social media sites share news and certain content.[146] This wariness of accuracy is on the rise as social media sites are increasingly exploited by aggregated new sources which stitch together multiple feeds to develop plausible correlations. Hemsley and colleagues (2018) refer to this phenomenon as ""pseudo-knowledge"" which develop false narratives and fake news that are supported through general analysis and ideology rather than facts.[147] Social media as a news source was further questioned as spikes in evidence surround major news events such as was captured in the United States 2016 presidential election[148] and again during the COVID-19 Pandemic.
 Social media are used to fulfill perceived social needs such as socializing with friends and family[4] as well as romance and flirting,[4] but not all needs can be fulfilled by social media.[149] For example, a 2003 article found that lonely individuals are more likely to use the Internet for emotional support than those who are not lonely.[150] A nationally representative survey from Common Sense Media in 2018 found that 40% of American teens ages 13–17 thought that social media was ""extremely"" or ""very"" important for them to keep up with their friends on a day-to-basis.[151] The same survey found that 33% of teens said social media was extremely or very important to have meaningful conversations with close friends, and 23% of teens said social media was extremely or very important to document and share highlights from their lives.[151] Recently, a Gallup poll from May 2020 showed that 53% of adult social media users in the United States thought that social media was a very or moderately important way to keep in touch with those they cannot otherwise see in-person due to social distancing measures related to the COVID-19 pandemic.[152]
 Sherry Turkle explores this topic in her book Alone Together as she discusses how people confuse social media usage with authentic communication.[153] She posits that people tend to act differently online and are less afraid to hurt each other's feelings. Additionally, some online behaviors can cause stress and anxiety, due to the permanence of online posts, the fear of being hacked, or of universities and employers exploring social media pages. Turkle also speculates that people are beginning to prefer texting to face-to-face communication, which can contribute to feelings of loneliness.[153] Nationally representative surveys from 2019 have found this to be the case with teens in the United States[151] and Mexico.[154] Some researchers have also found that exchanges that involved direct communication and reciprocation of messages correlated with fewer feelings of loneliness.[155] However, that same study showed that passively using social media without sending or receiving messages does not make people feel less lonely unless they were lonely to begin with.
 The term social media ""stalking"" or ""creeping"" have been popularized over the years, and this refers to looking at the person's ""timeline, status updates, tweets, and online bios"" to find information about them and their activities.[156] While social media creeping is common, it is considered to be poor form to admit to a new acquaintance or new date that you have looked through his or her social media posts, particularly older posts, as this will indicate that you were going through their old history.[156] A sub-category of creeping is creeping ex-partners' social media posts after a breakup to investigate if there is a new partner or new dating; this can lead to preoccupation with the ex, rumination, and negative feelings, all of which postpone recovery and increase feelings of loss.[157]
 Catfishing has become more prevalent since the advent of social media. Relationships formed with catfish can lead to actions such as supporting them with money and catfish will typically make excuses as to why they cannot meet up or be viewed on camera.[158]
 The more time people spend on Facebook, the less satisfied they feel about their life.[159] Self-presentation theory explains that people will consciously manage their self-image or identity related information in social contexts.[160] In fact, a critical aspect of social networking sites is the time invested in customizing a personal profile, and encourage a sort of social currency based on likes, followers, and comments.[161] Users also tend to segment their audiences based on the image they want to present, pseudonymity and use of multiple accounts across the same platform remain popular ways to negotiate platform expectations and segment audiences.[162]
 However, users may feel pressure to gain their peers' acceptance of their self-presentation. For example, in a 2016 peer-reviewed article by Trudy Hui Hui Chua and Leanne Chang, the authors found that teenage girls manipulate their self-presentation on social media to achieve a sense of beauty that is projected by their peers.[163] These authors also discovered that teenage girls compare themselves to their peers on social media and present themselves in certain ways in an effort to earn regard and acceptance. However, when users do not feel like they reached this regard and acceptance, this can actually lead to problems with self-confidence and self-satisfaction.[163] A nationally representative survey of American teens ages 13–17 by Common Sense Media found that 45% said getting ""likes"" on posts is at least somewhat important, and 26% at least somewhat agreed that they feel bad about themselves if nobody comments on or ""likes"" their photos.[151] Some evidence suggests that perceived rejection may lead to feeling emotional pain,[164] and some may partake in online retaliation such as online bullying.[165] Conversely, according to research from UCLA, users' reward circuits in their brains are more active when their own photos are liked by more peers.[166]
 Literature suggests that social media can breed a negative feedback loop of viewing and uploading photos, self-comparison, feelings of disappointment when perceived social success is not achieved, and disordered body perception.[167] In fact, one study shows that the microblogging platform, Pinterest is directly associated with disordered dieting behavior, indicating that for those who frequently look at exercise or dieting ""pins"" there is a greater chance that they will engage in extreme weight-loss and dieting behavior.[168]
 Social media can also function as a supportive system for adolescents' health, because by using social media, adolescents are able to mobilize around health issues that they themselves deem relevant.[169] For example, in a clinical study among adolescent patients undergoing treatment for obesity, the participants' expressed that through social media, they could find personalized weight-loss content as well as social support among other adolescents with obesity.[170][171] While social media can provide such information, there are a considerable amount of uninformed and incorrect sources which promote unhealthy and dangerous methods of weight loss.[171] As stated by the national eating disorder association there is a high correlation between weight loss content and disorderly eating among women who have been influenced by this negative content.[171] Therefore, there is a need for people to evaluate and identify reliable health information, competencies commonly known as health literacy. This has led to efforts by governments and public health organizations to use social media to interact with users, to limited success.[172]
 Other social media, such as pro-anorexia sites, have been found in studies to cause significant risk of harm by reinforcing negative health-related behaviors through social networking, especially in adolescents.[173][174][175] Social media affects the way a person views themself. The constant comparison to edited photos, of other individual's and their living situations, can cause many negative emotions. This can lead to not eating, and isolation. As more and more people continue to use social media for the wrong reasons, it increases the feeling of loneliness in adults.[176]
 During the coronavirus pandemic, the spread of information throughout social media regarding treatments against the virus has also influenced different health behaviors.[177] For example, People who use more social media and belief more in conspiracy theory in social media during the COVID-19 pandemic had worse mental health[178] and is predictive of their compliance to health behaviors such as hand-washing during the pandemic.[179]
 Social media platforms can serve as a breeding ground for addiction-related behaviors, with studies showing that excessive use can lead to the development of addiction-like symptoms. These symptoms include compulsive checking, mood modification, and withdrawal when not using social media, which can result in decreased face-to-face social interactions and contribute to the deterioration of interpersonal relationships and a sense of loneliness.[180]
 For example, adolescents who rely heavily on social media for health information and support may be more prone to these addiction-like behaviors. In a clinical study among adolescent patients undergoing treatment for obesity, participants expressed that they could find personalized weight-loss content and social support among other adolescents with obesity through social media.[181] However, social media also hosts a considerable amount of uninformed and incorrect sources promoting unhealthy and dangerous methods of weight loss. The National Eating Disorder Association states that there is a high correlation between weight loss content on social media and disordered eating among women influenced by this negative content.[181]
 News media and television journalism have been a key feature in the shaping of American collective memory for much of the 20th century.[182][183] Indeed, since the colonial era of the United States, news media has influenced collective memory and discourse about national development and trauma. In many ways, mainstream journalists have maintained an authoritative voice as the storytellers of the American past. Their documentary-style narratives, detailed exposés, and their positions in the present make them prime sources for public memory. Specifically, news media journalists have shaped collective memory on nearly every major national event—from the deaths of social and political figures to the progression of political hopefuls. Journalists provide elaborate descriptions of commemorative events in U.S. history and contemporary popular cultural sensations. Many Americans learn the significance of historical events and political issues through news media, as they are presented on popular news stations.[184] However, journalistic influence has grown less important, whereas social networking sites such as Facebook, YouTube and Twitter, provide a constant supply of alternative news sources for users.
 As social networking becomes more popular among older and younger generations, sites such as Facebook and YouTube gradually undermine the traditionally authoritative voices of news media. For example, American citizens contest media coverage of various social and political events as they see fit, inserting their voices into the narratives about America's past and present and shaping their own collective memories.[185][186] An example of this is the public explosion of the Trayvon Martin shooting in Sanford, Florida. News media coverage of the incident was minimal until social media users made the story recognizable through their constant discussion of the case. Approximately one month after Martin's death, its online coverage by everyday Americans garnered national attention from mainstream media journalists, in turn exemplifying media activism.[187]
 Social media use sometimes involves negative interactions between users.[188] Angry or emotional conversations can lead to real-world interactions, which can get users into dangerous situations. Some users have experienced threats of violence online and have feared these threats manifesting themselves offline. Related issues include cyberbullying, online harassment, and 'trolling'. According to cyberbullying statistics from the i-Safe Foundation, over half of adolescents and teens have been bullied online, and about the same number have engaged in cyberbullying.[189] Both the bully and the victim are negatively affected, and the intensity, duration, and frequency of bullying are the three aspects that increase the negative effects on both of them.[190]
 One phenomenon that is commonly studied with social media is the issue of social comparison. People compare their own lives to the lives of their friends through their friends' posts.[citation needed] Because people are motivated to portray themselves in a way that is appropriate to the situation and serves their best interests,[163] often the things posted online are the positive aspects of people's lives, making other people question why their own lives are not as exciting or fulfilling. One study in 2017 found that problematic social media use (i.e., feeling addicted to social media) was related to lower life satisfaction and self-esteem scores; the authors speculate that users may feel if their life is not exciting enough to put online it is not as good as their friends or family.[191]
 Studies have shown that self-comparison on social media can have dire effects on physical and mental health because they give us the ability to seek approval and compare ourselves.[192][193] In one study, women reported that social media are the most influential sources of their body image satisfaction; while men reported them as the second most impacting factor.[194]
 Social media has allowed for people to be constantly surrounded and aware of celebrity images and influencers who hold strong online presence with the number of followers they have. This constant online presence has meant that people are far more aware of what others look like and as such body comparisons have become an issue, as people are far more aware of what the desired body type is. A study produced by King university showed that 87% of women and 65% of men compared themselves to images found on social media.[195]
 There are efforts to combat these negative effects, such as the use of the tag #instagramversusreality and #instagramversusreallife, that have been used to promote body positivity. In a related study, women aged 18–30 were shown posts using this hashtag that contained side-by-side images of women in the same clothes and setting, but one image was enhanced for Instagram, while the other was an unedited, ""realistic"" version. Women who participated in this experiment noted a decrease in body dissatisfaction.[196]
 According to a study released in 2017 by researchers from the University of Pittsburgh, the link between sleep disturbance and the use of social media was clear. It concluded that blue light had a part to play—and how often they logged on, rather than time spent on social media sites, was a higher predictor of disturbed sleep, suggesting ""an obsessive 'checking'"".[197] The strong relationship of social media use and sleep disturbance has significant clinical ramifications for young adults health and well-being.[198] In a recent study, we have learned that people in the highest quartile for social media use per week report the most sleep disturbance. The median number of minutes of social media use per day is 61 minutes. Lastly, we have learned that females are more inclined to experience high levels of sleep disturbance than males.[199] Many teenagers suffer from sleep deprivation as they spend long hours at night on their phones, and this, in turn, could affect grades as they will be tired and unfocused in school.[200] In a study from 2011, it was found that time spent on Facebook has a strong negative relationship with overall GPA, but it was unclear if this was related to sleep disturbances.[201]
 One studied emotional effect of social media is 'Facebook depression', which is a type of depression that affects adolescents who spend too much of their free time engaging with social media sites.[10] This may lead to problems such as reclusiveness which can negatively damage one's health by creating feelings of loneliness and low self-esteem among young people.[10] Using a phone to look at social media before bed has become a popular trend among teenagers and this has led to a lack of sleep and inability to stay awake during school. Social media applications curate content that encourages users to keep scrolling to the point where they lose track of time.[198] There are studies that show children's self-esteem is positively affected by positive comments on social media and negatively affected self-esteem by negative comments. This affects the way that people look at themselves on a ""worthiness"" scale.[202] A 2017 study of almost 6,000 adolescent students showed that those who self-reported addiction-like symptoms of social media use were more likely to report low self-esteem and high levels of depressive symptoms.[203] From the findings on a population-based study, there is about 37% increase in the likelihood of major depression among adolescents.[204] In a different study conducted in 2007, those who used the most multiple social media platforms (7 to 11) had more than three times the risk of depression and anxiety than people who used the fewest (0 to 2).[205]
 A second emotional effect is social media burnout, which is defined by Bo Han as ambivalence, emotional exhaustion, and depersonalization.[206] Ambivalence refers to a user's confusion about the benefits she can get from using a social media site. Emotional exhaustion refers to the stress a user has when using a social media site. Depersonalization refers to the emotional detachment from a social media site a user experiences. The three burnout factors can all negatively influence the user's social media continuance. This study provides an instrument to measure the burnout a user can experience when his or her social media ""friends"" are generating an overwhelming amount of useless information (e.g., ""what I had for dinner"", ""where I am now"").
 A third emotional effect is the ""fear of missing out"" (FOMO), which is defined as the ""pervasive apprehension that others might be having rewarding experiences from which one is absent.""[207] FOMO has been classified by some as a form of social anxiety.[208] It is associated with checking updates on friends' activities on social media.[207] Some speculate that checking updates on friends' activities on social media may be associated with negative influences on people's psychological health and well-being because it could contribute to negative mood and depressed feelings.[209][210] Looking at friends' stories or posts of them attending parties, music festivals, vacations and other events on various social media applications can lead users to feel left out and upset because they are not having as much fun as others. This is a very common issue between young people using certain apps and it continues to affect their personal well-being.[211]
 On the other hand, social media can sometimes have a supportive effect on individuals who use it. Twitter has been used more by the medical community.[212] While Twitter can facilitate academic discussion among health professionals and students, it can also provide a supportive community for these individuals by fostering a sense of community and allowing individuals to support each other through tweets, likes, and comments.[213] Access to social media has also been seen a way to keep older adults connected, after the deaths of partners and the increased geographical distance between friends and loved ones.[214]
 The digital divide is a measure of disparity in the level of access to technology between households, socioeconomic levels or other demographic categories.[215][216] People who are homeless, living in poverty, elderly people and those living in rural or remote communities may have little or no access to computers and the Internet; in contrast, middle class and upper-class people in urban areas have very high rates of computer and Internet access. Other models argue that within a modern information society, some individuals produce Internet content while others only consume it,[217][218] which could be a result of disparities in the education system where only some teachers integrate technology into the classroom and teach critical thinking.[219] While social media has differences among age groups, a 2010 study in the United States found no racial divide.[220] Some zero-rating programs offer subsidized data access to certain websites on low-cost plans. Critics say that this is an anti-competitive program that undermines net neutrality and creates a ""walled garden""[221] for platforms like Facebook Zero. A 2015 study found that 65% of Nigerians, 61% of Indonesians, and 58% of Indians agree with the statement that ""Facebook is the Internet"" compared with only 5% in the US.[222]
 Eric Ehrmann contends that social media in the form of public diplomacy create a patina of inclusiveness that covers[223] traditional economic interests that are structured to ensure that wealth is pumped up to the top of the economic pyramid, perpetuating the digital divide and post-Marxian class conflict. He also voices concern over the trend that finds social utilities operating in a quasi-libertarian global environment of oligopoly that requires users in economically challenged nations to spend high percentages of annual income to pay for devices and services to participate in the social media lifestyle. Neil Postman also contends that social media will increase an information disparity between ""winners"" who are able to use the social media actively and ""losers"" who are not familiar with modern technologies or who do not have access to them. People with high social media skills may have better access to information about job opportunities, potential new friends, and social activities in their area, which may enable them to improve their standard of living and their quality of life.
 According to the Pew Research Center and other research works, a majority of Americans at least occasionally receive news from social media.[224][225] Because of recommendation algorithms on social media which filter and display news content which are likely to match their users' political preferences (known as a filter bubble), a potential impact of receiving news from social media includes an increase in political polarization due to selective exposure (see also: algorithmic radicalization). Political polarization refers to when an individual's stance on a topic is more likely to be strictly defined by their identification with a specific political party or ideology than on other factors. Selective exposure occurs when an individual favors information that supports their beliefs and avoids information that conflicts with their beliefs. A 2016 study using U.S. elections, conducted by Evans and Clark, revealed gender differences in the political use of Twitter between candidates.[226] Whilst politics is a male dominated arena, on social media the situation appears to be the opposite, with women discussing policy issues at a higher rate than their male counterparts. The study concluded that an increase in female candidates directly correlates to an increase in the amount of attention paid to policy issues, potentially heightening political polarization.[227]
 Efforts to combat selective exposure in social media may also cause an increase in political polarization.[228] A study examining Twitter activity conducted by Bail et al. paid Democrat and Republican participants to follow Twitter handles whose content was different from their political beliefs (Republicans received liberal content and Democrats received conservative content) over a six-week period.[228] At the end of the study, both Democrat and Republican participants were found to have increased political polarization in favor of their own parties, though only Republican participants had an increase that was statistically significant.[228]
 Though research has shown evidence that social media plays a role in increasing political polarization, it has also shown evidence that social media use leads to a persuasion of political beliefs.[229][230] An online survey consisting of 1,024 U.S. participants was conducted by Diehl, Weeks, and Gil de Zuñiga, which found that individuals who use social media were more likely to have their political beliefs persuaded than those who did not.[229] In particular, those using social media as a means to receive their news were the most likely to have their political beliefs changed.[229] Diehl et al. found that the persuasion reported by participants was influenced by the exposure to diverse viewpoints they experienced, both in the content they saw as well as the political discussions they participated in.[229] Similarly, a study by Hardy and colleagues conducted with 189 students from a Midwestern state university examined the persuasive effect of watching a political comedy video on Facebook.[230] Hardy et al. found that after watching a Facebook video of the comedian/political commentator John Oliver performing a segment on his show, participants were likely to be persuaded to change their viewpoint on the topic they watched (either payday lending or the Ferguson protests) to one that was closer to the opinion expressed by Oliver.[230] Furthermore, the persuasion experienced by the participants was found to be reduced if they viewed comments by Facebook users which contradicted the arguments made by Oliver.[230]
 Research has also shown that social media use may not have an effect on polarization at all.[231] A U.S. national survey of 1,032 participants conducted by Lee et al. found that participants who used social media were more likely to be exposed to a diverse number of people and amount of opinion than those who did not, although using social media was not correlated with a change in political polarization for these participants.[231]
 In a study examining the potential polarizing effects of social media on the political views of its users, Mihailidis and Viotty suggest that a new way of engaging with social media must occur to avoid polarization.[232] The authors note that media literacies (described as methods which give people skills to critique and create media) are important to using social media in a responsible and productive way, and state that these literacies must be changed further in order to have the most effectiveness.[232] In order to decrease polarization and encourage cooperation among social media users, Mihailidis and Viotty suggest that media literacies must focus on teaching individuals how to connect with other people in a caring way, embrace differences, and understand the ways in which social media has a real impact on the political, social, and cultural issues of the society they are a part of.[232]
 Recent research has demonstrated that social media, and media in general, have the power to increase the scope of stereotypes not only in children but people of all ages.[233] Both cases of stereotyping of the youth and the elderly are prime examples of ageism. The presumed characteristics of the individual being stereotyped can have both negative and positive connotations but frequently carry an opposing viewpoint. For example, the youth on social media platforms are often depicted as lazy, immature individuals who oftentimes have no drive or passion for other activities.[234] For example, during the COVID-19 pandemic, much of the youth were accused for the spread of the disease and were blamed for the continuous lockdowns across the world.[235] These misrepresentations make it difficult for the youth to find new efforts and prove others wrong, especially when a large group of individuals believe that the stereotypes are highly accurate. Considering the youthful groups that are present on social media are frequently in a new stage of their lives and preparing to make life-changing decisions, it is essential that the stereotypes are diminished so that they do not feel invalidated. Further, stereotyping often occurs for the elderly as they are presumed to be a group of individuals who are unaware of the proper functions and slang usage on social media.[236] These stereotypes often seek to exclude older generations from participating in trends or engaging them in other activities on digital platforms.
 Social media has allowed for mass cultural exchange and intercultural communication. As different cultures have different value systems, cultural themes, grammar, and world views, they also communicate differently.[237] The emergence of social media platforms fused together different cultures and their communication methods, blending together various cultural thinking patterns and expression styles.[238][better source needed]
 Social media has affected the way youth communicate, by introducing new forms of language.[239] Abbreviations have been introduced to cut down on the time it takes to respond online. The commonly known ""LOL"" has become globally recognized as the abbreviation for ""laugh out loud"" thanks to social media and use by people of all ages particularly as people grow up.
 Another trend that influences the way youth communicates is the use of hashtags. With the introduction of social media platforms such as Twitter, Facebook, and Instagram the hashtag was created to easily organize and search for information. Hashtags can be used when people want to advocate for a movement, store content or tweets from a movement for future use, and allow other social media users to contribute to a discussion about a certain movement by using existing hashtags. Using hashtags as a way to advocate for something online makes it easier and more accessible for more people to acknowledge it around the world.[240] As hashtags such as #tbt (""throwback Thursday"") become a part of online communication, it influenced the way in which youth share and communicate in their daily lives. Because of these changes in linguistics and communication etiquette, researchers of media semiotics[who?] have found that this has altered youth's communications habits and more.[241][vague]
 Social media is a great way to learn about your community and the world around you, but as social media progressed younger audiences have lowered their ability to effectively communicate. Because of the digital nature, teens have stopped worrying about the consequences that social media has. They often do not think about what they are sending and take longer to figure out what to say. In return, during real-life settings, it's harder for them to carry on conversations. Social media also creates a toxic environment where people cyberbully each other, so in person they act the same way and do not worry about the consequences. This can not only affect themselves but people around them.[242]
 Social media has offered a new platform for peer pressure with both positive and negative communication. From Facebook comments to likes on Instagram, how the youth communicate, and what is socially acceptable is now heavily based on social media.[243] Social media does make kids and young adults more susceptible to peer pressure. The American Academy of Pediatrics has also shown that bullying, the making of non-inclusive friend groups, and sexual experimentation have increased situations related to cyberbullying, issues with privacy, and the act of sending sexual images or messages to someone's mobile device. This includes issues of sexting and revenge porn among minors, and the resulting legal implications and issues, and resulting risk of trauma.[244][245][246][247] On the other hand, social media also benefits the youth and how they communicate.[248] Adolescents can learn basic social and technical skills that are essential in society.[248] Through the use of social media, kids and young adults are able to strengthen relationships by keeping in touch with friends and family, making more friends, and participating in community engagement activities and services.[10]
 Due to the business model of social media platforms - which is based on selling slots of highly personalized ads to advertising partners by collecting large amounts of user data - these platforms incentivize the distribution of content that keeps users on the platform for as long as possible. Socio-psychological research has already shown that populist, often extreme content in particular encourages users to stay on these platforms for longer, which in turn leads to such content being prioritized by the platforms' algorithms purely for economic reasons.[249] Various whistleblowers have already highlighted this problem with the platforms on several occasions. It is therefore a prime example of negative social externalities that are actually unplanned with the spread of technology, but are incentivized due to the business model and the way the technology works.
 Until recently, the narrative of self-regulation by platform providers, who determined the rules and processes of content moderation and the design of the algorithms used, largely prevailed in the European Union.[250] At the end of 2020, the European Commission presented two legislative proposals: The Digital Services Act (DSA) and the Digital Markets Act (DMA). Both proposals were adopted by the European Parliament in July 2022. The DSA will enter into force on 17 February 2024, the DMA in March 2024.[251] This legislation can basically be summarized in the following four objectives, articulated by MEPs: ""What is illegal offline must also be illegal online"".[252]
 ""Very large online platforms"" must therefore, among other things (a) delete illegal content (Russian propaganda, election interference, hate crimes and online harms such as harassment and child abuse) and better protect fundamental rights, (b) redesign their systems to ensure a ""high level of privacy, security and protection of minors"", by prohibiting advertising based on personal data, redesigning recommender systems to minimize risks for children and demonstrating this to the European Commission in a risk assessment, and (c) not using sensitive personal data such as race, gender and religion to target users with advertising.[253]  The legislative package therefore requires extensive content moderation and adaptation of the respective algorithm.
 According to the directive, a company that does not comply with the law could face a complete ban in Europe or fines of up to 6% of its global turnover. It remains to be seen whether this will actually have a deterrent effect on large platform providers such as Meta. Iverna McGowan, the director of the Centre for Democracy & Technology's Europe office, said that civil society in particular has a role to play in overseeing platforms, but also that national authorities lack adequate resources to enforce the law.[254]
 The regulatory problem is that both the prescribed rules for content moderation and the corresponding adaptation of the algorithms require extensive intervention by the platforms. However, it is not in their financial interest to identify and delete polarizing content, as they are incentivized to disseminate divisive content due to their advertising-based business model.[249]
 It is therefore unlikely that digital platforms such as Meta will make sufficient adjustments - also because the effectiveness of monitoring mechanisms to ensure that companies comply with the new European regulations will be low due to resource issues.Another problem is that, according to current regulations under the DSA, ""a country can have information deleted that is only illegal there but is not a problem at all elsewhere"", says Patrick Breyer (MEP).[255] If, for example, Hungary deletes a video that is critical of Viktor Orbán's government from the internet throughout the EU, this creates a problem. A different policy approach is therefore needed to solve the problem.
 Representatives of Ashoka's Tech & Humanity initiative, ""a global network of leading social entrepreneurs committed to ensuring tech works for the good of people and planet"" and 2018 Nobel Laureate Paul Romer[256] are in favor of taxing negative externalities of social media platforms due to the resource problem.[249] Similar to a CO2 tax - the resulting negative social effects should be measured and compensated for by a financial levy on the part of the companies.[257] The capital raised could then be used for awareness campaigns or education to offset negative effects such as political polarization, social division or increased suicide rates among minors. However, no consensus has yet emerged in the scientific community on how to measure the corresponding damage and convert it into a tax.
 Another proposal that is and has been the subject of lively academic debate and complements the proposals implemented by the EU Commission on data privacy, consumer protection and the fundamental intermediary liability of social media platforms is competition law.[258] The basic idea is to prevent the emergence of overly strong platforms or to restrict the market power of existing platforms by controlling mergers ex ante and tightening the relevant competition law. This is to be achieved through a supranational enforcement mechanism and the deterrent effect of high fines.
 Criticisms range from ease of use of specific platforms and their capabilities, disparity of information available, issues with trustworthiness and reliability of information presented,[259] impact of use on an individual's concentration, mental health,[260] ownership of media content, and the meaning of interactions created by social media. Although some social media platforms offer users the opportunity to cross-post between independently-run servers, the dominant social network platforms have been criticized for poor interoperability between platforms, which leads to the creation of information silos.[261] However, it is also argued that social media has positive effects, such as the democratization of the Internet[5] allowing individuals to advertise themselves and form friendships. Some note the term ""social"" cannot account for technological features of a platform alone, hence the level of sociability should be determined by the actual performances of its users.[262] There has been a decrease in face-to-face interactions as more social media platforms have been introduced, with the threat of cyberbullying and online sexual predators being more prevalent.[263][264] Social media may expose children to images of alcohol, tobacco, and sexual behaviors.[relevant?][265] It has been proven that individuals with no experience of cyber-bullying, often have better well-being than individuals who have been bullied online.[266]
 Twitter is increasingly a target of heavy activity of marketers. Their actions focuse on gaining massive numbers of followers, include use of advanced scripts and manipulation techniques.[267] British-American entrepreneur and author Andrew Keen criticized social media in his 2007 book The Cult of the Amateur, writing, ""Out of this anarchy, it suddenly became clear that what was governing the infinite monkeys now inputting away on the Internet was the law of digital Darwinism, the survival of the loudest and most opinionated. Under these rules, the only way to intellectually prevail is by infinite filibustering.""[268]
This is also relative to the issue ""justice"" in the social network. For example, the phenomenon ""Human flesh search engine"" in Asia raised the discussion of ""private-law"" brought by social network platform. Professor José van Dijck contends in her book The Culture of Connectivity (2013) that to understand the full weight of social media, their technological dimensions should be connected to the social and the cultural. She critically describes six social media platforms. One of her findings is the way Facebook had been successful in framing the term 'sharing' in such a way that third party use of user data is neglected in favor of intra-user connectedness. The fragmentation of modern society, in part due to social media, has been likened to a modern Tower of Babel.[269]
 Social media has become a regular source of news and information. Users have become so reliant that a 2021 poll by the Pew Research Center found roughly 70% of users regularly get their news from social media.[6] These platform's reliability and trustworthiness are questionable as a result of the amount of fake news and misinformation present. This is due to the lack of verification and regulation on information posted on platforms, as users can anonymously create posts containing misinformation and pass it off as truthful. While some social media platforms employ fact-checking to combat this,[270][271][272] most do not or do not employ it enough. Platforms have been found to magnify the spread of misinformation. In 2018, researchers found that fake news spreads almost 70% faster than truthful news on Twitter.[9] Social media accelerates misinformation as a result of two main reasons. Firstly, due to the heavy prevalence of bots on social media. Bad actors can use bots to mass post misinformation. Bots generate and publish posts significantly faster than human users, thus creating a platform where fake news posts greatly outnumber truthful reports.[13] Most platforms attempt to combat botting by human verification, yet even with these, heavy botting is still an issue.[14] Secondly fake news tends to receive more user engagement. Fake news posts contain ""novel"" or more ""new"" information, meaning users out of curiosity are more likely to engage. Because the posts receive more engagement they are more likely to be recommended, thus mass spreading the misinformation.[27]
 This issue becomes particularly bad in the immediate aftermath of an event before much information is known.[24] These ""information holes"" in the wake of an eventbecome filled by speculation and false information, which are shared by other users and sometimes by news organizations, amplifying misinformation in a feedback loop.[17] An example is the BLM protests in 2020. Users were exposed to fake news surrounding the protests resulting in political divisions and increased racial tension. As a result, a 2022 study found users exposed to fake news during that time were more likely to be against the protests than users who interacted with mostly accurate news.[25][26]
 Evgeny Morozov, a fellow at Georgetown University, contended that information uploaded to Twitter may have little relevance to the masses of people who do not use Twitter. In an article for the magazine Dissent titled ""Iran: Downside to the 'Twitter Revolution'"", Morozov wrote:
 [B]y its very design Twitter only adds to the noise: it's simply impossible to pack much context into its 140 characters. All other biases are present as well: in a country like Iran it's mostly pro-Western, technology-friendly and iPod-carrying young people who are the natural and most frequent users of Twitter. They are a tiny and, most important, extremely untypical segment of the Iranian population (the number of Twitter users in Iran — a country of more than seventy million people — was estimated at less than twenty thousand before the protests).[273] Professor Matthew Auer casts doubt on the conventional wisdom that social media are open and participatory. He speculates on the emergence of ""anti-social media"" used as ""instruments of pure control"".[274]
 Social media 'mining' is a type of data mining, a technique of analyzing data to detect patterns. Social media mining is a process of representing, analyzing, and extracting actionable patterns from data collected from people's activities on social media. Google mines data in many ways including using an algorithm in Gmail to analyze information in emails. This use of the information will then affect the type of advertisements shown to the user. Facebook has partnered with data mining companies such as Datalogix and BlueKai to use customer information for targeted advertising.[275] Massive amounts of data from social platforms allows scientists and machine learning researchers to extract insights and build product features.[276]
 Ethical questions of the extent to which a company should be able to utilize a user's information have arisen.[225] Users tend to click through Terms of Use agreements when signing up on platforms, and do not know how their information will be used. This leads to questions of privacy and surveillance when user data is recorded. Some social media outlets have added capture time and geotagging that helps provide information about the context of the data as well as making their data more accurate.
 In 2018, in a US Senate hearing held in response to revelations of data harvesting by Cambridge Analytica, Facebook chief executive Mark Zuckerberg faced questions, from privacy to the company's business model and mishandling of data. This was prompted by the revelation that Cambridge Analytica, a political consulting firm linked to the Trump campaign, harvested the data of an estimated 87 million Facebook users to psychologically profile voters during the 2016 United States presidential election. Zuckerberg was pressed to account for how third-party partners could take data without users' knowledge. Lawmakers questioned him on the proliferation of fake news on Facebook, Russian interference during the election and censorship of conservative media.[277]
 For Malcolm Gladwell, the role of social media in revolutions and protests is overstated.[278] On the one hand, social media makes it easier for individuals, and in this case activists, to express themselves. On the other, it is harder for that expression to have an impact.[278] Gladwell distinguishes between social media activism and high-risk activism, which brings real change. Activism and especially high-risk activism involves strong-tie relationships, hierarchies, coordination, motivation, exposing oneself to high risks, making sacrifices.[278] Gladwell discusses that social media are built around weak ties and argues that ""social networks are effective at increasing participation—by lessening the level of motivation that participation requires.""[278] According to him, ""Facebook activism succeeds not by motivating people to make a real sacrifice, but by motivating them to do the things that people do when they are not motivated enough to make a real sacrifice.""[278]
 Disputing Gladwell's theory, in the study ""Perceptions of Social Media for Politics: Testing the Slacktivism Hypothesis"" (2018), Nojin Kwak and colleagues conducted a survey which found that people who are politically expressive on social media are more likely to participate in offline political activity.[279]
 Social media content is generated by users. There has always been a huge debate on the ownership of the content because it is generated by the users and hosted by the company. Added to this is the danger to the security of information, which can be leaked to third parties with economic interests in the platform, or those who comb the data for their own databases.[280]
 In order for social media platforms to publish user content online, they must be issued a license from the copyright owners. A license is a legitimate right that allows them to carry out a specific task. Users grant a platform permission to use their content in accordance with its terms and conditions, even if users control the content. Although each platform's terms are different, generally they all give social media sites permission to utilize users' copyrighted works however they see fit.[281] Theoretically, platforms could make commercial use of and even sell or sublicense their license, and because each license specifically states that it is ""royalty free"", users would not be entitled to a share of the revenue.
 After being acquired by Facebook in 2012, Instagram revealed it intended to use user posts in adverts without seeking permission from or paying its users.[282][283] It backed down from these changes, with then-CEO Kevin Systrom writing in a blog post that ""it's not our intention to sell your photos"" and promising to update the terms of service to clarify this point.[284][285]
 Privacy rights advocates warn users about the collection of their personal data. Some information is captured without the user's knowledge or consent through electronic tracking and third-party applications. Data may be collected for law enforcement and governmental purposes,[274] by social media intelligence using data mining techniques.[280] Data and information may be collected for third party use. When information is shared on social media, that information is no longer private. There have been many cases in which young persons especially, share personal information, which can attract predators. It is important for users to monitor what they share and be aware of who they could potentially be sharing that information with. Teens are much more likely to share their personal information, such as email address, phone number, and school names.[286] Studies suggest teens are not aware of how much information can be accessed by third parties.
 There are arguments that ""privacy is dead"" and that with social media growing, some heavy social media users appear to have become unconcerned with privacy. Others argue, that people are still concerned about privacy, but are ignored by the companies running social networks, who profit from personal information. There is a disconnect between social media user's words and their actions. Studies suggest that surveys show that people want to keep their lives private, but their actions suggest otherwise. Every time someone creates a new social media account, they provide personal information that can include their name, birthdate, geographic location, and personal interests. Companies collect data on user behaviors. This data is stored and leveraged by companies to better target advertising to their users.[287] Another factor is ignorance of how accessible social media posts are. Some social media users who have been criticized for inappropriate comments stated that they did not realize that anyone outside their circle would read their posts. In fact, on some sites, unless a user selects higher privacy settings, their content is shared with a wide audience.
 According to a 2016 article on sharing privately and the effect social media has on expectations of privacy, ""1.18 billion people will log into their Facebook accounts, 500 million tweets will be sent, and there will be 95 million photos and videos posted on Instagram"" in a day. Much of the privacy concerns individuals face stem from their posts on a social network. Users have the choice to share voluntarily and this has become routine and normative for many. Social media is a community created on the behaviors of sharing, posting, liking, and communicating. Sharing has become a phenomenon which social media has accelerated.[288] Some desire privacy in some shape or form, yet also contribute to social media, which makes it difficult to maintain privacy.[289] Mills offers options for reform which include copyright and the application of the law of confidence; more radically, a change to the concept of privacy itself.
 A 2014 Pew Research Center survey found that 91% of Americans ""agree"" or ""strongly agree"" that people have lost control over how personal information is collected and used by all kinds of entities. Some 80% of social media users said they were concerned about advertisers and businesses accessing the data they share on social media platforms, and 64% said the government should do more to regulate advertisers.[290] The The Wall Street Journal stated in 2019, that according to UK law, Facebook did not protect certain aspects of user data.[291]
 The US government announced banning TikTok and WeChat from the States over national security concerns. The shutdown was announced for September 20, 2020. Access to TikTok was extended until 12 November 2020,[292] and a federal court ruling on October 30, 2020, has blocked further implementation of restrictions that would lead to TikTok's shutdown.[293]
 Additionally, in 2019 the Pentagon issued guidance to the US Army, Navy, Air Force, Marine Corps, Coast Guard and other government agencies that identified ""the potential risk associated with using the TikTok app and directs appropriate action for employees to take in order to safeguard their personal information.""[294] As a result, the Army, Navy, Air Force, Marine Corps, Coast Guard, Transportation Security Administration, and Department of Homeland Security banned the installation and use of TikTok on government devices, including blacklisting it on intranet services.[295]
 The commercial development of social media has been criticized as the actions of consumers in these settings have become increasingly value-creating, for example when consumers contribute to the marketing and branding of specific products by posting positive reviews. As such, value-creating activities also increase the value of a specific product, which could, according to marketing professors Bernad Cova and Daniele Dalli (2009), lead to what they refer to as ""double exploitation"".[296]
 As social media usage has become increasingly widespread, social media has to a large extent come to be subjected to commercialization by marketing companies and advertising agencies.[297] In 2014 Christofer Laurell, a digital marketing researcher, suggested that the social media landscape currently consists of three types of places because of this development: consumer-dominated places, professionally dominated places and places undergoing commercialization.[298] As social media becomes commercialized, this process has been shown to create novel forms of value networks stretching between consumer and producer[299] in which a combination of personal, private and commercial contents are created.[300]
 Social media addiction[301] has various social effects.
 As one of the biggest preoccupations among adolescents is social media usage, in 2011 researchers began using the term ""Facebook addiction disorder"" (F.A.D.), a form of internet addiction disorder.[302] FAD is characterized by compulsive use of the social networking site Facebook, which generally results in physical or psychological complications. The disorder, although not classified in the latest Diagnostic and Statistical Manual of Mental Disorders (DSM-5) or by the World Health Organization, has been the subject of several studies focusing on the negative effects of social media use on the psyche. One German study published in 2017 investigated a correlation between excessive use of the social networking site and narcissism; the results were published in the journal PLoS One. According to the findings: ""FAD was significantly positively related to the personality trait narcissism and to negative mental health variables (depression, anxiety, and stress symptoms)"".[303][304]
 In 2020, Netflix released The Social Dilemma, which raises concerns about the problematic effects of social media. In the documentary, mental health experts and former employees of social media companies explain how social media is designed to be addictive. One example that's shown is when an AI detects that someone has not visited Facebook for some time, it may choose different notifications that it predicts are most likely to cause them to re-visit the platform. This AI takes into account everything that each person has done on that platform.
 The documentary also raises concerns about the correlation between child and teen suicides and suicide attempts and increasing social media usage in the United States, particularly usage on mobile.[305]
 Turning off social media notifications temporarily or long-term may help reduce problematic social media use.[306] In certain cases and for some users, changes in their web browsing environments can be helpful in compensating for self-regulatory problems. For instance, a study involving 157 online learners on massive open online courses examined the impact of self-regulatory intervention on learners' web browsing behavior. The results showed that, on average, learners spend half of their time online on YouTube and social media, and less than 2% of visited websites account for nearly 80% of their time spent online. Further, the study found that modifying the learners' web environment, specifically by providing support in self-regulation, was associated with changes in behavior, including a reduction in time spent online, particularly on websites related to entertainment. This suggests there is a potential for interventions to improve self-regulatory skills, which may effectively help learners reduce excessive social media usage and manage their signs of social media misuse more effectively.[307]
 Having social media in the classroom was a controversial topic in the 2010s. Many parents and educators have been fearful of the repercussions of having social media in the classroom.[308] There are concerns that social media tools can be misused for cyberbullying or sharing inappropriate content. As result, cell phones have been banned from some classrooms, and some schools have blocked many popular social media websites. Many schools have realized that they need to loosen restrictions, teach digital citizenship skills, and even incorporate these tools into classrooms. Some schools permit students to use smartphones or tablet computers in class, as long as the students are using these devices for academic purposes, such as doing research. Using Facebook in class allows for the integration of multimodal content such as student-created photographs and video and URLs to other texts, in a platform that many students are already familiar with. Twitter can be used to enhance communication building and critical thinking and it provides students with an informal ""back channel"", and extend discussion outside of class time.
 Social media often features in political struggles to control public perception and online activity. In some countries, Internet police or secret police monitor or control citizens' use of social media. For example, in 2013 some social media was banned in Turkey after the Taksim Gezi Park protests. Both Twitter and YouTube were temporarily suspended in the country by a court's decision. A new law, passed by Turkish Parliament, has granted immunity to Telecommunications Directorate (TİB) personnel. The TİB was also given the authority to block access to specific websites without the need for a court order.[309] Yet TİB's 2014 blocking of Twitter was ruled by the constitutional court to violate free speech.[310] More recently, in the 2014 Thai coup d'état, the public was explicitly instructed not to 'share' or 'like' dissenting views on social media or face prison. In July of that same year, in response to WikiLeaks' release of a secret suppression order made by the Victorian Supreme Court, media lawyers were quoted in the Australian media to the effect that ""anyone who tweets a link to the WikiLeaks report, posts it on Facebook, or shares it in any way online could also face charges"".[311] On 27 July 2020, in Egypt, two women were sentenced to two years of imprisonment for posting TikTok videos, which the government claims are ""violating family values"".[312]
 Mastodon, GNU social, Diaspora, Friendica and other compatible software packages operate as a loose federation of mostly volunteer-operated servers, called the Fediverse, which connect with each other through the open source protocol ActivityPub. In early 2019, Mastodon successfully blocked the spread of violent right-wing extremism when the Twitter alternative Gab tried to associate with Mastodon, and their independent servers quickly contained its dissemination.[313]
 In December 2019, Twitter CEO Jack Dorsey made a similar suggestion, stating that efforts would be taken to achieve an ""open and decentralized standard for social media"". Rather than ""deplatforming"", such standards would allow a more scalable, and customizable approach to content moderation and censorship, and involve a number of companies, in the way that e-mail servers work.[citation needed]
 Deplatforming is a form of Internet censorship in which controversial speakers or speech are suspended, banned, or otherwise shut down by social media platforms and other service providers that normally provide a venue for free expression.[314] These kinds of actions are similar to alternative dispute resolution.[315]: 4  As early as 2015, platforms such as Reddit began to enforce selective bans based, for example, on terms of service that prohibit ""hate speech"".[316] According to technology journalist Declan McCullagh, ""Silicon Valley's efforts to pull the plug on dissenting opinions"" have included, as of 2018[update], Twitter, Facebook, and YouTube ""devising excuses to suspend ideologically disfavored accounts"".[317]
 Most people see social media platforms as censoring objectionable political views.[318]
 According to Danah Boyd (2011), the media plays a large role in shaping people's perceptions of specific social networking services. When looking at the site MySpace, after adults started to realize how popular the site was becoming with teens, news media became heavily concerned with teen participation and the potential dangers they faced using the site. As a result, teens avoided joining the site because of the associated risks (e.g. child predators and lack of control), and parents began to publicly denounce the site. Ultimately, the site was labeled as dangerous, and many were detracted from interacting with the site.[319]
 As Boyd also describes, when Facebook initially launched in 2004, it solely targeted college students and access was intentionally limited. Facebook started as a Harvard-only social networking service before expanding to all other Ivy League schools. It then made its way to other top universities and ultimately to a wider range of schools. Because of its origins, some saw Facebook as an ""elite"" social networking service. While it was very open and accepting to some, it seemed to outlaw and shun most others who did not fit that ""elite"" categorization. These narratives propagated by the media influenced the large movement of teenage users from one social networking service to another.[319]
 According to LikeWar: The Weaponization of Social Media,[320] the use of effective social media marketing techniques is not only limited to celebrities, corporations, and governments, but also extremist groups to carry out political objectives.[321] The use of social media by ISIS and Al-Qaeda has been used to influence in areas of operation and gain the attention of sympathizers. Social media platforms like YouTube, Twitter, Facebook, and encrypted-messaging applications have been used to increase recruiting of members, both locally and internationally.[322] Larger platforms like YouTube, Twitter, and others have received backlash for allowing this content (see Use of social media by the Islamic State of Iraq and the Levant). The use of social media to further extremist objectives is not only limited to Islamic terrorism, but also extreme nationalist groups, and more prominently, US right-wing extremist. As many traditional social media platforms banned hate speech, several platforms have become popular among right-wing extremists to carry out planning and communication including of events; these application became known as ""Alt-tech"". Platforms such as Telegram, Parler, and Gab were used during the January 6 United States Capitol attack, to coordinate attacks.[323] Members shared tips on how to avoid law enforcement and their plans on carrying out their objectives; users called for killings of law enforcement and politicians.[324]
 Social media content, like most content on the web, will continue to persist unless the user deletes it. This brings up the inevitable question of what to do once a social media user dies, and no longer has access to their content.[325] As it is a topic that is often left undiscussed, it is important to note that each social media platform, e.g., Twitter, Facebook, Instagram, LinkedIn, and Pinterest, has created its own guidelines for users who have died.[326] In most cases on social media, the platforms require a next-of-kin to prove that the user is deceased, and then give them the option of closing the account or maintaining it in a 'legacy' status.
 facilitates the building of relations
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['sleep disturbance', 'Women', 'it is a topic that is often left undiscussed', 'payday lending or the Ferguson protests', 'Ethical questions'], 'answer_start': [], 'answer_end': []}"
"A content management system (CMS) is computer software used to manage the creation and modification of digital content (content management).[1][2][3]
A CMS is typically used for enterprise content management (ECM) and web content management (WCM).
 ECM typically supports multiple users in a collaborative environment[4][5] by integrating document management, digital asset management, and record retention.[4]
 Alternatively, WCM is the collaborative authoring for websites and may include text and embed graphics, photos, video, audio, maps, and program code that display content and interact with the user.[6][7] ECM typically includes a WCM function.
 A CMS typically has two major components: a content management application (CMA), as the front-end user interface that allows a user, even with limited expertise, to add, modify, and remove content from a website without the intervention of a webmaster; and a content delivery application (CDA), that compiles the content and updates the website.
 There are two types of CMS installation: on-premises and cloud-based. On-premises installation means that the CMS software can be installed on the server. This approach is usually taken by businesses that want flexibility in their setup. Notable CMSs which can be installed on-premises are Wordpress.org, Drupal, Joomla, Grav, ModX and others. 
 The cloud-based CMS is hosted on the vendor environment. Examples of notable cloud-based CMSs are SquareSpace, Contentful, Wordpress.com, Webflow, Ghost and WIX.
 The core CMS features are: indexing, search and retrieval, format management, revision control, and management.[4]
 Features may vary depending on the system application but will typically include:[4]
 Popular additional features may include:[4]
 Digital asset management systems are another type of CMS. They manage content with clearly-defined author or ownership, such as documents, movies, pictures, phone numbers, and scientific data. Companies also use CMSs to store, control, revise, and publish documentation.
 There are also component content management systems (CCMS), which are CMSs that manage content at a modular level rather than as pages or articles. CCMSs are often used in technical communication, where many publications reuse the same content.
 Based on a survey, the most widely used content management system is WordPress, used by 42.8% of the top 10 million websites as of October 2021. (although, per definition, it is a blog system/website generator, not a fully-fledged content management system), followed by Shopify and Joomla.[8][9]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['enterprise content management', 'Wordpress.org, Drupal, Joomla', 'blog system/website generator', 'Wordpress.com, Webflow, Ghost and WIX', 'compiles the content and updates the website'], 'answer_start': [], 'answer_end': []}"
