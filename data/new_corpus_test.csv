context,questions,answers
"
 An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.
 Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, peripherals, and other resources.
 For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer – from cellular phones and video game consoles to web servers and supercomputers.
 In the personal computer market, as of September 2023[update], Microsoft Windows holds a dominant market share of around 68%. macOS by Apple Inc. is in second place (20%), and the varieties of Linux, including ChromeOS, are collectively in third place (7%).[3] In the mobile sector (including smartphones and tablets), as of September 2023[update], Android's share is 68.92%, followed by Apple's iOS and iPadOS with 30.42%, and other operating systems with .66%.[4] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems),[5][6] such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.
 Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. USB stick).
 A single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running concurrently. This is achieved by time-sharing, where the available processor time is divided between multiple processes. These processes are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and cooperative types. In preemptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. Unix-like operating systems, such as Linux—as well as non-Unix-like, such as AmigaOS—support preemptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking; 32-bit versions of both Windows NT and Win9x used preemptive multi-tasking.
 Single-user operating systems have no facilities to distinguish users but may allow multiple programs to run in tandem.[7] A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users.
 A distributed operating system manages a group of distinct, networked computers and makes them appear to be a single computer, as all computations are distributed (divided amongst the constituent computers).[8]
 Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines with less autonomy (e.g. PDAs). They are very compact and extremely efficient by design and are able to operate with a limited amount of resources. Windows CE and Minix 3 are some examples of embedded operating systems.
 A real-time operating system is an operating system that guarantees to process events or data by a specific moment in time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. Such an event-driven system switches between tasks based on their priorities or external events, whereas time-sharing operating systems switch tasks based on clock interrupts.
 A library operating system is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with the application and configuration code to construct a unikernel: a specialized, single address space, machine image that can be deployed to cloud or embedded environments.[further explanation needed]
 Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s.[9] Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.
 In the 1940s, the earliest electronic digital systems had no operating systems. Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plugboards. These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards. After programmable general-purpose computers were invented, machine languages(consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981).[full citation needed]
 In the early 1950s, a computer could execute only one program at a time. Each user had sole use of the computer for a limited period and would arrive at a scheduled time with their program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the universal Turing machine.[9]
 Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and compiling (generating machine code from human-readable symbolic code). This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England, the job queue was at one time a washing line (clothesline) from which tapes were hung with different colored clothes-pegs to indicate job priority.[citation needed]
 By the late 1950s, programs that one would recognize as an operating system were beginning to appear. Often pointed to as the earliest recognizable example is GM-NAA I/O, released in 1956 on the IBM 704. The first known example that actually referred to itself was the SHARE Operating System, a development of GM-NAA I/O, released in 1959. In a May 1960 paper describing the system, George Ryckman noted:
 The development of computer operating systems have materially aided the problem of getting a program or series of programs on and off the computer efficiently.[10] One of the more famous examples that is often found in discussions of early systems is the Atlas Supervisor, running on the Atlas in 1962.[11] It was referred to as such in a December 1961 article describing the system, but the context of ""the Operating System"" is more along the lines of ""the system operates in the fashion"". The Atlas team itself used the term ""supervisor"",[12] which was widely used along with ""monitor"". Brinch Hansen described it as ""the most significant breakthrough in the history of operating systems.""[13]
 Through the 1950s, many major features were pioneered in the field of operating systems on mainframe computers, including batch processing, input/output interrupting, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications. In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094, which in turn influenced the later 7040-PR-150 (7040/7044) and 1410-PR-155 (1410/7010) operating systems.
 During the 1960s, IBM's OS/360 introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines. IBM's current mainframe operating systems are distant descendants of this original system and modern machines are backward compatible with applications written for OS/360.[citation needed]
 OS/360 also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and file locking during updates. When a process is terminated for any reason, all of these resources are re-claimed by the operating system.
 The alternative CP-67 system for the S/360-67 started a whole line of IBM operating systems focused on the concept of virtual machines. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: DOS/360[a] (Disk Operating System), TSS/360 (Time Sharing System), TOS/360 (Tape Operating System), BOS/360 (Basic Operating System), and ACP (Airline Control Program), as well as a few non-IBM systems: MTS (Michigan Terminal System), MUSIC (Multi-User System for Interactive Computing), and ORVYL (Stanford Timesharing System).
 Control Data Corporation developed the SCOPE operating system in the 1960s, for batch processing. In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.
 In 1961, Burroughs Corporation introduced the B5000 with the MCP (Master Control Program) operating system. The B5000 was a stack machine designed to exclusively support high-level languages with no assembler;[b] indeed, the MCP was the first OS to be written exclusively in a high-level language (ESPOL, a dialect of ALGOL). MCP also introduced many other ground-breaking innovations, such as being the first commercial implementation of virtual memory. MCP is still in use today in the Unisys company's MCP/ClearPath line of computers.
 UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems.[14][15][16] Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.
 General Electric developed General Electric Comprehensive Operating Supervisor (GECOS), which primarily supported batch processing. After its acquisition by Honeywell, it was renamed General Comprehensive Operating System (GCOS).
 Bell Labs,[c] General Electric and MIT developed Multiplexed Information and Computing Service (Multics), which introduced the concept of ringed security privilege levels.
 Digital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time-sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community. RT-11 was a single-user real-time OS for the PDP-11 class minicomputer, and RSX-11 was the corresponding multi-user OS.
 From the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/44, 360/75, 360/91, 360/95 and 360/195) were microprogrammed implementations.
 The enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:
 The first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minicomputers; minimalistic operating systems were developed, often loaded from ROM and known as monitors. One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft's MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM's version of it was called IBM DOS or PC DOS).
 In the 1980s, Apple Computer Inc. (now Apple Inc.) introduced the Apple Macintosh alongside its popular Apple II series of microcomputers. The Macintosh had an innovative graphical user interface (GUI) and a mouse; it ran an operating system later known as the (classic) Mac OS. 
 The introduction of the Intel 80386 CPU chip in October 1985,[17] with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier superminicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft's operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NeXTSTEP operating system. NeXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X (macOS after latest name change).
 The GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply ""Linux"" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD.
 Unix was originally written in assembly language.[18] Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History).
 The Unix-like family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name ""UNIX"" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. ""UNIX-like"" is commonly used to refer to the large set of operating systems which resemble the original UNIX.
 Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.
 Five operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, Sun Microsystems's Solaris can run on multiple types of hardware, including x86 and SPARC servers, and PCs. Apple's macOS, a replacement for Apple's earlier (non-Unix) classic Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD. IBM's z/OS UNIX System Services includes a shell and utilities based on Mortice Kerns' InterOpen products.
 Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.
 A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP.
 In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.
 Steve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web.
 Developers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. After two years of legal disputes, the BSD project spawned a number of free derivatives, such as NetBSD and FreeBSD (both in 1993), and OpenBSD (from NetBSD in 1995).
 macOS (formerly ""Mac OS X"" and later ""OS X"") is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. macOS is the successor to the original classic Mac OS, which had been Apple's primary operating system since 1984. Unlike its predecessor, macOS is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997.
The operating system was first released in 1999 as Mac OS X Server 1.0, followed in March 2001 by a client version (Mac OS X v10.0 ""Cheetah""). Since then, six more distinct ""client"" and ""server"" editions of macOS have been released, until the two were merged in OS X 10.7 ""Lion"".
 Prior to its merging with macOS, the server edition – macOS Server – was architecturally identical to its desktop counterpart and usually ran on Apple's line of Macintosh server hardware. macOS Server included work group management and administration software tools that provide simplified access to key network services, including a mail transfer agent, a Samba server, an LDAP server, a domain name server, and others. With Mac OS X v10.7 Lion, all server aspects of Mac OS X Server have been integrated into the client version and the product re-branded as ""OS X"" (dropping ""Mac"" from the name). The server tools are now offered as an application.[19]
 First introduced as the OpenEdition upgrade to MVS/ESA System Product Version 4 Release 3, announced[20] February 1993 with support for POSIX and other standards.[21][22][23] z/OS UNIX System Services is built on top of MVS services and cannot run independently. While IBM initially introduced OpenEdition to satisfy FIPS requirements, several z/OS component now require UNIX services, e.g., TCP/IP.
 The Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel.
 Linux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smartwatches. Although estimates suggest that Linux is used on only 2.81% of all ""desktop"" (or laptop) PCs,[3] it has been widely adopted for use in servers[28] and embedded systems[29] such as cell phones. 
 Linux has superseded Unix on many platforms and is used on most supercomputers, including all 500 most powerful supercomputers on the TOP500 list — having displaced all competitors by 2017.[30] Linux is also commonly used on other small energy-efficient computers, such as smartphones and smartwatches. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google's Android, ChromeOS, and ChromiumOS.
 Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to x86 architecture based computers. As of 2022[update], its worldwide market share on all platforms was approximately 30%,[31] and on the desktop/laptop platforms, its market share was approximately 75%.[32] The latest version is Windows 11.
 Microsoft Windows was first released in 1985, as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS[33][34] and 16-bit Windows 3.x[35] drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and Arm microprocessors.[36] In the past, Windows NT supported additional architectures.
 Server editions of Windows are widely used, however, Windows' usage on servers is not as widespread as on personal computers as Windows competes against Linux and BSD for server market share.[37][38]
 ReactOS is a Windows-alternative operating system, which is being developed on the principles of Windows – without using any of Microsoft's code.
 There have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; classic Mac OS, the non-Unix precursor to Apple's macOS; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications.
 The z/OS operating system for IBM z/Architecture mainframe computers is still being used and developed, and 
OpenVMS, formerly from DEC, is still under active development by VMS Software Inc. The IBM i operating system for IBM AS/400 and IBM Power Systems midrange computers is also still being used and developed.
 Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is MINIX, while for example Singularity is used purely for research. Another example is the Oberon System designed at ETH Zürich by Niklaus Wirth, Jürg Gutknecht and a group of students at the former Computer Systems Institute in the 1980s. It was used mainly for research, teaching, and daily work in Wirth's group.
 Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' Plan 9.
 The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.
 With the aid of firmware and device drivers, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc.
 The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program typically involves the creation of a process by the operating system kernel, which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program, which then interacts with the user and with hardware devices. However, in some systems an application can request that the operating system execute another application within the same process, either as a subroutine or in a separate thread, e.g., the LINK and ATTACH facilities of OS/360 and successors.
 An interrupt (also known as an abort, exception, fault, signal,[39] or trap)[40] provides an efficient way for most operating systems to react to the environment. Interrupts cause the central processing unit (CPU) to have a control flow change away from the currently running program to an interrupt handler, also known as an interrupt service routine (ISR).[41][42] An interrupt service routine may cause the central processing unit (CPU) to have a context switch.[43][d] The details of how a computer processes an interrupt vary from architecture to architecture, and the details of how interrupt service routines behave vary from operating system to operating system.[44] However, several interrupt functions are common.[44] The architecture and operating system must:[44]
 A software interrupt is a message to a process that an event has occurred.[39] This contrasts with a hardware interrupt — which is a message to the central processing unit (CPU) that an event has occurred.[45] Software interrupts are similar to hardware interrupts — there is a change away from the currently running process.[46] Similarly, both hardware and software interrupts execute an interrupt service routine.
 Software interrupts may be normally occurring events. It is expected that a time slice will occur, so the kernel will have to perform a context switch.[47] A computer program may set a timer to go off after a few seconds in case too much data causes an algorithm to take too long.[48]
 Software interrupts may be error conditions, such as a malformed machine instruction.[48] However, the most common error conditions are division by zero and accessing an invalid memory address.[48]
 Users can send messages to the kernel to modify the behavior of a currently running process.[48] For example, in the command-line environment, pressing the interrupt character (usually Control-C) might terminate the currently running process.[48]
 To generate software interrupts for x86 CPUs, the INT assembly language instruction is available.[49] The syntax is INT X, where X is the offset number (in hexadecimal format) to the interrupt vector table.
 To generate software interrupts in Unix-like operating systems, the kill(pid,signum) system call will send a signal to another process.[50] pid is the process identifier of the receiving process. signum is the signal number (in mnemonic format)[e] to be sent. (The abrasive name of kill was chosen because early implementations only terminated the process.)[51]
 In Unix-like operating systems, signals inform processes of the occurrence of asynchronous events.[50] To communicate asynchronously, interrupts are required.[52] One reason a process needs to asynchronously communicate to another process solves a variation of the classic reader/writer problem.[53] The writer receives a pipe from the shell for its output to be sent to the reader's input stream.[54] The command-line syntax is alpha | bravo. alpha will write to the pipe when its computation is ready and then sleep in the wait queue.[55] bravo will then be moved to the ready queue and soon will read from its input stream.[56] The kernel will generate software interrupts to coordinate the piping.[56]
 Signals may be classified into 7 categories.[50] The categories are:
 Input/Output (I/O) devices are slower than the CPU. Therefore, it would slow down the computer if the CPU had to wait for each I/O to finish. Instead, a computer may implement interrupts for I/O completion, avoiding the need for polling or busy waiting.[57]
 Some computers require an interrupt for each character or word, costing a significant amount of CPU time. Direct memory access (DMA) is an architecture feature to allow devices to bypass the CPU and access main memory directly.[58] (Separate from the architecture, a device may perform direct memory access[f] to and from main memory either directly or via a bus.)[59][g]
 When a computer user types a key on the keyboard, typically the character appears immediately on the screen. Likewise, when a user moves a mouse, the cursor immediately moves across the screen. Each keystroke and mouse movement generates an interrupt called Interrupt-driven I/O. An interrupt-driven I/O occurs when a process causes an interrupt for every character[59] or word[60] transmitted.
 Devices such as hard disk drives, solid state drives, and magnetic tape drives can transfer data at a rate high enough that interrupting the CPU for every byte or word transferred, and having the CPU transfer the byte or word between the device and memory, would require too much CPU time. Data is, instead, transferred between the device and memory independently of the CPU by hardware such as a channel or a direct memory access controller; an interrupt is delivered only when all the data is transferred.[61]
 If a computer program executes a system call to perform a block I/O write operation, then the system call might execute the following instructions:
 While the writing takes place, the operating system will context switch to other processes as normal. When the device finishes writing, the device will interrupt the currently running process by asserting an interrupt request. The device will also place an integer onto the data bus.[65] Upon accepting the interrupt request, the operating system will:
 When the writing process has its time slice expired, the operating system will:[66]
 With the program counter now reset, the interrupted process will resume its time slice.[44]
 Modern computers support multiple modes of operation. CPUs with this capability offer at least two modes: user mode and supervisor mode. In general terms, supervisor mode operation allows unrestricted access to all machine resources, including all MPU instructions. User mode operation sets limits on instruction use and typically disallows direct access to machine resources. CPUs might have other modes similar to user mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.
 At power-on or reset, the system begins in supervisor mode. Once an operating system kernel has been loaded and started, the boundary between user mode and supervisor mode (also known as kernel mode) can be established.
 Supervisor mode is used by the kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is accessed, and communicating with devices such as disk drives and video display devices. User mode, in contrast, is used for almost everything else. Application programs, such as word processors and database managers, operate within user mode, and can only access machine resources by turning control over to the kernel, a process which causes a switch to supervisor mode. Typically, the transfer of control to the kernel is achieved by executing a software interrupt instruction, such as the Motorola 68000 TRAP instruction. The software interrupt causes the processor to switch from user mode to supervisor mode and begin executing code that allows the kernel to take control.
 In user mode, programs usually have access to a restricted set of processor instructions, and generally cannot execute any instructions that could potentially cause disruption to the system's operation. In supervisor mode, instruction execution restrictions are typically removed, allowing the kernel unrestricted access to all machine resources.
 The term ""user mode resource"" generally refers to one or more CPU registers, which contain information that the running program is not allowed to alter. Attempts to alter these resources generally cause a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting; for example, by forcibly terminating (""killing"") the program.
 Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by the programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.
 Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.
 Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which does not exist in all computers.
 In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt, which causes the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.
 Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.
 The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.
 If a program tries to access memory that is not in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault.
 When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet.
 In modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.
 ""Virtual memory"" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.[67]
 Multitasking refers to the running of multiple independent computer programs on the same computer, giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.
 An operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch.
 An early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop.
 Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.
 The philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)
 On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. AmigaOS is an exception, having preemptive multitasking from its first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it did not reach the home user market until Windows XP (since Windows NT was targeted at professionals).
 Access to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use of the drive's available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree.
 Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.
 While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers.
 A connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices.
 When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.
 Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ReiserFS, Reiser4, ext3, ext4 and Btrfs in Linux. However, in practice, third party drivers are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software).
 Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on. It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system.
 A device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.
 The key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system's point of view.
 Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do.
 Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface.
 Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's IP address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.
 Many operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware.
 Security means protecting users from other users of the same computer, as well as from those who seeking remote access to it over a network.[68]  Operating systems security rests on achieving the CIA triad: confidentiality (unauthorized users cannot access data), integrity (unauthorized users cannot modify data), and availability (ensuring that the system remains available to authorized users, even in the event of a denial of service attack).[69] As with other computer systems, isolating security domains—in the case of operating systems, the kernel, processes, and virtual machines—is key to achieving security.[70] Other ways to increase security include simplicity to minimize the attack surface, locking access to resources by default, checking all requests for authorization, principle of least authority (granting the minimum privilege essential for performing a task), privilege separation, and reducing shared data.[71]
 Some operating system designs are more secure than others. Those with no isolation between the kernel and applications are least secure, while those with a monolithic kernel like most general-purpose operating systems are still vulnerable if any part of the kernel is compromised. A more secure design features microkernels that separate the kernel's privileges into many separate security domains and reduce the consequences of a single kernel breach.[72] Unikernels are another approach that improves security by minimizing the kernel and separating out other operating systems functionality by application.[72]
 Most operating systems are written in C or C++, which can cause vulnerabilities. Despite various attempts to protect against them, a substantial number of vulnerabilities are caused by buffer overflow attacks, which are enabled by the lack of bounds checking.[73]  Hardware vulnerabilities, some of them caused by CPU optimizations, can also be used to compromise the operating system.[74] Programmers coding the operating system may have deliberately implanted vulnerabilities, such as back doors.[75]
 Operating systems security is hampered by their increasing complexity and the resulting inevitability of bugs.[76] Because formal verification of operating systems may not be feasible, operating systems developers use hardening to reduce vulnerabilities,[77] such as address space layout randomization, control-flow integrity,[78] access restrictions,[79] and other techniques.[80] Anyone can contribute code to open source operating systems, which have transparent code histories and distributed governance structures.[81] Their developers work together to find and eliminate security vulnerabilities, using techniques such as code review and type checking to avoid malicious code.[82][83] Andrew S. Tanenbaum advises releasing the source code of all operating systems, arguing that it prevents the developer from falsely believing it is secret and relying on security by obscurity.[84]
 Every computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported. The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present.
 Most of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of the classic Mac OS, the GUI is integrated into the kernel.
 While technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of context switches required for the GUI to perform its output functions. Other operating systems are modular, separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and macOS are also built this way. Modern releases of Microsoft Windows such as Windows Vista implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between Windows NT 4.0 and Windows Server 2003 exist mostly in kernel space. Windows 9x had very little distinction between the interface and the kernel.
 Many computer operating systems allow the user to install or create any user interface they desire. The X Window System in conjunction with GNOME or KDE Plasma 5 is a commonly found setup on most Unix and Unix-like (BSD, Linux, Solaris) systems. A number of Windows shell replacements have been released for Microsoft Windows, which offer alternatives to the included Windows shell, but the shell itself cannot be separated from Windows.
 Numerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to COSE and CDE failed for various reasons, and were eventually eclipsed by the widespread adoption of GNOME and K Desktop Environment. Prior to free software-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).
 Graphical user interfaces evolve over time. For example, Windows has modified its user interface almost every time a new major version of Windows is released, and the Mac OS GUI changed dramatically with the introduction of Mac OS X in 1999.[85]
 A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.
 An early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System.
 Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase.[86] Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.
 Some embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing.
 A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.[citation needed]
 In some cases, hobby development is in support of a ""homebrew"" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is her/his own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
 Examples of a hobby operating system include Syllable and TempleOS.
 If an application is written for use on a specific operating system, and is ported to another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.
 This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.
 Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['page fault', 'Syllable and TempleOS', 'command-line interface', 'the hobbyist is her/his own developer', 'interacts with the user and with hardware devices'], 'answer_start': [], 'answer_end': []}"
"
 Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]
 Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. 
 The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.
 There are many types of robots; they are used in many different environments and for many different uses. Although diverse in application and form, they all share three basic aspects when it comes to their design and construction:
 As more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed ""assembly robots"". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a ""welding robot"" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as ""heavy-duty robots"".[3]
 Current and potential applications include:
 At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[15] 
Potential power sources could be:
 Actuators are the ""muscles"" of a robot, the parts which convert stored energy into movement.[16] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.
 The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.
 Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.
 Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[17] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[18] and walking humanoid robots.[19][20]
 The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[21] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[22] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[23] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.
 Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[24][25][26]
 Muscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[27][28]
 EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[29] and to enable new robots to float,[30] fly, swim or walk.[31]
 Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[32] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[33] These motors are already available commercially and being used on some robots.[34][35]
 Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact ""muscle"" might allow future robots to outrun and outjump humans.[36]
 Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.
 Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[37][38] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.
 Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[39]
 Other common forms of sensing in robotics use lidar, radar, and sonar.[40] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.
 A definition of robotic manipulation has been provided by Matt Mason as: ""manipulation refers to an agent's control of its environment through selective contact"".[41]
 Robots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[42] while the ""arm"" is referred to as a manipulator.[43] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[44]
 One of the most common types of end-effectors are ""grippers"". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[45] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[46] Hands that are of a mid-level complexity include the Delft hand.[47][48] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.
 Suction end-effectors, powered by vacuum generators, are very simple astrictive[49] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.
 Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.
 Suction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.
 Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[50] and the Schunk hand.[51] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[52]
 For simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.
 Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[53] Many different balancing robots have been designed.[54] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[55]
 A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's ""Ballbot"" which is the approximate height and width of a person, and Tohoku Gakuin University's ""BallIP"".[56] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[57]
 Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[58][59] or by rotating the outer shells of the sphere.[60][61] These have also been referred to as an orb bot[62] or a ball bot.[63][64]
 Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.
 Tank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot ""Urbie"".[65]
 Walking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[66] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[67][68] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:
 The zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[69] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[70][71][72] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.
 Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[73] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[74] A quadruped was also demonstrated which could trot, run, pace, and bound.[75] For a full list of these robots, see the MIT Leg Lab Robots page.[76]
 A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[77] This technique was recently demonstrated by Anybots' Dexter Robot,[78] which is so stable, it can even jump.[79] Another example is the TU Delft Flame.
 Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[80][81]
 A modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[82] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.
 BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[83] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.
 Mammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[84] Examples of bat inspired BFRs include Bat Bot[85] and the DALER.[86] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[86] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[84] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[84]
 Bird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[87] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[87] An example of a raptor inspired BFR is the prototype by Savastano et al.[88] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[89]
 Insect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[90] and a dragonfly inspired BFR is the prototype by Hu et al.[91] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[92] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.
 A class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional ""opposed x-wing fashion"" while ""blowing"" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.
 Several snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[93] The Japanese ACM-R5 snake robot[94] can even navigate both on land and in water.[95]
 A small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[96] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[97]
 Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[98] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[99] and Stickybot.[100]
 China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named ""Speedy Freelander"". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[40]
 It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[101] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[102] Notable examples are the Essex University Computer Science Robotic Fish G9,[103] and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion.[104] The Aqua Penguin,[105] designed and built by Festo of Germany, copies the streamlined shape and propulsion by front ""flippers"" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.
 In 2014, iSplash-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[106] This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[107] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[108]
 Sailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos[109] built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.
 The mechanical structure of a robot must be controlled to perform tasks.[110] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[111] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.
 The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[110][111][112]
 At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a ""cognitive"" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[110] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.
 Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[111] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[113] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[112] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[112] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[112] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[114] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was developed by Michael Short and colleagues at the University of Sunderland in the UK in 2000 (pictured right).[112] The robot was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[114][115]
 Control systems may also have varying levels of autonomy.
 Another classification takes into account the interaction between human control and the machine motions.
 
 Computer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.
 In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.
 Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' ""eyes"" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.
 There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.
 Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[118] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.
 The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[119] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[120] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[120]
 Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[121] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[122] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first ""voice input system"" which recognized ""ten digits spoken by a single user with 100% accuracy"" in 1952.[123] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[124] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[125]
 Other hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[126] making it necessary to develop the emotional component of robotic voice through various techniques.[127][128] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[129][130] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[131] It was programmed to teach students in The Bronx, New York.[131]
 One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate ""down the road, then turn right"". It is likely that gestures will make up a part of the interaction between humans and robots.[132] A great many systems have been developed to recognize human hand gestures.[133]
 Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[134] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[135] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[136]
 Artificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[137]
 Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[138] Nevertheless, researchers are trying to create robots which appear to have a personality:[139][140] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[141]
 Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.
 
 Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.
 To describe the level of advancement of a robot, the term ""Generation Robots"" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[142]
 The study of motion can be divided into kinematics and dynamics.[143] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.
 In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for ""optimal"" performance and ways to optimize design, structure, and control of robots must be developed and implemented.
 Open source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.
 Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[144] and to explore the nature of evolution.[145] Because the process often requires many generations of robots to be simulated,[146] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[147] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]
 Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.
 Swarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [118]
 There has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[148]
 The main venues for robotics research are the international conferences ICRA and IROS.
 Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[151] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[152] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.
 Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[153] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation ""over some unspecified number of years"".[154] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[155] In a 2016 article in The Guardian, Stephen Hawking stated ""The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining"".[156]
 According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[157]
 A discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[158]
 The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[159]
 Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the ""man-robot merger"". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic ""human-robot collaboration"".
 In the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[160][161] aiming to protect employees from the risk of working with collaborative robots will have to be revised.
 Great user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[162]
 It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[163] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.
 Robotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.    
 Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.
 Robotics careers are widely predicted to grow during in the 21st century, as robots replace more manual and intellectual human work. Workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.
 In 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.
 Fully autonomous robots only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately, and more reliably than humans. They are also employed in some jobs that are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery,[164] weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.[165]
 
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['cybernetics', 'Wallbot[99] and Stickybot', 'it drives the non-conservative passivity bounds', 'people and other obstacles that are not stationary', 'social policy, not AI, causes unemployment'], 'answer_start': [], 'answer_end': []}"
"
 In physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force. Thus, string theory is a theory of quantum gravity.
 String theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has contributed a number of advances to mathematical physics, which have been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of its details.
 String theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in eleven dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the anti-de Sitter/conformal field theory correspondence (AdS/CFT correspondence), which relates string theory to another type of physical theory called a quantum field theory.
 One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, which has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics, and to question the value of continued research on string theory unification.
 In the 20th century, two theoretical frameworks emerged for formulating the laws of physics. The first is Albert Einstein's general theory of relativity, a theory that explains the force of gravity and the structure of spacetime at the macro-level. The other is quantum mechanics, a completely different formulation, which uses known probability principles to describe physical phenomena at the micro-level. By the late 1970s, these two frameworks had proven to be sufficient to explain most of the observed features of the universe, from elementary particles to atoms to the evolution of stars and the universe as a whole.[1]
 In spite of these successes, there are still many problems that remain to be solved. One of the deepest problems in modern physics is the problem of quantum gravity.[1] The general theory of relativity is formulated within the framework of classical physics, whereas the other fundamental forces are described within the framework of quantum mechanics. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity.[2] In addition to the problem of developing a consistent theory of quantum gravity, there are many other fundamental problems in the physics of atomic nuclei, black holes, and the early universe.[a]
 String theory is a theoretical framework that attempts to address these questions and many others. The starting point for string theory is the idea that the point-like particles of particle physics can also be modeled as one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle consistent with non-string models of elementary particles, with its mass, charge, and other properties determined by the vibrational state of the string. String theory's application as a form of quantum gravity proposes a vibrational state responsible for the graviton, a yet unproven quantum particle that is theorized to carry gravitational force.[3]
 One of the main developments of the past several decades in string theory was the discovery of certain 'dualities', mathematical transformations that identify one physical theory with another. Physicists studying string theory have discovered a number of these dualities between different versions of string theory, and this has led to the conjecture that all consistent versions of string theory are subsumed in a single framework known as M-theory.[4]
 Studies of string theory have also yielded a number of results on the nature of black holes and the gravitational interaction. There are certain paradoxes that arise when one attempts to understand the quantum aspects of black holes, and work on string theory has attempted to clarify these issues. In late 1997 this line of work culminated in the discovery of the anti-de Sitter/conformal field theory correspondence or AdS/CFT.[5] This is a theoretical result that relates string theory to other physical theories which are better understood theoretically. The AdS/CFT correspondence has implications for the study of black holes and quantum gravity, and it has been applied to other subjects, including nuclear[6] and condensed matter physics.[7][8]
 Since string theory incorporates all of the fundamental interactions, including gravity, many physicists hope that it will eventually be developed to the point where it fully describes our universe, making it a theory of everything. One of the goals of current research in string theory is to find a solution of the theory that reproduces the observed spectrum of elementary particles, with a small cosmological constant, containing dark matter and a plausible mechanism for cosmic inflation. While there has been progress toward these goals, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of details.[9]
 One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. The scattering of strings is most straightforwardly defined using the techniques of perturbation theory, but it is not known in general how to define string theory nonperturbatively.[10] It is also not clear whether there is any principle by which string theory selects its vacuum state, the physical state that determines the properties of our universe.[11] These problems have led some in the community to criticize these approaches to the unification of physics and question the value of continued research on these problems.[12]
 The application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields.[13]
 In quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory. Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations. One imagines that these diagrams depict the paths of point-like particles and their interactions.[13]
 The starting point for string theory is the idea that the point-like particles of quantum field theory can also be modeled as one-dimensional objects called strings.[14] The interaction of strings is most straightforwardly defined by generalizing the perturbation theory used in ordinary quantum field theory. At the level of Feynman diagrams, this means replacing the one-dimensional diagram representing the path of a point particle by a two-dimensional (2D) surface representing the motion of a string.[15] Unlike in quantum field theory, string theory does not have a full non-perturbative definition, so many of the theoretical questions that physicists would like to answer remain out of reach.[16]
 In theories of particle physics based on string theory, the characteristic length scale of strings is assumed to be on the order of the Planck length, or 10−35 meters, the scale at which the effects of quantum gravity are believed to become significant.[15] On much larger length scales, such as the scales visible in physics laboratories, such objects would be indistinguishable from zero-dimensional point particles, and the vibrational state of the string would determine the type of particle. One of the vibrational states of a string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force.[3]
 The original version of string theory was bosonic string theory, but this version described only bosons, a class of particles that transmit forces between the matter particles, or fermions. Bosonic string theory was eventually superseded by theories called superstring theories. These theories describe both bosons and fermions, and they incorporate a theoretical idea called supersymmetry. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa.[17]
 There are several versions of superstring theory: type I, type IIA, type IIB, and two flavors of heterotic string theory (SO(32) and E8×E8). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA, IIB and heterotic include only closed strings.[18]
 In everyday life, there are three familiar dimensions (3D) of space: height, width and length. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional (4D) spacetime. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.[19]
 In spite of the fact that the Universe is well described by 4D spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily.[b] There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics.[13] Finally, there exist scenarios in which there could actually be more than 4D of spacetime which have nonetheless managed to escape detection.[20]
 String theories require extra dimensions of spacetime for their mathematical consistency. In bosonic string theory, spacetime is 26-dimensional, while in superstring theory it is 10-dimensional, and in M-theory it is 11-dimensional. In order to describe real physical phenomena using string theory, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.[21]
 Compactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to ""close up"" on themselves to form circles.[22] In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.
 Compactification can be used to construct models in which spacetime is effectively four-dimensional. However, not every way of compactifying the extra dimensions produces a model with the right properties to describe nature. In a viable model of particle physics, the compact extra dimensions must be shaped like a Calabi–Yau manifold.[22] A Calabi–Yau manifold is a special space which is typically taken to be six-dimensional in applications to string theory. It is named after mathematicians Eugenio Calabi and Shing-Tung Yau.[23]
 Another approach to reducing the number of dimensions is the so-called brane-world scenario. In this approach, physicists assume that the observable universe is a four-dimensional subspace of a higher dimensional space. In such models, the force-carrying bosons of particle physics arise from open strings with endpoints attached to the four-dimensional subspace, while gravity arises from closed strings propagating through the larger ambient space. This idea plays an important role in attempts to develop models of real-world physics based on string theory, and it provides a natural explanation for the weakness of gravity compared to the other fundamental forces.[24]
 A notable fact about string theory is that the different versions of the theory all turn out to be related in highly nontrivial ways. One of the relationships that can exist between different string theories is called S-duality. This is a relationship that says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the SO(32) heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.[25]
 Another relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius R is equivalent to a string propagating around a circle of radius 1/R in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum p and winding number n in one description, it will have momentum n and winding number p in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.[25]
 In general, the term duality refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. Two theories related by a duality need not be string theories. For example, Montonen–Olive duality is an example of an S-duality relationship between quantum field theories. The AdS/CFT correspondence is an example of a duality that relates string theory to a quantum field theory. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be dual to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.[26]
 In string theory and other related theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For instance, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension p, these are called p-branes. The word brane comes from the word ""membrane"" which refers to a two-dimensional brane.[27]
 Branes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They have mass and can have other attributes such as charge. A p-brane sweeps out a (p+1)-dimensional volume in spacetime called its worldvolume. Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane.[27]
 In string theory, D-branes are an important class of branes that arise when one considers open strings. As an open string propagates through spacetime, its endpoints are required to lie on a D-brane. The letter ""D"" in D-brane refers to a certain mathematical condition on the system known as the Dirichlet boundary condition. The study of D-branes in string theory has led to important results such as the AdS/CFT correspondence, which has shed light on many problems in quantum field theory.[27]
 Branes are frequently studied from a purely mathematical point of view, and they are described as objects of certain categories, such as the derived category of coherent sheaves on a complex algebraic variety, or the Fukaya category of a symplectic manifold.[28] The connection between the physical notion of a brane and the mathematical notion of a category has led to important mathematical insights in the fields of algebraic and symplectic geometry[29] and representation theory.[30]
 Prior to 1995, theorists believed that there were five consistent versions of superstring theory (type I, type IIA, type IIB, and two versions of heterotic string theory). This understanding changed in 1995 when Edward Witten suggested that the five theories were just special limiting cases of an eleven-dimensional theory called M-theory. Witten's conjecture was based on the work of a number of other physicists, including Ashoke Sen, Chris Hull, Paul Townsend, and Michael Duff. His announcement led to a flurry of research activity now known as the second superstring revolution.[31]
 In the 1970s, many physicists became interested in supergravity theories, which combine general relativity with supersymmetry. Whereas general relativity makes sense in any number of dimensions, supergravity places an upper limit on the number of dimensions.[32] In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven.[33] In the same year, Eugene Cremmer, Bernard Julia, and Joël Scherk of the École Normale Supérieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.[34][35]
 Initially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.[35]
 In the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects.[35] Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.[35]
 Although there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation.[35] However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways. They found that a system of strongly interacting strings can, in some cases, be viewed as a system of weakly interacting strings. This phenomenon is known as S-duality. It was studied by Ashoke Sen in the context of heterotic strings in four dimensions[36][37] and by Chris Hull and Paul Townsend in the context of the type IIB theory.[38] Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.[39]
 At around the same time, as many physicists were studying the properties of strings, a small group of physicists were examining the possible applications of higher dimensional objects. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes.[40] Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle.[41] In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.[42]
 Speaking at a string theory conference in 1995, Edward Witten made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of higher-dimensional branes in string theory.[43] In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming different parts of his proposal.[44] Today this flurry of work is known as the second superstring revolution.[45]
 Initially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Hořava and Witten wrote ""As it has been proposed that the eleven-dimensional theory is a supermembrane theory but there are some reasons to doubt that interpretation, we will non-committally call it the M-theory, leaving to the future the relation of M to membranes.""[46] In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the M should stand for ""magic"", ""mystery"", or ""membrane"" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.[47]
 In mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes the behavior of a set of matrices within the framework of quantum mechanics.[48]
 One important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.[48]
 The development of the matrix model formulation of M-theory has led physicists to consider various connections between string theory and a branch of mathematics called noncommutative geometry. This subject is a generalization of ordinary geometry in which mathematicians define new geometric notions using tools from noncommutative algebra.[49] In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which spacetime is described mathematically using noncommutative geometry.[50] This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.[51][52]
 In general relativity, a black hole is defined as a region of spacetime in which the gravitational field is so strong that no particle or radiation can escape. In the currently accepted models of stellar evolution, black holes are thought to arise when massive stars undergo gravitational collapse, and many galaxies are thought to contain supermassive black holes at their centers. Black holes are also important for theoretical reasons, as they present profound challenges for theorists attempting to understand the quantum aspects of gravity. String theory has proved to be an important tool for investigating the theoretical properties of black holes because it provides a framework in which theorists can study their thermodynamics.[53]
 In the branch of physics called statistical mechanics, entropy is a measure of the randomness or disorder of a physical system. This concept was studied in the 1870s by the Austrian physicist Ludwig Boltzmann, who showed that the thermodynamic properties of a gas could be derived from the combined properties of its many constituent molecules. Boltzmann argued that by averaging the behaviors of all the different molecules in a gas, one can understand macroscopic properties such as volume, temperature, and pressure. In addition, this perspective led him to give a precise definition of entropy as the natural logarithm of the number of different states of the molecules (also called microstates) that give rise to the same macroscopic features.[54]
 In the twentieth century, physicists began to apply the same concepts to black holes. In most systems such as gases, the entropy scales with the volume. In the 1970s, the physicist Jacob Bekenstein suggested that the entropy of a black hole is instead proportional to the surface area of its event horizon, the boundary beyond which matter and radiation are lost to its gravitational attraction.[55] When combined with ideas of the physicist Stephen Hawking,[56] Bekenstein's work yielded a precise formula for the entropy of a black hole. The Bekenstein–Hawking formula expresses the entropy S as
 where c is the speed of light, k is the Boltzmann constant, ħ is the reduced Planck constant, G is Newton's constant, and A is the surface area of the event horizon.[57]
 Like any physical system, a black hole has an entropy defined in terms of the number of different microstates that lead to the same macroscopic features. The Bekenstein–Hawking entropy formula gives the expected value of the entropy of a black hole, but by the 1990s, physicists still lacked a derivation of this formula by counting microstates in a theory of quantum gravity. Finding such a derivation of this formula was considered an important test of the viability of any theory of quantum gravity such as string theory.[58]
 In a paper from 1996, Andrew Strominger and Cumrun Vafa showed how to derive the Beckenstein–Hawking formula for certain black holes in string theory.[59] Their calculation was based on the observation that D-branes—which look like fluctuating membranes when they are weakly interacting—become dense, massive objects with event horizons when the interactions are strong. In other words, a system of strongly interacting D-branes in string theory is indistinguishable from a black hole. Strominger and Vafa analyzed such D-brane systems and calculated the number of different ways of placing D-branes in spacetime so that their combined mass and charge is equal to a given mass and charge for the resulting black hole. Their calculation reproduced the Bekenstein–Hawking formula exactly, including the factor of 1/4.[60] Subsequent work by Strominger, Vafa, and others refined the original calculations and gave the precise values of the ""quantum corrections"" needed to describe very small black holes.[61][62]
 The black holes that Strominger and Vafa considered in their original work were quite different from real astrophysical black holes. One difference was that Strominger and Vafa considered only extremal black holes in order to make the calculation tractable. These are defined as black holes with the lowest possible mass compatible with a given charge.[63] Strominger and Vafa also restricted attention to black holes in five-dimensional spacetime with unphysical supersymmetry.[64]
 Although it was originally developed in this very particular and physically unrealistic context in string theory, the entropy calculation of Strominger and Vafa has led to a qualitative understanding of how black hole entropy can be accounted for in any theory of quantum gravity. Indeed, in 1998, Strominger argued that the original result could be generalized to an arbitrary consistent theory of quantum gravity without relying on strings or supersymmetry.[65] In collaboration with several other authors in 2010, he showed that some results on black hole entropy could be extended to non-extremal astrophysical black holes.[66][67]
 One approach to formulating string theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. This is a theoretical result which implies that string theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective.[6] The AdS/CFT correspondence was first proposed by Juan Maldacena in late 1997.[68] Important aspects of the correspondence were elaborated in articles by Steven Gubser, Igor Klebanov, and Alexander Markovich Polyakov,[69] and by Edward Witten.[70] By 2010, Maldacena's article had over 7000 citations, becoming the most highly cited article in the field of high energy physics.[c]
 In the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space.[6] In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left.[71] This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.[72]
 One can imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space.[71] It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.[72]
 This construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can ""stack up"" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.[71]
 An important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics.[73] One can therefore consider an auxiliary theory in which ""spacetime"" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the ""spacetime"" for a quantum field theory. The claim is that this quantum field theory is equivalent to a gravitational theory, such as string theory, in the bulk anti-de Sitter space in the sense that there is a ""dictionary"" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.[74]
 The discovery of the AdS/CFT correspondence was a major advance in physicists' understanding of string theory and quantum gravity. One reason for this is that the correspondence provides a formulation of string theory in terms of quantum field theory, which is well understood by comparison. Another reason is that it provides a general framework in which physicists can study and attempt to resolve the paradoxes of black holes.[53]
 In 1975, Stephen Hawking published a calculation which suggested that black holes are not completely black but emit a dim radiation due to quantum effects near the event horizon.[56] At first, Hawking's result posed a problem for theorists because it suggested that black holes destroy information. More precisely, Hawking's calculation seemed to conflict with one of the basic postulates of quantum mechanics, which states that physical systems evolve in time according to the Schrödinger equation. This property is usually referred to as unitarity of time evolution. The apparent contradiction between Hawking's calculation and the unitarity postulate of quantum mechanics came to be known as the black hole information paradox.[75]
 The AdS/CFT correspondence resolves the black hole information paradox, at least to some extent, because it shows how a black hole can evolve in a manner consistent with quantum mechanics in some contexts. Indeed, one can consider black holes in the context of the AdS/CFT correspondence, and any such black hole corresponds to a configuration of particles on the boundary of anti-de Sitter space.[76] These particles obey the usual rules of quantum mechanics and in particular evolve in a unitary fashion, so the black hole must also evolve in a unitary fashion, respecting the principles of quantum mechanics.[77] In 2005, Hawking announced that the paradox had been settled in favor of information conservation by the AdS/CFT correspondence, and he suggested a concrete mechanism by which black holes might preserve information.[78]
 In addition to its applications to theoretical problems in quantum gravity, the AdS/CFT correspondence has been applied to a variety of problems in quantum field theory. One physical system that has been studied using the AdS/CFT correspondence is the quark–gluon plasma, an exotic state of matter produced in particle accelerators. This state of matter arises for brief instants when heavy ions such as gold or lead nuclei are collided at high energies. Such collisions cause the quarks that make up atomic nuclei to deconfine at temperatures of approximately two trillion kelvin, conditions similar to those present at around 10−11 seconds after the Big Bang.[79]
 The physics of the quark–gluon plasma is governed by a theory called quantum chromodynamics, but this theory is mathematically intractable in problems involving the quark–gluon plasma.[d] In an article appearing in 2005, Đàm Thanh Sơn and his collaborators showed that the AdS/CFT correspondence could be used to understand some aspects of the quark-gluon plasma by describing it in the language of string theory.[80] By applying the AdS/CFT correspondence, Sơn and his collaborators were able to describe the quark-gluon plasma in terms of black holes in five-dimensional spacetime. The calculation showed that the ratio of two quantities associated with the quark-gluon plasma, the shear viscosity and volume density of entropy, should be approximately equal to a certain universal constant. In 2008, the predicted value of this ratio for the quark-gluon plasma was confirmed at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory.[7][81]
 The AdS/CFT correspondence has also been used to study aspects of condensed matter physics. Over the decades, experimental condensed matter physicists have discovered a number of exotic states of matter, including superconductors and superfluids. These states are described using the formalism of quantum field theory, but some phenomena are difficult to explain using standard field theoretic techniques. Some condensed matter theorists including Subir Sachdev hope that the AdS/CFT correspondence will make it possible to describe these systems in the language of string theory and learn more about their behavior.[7]
 So far some success has been achieved in using string theory methods to describe the transition of a superfluid to an insulator. A superfluid is a system of electrically neutral atoms that flows without any friction. Such systems are often produced in the laboratory using liquid helium, but recently experimentalists have developed new ways of producing artificial superfluids by pouring trillions of cold atoms into a lattice of criss-crossing lasers. These atoms initially behave as a superfluid, but as experimentalists increase the intensity of the lasers, they become less mobile and then suddenly transition to an insulating state. During the transition, the atoms behave in an unusual way. For example, the atoms slow to a halt at a rate that depends on the temperature and on the Planck constant, the fundamental parameter of quantum mechanics, which does not enter into the description of the other phases. This behavior has recently been understood by considering a dual description where properties of the fluid are described in terms of a higher dimensional black hole.[8]
 In addition to being an idea of considerable theoretical interest, string theory provides a framework for constructing models of real-world physics that combine general relativity and particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic or semi-realistic models based on string theory.
 Partly because of theoretical and mathematical difficulties and partly because of the extremely high energies needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.[12]
 The currently accepted theory describing elementary particles and their interactions is known as the standard model of particle physics. This theory provides a unified description of three of the fundamental forces of nature: electromagnetism and the strong and weak nuclear forces. Despite its remarkable success in explaining a wide range of physical phenomena, the standard model cannot be a complete description of reality. This is because the standard model fails to incorporate the force of gravity and because of problems such as the hierarchy problem and the inability to explain the structure of fermion masses or dark matter.
 String theory has been used to construct a variety of models of particle physics going beyond the standard model. Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles.[82] One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi–Yau manifold. Such compactifications offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct realistic or semi-realistic models of our four-dimensional world based on M-theory.[83]
 The Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. Despite its success in explaining many observed features of the universe including galactic redshifts, the relative abundance of light elements such as hydrogen and helium, and the existence of a cosmic microwave background, there are several questions that remain unanswered. For example, the standard Big Bang model does not explain why the universe appears to be the same in all directions, why it appears flat on very large distance scales, or why certain hypothesized particles such as magnetic monopoles are not observed in experiments.[84]
 Currently, the leading candidate for a theory going beyond the Big Bang is the theory of cosmic inflation. Developed by Alan Guth and others in the 1980s, inflation postulates a period of extremely rapid accelerated expansion of the universe prior to the expansion described by the standard Big Bang theory. The theory of cosmic inflation preserves the successes of the Big Bang while providing a natural explanation for some of the mysterious features of the universe.[85] The theory has also received striking support from observations of the cosmic microwave background, the radiation that has filled the sky since around 380,000 years after the Big Bang.[86]
 In the theory of inflation, the rapid initial expansion of the universe is caused by a hypothetical particle called the inflaton. The exact properties of this particle are not fixed by the theory but should ultimately be derived from a more fundamental theory such as string theory.[87] Indeed, there have been a number of attempts to identify an inflaton within the spectrum of particles described by string theory and to study inflation using string theory. While these approaches might eventually find support in observational data such as measurements of the cosmic microwave background, the application of string theory to cosmology is still in its early stages.[88]
 In addition to influencing research in theoretical physics, string theory has stimulated a number of major developments in pure mathematics. Like many developing ideas in theoretical physics, string theory does not at present have a mathematically rigorous formulation in which all of its concepts can be defined precisely. As a result, physicists who study string theory are often guided by physical intuition to conjecture relationships between the seemingly different mathematical structures that are used to formalize different parts of the theory. These conjectures are later proved by mathematicians, and in this way, string theory serves as a source of new ideas in pure mathematics.[89]
 After Calabi–Yau manifolds had entered physics as a way to compactify extra dimensions in string theory, many physicists began studying these manifolds. In the late 1980s, several physicists noticed that given such a compactification of string theory, it is not possible to reconstruct uniquely a corresponding Calabi–Yau manifold.[90] Instead, two different versions of string theory, type IIA and type IIB, can be compactified on completely different Calabi–Yau manifolds giving rise to the same physics. In this situation, the manifolds are called mirror manifolds, and the relationship between the two physical theories is called mirror symmetry.[28]
 Regardless of whether Calabi–Yau compactifications of string theory provide a correct description of nature, the existence of the mirror duality between different string theories has significant mathematical consequences. The Calabi–Yau manifolds used in string theory are of interest in pure mathematics, and mirror symmetry allows mathematicians to solve problems in enumerative geometry, a branch of mathematics concerned with counting the numbers of solutions to geometric questions.[28][91]
 Enumerative geometry studies a class of geometric objects called algebraic varieties which are defined by the vanishing of polynomials. For example, the Clebsch cubic illustrated on the right is an algebraic variety defined using a certain polynomial of degree three in four variables. A celebrated result of nineteenth-century mathematicians Arthur Cayley and George Salmon states that there are exactly 27 straight lines that lie entirely on such a surface.[92]
 Generalizing this problem, one can ask how many lines can be drawn on a quintic Calabi–Yau manifold, such as the one illustrated above, which is defined by a polynomial of degree five. This problem was solved by the nineteenth-century German mathematician Hermann Schubert, who found that there are exactly 2,875 such lines. In 1986, geometer Sheldon Katz proved that the number of curves, such as circles, that are defined by polynomials of degree two and lie entirely in the quintic is 609,250.[93]
 By the year 1991, most of the classical problems of enumerative geometry had been solved and interest in enumerative geometry had begun to diminish.[94] The field was reinvigorated in May 1991 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parkes showed that mirror symmetry could be used to translate difficult mathematical questions about one Calabi–Yau manifold into easier questions about its mirror.[95] In particular, they used mirror symmetry to show that a six-dimensional Calabi–Yau manifold can contain exactly 317,206,375 curves of degree three.[94] In addition to counting degree-three curves, Candelas and his collaborators obtained a number of more general results for counting rational curves which went far beyond the results obtained by mathematicians.[96]
 Originally, these results of Candelas were justified on physical grounds. However, mathematicians generally prefer rigorous proofs that do not require an appeal to physical intuition. Inspired by physicists' work on mirror symmetry, mathematicians have therefore constructed their own arguments proving the enumerative predictions of mirror symmetry.[e] Today mirror symmetry is an active area of research in mathematics, and mathematicians are working to develop a more complete mathematical understanding of mirror symmetry based on physicists' intuition.[102] Major approaches to mirror symmetry include the homological mirror symmetry program of Maxim Kontsevich[29] and the SYZ conjecture of Andrew Strominger, Shing-Tung Yau, and Eric Zaslow.[103]
 Group theory is the branch of mathematics that studies the concept of symmetry. For example, one can consider a geometric shape such as an equilateral triangle. There are various operations that one can perform on this triangle without changing its shape. One can rotate it through 120°, 240°, or 360°, or one can reflect in any of the lines labeled S0, S1, or S2 in the picture. Each of these operations is called a symmetry, and the collection of these symmetries satisfies certain technical properties making it into what mathematicians call a group. In this particular example, the group is known as the dihedral group of order 6 because it has six elements. A general group may describe finitely many or infinitely many symmetries; if there are only finitely many symmetries, it is called a finite group.[104]
 Mathematicians often strive for a classification (or list) of all mathematical objects of a given type. It is generally believed that finite groups are too diverse to admit a useful classification. A more modest but still challenging problem is to classify all finite simple groups. These are finite groups that may be used as building blocks for constructing arbitrary finite groups in the same way that prime numbers can be used to construct arbitrary whole numbers by taking products.[f] One of the major achievements of contemporary group theory is the classification of finite simple groups, a mathematical theorem that provides a list of all possible finite simple groups.[104]
 This classification theorem identifies several infinite families of groups as well as 26 additional groups which do not fit into any family. The latter groups are called the ""sporadic"" groups, and each one owes its existence to a remarkable combination of circumstances. The largest sporadic group, the so-called monster group, has over 1053 elements, more than a thousand times the number of atoms in the Earth.[105]
 A seemingly unrelated construction is the j-function of number theory. This object belongs to a special class of functions called modular functions, whose graphs form a certain kind of repeating pattern.[106] Although this function appears in a branch of mathematics that seems very different from the theory of finite groups, the two subjects turn out to be intimately related. In the late 1970s, mathematicians John McKay and John Thompson noticed that certain numbers arising in the analysis of the monster group (namely, the dimensions of its irreducible representations) are related to numbers that appear in a formula for the j-function (namely, the coefficients of its Fourier series).[107] This relationship was further developed by John Horton Conway and Simon Norton[108] who called it monstrous moonshine because it seemed so far fetched.[109]
 In 1992, Richard Borcherds constructed a bridge between the theory of modular functions and finite groups and, in the process, explained the observations of McKay and Thompson.[110][111] Borcherds' work used ideas from string theory in an essential way, extending earlier results of Igor Frenkel, James Lepowsky, and Arne Meurman, who had realized the monster group as the symmetries of a particular[which?] version of string theory.[112] In 1998, Borcherds was awarded the Fields medal for his work.[113]
 Since the 1990s, the connection between string theory and moonshine has led to further results in mathematics and physics.[105] In 2010, physicists Tohru Eguchi, Hirosi Ooguri, and Yuji Tachikawa discovered connections between a different sporadic group, the Mathieu group M24, and a certain version[which?] of string theory.[114] Miranda Cheng, John Duncan, and Jeffrey A. Harvey proposed a generalization of this moonshine phenomenon called umbral moonshine,[115] and their conjecture was proved mathematically by Duncan, Michael Griffin, and Ken Ono.[116] Witten has also speculated that the version of string theory appearing in monstrous moonshine might be related to a certain simplified model of gravity in three spacetime dimensions.[117]
 Some of the structures reintroduced by string theory arose for the first time much earlier as part of the program of classical unification started by Albert Einstein. The first person to add a fifth dimension to a theory of gravity was Gunnar Nordström in 1914, who noted that gravity in five dimensions describes both gravity and electromagnetism in four. Nordström attempted to unify electromagnetism with his theory of gravitation, which was however superseded by Einstein's general relativity in 1919. Thereafter, German mathematician Theodor Kaluza combined the fifth dimension with general relativity, and only Kaluza is usually credited with the idea. In 1926, the Swedish physicist Oskar Klein gave a physical interpretation of the unobservable extra dimension—it is wrapped into a small circle. Einstein introduced a non-symmetric metric tensor, while much later Brans and Dicke added a scalar component to gravity. These ideas would be revived within string theory, where they are demanded by consistency conditions.
 String theory was originally developed during the late 1960s and early 1970s as a never completely successful theory of hadrons, the subatomic particles like the proton and neutron that feel the strong interaction. In the 1960s, Geoffrey Chew and Steven Frautschi discovered that the mesons make families called Regge trajectories with masses related to spins in a way that was later understood by Yoichiro Nambu, Holger Bech Nielsen and Leonard Susskind to be the relationship expected from rotating strings. Chew advocated making a theory for the interactions of these trajectories that did not presume that they were composed of any fundamental particles, but would construct their interactions from self-consistency conditions on the S-matrix. The S-matrix approach was started by Werner Heisenberg in the 1940s as a way of constructing a theory that did not rely on the local notions of space and time, which Heisenberg believed break down at the nuclear scale. While the scale was off by many orders of magnitude, the approach he advocated was ideally suited for a theory of quantum gravity.
 Working with experimental data, R. Dolen, D. Horn and C. Schmid developed some sum rules for hadron exchange. When a particle and antiparticle scatter, virtual particles can be exchanged in two qualitatively different ways. In the s-channel, the two particles annihilate to make temporary intermediate states that fall apart into the final state particles. In the t-channel, the particles exchange intermediate states by emission and absorption. In field theory, the two contributions add together, one giving a continuous background contribution, the other giving peaks at certain energies. In the data, it was clear that the peaks were stealing from the background—the authors interpreted this as saying that the t-channel contribution was dual to the s-channel one, meaning both described the whole amplitude and included the other.
 The result was widely advertised by Murray Gell-Mann, leading Gabriele Veneziano to construct a scattering amplitude that had the property of Dolen–Horn–Schmid duality, later renamed world-sheet duality. The amplitude needed poles where the particles appear, on straight-line trajectories, and there is a special mathematical function whose poles are evenly spaced on half the real line—the gamma function— which was widely used in Regge theory. By manipulating combinations of gamma functions, Veneziano was able to find a consistent scattering amplitude with poles on straight lines, with mostly positive residues, which obeyed duality and had the appropriate Regge scaling at high energy. The amplitude could fit near-beam scattering data as well as other Regge type fits and had a suggestive integral representation that could be used for generalization.
 Over the next years, hundreds of physicists worked to complete the bootstrap program for this model, with many surprises. Veneziano himself discovered that for the scattering amplitude to describe the scattering of a particle that appears in the theory, an obvious self-consistency condition, the lightest particle must be a tachyon. Miguel Virasoro and Joel Shapiro found a different amplitude now understood to be that of closed strings, while Ziro Koba and Holger Nielsen generalized Veneziano's integral representation to multiparticle scattering. Veneziano and Sergio Fubini introduced an operator formalism for computing the scattering amplitudes that was a forerunner of world-sheet conformal theory, while Virasoro understood how to remove the poles with wrong-sign residues using a constraint on the states. Claud Lovelace calculated a loop amplitude, and noted that there is an inconsistency unless the dimension of the theory is 26. Charles Thorn, Peter Goddard and Richard Brower went on to prove that there are no wrong-sign propagating states in dimensions less than or equal to 26.
 In 1969–70, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind recognized that the theory could be given a description in space and time in terms of strings. The scattering amplitudes were derived systematically from the action principle by Peter Goddard, Jeffrey Goldstone, Claudio Rebbi, and Charles Thorn, giving a space-time picture to the vertex operators introduced by Veneziano and Fubini and a geometrical interpretation to the Virasoro conditions.
 In 1971, Pierre Ramond added fermions to the model, which led him to formulate a two-dimensional supersymmetry to cancel the wrong-sign states. John Schwarz and André Neveu added another sector to the fermi theory a short time later. In the fermion theories, the critical dimension was 10. Stanley Mandelstam formulated a world sheet conformal theory for both the bose and fermi case, giving a two-dimensional field theoretic path-integral to generate the operator formalism. Michio Kaku and Keiji Kikkawa gave a different formulation of the bosonic string, as a string field theory, with infinitely many particle types and with fields taking values not on points, but on loops and curves.
 In 1974, Tamiaki Yoneya discovered that all the known string theories included a massless spin-two particle that obeyed the correct Ward identities to be a graviton. John Schwarz and Joël Scherk came to the same conclusion and made the bold leap to suggest that string theory was a theory of gravity, not a theory of hadrons. They reintroduced Kaluza–Klein theory as a way of making sense of the extra dimensions. At the same time, quantum chromodynamics was recognized as the correct theory of hadrons, shifting the attention of physicists and apparently leaving the bootstrap program in the dustbin of history.
 String theory eventually made it out of the dustbin, but for the following decade, all work on the theory was completely ignored. Still, the theory continued to develop at a steady pace thanks to the work of a handful of devotees. Ferdinando Gliozzi, Joël Scherk, and David Olive realized in 1977 that the original Ramond and Neveu Schwarz-strings were separately inconsistent and needed to be combined. The resulting theory did not have a tachyon and was proven to have space-time supersymmetry by John Schwarz and Michael Green in 1984. The same year, Alexander Polyakov gave the theory a modern path integral formulation, and went on to develop conformal field theory extensively. In 1979, Daniel Friedan showed that the equations of motions of string theory, which are generalizations of the Einstein equations of general relativity, emerge from the renormalization group equations for the two-dimensional field theory. Schwarz and Green discovered T-duality, and constructed two superstring theories—IIA and IIB related by T-duality, and type I theories with open strings. The consistency conditions had been so strong, that the entire theory was nearly uniquely determined, with only a few discrete choices.
 In the early 1980s, Edward Witten discovered that most theories of quantum gravity could not accommodate chiral fermions like the neutrino. This led him, in collaboration with Luis Álvarez-Gaumé, to study violations of the conservation laws in gravity theories with anomalies, concluding that type I string theories were inconsistent. Green and Schwarz discovered a contribution to the anomaly that Witten and Alvarez-Gaumé had missed, which restricted the gauge group of the type I string theory to be SO(32). In coming to understand this calculation, Edward Witten became convinced that string theory was truly a consistent theory of gravity, and he became a high-profile advocate. Following Witten's lead, between 1984 and 1986, hundreds of physicists started to work in this field, and this is sometimes called the first superstring revolution.[citation needed]
 During this period, David Gross, Jeffrey Harvey, Emil Martinec, and Ryan Rohm discovered heterotic strings. The gauge group of these closed strings was two copies of E8, and either copy could easily and naturally include the standard model. Philip Candelas, Gary Horowitz, Andrew Strominger and Edward Witten found that the Calabi–Yau manifolds are the compactifications that preserve a realistic amount of supersymmetry, while Lance Dixon and others worked out the physical properties of orbifolds, distinctive geometrical singularities allowed in string theory. Cumrun Vafa generalized T-duality from circles to arbitrary manifolds, creating the mathematical field of mirror symmetry. Daniel Friedan, Emil Martinec and Stephen Shenker further developed the covariant quantization of the superstring using conformal field theory techniques. David Gross and Vipul Periwal discovered that string perturbation theory was divergent. Stephen Shenker showed it diverged much faster than in field theory suggesting that new non-perturbative objects were missing.[citation needed]
 In the 1990s, Joseph Polchinski discovered that the theory requires higher-dimensional objects, called D-branes and identified these with the black-hole solutions of supergravity. These were understood to be the new objects suggested by the perturbative divergences, and they opened up a new field with rich mathematical structure. It quickly became clear that D-branes and other p-branes, not just strings, formed the matter content of the string theories, and the physical interpretation of the strings and branes was revealed—they are a type of black hole. Leonard Susskind had incorporated the holographic principle of Gerardus 't Hooft into string theory, identifying the long highly excited string states with ordinary thermal black hole states. As suggested by 't Hooft, the fluctuations of the black hole horizon, the world-sheet or world-volume theory, describes not only the degrees of freedom of the black hole, but all nearby objects too.
 In 1995, at the annual conference of string theorists at the University of Southern California (USC), Edward Witten gave a speech on string theory that in essence united the five string theories that existed at the time, and giving birth to a new 11-dimensional theory called M-theory. M-theory was also foreshadowed in the work of Paul Townsend at approximately the same time. The flurry of activity that began at this time is sometimes called the second superstring revolution.[31]
 During this period, Tom Banks, Willy Fischler, Stephen Shenker and Leonard Susskind formulated matrix theory, a full holographic description of M-theory using IIA D0 branes.[48] This was the first definition of string theory that was fully non-perturbative and a concrete mathematical realization of the holographic principle. It is an example of a gauge-gravity duality and is now understood to be a special case of the AdS/CFT correspondence. Andrew Strominger and Cumrun Vafa calculated the entropy of certain configurations of D-branes and found agreement with the semi-classical answer for extreme charged black holes.[59] Petr Hořava and Witten found the eleven-dimensional formulation of the heterotic string theories, showing that orbifolds solve the chirality problem. Witten noted that the effective description of the physics of D-branes at low energies is by a supersymmetric gauge theory, and found geometrical interpretations of mathematical structures in gauge theory that he and Nathan Seiberg had earlier discovered in terms of the location of the branes.
 In 1997, Juan Maldacena noted that the low energy excitations of a theory near a black hole consist of objects close to the horizon, which for extreme charged black holes looks like an anti-de Sitter space.[68] He noted that in this limit the gauge theory describes the string excitations near the branes. So he hypothesized that string theory on a near-horizon extreme-charged black-hole geometry, an anti-de Sitter space times a sphere with flux, is equally well described by the low-energy limiting gauge theory, the N = 4 supersymmetric Yang–Mills theory. This hypothesis, which is called the AdS/CFT correspondence, was further developed by Steven Gubser, Igor Klebanov and Alexander Polyakov,[69] and by Edward Witten,[70] and it is now well-accepted. It is a concrete realization of the holographic principle, which has far-reaching implications for black holes, locality and information in physics, as well as the nature of the gravitational interaction.[53] Through this relationship, string theory has been shown to be related to gauge theories like quantum chromodynamics and this has led to a more quantitative understanding of the behavior of hadrons, bringing string theory back to its roots.[citation needed]
 To construct models of particle physics based on string theory, physicists typically begin by specifying a shape for the extra dimensions of spacetime. Each of these different shapes corresponds to a different possible universe, or ""vacuum state"", with a different collection of particles and forces. String theory as it is currently understood has an enormous number of vacuum states, typically estimated to be around 10500, and these might be sufficiently diverse to accommodate almost any phenomenon that might be observed at low energies.[118]
 Many critics of string theory have expressed concerns about the large number of possible universes described by string theory. In his book Not Even Wrong, Peter Woit, a lecturer in the mathematics department at Columbia University, has argued that the large number of different physical scenarios renders string theory vacuous as a framework for constructing models of particle physics. According to Woit,
 The possible existence of, say, 10500 consistent different vacuum states for superstring theory probably destroys the hope of using the theory to predict anything. If one picks among this large set just those states whose properties agree with present experimental observations, it is likely there still will be such a large number of these that one can get just about whatever value one wants for the results of any new observation.[119] Some physicists believe this large number of solutions is actually a virtue because it may allow a natural anthropic explanation of the observed values of physical constants, in particular the small value of the cosmological constant.[119] The anthropic principle is the idea that some of the numbers appearing in the laws of physics are not fixed by any fundamental principle but must be compatible with the evolution of intelligent life. In 1987, Steven Weinberg published an article in which he argued that the cosmological constant could not have been too large, or else galaxies and intelligent life would not have been able to develop.[120] Weinberg suggested that there might be a huge number of possible consistent universes, each with a different value of the cosmological constant, and observations indicate a small value of the cosmological constant only because humans happen to live in a universe that has allowed intelligent life, and hence observers, to exist.[121]
 String theorist Leonard Susskind has argued that string theory provides a natural anthropic explanation of the small value of the cosmological constant.[122] According to Susskind, the different vacuum states of string theory might be realized as different universes within a larger multiverse. The fact that the observed universe has a small cosmological constant is just a tautological consequence of the fact that a small value is required for life to exist.[123] Many prominent theorists and critics have disagreed with Susskind's conclusions.[124] According to Woit, ""in this case [anthropic reasoning] is nothing more than an excuse for failure. Speculative scientific ideas fail not just when they make incorrect predictions, but also when they turn out to be vacuous and incapable of predicting anything.""[125]
 It remains unknown whether string theory is compatible with a metastable, positive cosmological constant.
Some putative examples of such solutions do exist, such as the model described by Kachru et al. in 2003.[126] In 2018, a group of four physicists advanced a controversial conjecture which would imply that no such universe exists. This is contrary to some popular models of dark energy such as Λ-CDM, which requires a positive vacuum energy. However, string theory is likely compatible with certain types of quintessence, where dark energy is caused by a new field with exotic properties.[127]
 One of the fundamental properties of Einstein's general theory of relativity is that it is background independent, meaning that the formulation of the theory does not in any way privilege a particular spacetime geometry.[128]
 One of the main criticisms of string theory from early on is that it is not manifestly background-independent. In string theory, one must typically specify a fixed reference geometry for spacetime, and all other possible geometries are described as perturbations of this fixed one. In his book The Trouble With Physics, physicist Lee Smolin of the Perimeter Institute for Theoretical Physics claims that this is the principal weakness of string theory as a theory of quantum gravity, saying that string theory has failed to incorporate this important insight from general relativity.[129]
 Others have disagreed with Smolin's characterization of string theory. In a review of Smolin's book, string theorist Joseph Polchinski writes
 [Smolin] is mistaking an aspect of the mathematical language being used for one of the physics being described. New physical theories are often discovered using a mathematical language that is not the most suitable for them... In string theory, it has always been clear that the physics is background-independent even if the language being used is not, and the search for a more suitable language continues. Indeed, as Smolin belatedly notes, [AdS/CFT] provides a solution to this problem, one that is unexpected and powerful.[130] Polchinski notes that an important open problem in quantum gravity is to develop holographic descriptions of gravity which do not require the gravitational field to be asymptotically anti-de Sitter.[130] Smolin has responded by saying that the AdS/CFT correspondence, as it is currently understood, may not be strong enough to resolve all concerns about background independence.[131]
 Since the superstring revolutions of the 1980s and 1990s, string theory has been one of the dominant paradigms of high energy theoretical physics.[132] Some string theorists have expressed the view that there does not exist an equally successful alternative theory addressing the deep questions of fundamental physics. In an interview from 1987, Nobel laureate David Gross made the following controversial comments about the reasons for the popularity of string theory:
 The most important [reason] is that there are no other good ideas around. That's what gets most people into it. When people started to get interested in string theory they didn't know anything about it. In fact, the first reaction of most people is that the theory is extremely ugly and unpleasant, at least that was the case a few years ago when the understanding of string theory was much less developed. It was difficult for people to learn about it and to be turned on. So I think the real reason why people have got attracted by it is because there is no other game in town. All other approaches of constructing grand unified theories, which were more conservative to begin with, and only gradually became more and more radical, have failed, and this game hasn't failed yet.[133] Several other high-profile theorists and commentators have expressed similar views, suggesting that there are no viable alternatives to string theory.[134]
 Many critics of string theory have commented on this state of affairs. In his book criticizing string theory, Peter Woit views the status of string theory research as unhealthy and detrimental to the future of fundamental physics. He argues that the extreme popularity of string theory among theoretical physicists is partly a consequence of the financial structure of academia and the fierce competition for scarce resources.[135] In his book The Road to Reality, mathematical physicist Roger Penrose expresses similar views, stating ""The often frantic competitiveness that this ease of communication engenders leads to bandwagon effects, where researchers fear to be left behind if they do not join in.""[136] Penrose also claims that the technical difficulty of modern physics forces young scientists to rely on the preferences of established researchers, rather than forging new paths of their own.[137] Lee Smolin expresses a slightly different position in his critique, claiming that string theory grew out of a tradition of particle physics which discourages speculation about the foundations of physics, while his preferred approach, loop quantum gravity, encourages more radical thinking. According to Smolin,
 String theory is a powerful, well-motivated idea and deserves much of the work that has been devoted to it. If it has so far failed, the principal reason is that its intrinsic flaws are closely tied to its strengths—and, of course, the story is unfinished, since string theory may well turn out to be part of the truth. The real question is not why we have expended so much energy on string theory but why we haven't expended nearly enough on alternative approaches.[138] Smolin goes on to offer a number of prescriptions for how scientists might encourage a greater diversity of approaches to quantum gravity research.[139]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['condensed matter physics', 'Eugenio Calabi and Shing-Tung Yau', 'the second superstring revolution', 'quantum chromodynamics', 'unexpected and powerful'], 'answer_start': [], 'answer_end': []}"
"
 Street performance or busking is the act of performing in public places for gratuities. In many countries, the rewards are generally in the form of money but other gratuities such as food, drink or gifts may be given. Street performance is practiced all over the world and dates back to antiquity. People engaging in this practice are called street performers or buskers. Outside of New York, buskers is not a term generally used in American English.[1][2]
 Performances are anything that people find entertaining, including acrobatics, animal tricks, balloon twisting, caricatures, clowning, comedy, contortions, escapology, dance, singing, fire skills, flea circus, fortune-telling, juggling, magic, mime, living statue, musical performance, one man band, puppeteering, snake charming, storytelling or reciting poetry or prose, street art such as sketching and painting, street theatre, sword swallowing, ventriloquism and washboarding. Buskers may be solo performers or small groups.
 The term busking was first noted in the English language around the middle 1860s in Great Britain. The verb to busk, from the word busker, comes from the Spanish root word buscar, with the meaning ""to seek"".[3] The Spanish word buscar in turn evolved from the Indo-European word *bhudh-skō (""to win, conquer"").[4] It was used for many street acts, and was the title of a famous Spanish book about one of them, El Buscón. Today, the word is still used in Spanish but mostly reserved for female street sex workers, or mistresses of married men.[citation needed]
 There have been performances in public places for gratuities in every major culture in the world, dating back to antiquity. For many musicians, street performance was the most common means of employment before the advent of recording and personal electronics.[5] Prior to that, a person had to produce any music or entertainment, save for a few mechanical devices such as the barrel organ, the music box, and the piano roll. Organ grinders were commonly found busking in the 19th century and early 20th century.
 Busking is common among some Romani people. Romantic mention of Romani music, dancers and fortune tellers are found in all forms of song poetry, prose and lore. The Roma brought the word busking to England by way of their travels along the Mediterranean coast to Spain and the Atlantic Ocean and then up north to England and the rest of Europe.[citation needed]
 In medieval France, buskers were known by the terms troubadours and jongleurs. In northern France, they were known as trouveres. In old German, buskers were known as Minnesingers and Spielleute. In obsolete French, it evolved to busquer for ""seek, prowl"" and was generally used to describe prostitutes. In Russia, buskers are called skomorokh, and their first recorded history appears around the 11th century.[citation needed]
 Mariachis, Mexican bands that play a style of music by the same name, frequently busk when they perform while traveling through streets and plazas, as well as in restaurants and bars.[6]
 We like playing for big crowds, and the goal all along has been for people to pay a little to come and see us. But it all started on street corners, and that is still very connected to what we do. It's such a validating musical experience. Busking is a very humble and brave act that takes courage to do well. It's also about the energy of music being alive outside in a city ... You can walk right by it right in front of you. Sure, to some people you're just another guy with his hand out, so sometimes busking can be great social barometer. You're able to gauge who you live with on earth.[7]
 Ketch Secor, Old Crow Medicine Show Around the mid-19th century Japanese Chindonya started to be seen using their skills for advertising, and these street performers are still occasionally seen in Japan. Another Japanese street performance form dating from the Edo period is Nankin Tamasudare, in which the performer creates large figures using a bamboo mat.
 In the 19th century, Italian street musicians (mainly from Liguria, Emilia Romagna, Basilicata) began to roam worldwide in search of fortune. Musicians from Basilicata, especially the so-called Viggianesi, would later become professional instrumentalists in symphonic orchestras, especially in the United States.[8] The street musicians from Basilicata are sometimes cited as an influence on Hector Malot's Sans Famille.[9]
 In the United States, medicine shows proliferated in the 19th century. They were traveling vendors selling elixirs and potions which purportedly improved people's health. They would often employ entertainment acts as a way of drawing in potential clients and relaxing them. The people would often associate this feeling of well-being with the products sold. After these performances, they would ""pass the hat"".[citation needed]
 One-man bands have historically performed as buskers playing a variety of instruments simultaneously. One-man bands proliferated in urban areas in the 19th and early 20th centuries and still perform to this day. A current one-man band plays all their instruments acoustically usually combining a guitar, a harmonica, a drum and a tambourine. They may also include singing. Many still busk but some are booked to play at festivals and other events.[citation needed]
 Folk music has always been an important part of the busking scene. Cafe, restaurant, bar and pub busking is a mainstay of this art form. The delta bluesmen were mostly itinerant musicians emanating from the Mississippi Delta region of the USA around the early 1940s and on. B.B. King is one famous example who came from these roots.[citation needed]
 The counterculture of the hippies of the 1960s occasionally staged ""be-ins"", which resembled some present-day buskers festivals. Bands and performers would gather at public places and perform for free, passing the hat to make money. The San Francisco Bay Area was at the epicenter of this movement – be-ins were staged at Golden Gate Park and San Jose's Bee Stadium and other venues. Some of the bands that performed in this manner were Janis Joplin with Big Brother and the Holding Company, the Grateful Dead, Jefferson Airplane, Quicksilver Messenger Service, Country Joe and the Fish, Moby Grape and Jimi Hendrix.[citation needed]
 Christmas caroling can also be a form of busking, as wassailing included singing for alms, wassail or some other form of refreshment such as figgy pudding. In the Republic of Ireland, the traditional Wren Boys, and in England Morris Dancing can be considered part of the busking tradition.[citation needed]
 In India and Pakistan's Gujarati region, Bhavai is a form of street art where there are plays enacted in the village, the barot or the village singer also is part of the local entertainment scene.[citation needed]
 In the 2000s, some performers have begun ""Cyber Busking"". Artists post work or performances on the Internet for people to download or ""stream"" and if people like it they make a donation using PayPal.[citation needed]
 There are three basic forms of street performance: circle shows, walk-by acts, and stoplight performances.
 ""Circle shows"" are shows that tend to gather a crowd around them. They usually have a distinct beginning and end. Usually these are done in conjunction with street theatre, puppeteering, magicians, comedians, acrobats, jugglers and sometimes musicians. Circle shows can be the most lucrative. Sometimes the crowds attracted can be very large. A good busker will control the crowd so the patrons do not obstruct foot traffic.
 ""Walk-by acts"" are acts where the busker performs a musical, living statue or other act that does not have a distinct beginning or end, and the public usually watches for a brief time. A walk-by act may turn into a circle show if the act is unusual or very popular.
 ""Stoplight performances"" are performances in which performers present their act and get contributions from vehicle occupants on a crosswalk while the traffic lights are red. A variety of disciplines can be used in such a format (juggling, break dancing, even magic tricks). Because of the short period of time available to them, stoplight performers must have a very brief, condensed routine. This form is seen more commonly in Latin America than elsewhere.
 Buskers collect donations and tips from the public in a variety of containers and by different methods depending on the type of busking they are performing. For walk-by acts, their open, empty instrument case or a special can, box, or hat is often used. For circle shows the performer will typically collect money at the end of the show, although some performers will also collect during the show, as some audience members do not stay for the entire performance. 
 Sometimes a performer will employ a bottler, hat man, or pitch man to collect money from the audience and encourage them to contribute, sometimes by cajoling them in a humorous fashion. The term bottler is a British term that originated from the use of the top half of a bottle to collect money. The bottle had a leather flap inserted in the bottleneck and a leather pouch attached. This design allowed coins to be put in the bottle but did not allow them to be removed easily without the coins jingling against the glass. The first use of such contrivances was recorded by the famous Punch and Judy troupe of puppeteers in early Victorian times.[10]
 The place where a performance occurs is called a ""pitch"". A good pitch can be the key to success as a busker. An act that might make money at one place and time may not work at all in another setting. Popular pitches tend to be public places with large volumes of pedestrian traffic, high visibility, low background noise and as few elements of interference as possible. Good locations may include tourist spots, popular parks, entertainment districts including many restaurants, cafés, bars and pubs and theaters, subways and bus stops, outside the entrances to large concerts and sporting events, almost any plaza or town square as well as zócalos in Latin America and piazzas in other regions. Other places include shopping malls, strip malls, and outside supermarkets, although permission is usually required from management for these.
 In her book, Underground Harmonies: Music and Politics in the Subways of New York, Susie J. Tanenbaum examined how the adage ""Music hath charms to soothe the savage beast"" plays out in regards to busking. Her sociological studies showed that in areas where buskers regularly perform, crime rates tended to go down, and that those with higher education attainment tended to have a more positive view of buskers than did those of lesser educational attainment.[11] Some cities encourage busking in particular areas,[12] giving preference to city government-approved buskers and even publishing schedules of performances.[13]
 Many cities in the United States have particular areas known to be popular spots for buskers. Performers are found at many locations like Mallory Square in Key West, in New Orleans, in New York around Central Park, Washington Square, and the subway systems, in San Francisco, in Washington, D.C. around the transit centers, in Los Angeles around Venice Beach, the Santa Monica Third Street Promenade, and the Hollywood area, in Chicago on Maxwell Street, in the Delmar Loop district of St. Louis, and many other locations throughout the US.
Busking is still quite common in Scotland, Ireland (Grafton Street, Dublin), and England with musicians and other street performers of varying talent levels.
 The first recorded instances of laws affecting buskers were in ancient Rome in 462 BC. The Law of the Twelve Tables made it a crime to sing about or make parodies of the government or its officials in public places; the penalty was death.[14][15] Louis the Pious ""excluded histriones and scurrae, which included all entertainers without noble protection, from the privilege of justice"".[16] In 1530 Henry VIII ordered the licensing of minstrels and players, fortune-tellers, pardoners and fencers, as well as beggars who could not work. If they did not obey they could be whipped on two consecutive days.[17]
 In the United States under constitutional law and most European common law, the protection of artistic free speech extends to busking. In the U.S. and many countries, the designated places for free speech behavior are the public parks, streets, sidewalks, thoroughfares and town squares or plazas. Under certain circumstances even private property may be open to buskers, particularly if it is open to the general public and busking does not interfere with its function and management allows it or other forms of free speech behaviors or has a history of doing so.[18]
 While there is no universal code of conduct for buskers, there are common law practices that buskers must conform to. Most jurisdictions have corresponding statutory laws. In the UK busking regulation is not universal with most laws (if there are any) being governed by local councils.[19] Some towns in the British Isles limit the licenses issued to bagpipers because of the volume and difficulty of the instrument.[citation needed] In Great Britain places requiring licenses for buskers may also require auditions of anyone applying for a busking license.[citation needed] Oxford City Council have decided to enact a public spaces protection order. Some venues that do not regulate busking may still ask performers to abide by voluntary rules. Some places require a special permit to use electronically amplified sound and may have limits on the volume of sound produced.[20] It is common law that buskers or others should not impede pedestrian traffic flow, block or otherwise obstruct entrances or exits, or do things that endanger the public. It is common law that any disturbing or noisy behaviors may not be conducted after certain hours in the night. These curfew limitations vary from jurisdiction to jurisdiction. It is common law that ""performing blue"" (i.e. using material that is sexually explicit or any vulgar or obscene remarks or gestures) is generally prohibited unless performing for an adults-only environment such as in a bar or pub.
 In London, busking is prohibited in the entire area of the City of London. The London Underground provides busking permits for up to 39 pitches across 25 central London stations.[21] Most London boroughs do not license busking, but they have optional powers, under the London Local Authorities Act 2000, if there is sufficient reason to do so. Where these powers have not been adopted, councils can rely on other legislation including the Environmental Protection Act 1990 to deal with noise nuisance from buskers and the Highways Act 1980 to deal with obstructions. Camden Council is currently looking into further options to control the problem of nuisance buskers and the playing of amplified music to the detriment of local residents and businesses.[22]
 Buskers may find themselves targeted by thieves due to the very open and public nature of their craft. Buskers may have their earnings, instruments or props stolen. One particular technique that thieves use against buskers is to pretend to make a donation while actually taking money out instead, a practice known as ""dipping"" or ""skimming"". George Burns described his days as a youthful busker this way:[23]
 Sometimes the customers threw something in the hats. Sometimes they took something out of the hats. Sometimes they took the hats. Organisations
 Press
 Other
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the playing of amplified music', 'thieves', 'antiquity', 'playing of amplified music to the detriment of local residents and businesses', 'very open and public nature of their craft'], 'answer_start': [], 'answer_end': []}"
"Other reasons this message may be displayed:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['this message may be displayed:', 'this message may be displayed:', 'this message may be displayed', 'this message may be displayed:', 'reasons'], 'answer_start': [], 'answer_end': []}"
"
 Jazz is a music genre that originated in the African-American communities of New Orleans, Louisiana, in the late 19th and early 20th centuries, with its roots in blues, ragtime, European harmony and African rhythmic rituals.[1][2][3][4][5][6] Since the 1920s Jazz Age, it has been recognized as a major form of musical expression in traditional and popular music. Jazz is characterized by swing and blue notes, complex chords, call and response vocals, polyrhythms and improvisation.
 As jazz spread around the world, it drew on national, regional, and local musical cultures, which gave rise to different styles. New Orleans jazz began in the early 1910s, combining earlier brass band marches, French quadrilles, biguine, ragtime and blues with collective polyphonic improvisation. However, jazz did not begin as a single musical tradition in New Orleans or elsewhere.[7] In the 1930s, arranged dance-oriented swing big bands, Kansas City jazz (a hard-swinging, bluesy, improvisational style), and gypsy jazz (a style that emphasized musette waltzes) were the prominent styles. Bebop emerged in the 1940s, shifting jazz from danceable popular music toward a more challenging ""musician's music"" which was played at faster tempos and used more chord-based improvisation. Cool jazz developed near the end of the 1940s, introducing calmer, smoother sounds and long, linear melodic lines.[8]
 The mid-1950s saw the emergence of hard bop, which introduced influences from rhythm and blues, gospel, and blues to small groups and particularly to saxophone and piano. Modal jazz developed in the late 1950s, using the mode, or musical scale, as the basis of musical structure and improvisation, as did free jazz, which explored playing without regular meter, beat and formal structures. Jazz-rock fusion appeared in the late 1960s and early 1970s, combining jazz improvisation with rock music's rhythms, electric instruments, and highly amplified stage sound. In the early 1980s, a commercial form of jazz fusion called smooth jazz became successful, garnering significant radio airplay. Other styles and genres abound in the 21st century, such as Latin and Afro-Cuban jazz.
 The origin of the word jazz has resulted in considerable research, and its history is well documented. It is believed to be related to jasm, a slang term dating back to 1860 meaning 'pep, energy'.[9][10] The earliest written record of the word is in a 1912 article in the Los Angeles Times in which a minor league baseball pitcher described a pitch which he called a 'jazz ball' ""because it wobbles and you simply can't do anything with it"".[9][10]
 The use of the word in a musical context was documented as early as 1915 in the Chicago Daily Tribune.[10][11] Its first documented use in a musical context in New Orleans was in a November 14, 1916, Times-Picayune article about ""jas bands"".[12] In an interview with National Public Radio, musician Eubie Blake offered his recollections of the slang connotations of the term, saying: ""When Broadway picked it up, they called it 'J-A-Z-Z'. It wasn't called that. It was spelled 'J-A-S-S'. That was dirty, and if you knew what it was, you wouldn't say it in front of ladies.""[13] The American Dialect Society named it the Word of the 20th Century.[14]
 Jazz is difficult to define because it encompasses a wide range of music spanning a period of over 100 years, from ragtime to rock-infused fusion. Attempts have been made to define jazz from the perspective of other musical traditions, such as European music history or African music. But critic Joachim-Ernst Berendt argues that its terms of reference and its definition should be broader,[15] defining jazz as a ""form of art music which originated in the United States through the confrontation of the Negro with European music""[16] and arguing that it differs from European music in that jazz has a ""special relationship to time defined as 'swing'"". Jazz involves ""a spontaneity and vitality of musical production in which improvisation plays a role"" and contains a ""sonority and manner of phrasing which mirror the individuality of the performing jazz musician"".[15]
 A broader definition that encompasses different eras of jazz has been proposed by Travis Jackson: ""it is music that includes qualities such as swing, improvising, group interaction, developing an 'individual voice', and being open to different musical possibilities"".[17] Krin Gibbard argued that ""jazz is a construct"" which designates ""a number of musics with enough in common to be understood as part of a coherent tradition"".[18] Duke Ellington, one of jazz's most famous figures, said, ""It's all music.""[19]
 Although jazz is considered difficult to define, in part because it contains many subgenres, improvisation is one of its defining elements. The centrality of improvisation is attributed to the influence of earlier forms of music such as blues, a form of folk music which arose in part from the work songs and field hollers of African-American slaves on plantations. These work songs were commonly structured around a repetitive call-and-response pattern, but early blues was also improvisational. Classical music performance is evaluated more by its fidelity to the musical score, with less attention given to interpretation, ornamentation, and accompaniment. The classical performer's goal is to play the composition as it was written. In contrast, jazz is often characterized by the product of interaction and collaboration, placing less value on the contribution of the composer, if there is one, and more on the performer.[20] The jazz performer interprets a tune in individual ways, never playing the same composition twice. Depending on the performer's mood, experience, and interaction with band members or audience members, the performer may change melodies, harmonies, and time signatures.[21]
 In early Dixieland, a.k.a. New Orleans jazz, performers took turns playing melodies and improvising countermelodies. In the swing era of the 1920s–'40s, big bands relied more on arrangements which were written or learned by ear and memorized. Soloists improvised within these arrangements. In the bebop era of the 1940s, big bands gave way to small groups and minimal arrangements in which the melody was stated briefly at the beginning and most of the piece was improvised. Modal jazz abandoned chord progressions to allow musicians to improvise even more. In many forms of jazz, a soloist is supported by a rhythm section of one or more chordal instruments (piano, guitar), double bass, and drums. The rhythm section plays chords and rhythms that outline the composition structure and complement the soloist.[22] In avant-garde and free jazz, the separation of soloist and band is reduced, and there is license, or even a requirement, for the abandoning of chords, scales, and meters.
 Since the emergence of bebop, forms of jazz that are commercially oriented or influenced by popular music have been criticized. According to Bruce Johnson, there has always been a ""tension between jazz as a commercial music and an art form"".[17] Regarding the Dixieland jazz revival of the 1940s, Black musicians rejected it as being shallow nostalgia entertainment for white audiences.[23][24] On the other hand, traditional jazz enthusiasts have dismissed bebop, free jazz, and jazz fusion as forms of debasement and betrayal. An alternative view is that jazz can absorb and transform diverse musical styles.[25] By avoiding the creation of norms, jazz allows avant-garde styles to emerge.[17]
 For some African Americans, jazz has drawn attention to African-American contributions to culture and history. For others, jazz is a reminder of ""an oppressive and racist society and restrictions on their artistic visions"".[26] Amiri Baraka argues that there is a ""white jazz"" genre that expresses whiteness.[27] White jazz musicians appeared in the Midwest and in other areas throughout the U.S. Papa Jack Laine, who ran the Reliance band in New Orleans in the 1910s, was called ""the father of white jazz"".[28] The Original Dixieland Jazz Band, whose members were white, were the first jazz group to record, and Bix Beiderbecke was one of the most prominent jazz soloists of the 1920s.[29] The Chicago Style was developed by white musicians such as Eddie Condon, Bud Freeman, Jimmy McPartland, and Dave Tough. Others from Chicago such as Benny Goodman and Gene Krupa became leading members of swing during the 1930s.[30] Many bands included both Black and white musicians. These musicians helped change attitudes toward race in the U.S.[31]
 Female jazz performers and composers have contributed to jazz throughout its history. Although Betty Carter, Ella Fitzgerald, Adelaide Hall, Billie Holiday, Peggy Lee, Abbey Lincoln, Anita O'Day, Dinah Washington, and Ethel Waters were recognized for their vocal talent, less familiar were bandleaders, composers, and instrumentalists such as pianist Lil Hardin Armstrong, trumpeter Valaida Snow, and songwriters Irene Higginbotham and Dorothy Fields. Women began playing instruments in jazz in the early 1920s, drawing particular recognition on piano.[32]
 When male jazz musicians were drafted during World War II, many all-female bands replaced them.[32] The International Sweethearts of Rhythm, which was founded in 1937, was a popular band that became the first all-female integrated band in the U.S. and the first to travel with the USO, touring Europe in 1945. Women were members of the big bands of Woody Herman and Gerald Wilson. Beginning in the 1950s, many women jazz instrumentalists were prominent, some sustaining long careers. Some of the most distinctive improvisers, composers, and bandleaders in jazz have been women.[33] Trombonist Melba Liston is acknowledged as the first female horn player to work in major bands and to make a real impact on jazz, not only as a musician but also as a respected composer and arranger, particularly through her collaborations with Randy Weston from the late 1950s into the 1990s.[34][35]
 Jewish Americans played a significant role in jazz. As jazz spread, it developed to encompass many different cultures, and the work of Jewish composers in Tin Pan Alley helped shape the many different sounds that jazz came to incorporate.[36]
 Jewish Americans were able to thrive in Jazz because of the probationary whiteness that they were allotted at the time.[37] George Bornstein wrote that African Americans were sympathetic to the plight of the Jewish American and vice versa. As disenfranchised minorities themselves, Jewish composers of popular music saw themselves as natural allies with African Americans.[38]
 The Jazz Singer with Al Jolson is one example of how Jewish Americans were able to bring jazz, music that African Americans developed, into popular culture.[39] Benny Goodman was a vital Jewish American to the progression of Jazz. Goodman was the leader of a racially integrated band named King of Swing. His jazz concert in the Carnegie Hall in 1938 was the first ever to be played there. The concert was described by Bruce Eder as ""the single most important jazz or popular music concert in history"".[40]
 Shep Fields also helped to popularize ""Sweet"" Jazz music through his appearances and Big band remote broadcasts from such landmark venues as Chicago's Palmer House, Broadway's Paramount Theater and the Starlight Roof at the famed Waldorf-Astoria Hotel. He entertained audiences with a light elegant musical style which remained popular with audiences for nearly three decades from the 1930s until the late 1950s.[41][42][43]
 Jazz originated in the late-19th to early-20th century.  It developed out of many forms of music, including blues, spirituals, hymns, marches, vaudeville song, ragtime, and dance music.[44] It also incorporated interpretations of American and European classical music, entwined with African and slave folk songs and the influences of West African culture.[45] Its composition and style have changed many times throughout the years with each performer's personal interpretation and improvisation, which is also one of the greatest appeals of the genre.[46]
 By the 18th century, slaves in the New Orleans area gathered socially at a special market, in an area which later became known as Congo Square, famous for its African dances.[47]
 By 1866, the Atlantic slave trade had brought nearly 400,000 Africans to North America.[48] The slaves came largely from West Africa and the greater Congo River basin and brought strong musical traditions with them.[49] The African traditions primarily use a single-line melody and call-and-response pattern, and the rhythms have a counter-metric structure and reflect African speech patterns.[50]
 An 1885 account says that they were making strange music (Creole) on an equally strange variety of 'instruments'—washboards, washtubs, jugs, boxes beaten with sticks or bones and a drum made by stretching skin over a flour-barrel.[4][51]
 Lavish festivals with African-based dances to drums were organized on Sundays at Place Congo, or Congo Square, in New Orleans until 1843.[52] There are historical accounts of other music and dance gatherings elsewhere in the southern United States. Robert Palmer said of percussive slave music:
 Usually such music was associated with annual festivals, when the year's crop was harvested and several days were set aside for celebration. As late as 1861, a traveler in North Carolina saw dancers dressed in costumes that included horned headdresses and cow tails and heard music provided by a sheepskin-covered ""gumbo box"", apparently a frame drum; triangles and jawbones furnished the auxiliary percussion. There are quite a few [accounts] from the southeastern states and Louisiana dating from the period 1820–1850. Some of the earliest [Mississippi] Delta settlers came from the vicinity of New Orleans, where drumming was never actively discouraged for very long and homemade drums were used to accompany public dancing until the outbreak of the Civil War.[53]
 Another influence came from the harmonic style of hymns of the church, which black slaves had learned and incorporated into their own music as spirituals.[54] The origins of the blues are undocumented, though they can be seen as the secular counterpart of the spirituals. However, as Gerhard Kubik points out, whereas the spirituals are homophonic, rural blues and early jazz ""was largely based on concepts of heterophony"".[55]
 During the early 19th century an increasing number of black musicians learned to play European instruments, particularly the violin, which they used to parody European dance music in their own cakewalk dances. In turn, European American minstrel show performers in blackface popularized the music internationally, combining syncopation with European harmonic accompaniment. In the mid-1800s the white New Orleans composer Louis Moreau Gottschalk adapted slave rhythms and melodies from Cuba and other Caribbean islands into piano salon music. New Orleans was the main nexus between the Afro-Caribbean and African American cultures.
 The Black Codes outlawed drumming by slaves, which meant that African drumming traditions were not preserved in North America, unlike in Cuba, Haiti, and elsewhere in the Caribbean. African-based rhythmic patterns were retained in the United States in large part through ""body rhythms"" such as stomping, clapping, and patting juba dancing.[56]
 In the opinion of jazz historian Ernest Borneman, what preceded New Orleans jazz before 1890 was ""Afro-Latin music"", similar to what was played in the Caribbean at the time.[57] A three-stroke pattern known in Cuban music as tresillo is a fundamental rhythmic figure heard in many different slave musics of the Caribbean, as well as the Afro-Caribbean folk dances performed in New Orleans Congo Square and Gottschalk's compositions (for example ""Souvenirs From Havana"" (1859)). Tresillo (shown below) is the most basic and most prevalent duple-pulse rhythmic cell in sub-Saharan African music traditions and the music of the African Diaspora.[58][59]
 Tresillo is heard prominently in New Orleans second line music and in other forms of popular music from that city from the turn of the 20th century to present.[60] ""By and large the simpler African rhythmic patterns survived in jazz ... because they could be adapted more readily to European rhythmic conceptions,"" jazz historian Gunther Schuller observed. ""Some survived, others were discarded as the Europeanization progressed.""[61]
 In the post-Civil War period (after 1865), African Americans were able to obtain surplus military bass drums, snare drums and fifes, and an original African-American drum and fife music emerged, featuring tresillo and related syncopated rhythmic figures.[62] This was a drumming tradition that was distinct from its Caribbean counterparts, expressing a uniquely African-American sensibility. ""The snare and bass drummers played syncopated cross-rhythms,"" observed the writer Robert Palmer, speculating that ""this tradition must have dated back to the latter half of the nineteenth century, and it could have not have developed in the first place if there hadn't been a reservoir of polyrhythmic sophistication in the culture it nurtured.""[56]
 African-American music began incorporating Afro-Cuban rhythmic motifs in the 19th century when the habanera (Cuban contradanza) gained international popularity.[63] Musicians from Havana and New Orleans would take the twice-daily ferry between both cities to perform, and the habanera quickly took root in the musically fertile Crescent City. John Storm Roberts states that the musical genre habanera ""reached the U.S. twenty years before the first rag was published.""[64] For the more than quarter-century in which the cakewalk, ragtime, and proto-jazz were forming and developing, the habanera was a consistent part of African-American popular music.[64]
 Habaneras were widely available as sheet music and were the first written music which was rhythmically based on an African motif (1803).[65] From the perspective of African-American music, the ""habanera rhythm"" (also known as ""congo""),[65] ""tango-congo"",[66] or tango.[67] can be thought of as a combination of tresillo and the backbeat.[68] The habanera was the first of many Cuban music genres which enjoyed periods of popularity in the United States and reinforced and inspired the use of tresillo-based rhythms in African-American music.
 New Orleans native Louis Moreau Gottschalk's piano piece ""Ojos Criollos (Danse Cubaine)"" (1860) was influenced by the composer's studies in Cuba: the habanera rhythm is clearly heard in the left hand.[58]: 125  In Gottschalk's symphonic work ""A Night in the Tropics"" (1859), the tresillo variant cinquillo appears extensively.[69] The figure was later used by Scott Joplin and other ragtime composers.
 Comparing the music of New Orleans with the music of Cuba, Wynton Marsalis observes that tresillo is the New Orleans ""clavé"", a Spanish word meaning ""code"" or ""key"", as in the key to a puzzle, or mystery.[70] Although the pattern is only half a clave, Marsalis makes the point that the single-celled figure is the guide-pattern of New Orleans music. Jelly Roll Morton called the rhythmic figure the Spanish tinge and considered it an essential ingredient of jazz.[71]
 The abolition of slavery in 1865 led to new opportunities for the education of freed African Americans. Although strict segregation limited employment opportunities for most blacks, many were able to find work in entertainment. Black musicians were able to provide entertainment in dances, minstrel shows, and in vaudeville, during which time many marching bands were formed. Black pianists played in bars, clubs, and brothels, as ragtime developed.[72][73]
 Ragtime appeared as sheet music, popularized by African-American musicians such as the entertainer Ernest Hogan, whose hit songs appeared in 1895. Two years later, Vess Ossman recorded a medley of these songs as a banjo solo known as ""Rag Time Medley"".[74][75] Also in 1897, the white composer William Krell published his ""Mississippi Rag"" as the first written piano instrumental ragtime piece, and Tom Turpin published his ""Harlem Rag"", the first rag published by an African-American.
 Classically trained pianist Scott Joplin produced his ""Original Rags"" in 1898 and, in 1899, had an international hit with ""Maple Leaf Rag"", a multi-strain ragtime march with four parts that feature recurring themes and a bass line with copious seventh chords. Its structure was the basis for many other rags, and the syncopations in the right hand, especially in the transition between the first and second strain, were novel at the time.[76] The last four measures of Scott Joplin's ""Maple Leaf Rag"" (1899) are shown below.
 African-based rhythmic patterns such as tresillo and its variants, the habanera rhythm and cinquillo, are heard in the ragtime compositions of Joplin and Turpin. Joplin's ""Solace"" (1909) is generally considered to be in the habanera genre:[77][78] both of the pianist's hands play in a syncopated fashion, completely abandoning any sense of a march rhythm. Ned Sublette postulates that the tresillo/habanera rhythm ""found its way into ragtime and the cakewalk,""[79] whilst Roberts suggests that ""the habanera influence may have been part of what freed black music from ragtime's European bass"".[80]
 In the northeastern United States, a ""hot"" style of playing ragtime had developed, notably James Reese Europe's symphonic Clef Club orchestra in New York City, which played a benefit concert at Carnegie Hall in 1912.[81][82] The Baltimore rag style of Eubie Blake influenced James P. Johnson's development of stride piano playing, in which the right hand plays the melody, while the left hand provides the rhythm and bassline.[83]
 In Ohio and elsewhere in the mid-west the major influence was ragtime, until about 1919. Around 1912, when the four-string banjo and saxophone came in, musicians began to improvise the melody line, but the harmony and rhythm remained unchanged. A contemporary account states that blues could only be heard in jazz in the gut-bucket cabarets, which were generally looked down upon by the Black middle-class.[84]
 Blues is the name given to both a musical form and a music genre,[85] which originated in African-American communities of primarily the Deep South of the United States at the end of the 19th century from their spirituals, work songs, field hollers, shouts and chants and rhymed simple narrative ballads.[86]
 The African use of pentatonic scales contributed to the development of blue notes in blues and jazz.[87] As Kubik explains:
 Many of the rural blues of the Deep South are stylistically an extension and merger of basically two broad accompanied song-style traditions in the west central Sudanic belt:
 W. C. Handy became interested in folk blues of the Deep South while traveling through the Mississippi Delta. In this folk blues form, the singer would improvise freely within a limited melodic range, sounding like a field holler, and the guitar accompaniment was slapped rather than strummed, like a small drum which responded in syncopated accents, functioning as another ""voice"".[89] Handy and his band members were formally trained African-American musicians who had not grown up with the blues, yet he was able to adapt the blues to a larger band instrument format and arrange them in a popular music form.
 Handy wrote about his adopting of the blues:
 The primitive southern Negro, as he sang, was sure to bear down on the third and seventh tone of the scale, slurring between major and minor. Whether in the cotton field of the Delta or on the Levee up St. Louis way, it was always the same. Till then, however, I had never heard this slur used by a more sophisticated Negro, or by any white man. I tried to convey this effect ... by introducing flat thirds and sevenths (now called blue notes) into my song, although its prevailing key was major ... , and I carried this device into my melody as well.[90] The publication of his ""Memphis Blues"" sheet music in 1912 introduced the 12-bar blues to the world (although Gunther Schuller argues that it is not really a blues, but ""more like a cakewalk"").[91] This composition, as well as his later ""St. Louis Blues"" and others, included the habanera rhythm,[92] and would become jazz standards. Handy's music career began in the pre-jazz era and contributed to the codification of jazz through the publication of some of the first jazz sheet music.
 The music of New Orleans, Louisiana had a profound effect on the creation of early jazz. In New Orleans, slaves could practice elements of their culture such as voodoo and playing drums.[93] Many early jazz musicians played in the bars and brothels of the red-light district around Basin Street called Storyville.[94] In addition to dance bands, there were marching bands which played at lavish funerals (later called jazz funerals). The instruments used by marching bands and dance bands became the instruments of jazz: brass, drums, and reeds tuned in the European 12-tone scale. Small bands contained a combination of self-taught and formally educated musicians, many from the funeral procession tradition. These bands traveled in black communities in the deep south. Beginning in 1914, Louisiana Creole and African-American musicians played in vaudeville shows which carried jazz to cities in the northern and western parts of the U.S.[95] Jazz became international in 1914, when the Creole Band with cornettist Freddie Keppard performed the first ever jazz concert outside the United States, at the Pantages Playhouse Theatre in Winnipeg, Canada.[96]
 In New Orleans, a white bandleader named Papa Jack Laine integrated blacks and whites in his marching band. He was known as ""the father of white jazz"" because of the many top players he employed, such as George Brunies, Sharkey Bonano, and future members of the Original Dixieland Jass Band. During the early 1900s, jazz was mostly performed in African-American and mulatto communities due to segregation laws. Storyville brought jazz to a wider audience through tourists who visited the port city of New Orleans.[97] Many jazz musicians from African-American communities were hired to perform in bars and brothels. These included Buddy Bolden and Jelly Roll Morton in addition to those from other communities, such as Lorenzo Tio and Alcide Nunez. Louis Armstrong started his career in Storyville[98] and found success in Chicago. Storyville was shut down by the U.S. government in 1917.[99]
 Cornetist Buddy Bolden played in New Orleans from 1895 to 1906. No recordings by him exist. His band is credited with creating the big four: the first syncopated bass drum pattern to deviate from the standard on-the-beat march.[100] As the example below shows, the second half of the big four pattern is the habanera rhythm.
 Afro-Creole pianist Jelly Roll Morton began his career in Storyville. Beginning in 1904, he toured with vaudeville shows to southern cities, Chicago, and New York City. In 1905, he composed ""Jelly Roll Blues"", which became the first jazz arrangement in print when it was published in 1915. It introduced more musicians to the New Orleans style.[101]
 Morton considered the tresillo/habanera, which he called the Spanish tinge, an essential ingredient of jazz.[102] ""Now in one of my earliest tunes, ""New Orleans Blues,"" you can notice the Spanish tinge. In fact, if you can't manage to put tinges of Spanish in your tunes, you will never be able to get the right seasoning, I call it, for jazz.""[71]
 An excerpt of ""New Orleans Blues"" is shown below. In the excerpt, the left hand plays the tresillo rhythm, while the right hand plays variations on cinquillo.
 Morton was a crucial innovator in the evolution from the early jazz form known as ragtime to jazz piano, and could perform pieces in either style; in 1938, Morton made a series of recordings for the Library of Congress in which he demonstrated the difference between the two styles. Morton's solos, however, were still close to ragtime, and were not merely improvisations over chord changes as in later jazz, but his use of the blues was of equal importance.
 Morton loosened ragtime's rigid rhythmic feeling, decreasing its embellishments and employing a swing feeling.[103] Swing is the most important and enduring African-based rhythmic technique used in jazz. An oft quoted definition of swing by Louis Armstrong is: ""if you don't feel it, you'll never know it.""[104] The New Harvard Dictionary of Music states that swing is: ""An intangible rhythmic momentum in jazz...Swing defies analysis; claims to its presence may inspire arguments."" The dictionary does nonetheless provide the useful description of triple subdivisions of the beat contrasted with duple subdivisions:[105] swing superimposes six subdivisions of the beat over a basic pulse structure or four subdivisions. This aspect of swing is far more prevalent in African-American music than in Afro-Caribbean music. One aspect of swing, which is heard in more rhythmically complex Diaspora musics, places strokes in-between the triple and duple-pulse ""grids"".[106]
 New Orleans brass bands are a lasting influence, contributing horn players to the world of professional jazz with the distinct sound of the city whilst helping black children escape poverty. The leader of New Orleans' Camelia Brass Band, D'Jalma Ganier, taught Louis Armstrong to play trumpet; Armstrong would then popularize the New Orleans style of trumpet playing, and then expand it. Like Jelly Roll Morton, Armstrong is also credited with the abandonment of ragtime's stiffness in favor of swung notes. Armstrong, perhaps more than any other musician, codified the rhythmic technique of swing in jazz and broadened the jazz solo vocabulary.[107]
 The Original Dixieland Jass Band made the music's first recordings early in 1917, and their ""Livery Stable Blues"" became the earliest released jazz record.[108][109][110][111][112][113][114] That year, numerous other bands made recordings featuring ""jazz"" in the title or band name, but most were ragtime or novelty records rather than jazz. In February 1918 during World War I, James Reese Europe's ""Hellfighters"" infantry band took ragtime to Europe,[115][116] then on their return recorded Dixieland standards including ""Darktown Strutters' Ball"".[81]
 From 1920 to 1933, Prohibition in the United States banned the sale of alcoholic drinks, resulting in illicit speakeasies which became lively venues of the ""Jazz Age"", hosting popular music, dance songs, novelty songs, and show tunes. Jazz began to get a reputation as immoral, and many members of the older generations saw it as a threat to the old cultural values by promoting the decadent values of the Roaring 20s. Henry van Dyke of Princeton University wrote, ""... it is not music at all. It's merely an irritation of the nerves of hearing, a sensual teasing of the strings of physical passion.""[117] The New York Times reported that Siberian villagers used jazz to scare away bears, but the villagers had used pots and pans; another story claimed that the fatal heart attack of a celebrated conductor was caused by jazz.[117]
 In 1919, Kid Ory's Original Creole Jazz Band of musicians from New Orleans began playing in San Francisco and Los Angeles, where in 1922 they became the first black jazz band of New Orleans origin to make recordings.[118][119] During the same year, Bessie Smith made her first recordings.[120] Chicago was developing ""Hot Jazz"", and King Oliver joined Bill Johnson. Bix Beiderbecke formed The Wolverines in 1924.
 Despite its Southern black origins, there was a larger market for jazzy dance music played by white orchestras. In 1918, Paul Whiteman and his orchestra became a hit in San Francisco. He signed a contract with Victor and became the top bandleader of the 1920s, giving hot jazz a white component, hiring white musicians such as Bix Beiderbecke, Jimmy Dorsey, Tommy Dorsey, Frankie Trumbauer, and Joe Venuti. In 1924, Whiteman commissioned George Gershwin's Rhapsody in Blue, which was premiered by his orchestra. Jazz began to be recognized as a notable musical form. Olin Downes, reviewing the concert in The New York Times, wrote, ""This composition shows extraordinary talent, as it shows a young composer with aims that go far beyond those of his ilk, struggling with a form of which he is far from being master. ... In spite of all this, he has expressed himself in a significant and, on the whole, highly original form. ... His first theme ... is no mere dance-tune ... it is an idea, or several ideas, correlated and combined in varying and contrasting rhythms that immediately intrigue the listener.""[121]
 After Whiteman's band successfully toured Europe, huge hot jazz orchestras in theater pits caught on with other whites, including Fred Waring, Jean Goldkette, and Nathaniel Shilkret. According to Mario Dunkel, Whiteman's success was based on a ""rhetoric of domestication"" according to which he had elevated and rendered valuable (read ""white"") a previously inchoate (read ""black"") kind of music.[122]
 Whiteman's success caused black artists to follow suit, including Earl Hines (who opened in The Grand Terrace Cafe in Chicago in 1928), Washington, D.C.-native Duke Ellington (who opened at the Cotton Club in Harlem in 1927), Lionel Hampton, Fletcher Henderson, Claude Hopkins, and Don Redman, with Henderson and Redman developing the ""talking to one another"" formula for ""hot"" swing music.[123]
 In 1924, Louis Armstrong joined the Fletcher Henderson dance band for a year, as featured soloist. By 1924, one of Armstrong's favorite ""Sweet Jazz"" Big bands was also formed in Canada by Guy Lombardo. His Royal Canadians Orchestra specialized in performances of ""the Sweetest music this side of Heaven"" which transcended racial boundaries.[124][125]  The original New Orleans style was polyphonic, with theme variation and simultaneous collective improvisation. Armstrong was a master of his hometown style, but by the time he joined Henderson's band, he was already a trailblazer in a new phase of jazz, with its emphasis on arrangements and soloists. Armstrong's solos went well beyond the theme-improvisation concept and extemporized on chords, rather than melodies. According to Schuller, by comparison, the solos by Armstrong's bandmates (including a young Coleman Hawkins), sounded ""stiff, stodgy"", with ""jerky rhythms and a grey undistinguished tone quality"".[126] The following example shows a short excerpt of the straight melody of ""Mandy, Make Up Your Mind"" by George W. Meyer and Arthur Johnston (top), compared with Armstrong's solo improvisations (below) (recorded 1924).[127] Armstrong's solos were a significant factor in making jazz a true 20th-century language. After leaving Henderson's group, Armstrong formed his Hot Five band, where he popularized scat singing.[128]
 The 1930s belonged to popular swing big bands, in which some virtuoso soloists became as famous as the band leaders. Key figures in developing the ""big"" jazz band included bandleaders and arrangers Count Basie, Cab Calloway, Jimmy and Tommy Dorsey, Duke Ellington, Benny Goodman, Fletcher Henderson, Earl Hines, Harry James, Jimmie Lunceford, Glenn Miller and Artie Shaw. Although it was a collective sound, swing also offered individual musicians a chance to ""solo"" and improvise melodic, thematic solos which could at times be complex ""important"" music.
 Over time, social strictures regarding racial segregation began to relax in America: white bandleaders began to recruit black musicians and black bandleaders white ones. In the mid-1930s, Benny Goodman hired pianist Teddy Wilson, vibraphonist Lionel Hampton and guitarist Charlie Christian to join small groups. In the 1930s, Kansas City Jazz as exemplified by tenor saxophonist Lester Young marked the transition from big bands to the bebop influence of the 1940s. An early 1940s style known as ""jumping the blues"" or jump blues used small combos, uptempo music and blues chord progressions, drawing on boogie-woogie from the 1930s.
 While swing was reaching the height of its popularity, Duke Ellington spent the late 1920s and 1930s in Washington, D.C's jazz scene, developing an innovative musical idiom for his orchestra. Abandoning the conventions of swing, he experimented with orchestral sounds, harmony, and musical form with complex compositions that still translated well for popular audiences; some of his tunes became hits, and his own popularity spanned from the United States to Europe.[129]
 Ellington called his music American Music, rather than jazz, and liked to describe those who impressed him as ""beyond category"".[130] These included many musicians from his orchestra, some of whom are considered among the best in jazz in their own right, but it was Ellington who melded them into one of the most popular jazz orchestras in the history of jazz. He often composed for the style and skills of these individuals, such as ""Jeep's Blues"" for Johnny Hodges, ""Concerto for Cootie"" for Cootie Williams (which later became ""Do Nothing Till You Hear from Me"" with Bob Russell's lyrics), and ""The Mooche"" for Tricky Sam Nanton and Bubber Miley. He also recorded compositions written by his bandsmen, such as Juan Tizol's ""Caravan"" and ""Perdido"", which brought the ""Spanish Tinge"" to big-band jazz. Several members of the orchestra remained with him for several decades. The band reached a creative peak in the early 1940s, when Ellington and a small hand-picked group of his composers and arrangers wrote for an orchestra of distinctive voices who displayed tremendous creativity.[131]
 As only a limited number of American jazz records were released in Europe, European jazz traces many of its roots to American artists such as James Reese Europe, Paul Whiteman, and Lonnie Johnson, who visited Europe during and after World War I. It was their live performances which inspired European audiences' interest in jazz, as well as the interest in all things American (and therefore exotic) which accompanied the economic and political woes of Europe during this time.[132] The beginnings of a distinct European style of jazz began to emerge in this interwar period.
 British jazz began with a tour by the Original Dixieland Jazz Band in 1919. In 1926, Fred Elizalde and His Cambridge Undergraduates began broadcasting on the BBC. Thereafter jazz became an important element in many leading dance orchestras, and jazz instrumentalists became numerous.[133]
 This style entered full swing in France with the Quintette du Hot Club de France, which began in 1934. Much of this French jazz was a combination of African-American jazz and the symphonic styles in which French musicians were well-trained; in this, it is easy to see the inspiration taken from Paul Whiteman since his style was also a fusion of the two.[134] Belgian guitarist Django Reinhardt popularized gypsy jazz, a mix of 1930s American swing, French dance hall ""musette"", and Eastern European folk with a languid, seductive feel; the main instruments were steel stringed guitar, violin, and double bass. Solos pass from one player to another as guitar and bass form the rhythm section. Some researchers believe Eddie Lang and Joe Venuti pioneered the guitar-violin partnership characteristic of the genre,[135] which was brought to France after they had been heard live or on Okeh Records in the late 1920s.[136]
 The outbreak of World War II marked a turning point for jazz. The swing-era jazz of the previous decade had challenged other popular music as being representative of the nation's culture, with big bands reaching the height of the style's success by the early 1940s; swing acts and big bands traveled with U.S. military overseas to Europe, where it also became popular.[137] Stateside, however, the war presented difficulties for the big-band format: conscription shortened the number of musicians available; the military's need for shellac (commonly used for pressing gramophone records) limited record production; a shortage of rubber (also due to the war effort) discouraged bands from touring via road travel; and a demand by the musicians' union for a commercial recording ban limited music distribution between 1942 and 1944.[138]
 Many of the big bands who were deprived of experienced musicians because of the war effort began to enlist young players who were below the age for conscription, as was the case with saxophonist Stan Getz's entry in a band as a teenager.[139] This coincided with a nationwide resurgence in the Dixieland style of pre-swing jazz; performers such as clarinetist George Lewis, cornetist Bill Davison, and trombonist Turk Murphy were hailed by conservative jazz critics as more authentic than the big bands.[138] Elsewhere, with the limitations on recording, small groups of young musicians developed a more uptempo, improvisational style of jazz,[137] collaborating and experimenting with new ideas for melodic development, rhythmic language, and harmonic substitution, during informal, late-night jam sessions hosted in small clubs and apartments. Key figures in this development were largely based in New York and included pianists Thelonious Monk and Bud Powell, drummers Max Roach and Kenny Clarke, saxophonist Charlie Parker, and trumpeter Dizzy Gillespie.[138] This musical development became known as bebop.[137]
 Bebop and subsequent post-war jazz developments featured a wider set of notes, played in more complex patterns and at faster tempos than previous jazz.[139] According to Clive James, bebop was ""the post-war musical development which tried to ensure that jazz would no longer be the spontaneous sound of joy ... Students of race relations in America are generally agreed that the exponents of post-war jazz were determined, with good reason, to present themselves as challenging artists rather than tame entertainers.""[140] The end of the war marked ""a revival of the spirit of experimentation and musical pluralism under which it had been conceived"", along with ""the beginning of a decline in the popularity of jazz music in America"", according to American academic Michael H. Burchett.[137]
 With the rise of bebop and the end of the swing era after the war, jazz lost its cachet as pop music. Vocalists of the famous big bands moved on to being marketed and performing as solo pop singers; these included Frank Sinatra, Peggy Lee, Dick Haymes, and Doris Day.[139] Older musicians who still performed their pre-war jazz, such as Armstrong and Ellington, were gradually viewed in the mainstream as passé. Other younger performers, such as singer Big Joe Turner and saxophonist Louis Jordan, who were discouraged by bebop's increasing complexity, pursued more lucrative endeavors in rhythm and blues, jump blues, and eventually rock and roll.[137] Some, including Gillespie, composed intricate yet danceable pieces for bebop musicians in an effort to make them more accessible, but bebop largely remained on the fringes of American audiences' purview. ""The new direction of postwar jazz drew a wealth of critical acclaim, but it steadily declined in popularity as it developed a reputation as an academic genre that was largely inaccessible to mainstream audiences"", Burchett said. ""The quest to make jazz more relevant to popular audiences, while retaining its artistic integrity, is a constant and prevalent theme in the history of postwar jazz.""[137] During its swing period, jazz had been an uncomplicated musical scene; according to Paul Trynka, this changed in the post-war years:
 Suddenly jazz was no longer straightforward. There was bebop and its variants, there was the last gasp of swing, there were strange new brews like the progressive jazz of Stan Kenton, and there was a completely new phenomenon called revivalism – the rediscovery of jazz from the past, either on old records or performed live by aging players brought out of retirement. From now on it was no good saying that you liked jazz, you had to specify what kind of jazz. And that is the way it has been ever since, only more so. Today, the word 'jazz' is virtually meaningless without further definition.[139] In the early 1940s, bebop-style performers began to shift jazz from danceable popular music toward a more challenging ""musician's music"". The most influential bebop musicians included saxophonist Charlie Parker, pianists Bud Powell and Thelonious Monk, trumpeters Dizzy Gillespie and Clifford Brown, and drummer Max Roach. Divorcing itself from dance music, bebop established itself more as an art form, thus lessening its potential popular and commercial appeal.
 Composer Gunther Schuller wrote: ""In 1943 I heard the great Earl Hines band which had Bird in it and all those other great musicians. They were playing all the flatted fifth chords and all the modern harmonies and substitutions and Dizzy Gillespie runs in the trumpet section work. Two years later I read that that was 'bop' and the beginning of modern jazz ... but the band never made recordings.""[141]
 Dizzy Gillespie wrote: ""People talk about the Hines band being 'the incubator of bop' and the leading exponents of that music ended up in the Hines band. But people also have the erroneous impression that the music was new. It was not. The music evolved from what went before. It was the same basic music. The difference was in how you got from here to here to here...naturally each age has got its own shit.""[142]
 Since bebop was meant to be listened to, not danced to, it could use faster tempos. Drumming shifted to a more elusive and explosive style, in which the ride cymbal was used to keep time while the snare and bass drum were used for accents. This led to a highly syncopated music with a linear rhythmic complexity.[143]
 Bebop musicians employed several harmonic devices which were not previously typical in jazz, engaging in a more abstracted form of chord-based improvisation. Bebop scales are traditional scales with an added chromatic passing note;[144] bebop also uses ""passing"" chords, substitute chords, and altered chords. New forms of chromaticism and dissonance were introduced into jazz, and the dissonant tritone (or ""flatted fifth"") interval became the ""most important interval of bebop""[145] Chord progressions for bebop tunes were often taken directly from popular swing-era tunes and reused with a new and more complex melody or reharmonized with more complex chord progressions to form new compositions, a practice which was already well-established in earlier jazz, but came to be central to the bebop style. Bebop made use of several relatively common chord progressions, such as blues (at base, I–IV–V, but often infused with ii–V motion) and ""rhythm changes"" (I-vi-ii-V) – the chords to the 1930s pop standard ""I Got Rhythm"". Late bop also moved towards extended forms that represented a departure from pop and show tunes.
 The harmonic development in bebop is often traced back to a moment experienced by Charlie Parker while performing ""Cherokee"" at Clark Monroe's Uptown House, New York, in early 1942. ""I'd been getting bored with the stereotyped changes that were being used...and I kept thinking there's bound to be something else. I could hear it sometimes. I couldn't play it...I was working over 'Cherokee,' and, as I did, I found that by using the higher intervals of a chord as a melody line and backing them with appropriately related changes, I could play the thing I'd been hearing. It came alive.""[146] Gerhard Kubik postulates that harmonic development in bebop sprang from blues and African-related tonal sensibilities rather than 20th-century Western classical music. ""Auditory inclinations were the African legacy in [Parker's] life, reconfirmed by the experience of the blues tonal system, a sound world at odds with the Western diatonic chord categories. Bebop musicians eliminated Western-style functional harmony in their music while retaining the strong central tonality of the blues as a basis for drawing upon various African matrices.""[146]
 Samuel Floyd states that blues was both the bedrock and propelling force of bebop, bringing about a new harmonic conception using extended chord structures that led to unprecedented harmonic and melodic variety, a developed and even more highly syncopated, linear rhythmic complexity and a melodic angularity in which the blue note of the fifth degree was established as an important melodic-harmonic device; and reestablishment of the blues as the primary organizing and functional principle.[143] Kubik wrote:
 While for an outside observer, the harmonic innovations in bebop would appear to be inspired by experiences in Western ""serious"" music, from Claude Debussy to Arnold Schoenberg, such a scheme cannot be sustained by the evidence from a cognitive approach. Claude Debussy did have some influence on jazz, for example, on Bix Beiderbecke's piano playing. And it is also true that Duke Ellington adopted and reinterpreted some harmonic devices in European contemporary music. West Coast jazz would run into such debts as would several forms of cool jazz, but bebop has hardly any such debts in the sense of direct borrowings. On the contrary, ideologically, bebop was a strong statement of rejection of any kind of eclecticism, propelled by a desire to activate something deeply buried in self. Bebop then revived tonal-harmonic ideas transmitted through the blues and reconstructed and expanded others in a basically non-Western harmonic approach. The ultimate significance of all this is that the experiments in jazz during the 1940s brought back to African-American music several structural principles and techniques rooted in African traditions.[147] These divergences from the jazz mainstream of the time met a divided, sometimes hostile response among fans and musicians, especially swing players who bristled at the new harmonic sounds. To hostile critics, bebop seemed filled with ""racing, nervous phrases"".[148] But despite the friction, by the 1950s bebop had become an accepted part of the jazz vocabulary.
 The general consensus among musicians and musicologists is that the first original jazz piece to be overtly based in clave was ""Tanga"" (1943), composed by Cuban-born Mario Bauza and recorded by Machito and his Afro-Cubans in New York City. ""Tanga"" began as a spontaneous descarga (Cuban jam session), with jazz solos superimposed on top.[149]
 This was the birth of Afro-Cuban jazz. The use of clave brought the African timeline, or key pattern, into jazz. Music organized around key patterns convey a two-celled (binary) structure, which is a complex level of African cross-rhythm.[150] Within the context of jazz, however, harmony is the primary referent, not rhythm. The harmonic progression can begin on either side of clave, and the harmonic ""one"" is always understood to be ""one"". If the progression begins on the ""three-side"" of clave, it is said to be in 3–2 clave (shown below). If the progression begins on the ""two-side"", it is in 2–3 clave.[151]
 Mario Bauzá introduced bebop innovator Dizzy Gillespie to Cuban conga drummer and composer Chano Pozo. Gillespie and Pozo's brief collaboration produced some of the most enduring Afro-Cuban jazz standards. ""Manteca"" (1947) is the first jazz standard to be rhythmically based on clave. According to Gillespie, Pozo composed the layered, contrapuntal guajeos (Afro-Cuban ostinatos) of the A section and the introduction, while Gillespie wrote the bridge. Gillespie recounted: ""If I'd let it go like [Chano] wanted it, it would have been strictly Afro-Cuban all the way. There wouldn't have been a bridge. I thought I was writing an eight-bar bridge, but ... I had to keep going and ended up writing a sixteen-bar bridge.""[152] The bridge gave ""Manteca"" a typical jazz harmonic structure, setting the piece apart from Bauza's modal ""Tanga"" of a few years earlier.
 Gillespie's collaboration with Pozo brought specific African-based rhythms into bebop. While pushing the boundaries of harmonic improvisation, cu-bop also drew from African rhythm. Jazz arrangements with a Latin A section and a swung B section, with all choruses swung during solos, became common practice with many Latin tunes of the jazz standard repertoire. This approach can be heard on pre-1980 recordings of ""Manteca"", ""A Night in Tunisia"", ""Tin Tin Deo"", and ""On Green Dolphin Street"".
 Another jazz composition critical to the development of Afro-Cuban jazz was Bud Powell's ""Un Poco Loco,"" recorded with Curley Russell on bass and Max Roach on drums. Noted for its ""frenetic energy"" and ""clanging cowbell and polyrhythmic accompaniment,""[153] the composition combined Afro-Cuban rhythm with polytonality and preceded further use of modality and avant-garde harmony in Latin jazz.[154]
 Cuban percussionist Mongo Santamaria first recorded his composition ""Afro Blue"" in 1959.[155]
""Afro Blue"" was the first jazz standard built upon a typical African three-against-two (3:2) cross-rhythm, or hemiola.[156] The piece begins with the bass repeatedly playing 6 cross-beats per each measure of 128, or 6 cross-beats per 4 main beats—6:4 (two cells of 3:2).
 The following example shows the original ostinato ""Afro Blue"" bass line. The cross noteheads indicate the main beats (not bass notes).
 When John Coltrane covered ""Afro Blue"" in 1963, he inverted the metric hierarchy, interpreting the tune as a 34 jazz waltz with duple cross-beats superimposed (2:3). Originally a B♭ pentatonic blues, Coltrane expanded the harmonic structure of ""Afro Blue"".
 Perhaps the most respected Afro-cuban jazz combo of the late 1950s was vibraphonist Cal Tjader's band. Tjader had Mongo Santamaria, Armando Peraza, and Willie Bobo on his early recording dates.
 In the late 1940s, there was a revival of Dixieland, harking back to the contrapuntal New Orleans style. This was driven in large part by record company reissues of jazz classics by the Oliver, Morton, and Armstrong bands of the 1930s. There were two types of musicians involved in the revival: the first group was made up of those who had begun their careers playing in the traditional style and were returning to it (or continuing what they had been playing all along), such as Bob Crosby's Bobcats, Max Kaminsky, Eddie Condon, and Wild Bill Davison.[157] Most of these players were originally Midwesterners, although there were a small number of New Orleans musicians involved. The second group of revivalists consisted of younger musicians, such as those in the Lu Watters band, Conrad Janis, and Ward Kimball and his Firehouse Five Plus Two Jazz Band. By the late 1940s, Louis Armstrong's Allstars band became a leading ensemble. Through the 1950s and 1960s, Dixieland was one of the most commercially popular jazz styles in the US, Europe, and Japan, although critics paid little attention to it.[157]
 Hard bop is an extension of bebop (or ""bop"") music that incorporates influences from blues, rhythm and blues, and gospel, especially in saxophone and piano playing. Hard bop was developed in the mid-1950s, coalescing in 1953 and 1954; it developed partly in response to the vogue for cool jazz in the early 1950s and paralleled the rise of rhythm and blues. It has been described as ""funky"" and can be considered a relative of soul jazz.[158] Some elements of the genre were simplified from their bebop roots.[159]
 Miles Davis' 1954 performance of ""Walkin'"" at the first Newport Jazz Festival introduced the style to the jazz world.[160] Further leaders of hard bop's development included the Clifford Brown/Max Roach Quintet, Art Blakey's Jazz Messengers, the Horace Silver Quintet, and trumpeters Lee Morgan and Freddie Hubbard. The late 1950s to early 1960s saw hard boppers form their own bands as a new generation of blues- and bebop-influenced musicians entered the jazz world, from pianists Wynton Kelly and Tommy Flanagan[161] to saxophonists Joe Henderson and Hank Mobley. Coltrane, Johnny Griffin, Mobley, and Morgan all participated on the album A Blowin' Session (1957), considered by Al Campbell to have been one of the high points of the hard bop era.[162]
 Hard bop was prevalent within jazz for about a decade spanning from 1955 to 1965,[161] but has remained highly influential on mainstream[159] or ""straight-ahead"" jazz. It went into decline in the late 1960s through the 1970s due to the emergence of other styles such as jazz fusion, but again became influential following the Young Lions Movement and the emergence of neo-bop.[159]
 Modal jazz is a development which began in the later 1950s which takes the mode, or musical scale, as the basis of musical structure and improvisation. Previously, a solo was meant to fit into a given chord progression, but with modal jazz, the soloist creates a melody using one (or a small number of) modes. The emphasis is thus shifted from harmony to melody:[163] ""Historically, this caused a seismic shift among jazz musicians, away from thinking vertically (the chord), and towards a more horizontal approach (the scale)"",[164] explained pianist Mark Levine.
 The modal theory stems from a work by George Russell. Miles Davis introduced the concept to the greater jazz world with Kind of Blue (1959), an exploration of the possibilities of modal jazz which would become the best selling jazz album of all time. In contrast to Davis' earlier work with hard bop and its complex chord progression and improvisation, Kind of Blue was composed as a series of modal sketches in which the musicians were given scales that defined the parameters of their improvisation and style.[165]
 ""I didn't write out the music for Kind of Blue, but brought in sketches for what everybody was supposed to play because I wanted a lot of spontaneity,""[166] recalled Davis. The track ""So What"" has only two chords: D-7 and E♭-7.[167]
 Other innovators in this style include Jackie McLean,[168] and two of the musicians who had also played on Kind of Blue: John Coltrane and Bill Evans.
 Free jazz, and the related form of avant-garde jazz, broke through into an open space of ""free tonality"" in which meter, beat, and formal symmetry all disappeared, and a range of world music from India, Africa, and Arabia were melded into an intense, even religiously ecstatic or orgiastic style of playing.[169] While loosely inspired by bebop, free jazz tunes gave players much more latitude; the loose harmony and tempo was deemed controversial when this approach was first developed. The bassist Charles Mingus is also frequently associated with the avant-garde in jazz, although his compositions draw from myriad styles and genres.
 The first major stirrings came in the 1950s with the early work of Ornette Coleman (whose 1960 album Free Jazz: A Collective Improvisation coined the term) and Cecil Taylor. In the 1960s, exponents included Albert Ayler, Gato Barbieri, Carla Bley, Don Cherry, Larry Coryell, John Coltrane, Bill Dixon, Jimmy Giuffre, Steve Lacy, Michael Mantler, Sun Ra, Roswell Rudd, Pharoah Sanders, and John Tchicai. In developing his late style, Coltrane was especially influenced by the dissonance of Ayler's trio with bassist Gary Peacock and drummer Sunny Murray, a rhythm section honed with Cecil Taylor as leader. In November 1961, Coltrane played a gig at the Village Vanguard, which resulted in the classic Chasin' the 'Trane, which DownBeat magazine panned as ""anti-jazz"". On his 1961 tour of France, he was booed, but persevered, signing with the new Impulse! Records in 1960 and turning it into ""the house that Trane built"", while championing many younger free jazz musicians, notably Archie Shepp, who often played with trumpeter Bill Dixon, who organized the 4-day ""October Revolution in Jazz"" in Manhattan in 1964, the first free jazz festival.
 A series of recordings with the Classic Quartet in the first half of 1965 show Coltrane's playing becoming increasingly abstract, with greater incorporation of devices like multiphonics, utilization of overtones, and playing in the altissimo register, as well as a mutated return to Coltrane's sheets of sound. In the studio, he all but abandoned his soprano to concentrate on the tenor saxophone. In addition, the quartet responded to the leader by playing with increasing freedom. The group's evolution can be traced through the recordings The John Coltrane Quartet Plays, Living Space and Transition (both June 1965), New Thing at Newport (July 1965), Sun Ship (August 1965), and First Meditations (September 1965).
 In June 1965, Coltrane and 10 other musicians recorded Ascension, a 40-minute-long piece without breaks that included adventurous solos by young avant-garde musicians as well as Coltrane, and was controversial primarily for the collective improvisation sections that separated the solos. Dave Liebman later called it ""the torch that lit the free jazz thing"". After recording with the quartet over the next few months, Coltrane invited Pharoah Sanders to join the band in September 1965. While Coltrane used over-blowing frequently as an emotional exclamation-point, Sanders would opt to overblow his entire solo, resulting in a constant screaming and screeching in the altissimo range of the instrument.
 Free jazz was played in Europe in part because musicians such as Ayler, Taylor, Steve Lacy, and Eric Dolphy spent extended periods of time there, and European musicians such as Michael Mantler and John Tchicai traveled to the U.S. to experience American music firsthand. European contemporary jazz was shaped by Peter Brötzmann, John Surman, Krzysztof Komeda, Zbigniew Namysłowski, Tomasz Stanko, Lars Gullin, Joe Harriott, Albert Mangelsdorff, Kenny Wheeler, Graham Collier, Michael Garrick and Mike Westbrook. They were eager to develop approaches to music that reflected their heritage.
 Since the 1960s, creative centers of jazz in Europe have developed, such as the creative jazz scene in Amsterdam. Following the work of drummer Han Bennink and pianist Misha Mengelberg, musicians started to explore by improvising collectively until a form (melody, rhythm, a famous song) is found Jazz critic Kevin Whitehead documented the free jazz scene in Amsterdam and some of its main exponents such as the ICP (Instant Composers Pool) orchestra in his book New Dutch Swing. Since the 1990s Keith Jarrett has defended free jazz from criticism. British writer Stuart Nicholson has argued European contemporary jazz has an identity different from American jazz and follows a different trajectory.[170]
 Latin jazz is jazz that employs Latin American rhythms and is generally understood to have a more specific meaning than simply jazz from Latin America. A more precise term might be Afro-Latin jazz, as the jazz subgenre typically employs rhythms that either have a direct analog in Africa or exhibit an African rhythmic influence beyond what is ordinarily heard in other jazz. The two main categories of Latin jazz are Afro-Cuban jazz and Brazilian jazz.
 In the 1960s and 1970s, many jazz musicians had only a basic understanding of Cuban and Brazilian music, and jazz compositions which used Cuban or Brazilian elements were often referred to as ""Latin tunes"", with no distinction between a Cuban son montuno and a Brazilian bossa nova. Even as late as 2000, in Mark Gridley's Jazz Styles: History and Analysis, a bossa nova bass line is referred to as a ""Latin bass figure"".[171] It was not uncommon during the 1960s and 1970s to hear a conga playing a Cuban tumbao while the drumset and bass played a Brazilian bossa nova pattern. Many jazz standards such as ""Manteca"", ""On Green Dolphin Street"" and ""Song for My Father"" have a ""Latin"" A section and a swung B section. Typically, the band would only play an even-eighth ""Latin"" feel in the A section of the head and swing throughout all of the solos. Latin jazz specialists like Cal Tjader tended to be the exception. For example, on a 1959 live Tjader recording of ""A Night in Tunisia"", pianist Vince Guaraldi soloed through the entire form over an authentic mambo.[172]
 For most of its history, Afro-Cuban jazz had been a matter of superimposing jazz phrasing over Cuban rhythms. But by the end of the 1970s, a new generation of New York City musicians had emerged who were fluent in both salsa dance music and jazz, leading to a new level of integration of jazz and Cuban rhythms. This era of creativity and vitality is best represented by the Gonzalez brothers Jerry (congas and trumpet) and Andy (bass).[173] During 1974–1976, they were members of one of Eddie Palmieri's most experimental salsa groups: salsa was the medium, but Palmieri was stretching the form in new ways. He incorporated parallel fourths, with McCoy Tyner-type vamps. The innovations of Palmieri, the Gonzalez brothers and others led to an Afro-Cuban jazz renaissance in New York City.
 This occurred in parallel with developments in Cuba[174] The first Cuban band of this new wave was Irakere. Their ""Chékere-son"" (1976) introduced a style of ""Cubanized"" bebop-flavored horn lines that departed from the more angular guajeo-based lines which were typical of Cuban popular music and Latin jazz up until that time. It was based on Charlie Parker's composition ""Billie's Bounce"", jumbled together in a way that fused clave and bebop horn lines.[175] In spite of the ambivalence of some band members towards Irakere's Afro-Cuban folkloric / jazz fusion, their experiments forever changed Cuban jazz: their innovations are still heard in the high level of harmonic and rhythmic complexity in Cuban jazz and in the jazzy and complex contemporary form of popular dance music known as timba.
 Brazilian jazz, such as bossa nova, is derived from samba, with influences from jazz and other 20th-century classical and popular music styles. Bossa is generally moderately paced, with melodies sung in Portuguese or English, whilst the related jazz-samba is an adaptation of street samba into jazz.
 The bossa nova style was pioneered by Brazilians João Gilberto and Antônio Carlos Jobim and was made popular by Elizete Cardoso's recording of ""Chega de Saudade"" on the Canção do Amor Demais LP. Gilberto's initial releases, and the 1959 film Black Orpheus, achieved significant popularity in Latin America; this spread to North America via visiting American jazz musicians. The resulting recordings by Charlie Byrd and Stan Getz cemented bossa nova's popularity and led to a worldwide boom, with 1963's Getz/Gilberto, numerous recordings by famous jazz performers such as Ella Fitzgerald and Frank Sinatra, and the eventual entrenchment of the bossa nova style as a lasting influence in world music.
 Brazilian percussionists such as Airto Moreira and Naná Vasconcelos also influenced jazz internationally by introducing Afro-Brazilian folkloric instruments and rhythms into a wide variety of jazz styles, thus attracting a greater audience to them.[176][177][178]
 While bossa nova has been labeled as jazz by music critics, namely those from outside of Brazil, it has been rejected by many prominent bossa nova musicians such as Jobim, who once said ""Bossa nova is not Brazilian jazz.""[179][180]
 The first jazz standard composed by a non-Latino to use an overt African 128 cross-rhythm was Wayne Shorter's ""Footprints"" (1967).[181] On the version recorded on Miles Smiles by Miles Davis, the bass switches to a 44 tresillo figure at 2:20. ""Footprints"" is not, however, a Latin jazz tune: African rhythmic structures are accessed directly by Ron Carter (bass) and Tony Williams (drums) via the rhythmic sensibilities of swing. Throughout the piece, the four beats, whether sounded or not, are maintained as the temporal referent. The following example shows the 128 and 44 forms of the bass line. The slashed noteheads indicate the main beats (not bass notes), where one ordinarily taps their foot to ""keep time"".
 The use of pentatonic scales was another trend associated with Africa. The use of pentatonic scales in Africa probably goes back thousands of years.[182]
 McCoy Tyner perfected the use of the pentatonic scale in his solos,[183] and also used parallel fifths and fourths, which are common harmonies in West Africa.[184]
 The minor pentatonic scale is often used in blues improvisation, and like a blues scale, a minor pentatonic scale can be played over all of the chords in a blues. The following pentatonic lick was played over blues changes by Joe Henderson on Horace Silver's ""African Queen"" (1965).[185]
 Jazz pianist, theorist, and educator Mark Levine refers to the scale generated by beginning on the fifth step of a pentatonic scale as the V pentatonic scale.[186]
 Levine points out that the V pentatonic scale works for all three chords of the standard II–V–I jazz progression.[187] This is a very common progression, used in pieces such as Miles Davis' ""Tune Up"". The following example shows the V pentatonic scale over a II–V–I progression.[188]
 Accordingly, John Coltrane's ""Giant Steps"" (1960), with its 26 chords per 16 bars, can be played using only three pentatonic scales. Coltrane studied Nicolas Slonimsky's Thesaurus of Scales and Melodic Patterns, which contains material that is virtually identical to portions of ""Giant Steps"".[189] The harmonic complexity of ""Giant Steps"" is on the level of the most advanced 20th-century art music. Superimposing the pentatonic scale over ""Giant Steps"" is not merely a matter of harmonic simplification, but also a sort of ""Africanizing"" of the piece, which provides an alternate approach for soloing. Mark Levine observes that when mixed in with more conventional ""playing the changes"", pentatonic scales provide ""structure and a feeling of increased space"".[190]
 As noted above, jazz has incorporated from its inception aspects of African-American sacred music including spirituals and hymns. Secular jazz musicians often performed renditions of spirituals and hymns as part of their repertoire or isolated compositions such as ""Come Sunday"", part of ""Black and Beige Suite"" by Duke Ellington. Later many other jazz artists borrowed from black gospel music. However, it was only after World War II that a few jazz musicians began to compose and perform extended works intended for religious settings or as religious expression. Since the 1950s, sacred and liturgical music has been performed and recorded by many prominent jazz composers and musicians.[191] The ""Abyssinian Mass"" by Wynton Marsalis (Blueengine Records, 2016) is a recent example.
 Relatively little has been written about sacred and liturgical jazz. In a 2013 doctoral dissertation, Angelo Versace examined the development of sacred jazz in the 1950s using disciplines of musicology and history. He noted that the traditions of black gospel music and jazz were combined in the 1950s to produce a new genre, ""sacred jazz"".[192] Versace maintained that the religious intent separates sacred from secular jazz. Most prominent in initiating the sacred jazz movement were pianist and composer Mary Lou Williams, known for her jazz masses in the 1950s and Duke Ellington. Prior to his death in 1974 in response to contacts from Grace Cathedral in San Francisco, Duke Ellington wrote three Sacred Concerts: 1965 – A Concert of Sacred Music; 1968 – Second Sacred Concert; 1973 – Third Sacred Concert.
 The most prominent form of sacred and liturgical jazz is the jazz mass. Although most often performed in a concert setting rather than church worship setting, this form has many examples. An eminent example of composers of the jazz mass was Mary Lou Williams. Williams converted to Catholicism in 1957, and proceeded to compose three masses in the jazz idiom.[193] One was composed in 1968 to honor the recently assassinated Martin Luther King Jr. and the third was commissioned by a pontifical commission. It was performed once in 1975 in St Patrick's Cathedral in New York City. However the Catholic Church has not embraced jazz as appropriate for worship. In 1966 Joe Masters recorded ""Jazz Mass"" for Columbia Records. A jazz ensemble was joined by soloists and choir using the English text of the Roman Catholic Mass.[194] Other examples include ""Jazz Mass in Concert"" by Lalo Schiffrin (Aleph Records, 1998, UPC 0651702632725) and ""Jazz Mass"" by Vince Guaraldi (Fantasy Records, 1965). In England, classical composer Will Todd recorded his ""Jazz Missa Brevis"" with a jazz ensemble, soloists and the St Martin's Voices on a 2018 Signum Records release, ""Passion Music/Jazz Missa Brevis"" also released as ""Mass in Blue"", and jazz organist James Taylor composed ""The Rochester Mass"" (Cherry Red Records, 2015).[195] In 2013, Versace put forth bassist Ike Sturm and New York composer Deanna Witkowski as contemporary exemplars of sacred and liturgical jazz.[192]
 In the late 1960s and early 1970s, the hybrid form of jazz-rock fusion was developed by combining jazz improvisation with rock rhythms, electric instruments and the highly amplified stage sound of rock musicians such as Jimi Hendrix and Frank Zappa. Jazz fusion often uses mixed meters, odd time signatures, syncopation, complex chords, and harmonies.
 According to AllMusic:
 ... until around 1967, the worlds of jazz and rock were nearly completely separate. [However, ...] as rock became more creative and its musicianship improved, and as some in the jazz world became bored with hard bop and did not want to play strictly avant-garde music, the two different idioms began to trade ideas and occasionally combine forces.[196] In 1969, Davis fully embraced the electric instrument approach to jazz with In a Silent Way, which can be considered his first fusion album. Composed of two side-long suites edited heavily by producer Teo Macero, this quiet, static album would be equally influential to the development of ambient music.
 As Davis recalls:
 The music I was really listening to in 1968 was James Brown, the great guitar player Jimi Hendrix, and a new group who had just come out with a hit record, ""Dance to the Music"", Sly and the Family Stone ... I wanted to make it more like rock. When we recorded In a Silent Way I just threw out all the chord sheets and told everyone to play off of that.[197] Two contributors to In a Silent Way also joined organist Larry Young to create one of the early acclaimed fusion albums: Emergency! (1969) by The Tony Williams Lifetime.
 Weather Report's self-titled electronic and psychedelic Weather Report debut album caused a sensation in the jazz world on its arrival in 1971, thanks to the pedigree of the group's members (including percussionist Airto Moreira), and their unorthodox approach to music. The album featured a softer sound than would be the case in later years (predominantly using acoustic bass with Shorter exclusively playing soprano saxophone, and with no synthesizers involved), but is still considered a classic of early fusion. It built on the avant-garde experiments which Joe Zawinul and Shorter had pioneered with Miles Davis on Bitches Brew, including an avoidance of head-and-chorus composition in favor of continuous rhythm and movement – but took the music further. To emphasize the group's rejection of standard methodology, the album opened with the inscrutable avant-garde atmospheric piece ""Milky Way"", which featured by Shorter's extremely muted saxophone inducing vibrations in Zawinul's piano strings while the latter pedaled the instrument. DownBeat described the album as ""music beyond category"", and awarded it Album of the Year in the magazine's polls that year.
 Weather Report's subsequent releases were creative funk-jazz works.[198]
 Although some jazz purists protested against the blend of jazz and rock, many jazz innovators crossed over from the contemporary hard bop scene into fusion. As well as the electric instruments of rock (such as electric guitar, electric bass, electric piano and synthesizer keyboards), fusion also used the powerful amplification, ""fuzz"" pedals, wah-wah pedals and other effects that were used by 1970s-era rock bands. Notable performers of jazz fusion included Miles Davis, Eddie Harris, keyboardists Joe Zawinul, Chick Corea, and Herbie Hancock, vibraphonist Gary Burton, drummer Tony Williams, violinist Jean-Luc Ponty, guitarists Larry Coryell, Al Di Meola, John McLaughlin, Ryo Kawasaki, and Frank Zappa, saxophonist Wayne Shorter and bassists Jaco Pastorius and Stanley Clarke. Jazz fusion was also popular in Japan, where the band Casiopea released more than thirty fusion albums.
 According to jazz writer Stuart Nicholson, ""just as free jazz appeared on the verge of creating a whole new musical language in the 1960s ... jazz-rock briefly suggested the promise of doing the same"" with albums such as Williams' Emergency! (1970) and Davis' Agharta (1975), which Nicholson said ""suggested the potential of evolving into something that might eventually define itself as a wholly independent genre quite apart from the sound and conventions of anything that had gone before."" This development was stifled by commercialism, Nicholson said, as the genre ""mutated into a peculiar species of jazz-inflected pop music that eventually took up residence on FM radio"" at the end of the 1970s.[199]
 Although jazz-rock fusion reached the height of its popularity in the 1970s, the use of electronic instruments and rock-derived musical elements in jazz continued in the 1990s and 2000s. Musicians using this approach include Pat Metheny, John Abercrombie, John Scofield and the Swedish group e.s.t. Since the beginning of the 1990s, electronic music had significant technical improvements that popularized and created new possibilities for the genre. Jazz elements such as improvisation, rhythmic complexities and harmonic textures were introduced to the genre and consequently had a big impact in new listeners and in some ways kept the versatility of jazz relatable to a newer generation that did not necessarily relate to what the traditionalists call real jazz (bebop, cool and modal jazz).[200] Artists such as Squarepusher, Aphex Twin, Flying Lotus and sub genres like IDM, drum 'n' bass, jungle and techno ended up incorporating a lot of these elements.[201] Squarepusher being cited as one big influence for jazz performers drummer Mark Guiliana and pianist Brad Mehldau, showing the correlations between jazz and electronic music are a two-way street.[202]
 By the mid-1970s, the sound known as jazz-funk had developed, characterized by a strong back beat (groove), electrified sounds[203] and, often, the presence of electronic analog synthesizers. Jazz-funk also draws influences from traditional African music, Afro-Cuban rhythms and Jamaican reggae, notably Kingston bandleader Sonny Bradshaw. Another feature is the shift of emphasis from improvisation to composition: arrangements, melody and overall writing became important. The integration of funk, soul, and R&B music into jazz resulted in the creation of a genre whose spectrum is wide and ranges from strong jazz improvisation to soul, funk or disco with jazz arrangements, jazz riffs and jazz solos, and sometimes soul vocals.[204]
 Early examples are Herbie Hancock's Headhunters band and Miles Davis' On the Corner album, which, in 1972, began Davis' foray into jazz-funk and was, he claimed, an attempt at reconnecting with the young black audience which had largely forsaken jazz for rock and funk. While there is a discernible rock and funk influence in the timbres of the instruments employed, other tonal and rhythmic textures, such as the Indian tambora and tablas and Cuban congas and bongos, create a multi-layered soundscape. The album was a culmination of sorts of the musique concrète approach that Davis and producer Teo Macero had begun to explore in the late 1960s.
 The 1980s saw something of a reaction against the fusion and free jazz that had dominated the 1970s. Trumpeter Wynton Marsalis emerged early in the decade, and strove to create music within what he believed was the tradition, rejecting both fusion and free jazz and creating extensions of the small and large forms initially pioneered by artists such as Louis Armstrong and Duke Ellington, as well as the hard bop of the 1950s. It is debatable whether Marsalis' critical and commercial success was a cause or a symptom of the reaction against Fusion and Free Jazz and the resurgence of interest in the kind of jazz pioneered in the 1960s (particularly modal jazz and post-bop); nonetheless there were many other manifestations of a resurgence of traditionalism, even if fusion and free jazz were by no means abandoned and continued to develop and evolve.
 For example, several musicians who had been prominent in the fusion genre during the 1970s began to record acoustic jazz once more, including Chick Corea and Herbie Hancock. Other musicians who had experimented with electronic instruments in the previous decade had abandoned them by the 1980s; for example, Bill Evans, Joe Henderson, and Stan Getz. Even the 1980s music of Miles Davis, although certainly still fusion, adopted a far more accessible and recognizably jazz-oriented approach than his abstract work of the mid-1970s, such as a return to a theme-and-solos approach.
 
A similar reaction[vague] took place against free jazz. According to Ted Gioia: the very leaders of the avant garde started to signal a retreat from the core principles of free jazz. Anthony Braxton began recording standards over familiar chord changes. Cecil Taylor played duets in concert with Mary Lou Williams, and let her set out structured harmonies and familiar jazz vocabulary under his blistering keyboard attack. And the next generation of progressive players would be even more accommodating, moving inside and outside the changes without thinking twice. Musicians such as David Murray or Don Pullen may have felt the call of free-form jazz, but they never forgot all the other ways one could play African-American music for fun and profit.[205] Pianist Keith Jarrett—whose bands of the 1970s had played only original compositions with prominent free jazz elements—established his so-called 'Standards Trio' in 1983, which, although also occasionally exploring collective improvisation, has primarily performed and recorded jazz standards. Chick Corea similarly began exploring jazz standards in the 1980s, having neglected them for the 1970s.
 In 1987, the United States House of Representatives and Senate passed a bill proposed by Democratic Representative John Conyers Jr. to define jazz as a unique form of American music, stating ""jazz is hereby designated as a rare and valuable national American treasure to which we should devote our attention, support and resources to make certain it is preserved, understood and promulgated."" It passed in the House on September 23, 1987, and in the Senate on November 4, 1987.[206]
 In 2001, Ken Burns's documentary Jazz premiered on PBS, featuring Wynton Marsalis and other experts reviewing the entire history of American jazz to that time. It received some criticism, however, for its failure to reflect the many distinctive non-American traditions and styles in jazz that had developed, and its limited representation of US developments in the last quarter of the 20th century.
 The emergence of young jazz talent beginning to perform in older, established musicians' groups further impacted the resurgence of traditionalism in the jazz community. In the 1970s, the groups of Betty Carter and Art Blakey and the Jazz Messengers retained their conservative jazz approaches in the midst of fusion and jazz-rock, and in addition to difficulty booking their acts, struggled to find younger generations of personnel to authentically play traditional styles such as hard bop and bebop. In the late 1970s, however, a resurgence of younger jazz players in Blakey's band began to occur. This movement included musicians such as Valery Ponomarev and Bobby Watson, Dennis Irwin and James Williams. In the 1980s, in addition to Wynton and Branford Marsalis, the emergence of pianists in the Jazz Messengers such as Donald Brown, Mulgrew Miller, and later, Benny Green, bassists such as Charles Fambrough, Lonnie Plaxico (and later, Peter Washington and Essiet Essiet) horn players such as Bill Pierce, Donald Harrison and later Javon Jackson and Terence Blanchard emerged as talented jazz musicians, all of whom made significant contributions in the 1990s and 2000s.
 The young Jazz Messengers' contemporaries, including Roy Hargrove, Marcus Roberts, Wallace Roney and Mark Whitfield were also influenced by Wynton Marsalis's emphasis toward jazz tradition. These younger rising stars rejected avant-garde approaches and instead championed the acoustic jazz sound of Charlie Parker, Thelonious Monk and early recordings of the first Miles Davis quintet. This group of ""Young Lions"" sought to reaffirm jazz as a high art tradition comparable to the discipline of classical music.[207]
 In addition, Betty Carter's rotation of young musicians in her group foreshadowed many of New York's preeminent traditional jazz players later in their careers. Among these musicians were Jazz Messenger alumni Benny Green, Branford Marsalis and Ralph Peterson Jr., as well as Kenny Washington, Lewis Nash, Curtis Lundy, Cyrus Chestnut, Mark Shim, Craig Handy, Greg Hutchinson and Marc Cary, Taurus Mateen and Geri Allen. O.T.B. ensemble included a rotation of young jazz musicians such as Kenny Garrett, Steve Wilson, Kenny Davis, Renee Rosnes, Ralph Peterson Jr., Billy Drummond, and Robert Hurst.[208]
 Starting in the 1990s, a number of players from largely straight-ahead or post-bop backgrounds emerged as a result of the rise of neo-traditionalist jazz, including pianists Jason Moran and Vijay Iyer, guitarist Kurt Rosenwinkel, vibraphonist Stefon Harris, trumpeters Roy Hargrove and Terence Blanchard, saxophonists Chris Potter and Joshua Redman, clarinetist Ken Peplowski and bassist Christian McBride.
 In the early 1980s, a commercial form of jazz fusion called ""pop fusion"" or ""smooth jazz"" became successful, garnering significant radio airplay in ""quiet storm"" time slots at radio stations in urban markets across the U.S. This helped to establish or bolster the careers of vocalists including Al Jarreau, Anita Baker, Chaka Khan, and Sade, as well as saxophonists including Grover Washington Jr., Kenny G, Kirk Whalum, Boney James, and David Sanborn. In general, smooth jazz is downtempo (the most widely played tracks are of 90–105 beats per minute), and has a lead melody-playing instrument (saxophone, especially soprano and tenor, and legato electric guitar are popular).
 In his Newsweek article ""The Problem With Jazz Criticism"",[209] Stanley Crouch considers Miles Davis' playing of fusion to be a turning point that led to smooth jazz. Critic Aaron J. West has countered the often negative perceptions of smooth jazz, stating:
 I challenge the prevalent marginalization and malignment of smooth jazz in the standard jazz narrative. Furthermore, I question the assumption that smooth jazz is an unfortunate and unwelcomed evolutionary outcome of the jazz-fusion era. Instead, I argue that smooth jazz is a long-lived musical style that merits multi-disciplinary analyses of its origins, critical dialogues, performance practice, and reception.[210] Acid jazz developed in the UK in the 1980s and 1990s, influenced by jazz-funk and electronic dance music. Acid jazz often contains various types of electronic composition (sometimes including sampling or live DJ cutting and scratching), but it is just as likely to be played live by musicians, who often showcase jazz interpretation as part of their performance. Richard S. Ginell of AllMusic considers Roy Ayers ""one of the prophets of acid jazz"".[211]
 Nu jazz is influenced by jazz harmony and melodies, and there are usually no improvisational aspects. It can be very experimental in nature and can vary widely in sound and concept. It ranges from the combination of live instrumentation with the beats of jazz house (as exemplified by St Germain, Jazzanova, and Fila Brazillia) to more band-based improvised jazz with electronic elements (for example, The Cinematic Orchestra, Kobol and the Norwegian ""future jazz"" style pioneered by Bugge Wesseltoft, Jaga Jazzist, and Nils Petter Molvær).
 Jazz rap developed in the late 1980s and early 1990s and incorporates jazz influences into hip hop. In 1988, Gang Starr released the debut single ""Words I Manifest"", which sampled Dizzy Gillespie's 1962 ""Night in Tunisia"", and Stetsasonic released ""Talkin' All That Jazz"", which sampled Lonnie Liston Smith. Gang Starr's debut LP No More Mr. Nice Guy (1989) and their 1990 track ""Jazz Thing"" sampled Charlie Parker and Ramsey Lewis. The groups which made up the Native Tongues Posse tended toward jazzy releases: these include the Jungle Brothers' debut Straight Out the Jungle (1988), and A Tribe Called Quest's People's Instinctive Travels and the Paths of Rhythm (1990) and The Low End Theory (1991). Rap duo Pete Rock & CL Smooth incorporated jazz influences on their 1992 debut Mecca and the Soul Brother. Rapper Guru's Jazzmatazz series began in 1993 using jazz musicians during the studio recordings.
 Although jazz rap had achieved little mainstream success, Miles Davis' final album Doo-Bop (released posthumously in 1992) was based on hip hop beats and collaborations with producer Easy Mo Bee. Davis' ex-bandmate Herbie Hancock also absorbed hip-hop influences in the mid-1990s, releasing the album Dis Is Da Drum in 1994.
 The mid-2010s saw an increased influence of R&B, hip-hop, and pop music on jazz. In 2015, Kendrick Lamar released his third studio album, To Pimp a Butterfly. The album heavily featured prominent contemporary jazz artists such as Thundercat[212] and redefined jazz rap with a larger focus on improvisation and live soloing rather than simply sampling. In that same year, saxophonist Kamasi Washington released his nearly three-hour long debut, The Epic. Its hip-hop inspired beats and R&B vocal interludes was not only acclaimed by critics for being innovative in keeping jazz relevant,[213] but also sparked a small resurgence in jazz on the internet.
 The relaxation of orthodoxy which was concurrent with post-punk in London and New York City led to a new appreciation of jazz. In London, the Pop Group began to mix free jazz and dub reggae into their brand of punk rock.[214] In New York, No Wave took direct inspiration from both free jazz and punk. Examples of this style include Lydia Lunch's Queen of Siam,[215] Gray, the work of James Chance and the Contortions (who mixed Soul with free jazz and punk)[215] and the Lounge Lizards[215] (the first group to call themselves ""punk jazz"").
 John Zorn took note of the emphasis on speed and dissonance that was becoming prevalent in punk rock, and incorporated this into free jazz with the release of the Spy vs. Spy album in 1986, a collection of Ornette Coleman tunes done in the contemporary thrashcore style.[216] In the same year, Sonny Sharrock, Peter Brötzmann, Bill Laswell, and Ronald Shannon Jackson recorded the first album under the name Last Exit, a similarly aggressive blend of thrash and free jazz.[217] These developments are the origins of jazzcore, the fusion of free jazz with hardcore punk.
 The M-Base movement started in the 1980s, when a loose collective of young African-American musicians in New York which included Steve Coleman, Greg Osby, and Gary Thomas developed a complex but grooving[218] sound.
 In the 1990s, most M-Base participants turned to more conventional music, but Coleman, the most active participant, continued developing his music in accordance with the M-Base concept.[219]
 Coleman's audience decreased, but his music and concepts influenced many musicians, according to pianist Vijay Iver and critic Ben Ratlifff of The New York Times.[220][221]
 M-Base changed from a movement of a loose collective of young musicians to a kind of informal Coleman ""school"",[222] with a much advanced but already originally implied concept.[223] Steve Coleman's music and M-Base concept gained recognition as ""next logical step"" after Charlie Parker, John Coltrane, and Ornette Coleman.[224]
 Since the 1990s, jazz has been characterized by a pluralism in which no one style dominates, but rather a wide range of styles and genres are popular. Individual performers often play in a variety of styles, sometimes in the same performance. Pianist Brad Mehldau and The Bad Plus have explored contemporary rock music within the context of the traditional jazz acoustic piano trio, recording instrumental jazz versions of songs by rock musicians. The Bad Plus have also incorporated elements of free jazz into their music. A firm avant-garde or free jazz stance has been maintained by some players, such as saxophonists Greg Osby and Charles Gayle, while others, such as James Carter, have incorporated free jazz elements into a more traditional framework.
 Harry Connick Jr. began his career playing stride piano and the Dixieland jazz of his home, New Orleans, beginning with his first recording when he was 10 years old.[225] Some of his earliest lessons were at the home of pianist Ellis Marsalis.[226] Connick had success on the pop charts after recording the soundtrack to the movie When Harry Met Sally, which sold over two million copies.[225] Crossover success has also been achieved by Diana Krall, Norah Jones, Cassandra Wilson, Kurt Elling, and Jamie Cullum.
 Additionally, the era saw the release of recordings and videos from the previous century, such as a Just Jazz tape broadcast by a band led by Gene Ammons[227] and studio archives such as Just Coolin' by Art Blakey and the Jazz Messengers.[228]
 An internet-aided trend of 2010's jazz was that of extreme reharmonization, inspired by both virtuosic players known for their speed and rhythm such as Art Tatum, as well as players known for their ambitious voicings and chords such as Bill Evans. Supergroup Snarky Puppy adopted this trend, allowing players like Cory Henry[229] to shape the grooves and harmonies of modern jazz soloing. YouTube phenomenon Jacob Collier also gained recognition for his ability to play an incredibly large number of instruments and his ability to use microtones, advanced polyrhythms, and blend a spectrum of genres in his largely homemade production process.[230][231]
 Other jazz musicians gained popularity through social media during the 2010s and 2020s. These included Joan Chamorro, a bassist and bandleader based in Barcelona whose big band and jazz combo videos have received tens of millions of views on YouTube,[232] and Emmet Cohen, who broadcast a series of performances live from New York starting in March 2020.[233]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['musicology and history', 'Eddie Lang and Joe Venuti', 'The quest to make jazz more relevant to popular audiences', 'African rhythmic rituals', 'unorthodox'], 'answer_start': [], 'answer_end': []}"
"International law (also known as public international law and the law of nations) is the set of rules, norms, and standards generally recognized as binding between states. It establishes norms for states across a broad range of domains, including war and diplomacy, economic relations, and human rights. International law differs from state-based domestic legal systems in that it is primarily, though not exclusively, applicable to states, rather than to individuals, and operates largely through consent, since there is no universally accepted authority to enforce it upon sovereign states. States may choose to not abide by international law, and even to breach a treaty but such violations, particularly of peremptory norms, can be met with disapproval by others and in some cases coercive action ranging from diplomatic and economic sanctions to war.
 With origins tracing back to antiquity, states have a long history of negotiating interstate agreements. An initial framework was conceptualised by the Ancient Romans and this idea of ius gentium has been used by various academics to establish the modern concept of international law. The sources of international law include international custom (general state practice accepted as law), treaties, and general principles of law recognised by most national legal systems. Although international law may also be reflected in international comity—the practices adopted by states to maintain good relations and mutual recognition—such traditions are not legally binding. The relationship and interaction between a national legal system and international law is complex and variable. National law may become international law when treaties permit national jurisdiction to supranational tribunals such as the European Court of Human Rights or the International Criminal Court. Treaties such as the Geneva Conventions require national law to conform to treaty provisions. National laws or constitutions may also provide for the implementation or integration of international legal obligations into domestic law.
 The modern term ""international law"" was originally coined by Jeremy Bentham in his 1789 book Introduction to the Principles of Morals and Legislation to replace the older law of nations, a direct translation of the late medieval concepts of ius gentium, used by Hugo Grotius, and droits des gens, used by Emer de Vattel.[1][2] The definition of international law has been debated; Bentham referred specifically to relationships between states which has been criticised for its narrow scope.[3] Lassa Oppenheim defined it in his treatise as ""a law between sovereign and equal states based on the common consent of these states"" and this definition has been largely adopted by international legal scholars.[4]
 There is a distinction between public and private international law; the latter is concerned with whether national courts can claim jurisdiction over cases with a foreign element and the application of foreign judgments in domestic law, whereas public international law covers rules with an international origin.[5] The difference between the two areas of law has been debated as scholars disagree about the nature of their relationship. Joseph Story, who originated the term ""private international law"", emphasised that it must be governed by the principles of public international law but other academics view them as separate bodies of law.[6][7] Another term, transnational law, is sometimes used to refer to a body of both national and international rules that transcend the nation state, although some academics emphasise that it is distinct from either type of law. It was defined by Philip Jessup as ""all law which regulates actions or events that transcend national frontiers"".[8]
 A more recent concept is supranational law, which was described in a 1969 paper as ""[a] relatively new word in the vocabulary of politics"".[9] Systems of supranational law arise when nations explicitly cede their right to make decisions to this system's judiciary and legislature, which then have the right to make laws that are directly effective in each member state.[9][10] This has been described as ""a level of international integration beyond mere intergovernmentalism yet still short of a federal system"".[9] The most common example of a supranational system is the European Union.[10]
 The origins of international law can be traced back to antiquity.[12] Among the earliest recorded examples are peace treaties between the Mesopotamian city-states of Lagash and Umma (approximately 3100 BCE), and an agreement between the Egyptian pharaoh, Ramesses II, and the Hittite king, Ḫattušili III, concluded in 1279 BCE.[11] Interstate pacts and agreements were negotiated and agreed upon by polities across the world, from the eastern Mediterranean to East Asia.[13] In Ancient Greece, many early peace treaties were negotiated between its city-states and, occasionally, with neighbouring states.[14] The Roman Empire established an early conceptual framework for international law, jus gentium, which governed the status of foreigners living in Rome and relations between foreigners and Roman citizens.[15][16] Adopting the Greek concept of natural law, the Romans conceived of jus gentium as being universal.[17] However, in contrast to modern international law, the Roman law of nations applied to relations with and between foreign individuals rather than among political units such as states.[18]
 Beginning with the Spring and Autumn period of the eighth century BCE, China was divided into numerous states that were often at war with each other. Rules for diplomacy and treaty-making emerged, including notions regarding just grounds for war, the rights of neutral parties, and the consolidation and partition of states; these concepts were sometimes applied to relations with barbarians along China's western periphery beyond the Central Plains.[19][20] The subsequent Warring States period saw the development of two major schools of thought, Confucianism and Legalism, both of which held that the domestic and international legal spheres were closely interlinked, and sought to establish competing normative principles to guide foreign relations.[20][21] Similarly, the Indian subcontinent was divided into various states, which over time developed rules of neutrality, treaty law, and international conduct, and established both temporary and permanent embassies.[22][23]
 Following the collapse of the western Roman Empire in the fifth century CE, Europe fragmented into numerous often-warring states for much of the next five centuries. Political power was dispersed across a range of entities, including the Church, mercantile city-states, and kingdoms, most of which had overlapping and ever-changing jurisdictions. As in China and India, these divisions prompted the development of rules aimed at providing stable and predictable relations. Early examples include canon law, which governed ecclesiastical institutions and clergy throughout Europe; the lex mercatoria (""merchant law""), which concerned trade and commerce; and various codes of maritime law, such as the Rolls of Oléron— aimed at regulating shipping in North-western Europe — and the later Laws of Wisby, enacted among the commercial Hanseatic League of northern Europe and the Baltic region.[24]
 In the Islamic world, Muhammad al-Shaybani published Al-Siyar Al-Kabīr in the eighth century, which served as a fundamental reference work for siyar, a subset of Sharia law, which governed foreign relations.[25][26] This was based on the division of the world into three categories: the dar al-Islam, where Islamic law prevailed; the dar al-sulh, non-Islamic realms that concluded an armistice with a Muslim government; and the dar al-harb, non-Islamic lands which were contested through jihad.[27][28] Islamic legal principles concerning military conduct served as precursors to modern international humanitarian law and institutionalised limitations on military conduct, including guidelines for commencing war, distinguishing between civilians and combatants and caring for the sick and wounded.[29][30]
 During the European Middle Ages, international law was concerned primarily with the purpose and legitimacy of war, seeking to determine what constituted ""just war"".[31] The Greco-Roman concept of natural law was combined with religious principles by Jewish philosopher Maimonides (1135–1204) and Christian theologian Thomas Aquinas (1225–1274) to create the new discipline of the ""law of nations"", which unlike its eponymous Roman predecessor, applied natural law to relations between states.[32][33] In Islam, a similar framework was developed wherein the law of nations was derived, in part, from the principles and rules set forth in treaties with non-Muslims.[34]
 
The 15th century witnessed a confluence of factors that contributed to an accelerated development of international law. Italian jurist Bartolus de Saxoferrato (1313–1357) was considered the founder of private international law. Another Italian jurist, Baldus de Ubaldis (1327–1400), provided commentaries and compilations of Roman, ecclesiastical, and feudal law, creating an organised source of law that could be referenced by different nations.[citation needed] Alberico Gentili (1552–1608) took a secular view to international law, authoring various books on issues in international law, notably Law of War, which provided comprehensive commentary on the laws of war and treaties.[35] Francisco de Vitoria (1486–1546), who was concerned with the treatment of indigenous peoples by Spain, invoked the law of nations as a basis for their innate dignity and rights, articulating an early version of sovereign equality between peoples.[36] Francisco Suárez (1548–1617) emphasised that international law was founded upon natural law and human positive law.[37][38] Dutch jurist Hugo Grotius (1583–1645) is widely regarded as the father of international law,[39] being one of the first scholars to articulate an international order that consists of a ""society of states"" governed not by force or warfare but by actual laws, mutual agreements, and customs.[40] Grotius secularised international law;[41] his 1625 work, De Jure Belli ac Pacis, laid down a system of principles of natural law that bind all nations regardless of local custom or law.[39] He inspired two nascent schools of international law, the naturalists and the positivists.[42] In the former camp was German jurist Samuel von Pufendorf (1632–1694), who stressed the supremacy of the law of nature over states.[43][44] His 1672 work, Of the Law of Nature and Nations, expanded on the theories of Grotius and grounded natural law to reason and the secular world, asserting that it regulated only external acts of states.[43] Pufendorf challenged the Hobbesian notion that the state of nature was one of war and conflict, arguing that the natural state of the world is actually peaceful but weak and uncertain without adherence to the law of nations.[45] The actions of a state consist of nothing more than the sum of the individuals within that state, thereby requiring the state to apply a fundamental law of reason, which is the basis of natural law. He was among the earliest scholars to expand international law beyond European Christian nations, advocating for its application and recognition among all peoples on the basis of shared humanity.[46]
 In contrast, positivist writers, such as Richard Zouche (1590–1661) in England and Cornelis van Bynkershoek (1673–1743) in the Netherlands, argued that international law should derive from the actual practice of states rather than Christian or Greco-Roman sources. The study of international law shifted away from its core concern on the law of war and towards the domains such as the law of the sea and commercial treaties.[47] The positivist school grew more popular as it reflected accepted views of state sovereignty and was consistent with the empiricist approach to philosophy that was then gaining acceptance in Europe.[48]
 The developments of the 17th century culminated at the conclusion of the Peace of Westphalia in 1648, which is considered the seminal event in international law.[49] The resulting Westphalian sovereignty is said to have established the current international legal order characterised by independent nation states, which have equal sovereignty regardless of their size and power, defined primarily by non-interference in the domestic affairs of sovereign states, although historians have challenged this narrative.[50] The idea of nationalism further solidified the concept and formation of nation-states.[51] Elements of the naturalist and positivist schools were synthesised, notably by German philosopher Christian Wolff (1679–1754) and Swiss jurist Emer de Vattel (1714–1767), both of whom sought a middle-ground approach.[52][53] During the 18th century, the positivist tradition gained broader acceptance, although the concept of natural rights remained influential in international politics, particularly through the republican revolutions of the United States and France.[citation needed]
 Until the mid-19th century, relations between states were dictated mostly by treaties, agreements between states to behave in a certain way, unenforceable except by force, and nonbinding except as matters of honour and faithfulness.[citation needed] One of the first instruments of modern armed conflict law was the Lieber Code of 1863, which governed the conduct of warfare during the American Civil War, and is noted for codifying rules and articles of war adhered to by nations across the world, including the United Kingdom, Prussia, Serbia and Argentina.[54] In the years that followed, numerous other treaties and bodies were created to regulate the conduct of states towards one another, including the Permanent Court of Arbitration in 1899, and the Hague and Geneva Conventions, the first of which was passed in 1864.[55][56]
 Colonial expansion by European powers reached its peak in the late 19th century and its influence began to wane following the unprecedented bloodshed of World War I, which spurred the creation of international organisations. Right of conquest was generally recognized as international law before World War II.[57] The League of Nations was founded to safeguard peace and security.[58][59] International law began to incorporate notions such as self-determination and human rights.[60] The United Nations (UN) was established in 1945 to replace the League, with an aim of maintaining collective security.[61] A more robust international legal order followed, buttressed by institutions such as the International Court of Justice (ICJ) and the UN Security Council (UNSC).[62] The International Law Commission (ILC) was established in 1947 to develop and codify international law.[61]
 In the 1940s through the 1970s, the dissolution of the Soviet bloc and decolonisation across the world resulted in the establishment of scores of newly independent states.[63] As these former colonies became their own states, they adopted European views of international law.[64] A flurry of institutions, ranging from the International Monetary Fund (IMF) and the International Bank for Reconstruction and Development (World Bank) to the World Health Organization furthered the development of a multilateralist approach as states chose to compromise on sovereignty to benefit from international cooperation.[65] Since the 1980s, there has been an increasing focus on the phenomenon of globalisation and on protecting human rights on the global scale, particularly when minorities or indigenous communities are involved, as concerns are raised that globalisation may be increasing inequality in the international legal system.[66]
 The sources of international law applied by the community of nations are listed in Article 38(1) of the Statute of the International Court of Justice, which is considered authoritative in this regard. These categories are, in order, international treaties, customary international law, general legal principles and judicial decisions and the teachings of prominent legal scholars as ""a subsidiary means for the determination of rules of law"".[67] It was originally considered that the arrangement of the sources sequentially would suggest an implicit hierarchy of sources; however, the statute does not provide for a hierarchy and other academics have argued that therefore the sources must be equivalent.[68][69]
 General principles of law have been defined in the Statute as ""general principles of law recognized by civilized nations"" but there is no academic consensus about what is included within this scope.[70][71] They are considered to be derived from both national and international legal systems, although including the latter category has led to debate about potential cross-over with international customary law.[72][73] The relationship of general principles to treaties or custom has generally been considered to be ""fill[ing] the gaps"" although there is still no conclusion about their exact relationship in the absence of a hierarchy.[74]
 A treaty is defined in Article 2 of the Vienna Convention on the Law of Treaties (VCLT) as ""an international agreement concluded between States in written form and governed by international law, whether embodied in a single instrument or in two or more related instruments and whatever its particular designation"".[75] The definition specifies that the parties must be states, however international organisations are also considered to have the capacity to enter treaties.[75][76] Treaties are binding through the principle of pacta sunt servanda, which allows states to create legal obligations on themselves through consent.[77][78] The treaty must be governed by international law; however it will likely be interpreted by national courts.[79] The VCLT, which codifies several bedrock principles of treaty interpretation, holds that a treaty ""shall be interpreted in good faith in accordance with the ordinary meaning to be given to the terms of the treaty in their context and in the light of its object and purpose"".[80] This represents a compromise between three theories of interpretation: the textual approach which looks to the ordinary meaning of the text, the subjective approach which considers factors such as the drafters' intention, and the teleological approach which interprets a treaty according to its objective and purpose.[80][81]
 A state must express its consent to be bound by a treaty through signature, exchange of instruments, ratification, acceptance, approval or accession. Accession refers to a state choosing to become party to a treaty that it is unable to sign, such as when establishing a regional body. Where a treaty states that it will be enacted through ratification, acceptance or approval, the parties must sign to indicate acceptance of the wording but there is no requirement on a state to later ratify the treaty, although they may still be subject to certain obligations.[82] When signing or ratifying a treaty, a state can make a unilateral statement to negate or amend certain legal provisions which can have one of three effects: the reserving state is bound by the treaty but the effects of the relevant provisions are precluded or changes, the reserving state is bound by the treaty but not the relevant provisions, or the reserving state is not bound by the treaty.[83][84] An interpretive declaration is a separate process, where a state issues a unilateral statement to specify or clarify a treaty provision. This can affect the interpretation of the treaty but it is generally not legally binding.[85][86] A state is also able to issue a conditional declaration stating that it will consent to a given treaty only on the condition of a particular provision or interpretation.[87]
 Article 54 of the VCLT provides that either party may terminate or withdraw from a treaty in accordance with its terms or at any time with the consent of the other party, with 'termination' applying to a bilateral treaty and 'withdrawal' applying to a multilateral treaty.[88] Where a treaty does not have provisions allowing for termination or withdrawal, such as the Genocide Convention, it is prohibited unless the right was implied into the treaty or the parties had intended to allow for it.[89] A treaty can also be held invalid, including where parties act ultra vires or negligently, where execution has been obtained through fraudulent, corrupt or forceful means, or where the treaty contradicts peremptory norms.[90]
 Customary international law requires two elements: a consistent practice of states and the conviction of those states that the consistent practice is required by a legal obligation, referred to as opinio juris.[91][92] Custom distinguishes itself from treaty law as it is binding on all states, regardless of whether they have participated in the practice, with the exception of states who have been persistent objectors during the process of the custom being formed and special or local forms of customary law.[93] The requirement for state practice relates to the practice, either through action or failure to act, of states in relation to other states or international organisations.[94] There is no legal requirement for state practice to be uniform or for the practice to be long-running, although the ICJ has set a high bar for enforcement in the cases of Anglo-Norwegian Fisheries and North Sea Continental Shelf.[95] There has been legal debate on this topic with the only prominent view on the length of time necessary to establish custom explained by Humphrey Waldock as varying ""according to the nature of the case"".[96] The practice is not required to be followed universally by states, but there must be a ""general recognition"" by states ""whose interests are specially affected"".[97]
 The second element of the test, opinio juris, the belief of a party that a particular action is required by the law is referred to as the subjective element.[98] The ICJ has stated in dictum in North Sea Continental Shelf that, ""Not only must the acts concerned amount to a settled practice, but they must also be such, or be carried out in such a way, as to be evidence of a belief that this practice is rendered obligatory by the existence of a rule of law requiring it"".[99] A committee of the International Law Association has argued that there is a general presumption of an opinio juris where state practice is proven but it may be necessary if the practice suggests that the states did not believe it was creating a precedent.[99] The test in these circumstances is whether opinio juris can be proven by the states' failure to protest.[100] Other academics believe that intention to create customary law can be shown by states including the principle in multiple bilateral and multilateral treaties, so that treaty law is necessary to form customs.[101]
 The adoption of the VCLT in 1969 established the concept of jus cogens, or peremptory norms, which are ""a norm accepted and recognized by the international community of States as a whole as a norm from which no derogation is permitted and which can be modified only by a subsequent norm of general international law having the same character"".[102] Where customary or treaty law conflicts with a peremptory norm, it will be considered invalid, but there is no agreed definition of jus cogens.[103] Academics have debated what principles are considered peremptory norms but the mostly widely agreed is the principle of non-use of force.[104] The next year, the ICJ defined erga omnes obligations as those owed to ""the international community as a whole"", which included the illegality of genocide and human rights.[102]
 There are generally two approaches to the relationship between international and national law, namely monism and dualism.[105] Monism assumes that international and national law are part of the same legal order.[106] Therefore, a treaty can directly become part of national law without the need for enacting legislation, although they will generally need to be approved by the legislature. Once approved, the content of the treaty is considered as a law that has a higher status than national laws. Examples of countries with a monism approach are France and the Netherlands.[107] The dualism approach considers that national and international law are two separate legal orders, so treaties are not granted a special status.[105][108] The rules in a treaty can only be considered national law if the contents of the treaty have been enacted first.[108] An example is the United Kingdom; after the country ratified the European Convention on Human Rights, the convention was only considered to have the force of law in national law after Parliament passed the Human Rights Act 1998.[109]
 In practice, the division of countries between monism and dualism is often more complicated; countries following both approaches may accept peremptory norms as being automatically binding and they may approach treaties, particularly later amendments or clarifications, differently than they would approach customary law.[110] Many countries with older or unwritten constitutions do not have explicit provision for international law in their domestic system and there has been an upswing in support for monism principles in relation to human rights and humanitarian law, as most principles governing these concepts can be found in international law.[111]
 A state is defined under Article 1 of the Montevideo Convention on the Rights and Duties of States as a legal person with a permanent population, a defined territory, government and capacity to enter relations with other states. There is no requirement on population size, allowing micro-states such as San Marino and Monaco to be admitted to the UN, and no requirement of fully defined boundaries, allowing Israel to be admitted despite border disputes. There was originally an intention that a state must have self-determination, but now the requirement is for a stable political environment. The final requirement of being able to enter relations is commonly evidenced by independence and sovereignty.[112]
 Under the principle of par in parem non habet imperium, all states are sovereign and equal,[113] but state recognition often plays a significant role in political conceptions. A country may recognise another nation as a state and, separately, it may recognise that nation's government as being legitimate and capable of representing the state on the international stage.[114][115] There are two theories on recognition; the declaratory theory sees recognition as commenting on a current state of law which has been separately satisfied whereas the constitutive theory states that recognition by other states determines whether a state can be considered to have legal personality.[116] States can be recognised explicitly through a released statement or tacitly through conducting official relations, although some countries have formally interacted without conferring recognition.[117]
 Throughout the 19th century and the majority of the 20th century, states were protected by absolute immunity, so they could not face criminal prosecution for any actions. However a number of countries began to distinguish between acta jure gestionis, commercial actions, and acta jure imperii, government actions; the restrictive theory of immunity said states were immune where they were acting in a governmental capacity but not a commercial one. The European Convention on State Immunity in 1972 and the UN Convention on Jurisdictional Immunities of States and their Property attempt to restrict immunity in accordance with customary law.[118]
 Historically individuals have not been seen as entities in international law, as the focus was on the relationship between states.[119][120] As human rights have become more important on the global stage, being codified by the UN General Assembly (UNGA) in the Universal Declaration of Human Rights in 1948, individuals have been given the power to defend their rights to judicial bodies.[121] International law is largely silent on the issue of nationality law with the exception of cases of dual nationality or where someone is claiming rights under refugee law but as, argued by the political theorist Hannah Arendt, human rights are often tied to someone's nationality.[122] The European Court of Human Rights allows individuals to petition the court where their rights have been violated and national courts have not intervened and the Inter-American Court of Human Rights and the African Court on Human and Peoples' Rights have similar powers.[121]
 Traditionally, sovereign states and the Holy See were the sole subjects of international law. With the proliferation of international organisations over the last century, they have also been recognised as relevant parties.[123] One definition of international organisations comes from the ILC's 2011 Draft Articles on the Responsibility of International Organizations which in Article 2(a) states that it is ""an organization established by treaty or other instrument governed by international law and possessing its own international legal personality"".[124] This definition functions as a starting point but does not recognise that organisations can have no separate personality but nevertheless function as an international organisation.[124] The UN Economic and Social Council has emphasised a split between inter-government organisations (IGOs), which are created by inter-governmental agreements, and international non-governmental organisations (INGOs).[125] All international organisations have members; generally this is restricted to states, although it can include other international organisations.[126] Sometimes non-members will be allowed to participate in meetings as observers.[127]
 The Yearbook of International Organizations sets out a list of international organisations, which include the UN, the WTO, the World Bank and the IMF.[128][129] Generally organisations consist of a plenary organ, where member states can be represented and heard; an executive organ, to decide matters within the competence of the organisation; and an administrative organ, to execute the decisions of the other organs and handle secretarial duties.[130] International organisations will typically provide for their privileges and immunity in relation to its member states in their constitutional documents or in multilateral agreements, such as the Convention on the Privileges and Immunities of the United Nations.[131] These organisations also have the power to enter treaties, using the Vienna Convention on the Law of Treaties between States and International Organizations or between International Organizations as a basis although it is not yet in force.[76] They may also have the right to bring legal claims against states depending, as set out in Reparation for Injuries, where they have legal personality and the right to do so in their constitution.[132]
 The UNSC has the power under Chapter VII of the UN Charter to take decisive and binding actions against states committing ""a threat to the peace, breach of the peace, or an act of aggression"" for collective security although prior to 1990, it has only intervened once, in the case of Korea in 1950.[133][134] This power can only be exercised, however, where a majority of member states vote for it, as well as receiving the support of the permanent five members of the UNSC.[135] This can be followed up with economic sanctions, military action, and similar uses of force.[136] The UNSC also has a wide discretion under Article 24, which grants ""primary responsibility"" for issues of international peace and security.[133] The UNGA, concerned during the Cold War with the requirement that the USSR would have to authorise any UNSC action, adopted the ""Uniting for Peace"" resolution of 3 November 1950, which allowed the organ to pass recommendations to authorize the use of force. This resolution also led to the practice of UN peacekeeping, which has been notably been used in East Timor and Kosovo.[137]
 There are more than one hundred international courts in the global community, although states have generally been reluctant to allow their sovereignty to be limited in this way.[138] The first known international court was the Central American Court of Justice, prior to World War I, when the Permanent Court of International Justice (PCIJ) was established. The PCIJ was replaced by the ICJ, which is the best known international court due to its universal scope in relation to geographical jurisdiction and subject matter.[139] There are additionally a number of regional courts, including the Court of Justice of the European Union, the EFTA Court and the Court of Justice of the Andean Community.[140] Interstate arbitration can also be used to resolve disputes between states, leading in 1899 to the creation of the Permanent Court of Arbitration which facilitates the process by maintaining a list of arbitrators. This process was used in the Island of Palmas case and to resolve disputes during the Eritrean-Ethiopian war.[141]
 The ICJ operates as one of the six organs of the UN, based out of the Hague with a panel of fifteen permanent judges.[142] It has jurisdiction to hear cases involving states but cannot get involved in disputes involving individuals or international organizations. The states that can bring cases must be party to the Statute of the ICJ, although in practice most states are UN members and would therefore be eligible. The court has jurisdiction over all cases that are referred to it and all matters specifically referred to in the UN Charter or international treaties, although in practice there are no relevant matters in the UN Charter.[143] The ICJ may also be asked by an international organisation to provide an advisory opinion on a legal question, which are generally considered non-binding but authoritative.[144]
 Conflict of laws, also known as private international law, was originally concerned with choice of law, determining which nation's laws should govern a particular legal circumstance.[145][146] Historically the comity theory has been used although the definition is unclear, sometimes referring to reciprocity and sometimes being used as a synonym for private international law.[147][148] Story distinguished it from ""any absolute paramount obligation, superseding all discretion on the subject"".[148] There are three aspects to conflict of laws – determining which domestic court has jurisdiction over a dispute, determining if a domestic court has jurisdiction and determining whether foreign judgments can be enforced. The first question relates to whether the domestic court or a foreign court is best placed to decide the case.[149] When determining the national law that should apply, the lex causae is the law that has been chosen to govern the case, which is generally foreign, and the lexi fori is the national law of the court making the determination. Some examples are lex domicilii, the law of the domicile, and les patriae, the law of the nationality.[150]
 The rules which are applied to conflict of laws will vary depending on the national system determining the question. There have been attempts to codify an international standard to unify the rules so differences in national law cannot lead to inconsistencies, such as through the Hague Convention on the Recognition and Enforcement of Foreign Judgments in Civil and Commercial Matters and the Brussels Regulations.[151][152][153] These treaties codified practice on the enforcement of international judgments, stating that a foreign judgment would be automatically recognised and enforceable where required in the jurisdiction where the party resides, unless the judgement was contrary to public order or conflicted with a local judgment between the same parties. On a global level, the New York Convention on the Recognition and Enforcement of Foreign Arbitral Awards was introduced in 1958 to internationalise the enforcement of arbitral awards, although it does not have jurisdiction over court judgments.[154]
 A state must prove that it has jurisdiction before it can exercise its legal authority.[155] This concept can be divided between prescriptive jurisdiction, which is the authority of a legislature to enact legislation on a particular issue, and adjudicative jurisdiction, which is the authority of a court to hear a particular case.[156] This aspect of private international law should first be resolved by reference to domestic law, which may incorporate international treaties or other supranational legal concepts, although there are consistent international norms.[157] There are five forms of jurisdiction which are consistently recognised in international law; an individual or act can be subject to multiple forms of jurisdiction.[158][159] The first is the territorial principle, which states that a nation has jurisdiction over actions which occur within its territorial boundaries.[160] The second is the nationality principle, also known as the active personality principle, whereby a nation has jurisdiction over actions committed by its nationals regardless of where they occur. The third is the passive personality principle, which gives a country jurisdiction over any actions which harm its nationals.[161] The fourth is the protective principle, where a nation has jurisdiction in relation to threats to its ""fundamental national interests"". The final form is universal jurisdiction, where a country has jurisdiction over certain acts based on the nature of the crime itself.[161][162]
 Following World War II, the modern system for international human rights was developed to make states responsible for their human rights violations.[163] The UN Economic and Security Council established the UN Commission on Human Rights in 1946, which developed the Universal Declaration of Human Rights (UDHR), which established non-binding international human rights standards, for work, standards of living, housing and education, non-discrimination, a fair trial and prohibition of torture. Two further human rights treaties were adopted by the UN in 1966, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR). These two documents along with the UDHR are considered the International Bill of Human Rights.[164]
 Non-domestic human rights enforcement operates at both the international and regional levels. Established in 1993, the Office of the UN High Commissioner for Human Rights supervises Charter-based and treaty-based procedures.[164] The former are based on the UN Charter and operate under the UN Human Rights Council, where each global region is represented by elected member states. The Council is responsible for Universal Periodic Review, which requires each UN member state to review its human rights compliance every four years, and for special procedures, including the appointment of special rapporteurs, independent experts and working groups.[165] The treaty-based procedure allows individuals to rely on the nine primary human rights treaties:
 The regional human rights enforcement systems operate in Europe, Africa and the Americas through the European Court of Human Rights, the Inter-American Court of Human Rights and the African Court on Human and Peoples' Rights.[167] International human rights has faced criticism for its Western focus, as many countries were subject to colonial rule at the time that the UDHR was drafted, although many countries in the Global South have led the development of human rights on the global stage in the intervening decades.[168]
 International labour law is generally defined as ""the substantive rules of law established at the international level and the procedural rules relating to their adoption and implementation"". It operates primarily through the International Labor Organization (ILO), a UN agency with the mission of protecting employment rights which was established in 1919.[169][170] The ILO has a constitution setting out a number of aims, including regulating work hours and labour supply, protecting workers and children and recognising equal pay and the right to free association, as well as the Declaration of Philadelphia of 1944, which re-defined the purpose of the ILO.[170][171] The 1998 Declaration on Fundamental Principles and Rights at Work further binds ILO member states to recognise fundamental labour rights including free association, collective bargaining and eliminating forced labour, child labour and employment discrimination.[171]
 The ILO have also created labour standards which are set out in their conventions and recommendations. Member states then have the choice as to whether or not to ratify and implement these standards.[171] The secretariat of the ILO is the International Labour Office, which can be consulted by states to determine the meaning of a convention, which forms the ILO's case law. Although the Right to Organise Convention does not provide an explicit right to strike, this has been interpreted into the treaty through case law.[172][173] The UN does not specifically focus on international labour law, although some of its treaties cover the same topics. Many of the primary human rights conventions also form part of international labour law, providing protection in employment and against discrimination on the grounds of gender and race.[174]
 It has been claimed that there is no concept of discrete international environmental law, with the general principles of international law instead being applied to these issues.[175] Since the 1960s, a number of treaties focused on environmental protection were ratified, including the Declaration of the United Nations Conference on the Human Environment of 1972, the World Charter for Nature of 1982, and the Vienna Convention for the Protection of the Ozone Layer of 1985. States generally agreed to co-operate with each other in relation to environmental law, as codified by principle 24 of the Rio Declaration of 1972.[176] Despite these, and other, multilateral environmental agreements covering specific issues, there is no overarching policy on international environmental protection or one specific international organisation, with the exception of the UN Environmental Programme. Instead, a general treaty setting out the framework for tackling an issue has then been supplemented by more specific protocols.[177]
 Climate change has been one of the most important and heavily debated topics in recent environmental law. The United Nations Framework Convention on Climate Change, intended to set out a framework for the mitigation of greenhouse gases and responses to resulting environmental changes, was introduced in 1992 and came into force two years later. As of 2023, 198 states were a party.[178][179] Separate protocols have been introduced through conferences of the parties, including the Kyoto Protocol which was introduced in 1997 to set specific targets for greenhouse gas reduction and the 2015 Paris Agreement which set the goal of keeping global warming at least below 2 °C (3.6 °F) above pre-industrial levels.[180]
 Individuals and organisations have some rights under international environmental law as the Aarhus Convention in 1998 set obligations on states to provide information and allow public input on these issues.[181] However few disputes under the regimes set out in environmental agreements are referred to the ICJ, as the agreements tend to specify their compliance procedures. These procedures generally focus on encouraging the state to once again become compliant through recommendations but there is still uncertainty on how these procedures should operate and efforts have been made to regulate these processes although some worry that this will undercut the efficiency of the procedures themselves.[182]
 Legal territory can be divided into four categories. There is territorial sovereignty which covers land and territorial sea, including the airspace above it and the subsoil below it, territory outside the sovereignty of any state, res nullius which is not yet within territorial sovereignty but is territory that is legally capable of being acquired by a state and res communis which is territory that cannot be acquired by a state.[183] There have historically been five methods of acquiring territorial sovereignty, reflecting Roman property law: occupation, accretion, cession, conquest and prescription.[184]
 The law of the sea is the area of international law concerning the principles and rules by which states and other entities interact in maritime matters. It encompasses areas and issues such as navigational rights, sea mineral rights, and coastal waters jurisdiction.[185] The law of the sea was primarily composed of customary law until the 20th century, beginning with the League of Nations Codification Conference in 1930, the UN Conference on the Law of the Sea and the adoption of the UNCLOS in 1982.[186] The UNCLOS was particularly notable for making international courts and tribunals responsible for the law of the sea.[187]
 The boundaries of a nation's territorial sea were initially proposed to be three miles in the late 18th century.[188] The UNCLOS instead defined it as being at most 12 nautical miles from the baseline (usually the coastal low-water mark) of a state; both military and civilian foreign ships are allowed innocent passage through these waters despite the sea being within the state's sovereignty.[189][190] A state can have jurisdiction beyond its territorial waters where it claims a contiguous zone of up to 24 nautical miles from its baseline for the purpose of preventing the infringement of its ""customs, fiscal, immigration and sanitary regulations"".[191] States are also able to claim an exclusive economic zone (EEZ) following passage of the UNCLOS, which can stretch up to 200 nautical miles from the baseline and gives the sovereign state rights over natural resources. Some states have instead chosen to retain their exclusive fishery zones, which cover the same territory.[192] There are specific rules in relation to the continental shelf, as this can extend further than 200 nautical miles. The International Tribunal for the Law of the Sea has specified that a state has sovereign rights over the resources of the entire continental shelf, regardless of its distance from the baseline, but different rights apply to the continental shelf and the water column above it where it is further than 200 nautical miles from the coast.[193]
 The UNCLOS defines the high seas as all parts of the sea that are not within a state's EEZ, territorial sea or internal waters.[194] There are six freedoms of the high seas—navigation, overflight, laying submarine cables and pipelines, constructing artificial islands, fishing and scientific research—some of which are subject to legal restrictions.[195] Ships in the high seas are deemed to have the nationality of the flag that they have the right to fly and no other state can exercise jurisdiction over them; the exception is ships used for piracy, which are subject to universal jurisdiction.[196]
 In 1944, the Bretton Woods Conference established the International Bank for Reconstruction and Development (later the World Bank) and the IMF. At the conference, the International Trade Organization was proposed but failed to be instituted due to the refusal of the United States to ratify its charter. Three years later, Part IV of the statute was adopted to create the General Agreement on Tariffs and Trade, which operated between 1948 and 1994, when the WTO was established. The OPEC, which banded together to control global oil supply and prices, caused the previous reliance on fixed currency exchange rates to be dropped in favour of floating exchange rates in 1971. During this recession, British Prime Minister Margaret Thatcher and US President Ronald Reagan pushed for free trade and deregulation under a neo-liberal agenda known as the Washington Consensus.[197]
 The law relating to the initiation of armed conflict is jus ad bellum.[198] This was codified in 1928 in the Kellogg–Briand Pact, which stated that conflicts should be settled through peaceful negotiations with the exception, through reservations drafted by some state parties, of self-defence.[199] These fundamental principles were re-affirmed in the UN Charter, which provided for ""an almost absolute prohibition on the use of force"", with the only three exceptions.[200][201] The first involves force authorised by the UNSC, as the entity is responsible in the first instance for responding to breaches or threats to the peace and acts of aggression, including the use of force or peacekeeping missions.[202] The second exception is where a state is acting in individual or collective self-defence. A state is allowed to act in self-defence in the case of an ""armed attack"" but the intention behind this exception has been challenged, particularly as nuclear weapons have become more common, with many states relying instead on the customary right of self-defence as set out in the Caroline test.[203][204] The ICJ considered collective self-defence in Nicaragua v. United States, where the U.S. unsuccessfully argued that it had mined harbours in Nicaragua in pre-emption of an attack by the Sandinista government against another member of the Organization of American States.[205] The final exception is where the UNSC delegates its responsibility for collective security to a regional organisation, such as NATO.[206]
 International humanitarian law (IHL) is an effort to ""mitigate the human suffering caused by war"" and it is often complementary to the law of armed conflict and international human rights law.[207] The concept of jus in bello (law in war) covers IHL, which is distinct from jus ad bellum.[198] Its scope lasts from the initiation of conflict until a peaceful settlement is reached.[208] There are two main principles in IHL; the principle of distinction dictates that combatants and non-combatants must be treated differently and the principle of not causing disproportionate suffering to combatants. In Legality of the Threat or Use of Nuclear Weapons, the ICJ described these concepts as ""intransgressible principles of international customary law"".[209]
 The two Hague Conventions of 1899 and 1907 considered restrictions on the conduct of war and the Geneva Conventions of 1949, which were organised by the International Committee of the Red Cross, considered the protection of innocent parties in conflict zones.[210] The First Geneva Convention covers wounded and ill combatants, the Second Geneva Convention covers combatants at sea who are wounded, ill or shipwrecked, the Third Geneva Convention covers prisoners of war and the Fourth Geneva Convention covers civilians.[209] These conventions were supplemented the additional Protocol I and Protocol II, which were codified in 1977.[210] Initially IHL conventions were only considered to apply to a conflict if all parties had ratified the relevant convention under the si omnes clause, but this posed concerns and the Martens clause began to be implemented, providing that the law would generally be deemed to apply.[211]
 There have been various agreements to outlaw particular types of weapons, such as the Chemical Weapons Convention and the Biological Weapons Convention. The use of nuclear weapons was determined to be in conflict with principles of IHL by the ICJ in 1995, although the court also held that it ""cannot conclude definitively whether the threat or use of nuclear weapons would be lawful or unlawful in an extreme circumstance of self-defence.""[212] Multiple treaties have attempted to regulate the use of these weapons, including the Non-Proliferation Treaty and the Joint Comprehensive Plan of Action, but key states have failed to sign or have withdrawn. There have been similar debates on the use of drones and cyberwarefare on the international stage.[213]
 International criminal law sets out the definition of international crimes and compels states to prosecute these crimes.[214] While war crimes were prosecuted throughout history, this has historically been done by national courts.[215] The International Military Tribunal in Nuremberg and the International Military Tribunal for the Far East in Tokyo were established at the end of World War II to prosecute key actors in Germany and Japan.[216] The jurisdiction of the tribunals was limited to crimes against peace (based on the Kellogg–Briand Pact), war crimes (based on the Hague Conventions) and crimes against humanity, establishing new categories of international crime.[217][218] Throughout the twentieth century, the separate crimes of genocide, torture and terrorism were also recognised.[218]
 Initially these crimes were intended to be prosecuted by national courts and subject to their domestic procedures.[219] The Geneva Conventions of 1949, the Additional Protocols of 1977 and the 1984 UN Convention against Torture mandated that the national courts of the contracting countries must prosecute these offenses where the perpetrator is on their territory or extradite them to any other interested state.[220] It was in the 1990s that two ad hoc tribunals, the International Criminal Tribunal for the Former Yugoslavia (ICTY) and the International Criminal Tribunal for Rwanda (ICTR), were established by the UNSC to address specific atrocities.[221][222] The ICTY had authority to prosecute war crimes, crimes against humanity and genocide occurring in Yugoslavia after 1991 and the ICTR had authority to prosecute genocide, crimes against humanity and grave breaches of the 1949 Geneva Conventions during the 1994 Rwandan genocide.[223][224]
 The International Criminal Court (ICC), established by the 1998 Rome Statute, is the first and only permanent international court to prosecute genocide, war crimes, crimes against humanity, and the crime of aggression.[225] There are 123 state parties to the ICC although a number of states have declared their opposition to the court; it has been criticised by African countries including The Gambia and Kenya for ""imperialist"" prosecutions.[226][227] One particular aspect of the court that has received scrutiny is the principle of complementarity, whereby the ICC only has jurisdiction if the national courts of a state with jurisdiction are ""unwilling or unable to prosecute"" or where a state has investigated but chosen not to prosecute a case.[228][229] The United States has a particularly complicated relationship with the ICC; originally signing the treaty in 2000, the US stated in 2002 that it did not intend to become a party as it believed the ICC threatened its national sovereignty and the country does not recognise the court's jurisdiction.[230][231]
 Hybrid courts are the most recent type of international criminal court; they aim to combine both national and international components, operating in the jurisdiction where the crimes in question occurred.[232][233] International courts have been criticised for a lack of legitimacy, as they can seem disconnected from the crimes that have occurred, but the hybrid courts are able to provide the resources that may be lacking in countries facing the aftermath of serious conflict.[232] There has been debate about what courts can be included within this definition, but generally the Special Panels for Serious Crimes in East Timor, the Kosovo Specialist Chambers, the Special Court for Sierra Leone, the Special Tribunal for Lebanon and the Extraordinary Chambers in the Courts of Cambodia have been listed.[234][225][233]
 International legal theory comprises a variety of theoretical and methodological approaches used to explain and analyse the content, formation and effectiveness of international law and institutions and to suggest improvements. Some approaches center on the question of compliance: why states follow international norms in the absence of a coercive power that ensures compliance. Other approaches focus on the problem of the formation of international rules: why states voluntarily adopt international law norms, that limit their freedom of action, in the absence of a world legislature; while other perspectives are policy oriented: they elaborate theoretical frameworks and instruments to criticize the existing norms and to make suggestions on how to improve them. Some of these approaches are based on domestic legal theory, some are interdisciplinary, and others have been developed expressly to analyse international law. Classical approaches to International legal theory are the natural law, the Eclectic and the legal positivism schools of thought.[235][page needed]
 The natural law approach argues that international norms should be based on axiomatic truths. The 16th-century natural law writer de Vitoria examined the questions of the just war, the Spanish authority in the Americas, and the rights of the Native American peoples. In 1625, Grotius argued that nations as well as persons ought to be governed by universal principle based on morality and divine justice while the relations among polities ought to be governed by the law of peoples, the jus gentium, established by the consent of the community of nations on the basis of the principle of pacta sunt servanda, that is, on the basis of the observance of commitments. On his part, de Vattel argued instead for the equality of states as articulated by 18th-century natural law and suggested that the law of nations was composed of custom and law on the one hand, and natural law on the other. During the 17th century, the basic tenets of the Grotian or eclectic school, especially the doctrines of legal equality, territorial sovereignty, and independence of states, became the fundamental principles of the European political and legal system and were enshrined in the 1648 Peace of Westphalia.[citation needed]
 The early positivist school emphasized the importance of custom and treaties as sources of international law. In the 16th-century, Gentili used historical examples to posit that positive law (jus voluntarium) was determined by general consent. van Bynkershoek asserted that the bases of international law were customs and treaties commonly consented to by various states, while John Jacob Moser emphasized the importance of state practice in international law. The positivism school narrowed the range of international practice that might qualify as law, favouring rationality over morality and ethics. The 1815 Congress of Vienna marked the formal recognition of the political and international legal system based on the conditions of Europe.[citation needed] Modern legal positivists consider international law as a unified system of rules that emanates from the states' will. International law, as it is, is an ""objective"" reality that needs to be distinguished from law ""as it should be"". Classic positivism demands rigorous tests for legal validity and it deems irrelevant all extralegal arguments.[236]
 John Austin asserted that due to the principle of par in parem non habet imperium, ""so-called"" international law, lacking a sovereign power and so unenforceable, was not really law at all, but ""positive morality"", consisting of ""opinions and sentiments...more ethical than legal in nature.""[237] Since states are few in number, diverse and atypical in character, unindictable, lack a centralised sovereign power, and their agreements unpoliced and decentralised, Martin Wight argued that international society is better described as anarchy.[238]
 Hans Morgenthau believed international law to be the weakest and most primitive system of law enforcement; he likened its decentralised nature to the law that prevails in preliterate tribal societies. Monopoly on violence is what makes domestic law enforceable; but between nations, there are multiple competing sources of force. The confusion created by treaty laws, which resemble private contracts between persons, is mitigated only by the relatively small number of states.[239] He asserted that no state may be compelled to submit a dispute to an international tribunal, making laws unenforceable and voluntary. International law is also unpoliced, lacking agencies for enforcement. He cites a 1947 US opinion poll in which 75% of respondents wanted ""an international police to maintain world peace"", but only 13% wanted that force to exceed the US armed forces. Later surveys have produced similar contradictory results.[240]
 International law is currently navigating a complex array of challenges and controversies that have underscored the dynamic nature of international relations in the 21st century. Some of these challenges include enforcement difficulties, the impact of technological advancements, climate change, and worldwide pandemics.[241] The possible re-emergence of right of conquest as international law is contentious.[242]
 Among the most pressing issues are enforcement difficulties, where the lack of a centralized global authority often leads to non-compliance with international norms, particularly evident in violations of International Humanitarian Law (IHL). Sovereignty disputes further complicate the international legal landscape, as conflicts over territorial claims and jurisdictional boundaries arise, challenging the principles of non-interference and peaceful resolution. Furthermore, the emergence of new global powers introduces additional layers of complexity, as these nations assert their interests and challenge established norms, necessitating a reevaluation of the global legal order to accommodate shifting power dynamics.[243]
 Cybersecurity has also emerged as a critical concern, with international law striving to address the threats posed by cyber-attacks to national security, infrastructure, and individual privacy. Climate change demands unprecedented international cooperation, as evidenced by agreements like the Paris Agreement, though disparities in responsibilities among nations pose significant challenges to collective action.[244]
 The COVID-19 pandemic has further highlighted the interconnectedness of the global community, emphasizing the need for coordinated efforts to manage health crises, vaccine distribution, and economic recovery.[245]
 These contemporary issues underscore the need for ongoing adaptation and cooperation within the framework of international law to address the multifaceted challenges of the modern world, ensuring a just, peaceful, and sustainable global order.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Climate change', 'prominent legal scholars', 'individuals have not been seen as entities in international law', 'war and diplomacy, economic relations, and human rights', 'economic sanctions, military action, and similar uses of force'], 'answer_start': [], 'answer_end': []}"
"
 Art of Central Asia
 Art of East Asia
 Art of South Asia
 Art of Southeast Asia
 Art of Europe
 Art of Africa
 Art of the Americas
 Art of Oceania
 The history of architecture traces the changes in architecture through various traditions, regions, overarching stylistic trends, and dates. The beginnings of all these traditions is thought to be humans satisfying the very basic need of shelter and protection.[1] The term ""architecture"" generally refers to buildings, but in its essence is much broader, including fields we now consider specialized forms of practice, such as urbanism, civil engineering, naval, military,[2] and landscape architecture.
 Trends in architecture were influenced, among other factors, by technological innovations, particularly in the 19th, 20th and 21st centuries. The improvement and/or use of steel, cast iron, tile, reinforced concrete, and glass helped for example Art Nouveau appear and made Beaux Arts more grandiose.[3]
 Humans and their ancestors have been creating various types of shelters for at least hundreds of thousands of years, and shelter-building may have been present early in hominin evolution. All great apes will construct ""nests"" for sleeping, albeit at different frequencies and degrees of complexity. Chimpanzees regularly make nests out of bundles of branches woven together;[4] these vary depending on the weather (nests have thicker bedding when cool and are built with larger, stronger supports in windy or wet weather).[5] Orangutans currently make the most complex nests out of all non-human great apes, complete with roofs, blankets, pillows, and ""bunks"".[6]
 It has been argued that nest-building practices were crucial to the evolution of human creativity and construction skill moreso than tool use, as hominins became required to build nests not just in uniquely adapted circumstances but as forms of signalling.[7] Retaining arboreal features like highly prehensile hands for the expert construction of nests and shelters would have also benefitted early hominins in unpredictable environments and changing climates.[5] Many hominins, especially the earliest ones such as Ardipithecus[8] and Australopithecus[9] retained such features and may have chosen to build nests in trees where available. The development of a ""home base"" 2 million years ago may have also fostered the evolution of constructing shelters or protected caches.[10] Regardless of the complexity of nest-building, early hominins may still have still slept in more or less 'open' conditions, unless the opportunity of a rock shelter was afforded.[7] These rock shelters could be used as-is with little more amendments than nests and hearths, or in the case of established bases —especially among later hominins— they could be personalized with rock art (in the case of Lascaux) or other types of aesthetic structures (in the case of the Bruniquel Cave among the Neanderthals)[11] In cases of sleeping in open ground, Dutch ethologist Adriaan Kortlandt once proposed that hominins could have built temporary enclosures of thorny bushes to deter predators, which he supported using tests that showed lions becoming averse to food if near thorny branches.[12]
 In 2000, archaeologists at the Meiji University in Tokyo claimed to have found 2 pentagonal alignments of post holes on a hillside near the village of Chichibu, interpreting it as two huts dated around 500,000 years old and built by Homo erectus.[13] Currently, the earliest confirmed purpose-built structures are in France at the site of Terra Amata, along with the earliest evidence of artificial fire, c. 400,000 years ago.[14] Due to the perishable nature of shelters of this time, it is difficult to find evidence for dwellings beyond hearths and the stones that may make up a dwelling's foundation. Near Wadi Halfa, Sudan, the Arkin 8 site contains 100,000 year old circles of sandstone that were likely the anchor stones for tents.[15] In eastern Jordan, post hole markings in the soil give evidence to houses made of poles and thatched brush around 20,000 years ago.[16] In areas where bone — especially mammoth bone — is a viable material, evidence of structures preserve much more easily, such as the mammoth-bone dwellings among the Mal'ta-Buret' culture 24–15,000 years ago and at Mezhirich 15,000 years ago. The Upper Paleolithic in general is characterized by the expansion and cultural growth of anatomically modern humans (as well as the cultural growth of Neanderthals, despite their steady extinction at this time), and although we currently lack data for dwellings built before this time, the dwellings of this era begin to more commonly show signs of aesthetic modification, such as at Mezhirich where engraved mammoth tusks may have formed the ""facade"" of a dwelling.[17]
 Architectural advances are an important part of the Neolithic period (10,000-2000 BC), during which some of the major innovations of human history occurred. The domestication of plants and animals, for example, led to both new economics and a new relationship between people and the world, an increase in community size and permanence, a massive development of material culture and new social and ritual solutions to enable people to live together in these communities. New styles of individual structures and their combination into settlements provided the buildings required for the new lifestyle and economy, and were also an essential element of change.[21]
 Although many dwellings belonging to all prehistoric periods and also some clay models of dwellings have been uncovered enabling the creation of faithful reconstructions, they seldom included elements that may relate them to art. Some exceptions are provided by wall decorations and by finds that equally apply to Neolithic and Chalcolithic rites and art.
 In South and Southwest Asia, Neolithic cultures appear soon after 10,000 BC, initially in the Levant (Pre-Pottery Neolithic A and Pre-Pottery Neolithic B) and from there spread eastwards and westwards. There are early Neolithic cultures in Southeast Anatolia, Syria and Iraq by 8000 BC, and food-producing societies first appear in southeast Europe by 7000 BC, and Central Europe by c. 5500 BC (of which the earliest cultural complexes include the Starčevo-Koros (Cris), Linearbandkeramic, and Vinča).[22][23][24][25]
 Neolithic settlements and ""cities"" include:
 Mesopotamia is most noted for its construction of mud-brick buildings and the construction of ziggurats, occupying a prominent place in each city and consisting of an artificial mound, often rising in huge steps, surmounted by a temple. The mound was no doubt to elevate the temple to a commanding position in what was otherwise a flat river valley. The great city of Uruk had a number of religious precincts, containing many temples larger and more ambitious than any buildings previously known.[32]
 The word ziggurat is an anglicized form of the Akkadian word ziqqurratum, the name given to the solid stepped towers of mud brick. It derives from the verb zaqaru, (""to be high""). The buildings are described as being like mountains linking Earth and heaven. The Ziggurat of Ur, excavated by Leonard Woolley, is 64 by 46 meters at base and originally some 12 meters in height with three stories. It was built under Ur-Nammu (circa 2100 B.C.) and rebuilt under Nabonidus (555–539 B.C.), when it was increased in height to probably seven stories.[33]
 Modern imaginings of ancient Egypt are heavily influenced by the surviving traces of monumental architecture. Many formal styles and motifs were established at the dawn of the pharaonic state, around 3100 BC. The most iconic Ancient Egyptian buildings are the pyramids, built during the Old and Middle Kingdoms (c.2600–1800 BC) as tombs for the pharaoh. However, there are also impressive temples, like the Karnak Temple Complex.
 The Ancient Egyptians believed in the afterlife. They also believed that in order for their soul (known as ka) to live eternally in their afterlife, their bodies would have to remain intact for eternity. So, they had to create a way to protect the deceased from damage and grave robbers. This way, the mastaba was born. These were adobe structures with flat roofs, which had underground rooms for the coffin, about 30 m down. Imhotep, an ancient Egyptian priest and architect, had to design a tomb for the Pharaoh Djoser. For this, he placed five mastabas, one above the next, this way creating the first Egyptian pyramid, the Pyramid of Djoser at Saqqara (c.2667–2648 BC), which is a step pyramid. The first smooth-sided one was built by Pharaoh Sneferu, who ruled between c.2613 and 2589 BC. The most imposing one is the Great Pyramid of Giza, made for Sneferu's son: Khufu (c.2589–2566 BC), being the last surviving wonder of the ancient world and the largest pyramid in Egypt. The stone blocks used for pyramids were held together by mortar, and the entire structure was covered with highly polished white limestone, with their tops topped in gold. What we see today is actually the core structure of the pyramid. Inside, narrow passages led to the royal burial chambers. Despite being highly associated with the Ancient Egypt, pyramids have been built by other civilisations too, like the Mayans.
 Due to the lack of resources and a shift in power towards priesthood, ancient Egyptians stepped away from pyramids, and temples became the focal point of cult construction. Just like the pyramids, Ancient Egyptian temples were also spectacular and monumental. They evolved from small shrines made of perishable materials to large complexes, and by the New Kingdom (circa 1550–1070 BC) they have become massive stone structures consisting of halls and courtyards. The temple represented a sort of 'cosmos' in stone, a copy of the original mound of creation on which the god could rejuvenate himself and the world. The entrance consisted of a twin gateway (pylon), symbolizing the hills of the horizon. Inside there were columned halls symbolizing a primeval papyrus thicket. It was followed by a series of hallways of decreasing size, until the sanctuary was reached, where a god's cult statue was placed. Back in ancient times, temples were painted in bright colours, mainly red, blue, yellow, green, orange, and white. Because of the desert climate of Egypt, some parts of these painted surfaces were preserved well, especially in interiors.
 An architectural element specific to ancient Egyptian architecture is the cavetto cornice (a concave moulding), introduced by the end of the Old Kingdom. It was widely used to accentuate the top of almost every formal pharaonic building. Because of how often it was used, it will later decorate many Egyptian Revival buildings and objects.[41][38]
 The first Urban Civilization in the Indian subcontinent is traceable originally to the Indus Valley civilisation mainly in Mohenjodaro and Harappa, now in modern-day Pakistan as well western states of the Republic of India. The earliest settlements are seen during the Neolithic period in Merhgarh, Balochistan. The civilization's cities were noted for their urban planning with baked brick buildings, elaborate drainage and water systems, and handicraft (carnelian products, seal carving). This civilisation transitioned from the Neolithic period into the Chalcolithic period and beyond with their expertise in metallurgy (copper, bronze, lead, and tin).[44] Their urban centres possibly grew to contain between 30,000 and 60,000 individuals,[45] and the civilisation itself may have contained between one and five million individuals.[46]
 Since the advent of the Classical Age in Athens, in the 5th century BC, the Classical way of building has been deeply woven into Western understanding of architecture and, indeed, of civilization itself.[53] From circa 850 BC to circa 300 AD, ancient Greek culture flourished on the Greek mainland, on the Peloponnese, and on the Aegean islands. However, Ancient Greek architecture is best known for its temples, many of which are found throughout the region, and the Parthenon is a prime example of this. Later, they will serve as inspiration for Neoclassical architects during the late 18th and the 19th century. The most well-known temples are the Parthenon and the Erechtheion, both on the Acropolis of Athens. Another type of important Ancient Greek buildings were the theatres. Both temples and theatres used a complex mix of optical illusions and balanced ratios.
 Ancient Greek temples usually consist of a base with continuous stairs of a few steps at each edges (known as crepidoma), a cella (or naos) with a cult statue in it, columns, an entablature, and two pediments, one on the front side and another in the back. By the 4th century BC, Greek architects and stonemasons had developed a system of rules for all buildings known as the orders: the Doric, the Ionic, and the Corinthian. They are most easily recognised by their columns (especially by the capitals). The Doric column is stout and basic, the Ionic one is slimmer and has four scrolls (called volutes) at the corners of the capital, and the Corinthian column is just like the Ionic one, but the capital is completely different, being decorated with acanthus leafs and four scrolls.[47] Besides columns, the frieze was different based on order. While the Doric one has metopes and triglyphs with guttae, Ionic and Corinthian friezes consist of one big continuous band with reliefs.
 Besides the columns, the temples were highly decorated with sculptures, in the pediments, on the friezes, metopes and triglyphs. Ornaments used by Ancient Greek architects and artists include palmettes, vegetal or wave-like scrolls, lion mascarons (mostly on lateral cornices), dentils, acanthus leafs, bucrania, festoons, egg-and-dart, rais-de-cœur, beads, meanders, and acroteria at the corners of the pediments. Pretty often, ancient Greek ornaments are used continuously, as bands. They will later be used in Etruscan, Roman and in the post-medieval styles that tried to revive Greco-Roman art and architecture, like Renaissance, Baroque, Neoclassical etc.
 Looking at the archaeological remains of ancient and medieval buildings it is easy to perceive them as limestone and concrete in a grey taupe tone and make the assumption that ancient buildings were monochromatic. However, architecture was polychromed in much of the Ancient and Medieval world. One of the most iconic Ancient buildings, the Parthenon (c. 447–432 BC) in Athens, had details painted with vibrant reds, blues and greens. Besides ancient temples, Medieval cathedrals were never completely white. Most had colored highlights on capitals and columns.[54] This practice of coloring buildings and artworks was abandoned during the early Renaissance. This is because Leonardo da Vinci and other Renaissance artists, including Michelangelo, promoted a color palette inspired by the ancient Greco-Roman ruins, which because of neglect and constant decay during the Middle Ages, became white despite being initially colorful. The pigments used in the ancient world were delicate and especially susceptible to weathering. Without necessary care, the colors exposed to rain, snow, dirt, and other factors, vanished over time, and this way Ancient buildings and artworks became white, like they are today and were during the Renaissance.[55]
 The architecture of ancient Rome has been one of the most influential in the world. Its legacy is evident throughout the medieval and early modern periods, and Roman buildings continue to be reused in the modern era in both New Classical and Postmodern architecture. It was particularly influenced by Greek and Etruscan styles. A range of temple types was developed during the republican years (509–27 BC), modified from Greek and Etruscan prototypes.
 Wherever the Roman army conquered, they established towns and cities, spreading their empire and advancing their architectural and engineering achievements. While the most important works are to be found in Italy, Roman builders also found creative outlets in the western and eastern provinces, of which the best examples preserved are in modern-day North Africa, Turkey, Syria and Jordan. Extravagant projects appeared, like the Arch of Septimius Severus in Leptis Magna (present-day Libya, built in 216 AD), with broken pediments on all sides, or the Arch of Caracalla in Thebeste (present-day Algeria, built in c.214 AD), with paired columns on all sides, projecting entablatures and medallions with divine busts. Due to the fact that the empire was formed from multiple nations and cultures, some buildings were the product of combining the Roman style with the local tradition. An example is the Palmyra Arch (present-day Syria, built in c.212–220), some of its arches being embellished with a repeated band design consisting of four ovals within a circle around a rosette, which are of Eastern origin.
 Among the many Roman architectural achievements were domes (which were created for temples), baths, villas, palaces and tombs. The most well known example is the one of the Pantheon in Rome, being the largest surviving Roman dome and having a large oculus at its centre. Another important innovation is the rounded stone arch, used in arcades, aqueducts and other structures. Besides the Greek orders (Doric, Ionic and Corinthian), the Romans invented two more. The Tuscan order was influenced by the Doric, but with un-fluted columns and a simpler entablature with no triglyphs or guttae, while the Composite was a mixed order, combining the volutes of the Ionic order capital with the acanthus leaves of the Corinthian order.
 Between 30 and 15 BC, the architect and engineer Marcus Vitruvius Pollio published a major treatise, De architectura, which influenced architects around the world for centuries. As the only treatise on architecture to survive from antiquity, it has been regarded since the Renaissance as the first book on architectural theory, as well as a major source on the canon of classical architecture.[60]
 Just like the Greeks, the Romans built amphiteatres too. The largest amphitheatre ever built, the Colosseum in Rome, could hold around 50,000 spectators. Another iconic Roman structure that demonstrates their precision and technological advancement is the Pont du Gard in southern France, the highest surviving Roman aqueduct.[61][57]
 From over 3,000 years before the Europeans 'discovered' America, complex societies had already been established across North, Central and South America. The most complex ones were in Mesoamerica, notably the Mayans, the Olmecs and the Aztecs, but also Incas in South America. Structures and buildings were often aligned with astronomical features or with the cardinal directions.
 Much of the Mesoamerican architecture developed through cultural exchange – for example the Aztecs learnt much from earlier Mayan architecture. Many cultures built entire cities, with monolithic temples and pyramids decoratively carved with animals, gods and kings. Most of these cities had a central plaza with governmental buildings and temples, plus public ball courts, or tlachtli, on raised platforms. Just like in ancient Egypt, here were built pyramids too, being generally stepped. They were probably not used as burial chambers, but had important religious sites at the top.[64] They had few rooms, as interiors mattered less than the ritual presence of these imposing structures and the public ceremonies they hosted; so, platforms, altars, processional stairs, statuary, and carving were all important.[67]
 Inca architecture originated from the Tiwanaku styles, founded in the 2nd century B.C.E.. The Incas used topography and land materials in their designs, with the capital city of Cuzco still containing many examples. The famous Machu Picchu royal estate is a surviving example, along with Sacsayhuamán and Ollantaytambo. The Incas also developed a road system along the western continent, placing their distinctive architecture along the way, visually asserting their imperial rule along the frontier. Other groups such as the Muisca did not construct grand architecture of stone based materials, but rather made of materials like wood and clay.
 After the fall of the Indus Valley, South Asian architecture entered the Dharmic period which saw the development of Ancient Indian architectural styles which further developed into various unique forms in the Middle Ages, along with the combination of Islamic styles, and later, other global traditions.
 Buddhist architecture developed in the Indian subcontinent during the 4th and 2nd century BC, and spread first to China and then further across Asia. Three types of structures are associated with the religious architecture of early Buddhism: monasteries (viharas), places to venerate relics (stupas), and shrines or prayer halls (chaityas, also called chaitya grihas), which later came to be called temples in some places. The most iconic Buddhist type of building is the stupa, which consists of a domed structure containing relics, used as a place of meditation to commemorate Buddha. The dome symbolised the infinite space of the sky.[68]
 Buddhism had a significant influence on Sri Lankan architecture after its introduction,[69] and ancient Sri Lankan architecture was mainly religious, with over 25 styles of Buddhist monasteries.[70] Monasteries were designed using the Manjusri Vasthu Vidya Sastra, which outlines the layout of the structure.
 After the fall of the Gupta empire, Buddhism mainly survived in Bengal under the Palas,[71] and has had a significant impact on pre-Islamic Bengali architecture of that period.[72]
 Across the Indian subcontinent, Hindu architecture evolved from simple rock-cut cave shrines to monumental temples. From the 4th to 5th centuries AD, Hindu temples were adapted to the worship of different deities and regional beliefs, and by the 6th or 7th centuries larger examples had evolved into towering brick or stone-built structures that symbolise the sacred five-peaked Mount Meru. Influenced by early Buddhist stupas, the architecture was not designed for collective worship, but had areas for worshippers to leave offerings and perform rituals.[73]
 Many Indian architectural styles for structures such as temples, statues, homes, markets, gardens and planning are as described in Hindu texts.[74][75] The architectural guidelines survive in Sanskrit manuscripts and in some cases also in other regional languages. These include the Vastu shastras, Shilpa Shastras, the Brihat Samhita, architectural portions of the Puranas and the Agamas, and regional texts such as the Manasara among others.[76][77]
 Since this architectural style emerged in the classical period, it has had a considerable influence on various medieval architectural styles like that of the Gurjaras, Dravidians, Deccan, Odias, Bengalis, and the Assamese.
 This style of North Indian architecture has been observed in Hindu as well as Jain places of worship and congregation. It emerged in the 11th to 13th centuries under the Chaulukya (Solanki) period.[79] It eventually became more popular among the Jain communities who spread it in the greater region and across the world.[80] These structures have the unique features like a large number of projections on external walls with sharply carved statues, and several urushringa spirelets on the main shikhara.
 The Himalayas are inhabited by various people groups including the Paharis, Sino-Tibetans, Kashmiris, and many more groups. Being from different religious and ethnic backgrounds, the architecture has also had multiple influences. Considering the logistical difficulties and slower pace of life in the Himalayas, artisans have that the time to make intricate wood carvings and paintings accompanied by ornamental metal work and stone sculptures that are reflected in religious as well as civic and military buildings. These styles exist in different forms from Tibet and Kashmir to Assam and Nagaland.[81] A common feature is observed in the slanted layered roofs on temples, mosques, and civic buildings.[82]
 This is an architectural style that emerged in the southern part of the Indian subcontinent and in Sri Lanka. These include Hindu temples with a unique style that involves a shorter pyramidal tower over the garbhagriha or sanctuary called a vimana, where the north has taller towers, usually bending inwards as they rise, called shikharas. These also include secular buildings that may or may not have slanted roofs based on the geographical region. In the Tamil country, this style is influenced by the Sangam period as well as the styles of the great dynasties that ruled it. This style varies in the region to its west in Kerala that is influenced by geographic factors like western trade and the monsoons which result in sloped roofs.[85] Further north, the Karnata Dravida style varies based on the diversity of influences, often relaying much about the artistic trends of the rulers of twelve different dynasties.[86]
 The ancient Kalinga region corresponds to the present-day eastern Indian areas of Odisha, West Bengal and northern Andhra Pradesh. Its architecture reached a peak between the 9th and 12th centuries under the patronage of the Somavamsi dynasty of Odisha. Lavishly sculpted with hundreds of figures, Kalinga temples usually feature repeating forms such as horseshoes. Within the protective walls of the temple complex are three main buildings with distinctive curved towers called deul or deula and prayer halls called jagmohan.[88]
 
 Chinese and Confucian culture has had a significant influence on the art and architecture in the Sinosphere (mainly Vietnam, Korea, Japan).[89]
 What is recognised today as Chinese culture has its roots in the Neolithic period (10,000–2000 BC), covering the cultural sites of Yangshao, Longshan, and Liangzhu in central China. Sections of present-day north-east China also contain sites of the Neolithic Hongshan culture that manifested aspects of proto-Chinese culture. Native Chinese belief systems included naturalistic, animistic and hero worship. In general, open-air platforms (tan, or altar) were used for worshipping naturalistic deities, such as the gods of wind and earth, whereas formal buildings (miao, or temple) were for heroes and deceased ancestors.
 Most early buildings in China were timber structures. Columns with sets of brackets on the face of the buildings, mostly in even numbers, made the central intercolumnal space the largest interior opening. Heavily tiled roofs sat squarely on the timber building with walls constructed in brick or pounded earth.
 The transmission of Buddhism into China around the 1st century AD led to a new era of religious practices, and so to new building types. Places of worship in form of cave temples appeared in China, based on Indian rock-cut ones. Another new building type introduced by Buddhism was the Chinese form of stupa (ta) or pagoda. In India, stupas were erected to commemorate well-known people or teachers: consequently, the Buddhist tradition adapted the structure to remember the great teacher, the Buddha. In The Chinese pagoda shared a similar symbolism with the Indian stupa and was built with sponsorship mainly from imperial patrons who hoped to gain earthly merits for the next life. Buddhism reached its peak from the 6th to the 8th centuries when there was an unprecedented number of monasteries thought China. More than 4,600 official and 40,000 unofficial monasteries were built. They varies in size by the number of cloisters they contained, ranging from 6 to 120. Each cloister consisted of a main stand-alone building – a hall, pagoda of pavilion – and was surrounded by a covered corridor in a rectangular compounded served by a gate building.[90]
 Korean architecture, especially post Choson period showcases Ming-Qing influences.[91]
 Traditionally, Japanese architecture was made of wood and fusuma (sliding doors) in place of walls, allowing internal space to be altered to suit different purposes. The introduction of Buddhism in the mid 6th century, via the neighbouring Korean kingdom of Paekche, initiated large-scale wooden temple building with an emphasis on simplicity, and much of the architecture was imported from China and other Asian cultures. By the end of this century, Japan was constructing Continental-style monasteries, notably the temple, known as Horyu-ji in Ikaruga.[92] In contrast with Western architecture, Japanese structures rarely use stone, except for specific elements such as foundations. Walls are light, thin, never load-bearing and often movable.[60]
 From the start of the 9th century to the early 15th century, Khmer kings rules over a vad Hindu-Buddhist empire in Southeast Asia. Angkor, in present-day Cambodia, was its capital city, and most of its surviving buildings are east-facing stone temples, many of them constructed in pyramidal, tiered form consisting of five square structures with towers, or prasats, that represent the sacred five-peaked Mount Meru of Hindu, Jain and Buddhist doctrine. As the residences of gods, temples were made of durable materials such as sandstone, brick or laterite, a clay-like substance that dries hard.[94]
 Cham architecture in Vietnam also follows a similar style.[93]
 Traditional Sub-Saharan African architecture is diverse, varying significantly across regions. Included among traditional house types, are huts, sometimes consisting of one or two rooms, as well as various larger and more complex structures.
 In much of West Africa, rectangular houses with peaked roofs and courtyards, sometimes consisting of several rooms and courtyards, are also traditionally found (sometimes decorated, with adobe reliefs as among the Ashanti of Ghana,[96][97] or carved pillars as among the Yoruba people of Nigeria, especially in palaces and the dwellings of the wealthy)[98] Besides the regular rectangular type of dwelling with a sharp roof, widespread in West Africa and Madagascar, there also other types of houses: beehive houses made from a circle of stones topped with a domed roof, and the round one, with a cone-shaped roof. The first type, which also existed in America, is characteristic especially for Southern Africa. These were used by Bantu-speaking groups in southern and parts of east Africa, which was made with mud, poles, thatch, and cow dung (rectangular houses were more common among the Bantu-speaking peoples of the greater Congo region and central Africa). The round hut with a cone-shaped roof is widespread especially in Sudan and Eastern Africa, but is also present in Colombia and New Caledonia, as well as in the Western Sudan and Sahel regions of west Africa, where they are sometimes arranged into compounds.[99] A distinct style of traditional wooden architecture exists among the Grassland peoples of Cameroon, such as the Bamileke.
 In several West African societies, including the kingdom of Benin (and of other Edo peoples), and the kingdoms of the Yoruba, Hausa, at sites like Jenne-Jeno (a pre-Islamic city in Mali),[100][101] and elsewhere, towns and cities were surrounded by large walls of mud brick or adobe,[102] and sometimes by monumental moats and earthworks, such as Sungbo's Eredo (in the Nigerian Yoruba kingdom of Ijebu) and the Walls of Benin (of the Nigerian Kingdom of Benin).[103][104] In medieval southern Africa, a tradition existed of fortified stone settlements such as Great Zimbabwe and Khami.
 The famed Benin City of southwest Nigeria (capital of the Kingdom of Benin) destroyed by the Punitive Expedition, was a large complex of homes in coursed clay, with hipped roofs of shingles or palm leaves. The Palace had a sequence of ceremonial rooms, and was decorated with brass plaques. It was surrounded by a monumental complex of earthworks and walls whose construction is thought to have begun by the early Middle Ages.[103][104][105][106]
 In the Western Sahel region, Islamic influence was a major contributing factor to architectural development from the later ages of the Kingdom of Ghana. At Kumbi Saleh, locals lived in domed-shaped dwellings in the king's section of the city, surrounded by a great enclosure. Traders lived in stone houses in a section which possessed 12 beautiful mosques, as described by al-bakri, with one centered on Friday prayer.[107] The king is said to have owned several mansions, one of which was sixty-six feet long, forty-two feet wide, contained seven rooms, was two stories high, and had a staircase; with the walls and chambers filled with sculpture and painting.[108]
 Sahelian architecture initially grew from the two cities of Djenné and Timbuktu. The Sankore Mosque in Timbuktu, constructed from mud on timber, was similar in style to the Great Mosque of Djenné. The rise of kingdoms in the West African coastal region produced architecture which drew on indigenous traditions, utilizing wood, mud-brick and adobe. Though later acquiring Islamic influences, the style also had roots in local pre-Islamic building styles, such as those found in ancient settlements like Jenne-Jeno, Dia, Mali, and Dhar Tichitt,[109] some of which employed a traditional sahelian style of cylindrical mud brick.[100]
 Ethiopian architecture (including modern-day Eritrea) expanded from the Aksumite style and incorporated new traditions with the expansion of the Ethiopian state. Styles incorporated more wood and rounder structures in domestic architecture in the center of the country and the south, and these stylistic influences were manifested in the construction of churches and monasteries. Throughout the medieval period, Aksumite architecture and influences and its monolithic tradition persisted, with its influence strongest in the early medieval (Late Aksumite) and Zagwe periods (when the rock-cut monolithic churches of Lalibela were carved). Throughout the medieval period, and especially from the 10th to 12th centuries, churches were hewn out of rock throughout Ethiopia, especially during the northernmost region of Tigray, which was the heart of the Aksumite Empire. The most famous example of Ethiopian rock-hewn architecture are the eleven monolithic churches of Lalibela, carved out of the red volcanic tuff found around the town.[110] During the early modern period in Ethiopia, the absorption of new diverse influences such as Baroque, Arab, Turkish and Gujarati style began with the arrival of Portuguese Jesuit missionaries in the 16th and 17th centuries.
 Most Oceanic buildings consist of huts, made of wood and other vegetal materials. Art and architecture have often been closely connected—for example, storehouses and meetinghouses are often decorated with elaborate carvings—and so they are presented together in this discussion. The architecture of the Pacific Islands was varied and sometimes large in scale. Buildings reflected the structure and preoccupations of the societies that constructed them, with considerable symbolic detail. Technically, most buildings in Oceania were no more than simple assemblages of poles held together with cane lashings; only in the Caroline Islands were complex methods of joining and pegging known. Fakhua shen, Taboa shen and Kuhua shen (the shen triplets) designed the first oceanian architecture.
 An important Oceanic archaeological site is Nan Madol from the Federated States of Micronesia. Nan Madol was the ceremonial and political seat of the Saudeleur Dynasty, which united Pohnpei's estimated 25,000 people until about 1628.[111] Set apart between the main island of Pohnpei and Temwen Island, it was a scene of human activity as early as the first or second century AD. By the 8th or 9th century, islet construction had started, with construction of the distinctive megalithic architecture beginning 1180–1200 AD.[112]
 Due to the extent of the Islamic conquests, Islamic architecture encompasses a wide range of architectural styles from the foundation of Islam (7th century) to the present day. Early Islamic architecture was influenced by Roman, Byzantine, Persian, Mesopotamian architecture and all other lands which the Early Muslim conquests conquered in the 7th and 8th centuries.[118][119] Further east, it was also influenced by Chinese and Indian architecture as Islam spread to Southeast Asia. This wide and long history has given rise to many local architectural styles, including but not limited to: Umayyad, Abbasid, Persian, Moorish, Fatimid, Mamluk, Ottoman, Indo-Islamic (particularly Mughal), Sino-Islamic and Sahelian architecture.
 Some distinctive structures in Islamic architecture are mosques, madrasas, tombs, palaces, baths, and forts. Notable types of Islamic religious architecture include hypostyle mosques, domed mosques and mausoleums, structures with vaulted iwans, and madrasas built around central courtyards. In secular architecture, major examples of preserved historic palaces include the Alhambra and the Topkapi Palace. Islam does not encourage the worship of idols; therefore the architecture tends to be decorated with Arabic calligraphy (including Qur'anic verses or other poetry) and with more abstract motifs such as geometric patterns, muqarnas, and arabesques, as opposed to illustrations of scenes and stories.[120][121][122][123]
 Surviving examples of medieval secular architecture mainly served for defense across various parts of Europe. Castles and fortified walls provide the most notable remaining non-religious examples of medieval architecture. New types of civic, military, as well as religious buildings of new styles begin to pop up in this region during this period.
 Byzantine architects built city walls, palaces, hippodromes, bridges, aqueducts, and churches. They built many types of churches, including the basilica (the most widespread type, and the one that reached the greatest development). After the early period, the most common layout was the cross-in-square with five domes, also found in Moscow, Novgorod or Kiev, as well as in Romania, Bulgaria, Serbia, North Macedonia and Albania. Through modifications and adaptations of local inspiration, the Byzantine style will be used as the main source of inspiration for architectural styles in all Eastern Orthodox countries.[129] For example, in Romania, the Brâncovenesc style is highly based on Byzantine architecture, but also has individual Romanian characteristics.
 Just as the Parthenon is the most famous building of Ancient Greek architecture, Hagia Sophia remains the iconic church of Orthodox Christianity. In Greek and Roman temples, the exterior was the most important part of the temple, where sacrifices were made; the interior, where the cult statue of the deity to whom the temple was built was kept, often had limited access by the general public. But Christian liturgies are held in the interior of the churches, Byzantine exteriors usually have little if any ornamentation.[130]
 Byzantine architecture often featured marble columns, coffered ceilings and sumptuous decoration, including the extensive use of mosaics with golden backgrounds.[131] The building material used by Byzantine architects was no longer marble, which was very appreciated by the Ancient Greeks. They used mostly stone and brick, and also thin alabaster sheets for windows.[132] Mosaics were used to cover brick walls, and any other surface where fresco would not resist. Good examples of mosaics from the proto-Byzantine era are in Hagios Demetrios in Thessaloniki (Greece), the Basilica of Sant'Apollinare Nuovo and the Basilica of San Vitale, both in Ravenna (Italy), and Hagia Sophia in Istanbul.
 From the very beginning of the formation of feudal relations, the architecture and urban planning of Armenia entered a new stage. The ancient Armenian cities experienced economic decline; only Artashat and Tigranakert retained their importance. The importance of the cities of Dvin and Karin (Erzurum) increased. The construction of the city of Arshakavan by the king of Great Armenia Arshak II was not completely completed. Christianity brought to life a new architecture of religious buildings, which was initially nourished by the traditions of the old, ancient architecture.
 Churches of the 4th-5th centuries are mainly basilicas (Kasakh, 4th-5th centuries, Ashtarak, 5th century, Akhts, 4th century, Yeghvard, 5th century). Some basilicas of Armenian architecture belong to the so-called “Western type” of basilica churches. Of these, the most famous are the churches of Tekor (5th century), Yererouk (IV-V centuries), Dvin (470), Tsitsernavank (IV-5 centuries). The three-nave Yereruyk basilica stands on a 6-step stylobate, presumably built on the site of an earlier pre-Christian temple. The basilicas of Karnut (5th century), Yeghvard (5th century), Garni (IV century), Zovuni (5th century), Tsaghkavank (VI century), Dvina (553–557), Tallinn (5th century) have also been preserved c.), Tanaat (491), Jarjaris (IV-V centuries), Lernakert (IV-V centuries), etc.[136]
 The term 'Romanesque' is rooted in the 19th century, when it was coined to describe medieval churches built from the 10th to 12th century, before the rise of steeply pointed arches, flying buttresses and other Gothic elements. This style of architecture emerged nearly simultaneously in multiple countries (France, Germany, Italy, Spain).[142] For 19th-century critics, the Romanesque reflected the architecture of stonemasons who evidently admired the heavy barrel vaults and intricate carved capitals of the ancient Romans, but whose own architecture was considered derivative and degenerate, lacking the sophistication of their classical models.
 Scholars in the 21st century are less inclined to understand the architecture of this period as a 'failure' to reproduce the achievements of the past, and are far more likely to recognise its profusion of experimental forms, as a series of creative new inventions. At the time, however, research has questioned the value of Romanesque as a stylistic term. On the surface, it provides a convenient designation for buildings that share a common vocabulary of rounded arches and thick stone masonry, and appear in between the Carolingian revival of classical antiquity in the 9th century and the swift evolution of Gothic architecture after the second half of the 12th century. One problem, however, is that the term encompasses a broad array of regional variations, some with closer links to Rome than others. It should also be noted that the distinction between Romanesque architecture and its immediate predecessors and followers is not at all clear. There is little evidence that medieval viewers were concerned with the stylistic distinctions that we observe today, making the slow evolution of medieval architecture difficult to separate into neat chronological categories. Nevertheless, Romanesque remains a useful word despite its limitations, because it reflects a period of intensive building activity that maintained some continuity with the classical past, but freely reinterpreted ancient forms in a new distinctive manner.[21]
 Romanesque cathedrals can be easily differentiated from Gothic and Byzantine ones, since they are characterized by the wide use of thick piers and columns, round arches and severity. Here, the possibilities of the round-arch arcade in both a structural and a spatial sense were once again exploited to the full. Unlike the sharp pointed arch of the later Gothic, the Romanesque round arch required the support of massive piers and columns. In comparison to Byzantine churches, Romanesque ones tend to lack complex ornamentation both on the exterior and interior. An example of this is the Périgueux Cathedral (Périgueux, France), built in the early 12th century and designed on the model of St. Mark's Basilica in Venice, but lacking mosaics, leaving its interior very austere and minimalistic.[143]
 Gothic architecture began with a series of experiments, which were conducted to fulfil specific requests by patrons and to accommodate the ever-growing number of pilgrims visiting sites that housed precious relics. Pilgrims in the high Middle Ages (circa 1000 to 1250 AD) increasingly travelled to well-known pilgrimage sites, but also to local sites where local and national saints were reputed to have performed miracles. The churches and monasteries housing important relics therefore wanted to heighten the popularity of their respective saints and build appropriate shrines for them. These shrines were not merely gem-encrusted reliquaries, but more importantly took the form of powerful architectural settings characterised by coloured light emitting from the large areas of stained glass. The use of stained glass, however, is not the only defining element of Gothic architecture and neither are the pointed arch, the ribbed vault, the rose window or the flying buttress, as many of these elements were used in one way or another in preceding architectural traditions. It was rather the combination and constant refinement of these elements, along with the quick response to the rapidly changing building techniques of the time, that fuelled the Gothic movement in architecture.
 Consequently, it is difficult to point to one element or the exact place where Gothic first emerged; however, it is traditional to initiate a discussion of Gothic architecture with the Basilica of St Denis (circa 1135–1344) and its patrons, Abbot Suger, who began to rebuild the west front and the choir of the church. As he wrote in his De Administratione, the old building could no longer accommodate the large volumes of pilgrims who were coming to venerate the relics of St Denis, and the solution for this twofold: a west façade with three large portals and the innovative new choir, which combined an ambulatory with radiating chapels that were unique as they were not separated by walls. Instead a row of slim columns was inserted between the chapels and the choir arcade to support the rib vaults. The result enabled visitors to circulate around the altar and come within reach of the relics without actually disrupting the altar space, while also experiencing the large stained-glass windows within the chapels. As confirmed by Suger, the desire for more stained-glass was not necessarily to bring more daylight into the building but rather to fill the space with a continuous ray of colorful light, rather like mosaics or precious stones, which would make the wall vanish. The demand for ever more stained-glass windows and the search for techniques that would support them are constant throughout the development of Gothic architecture, as is evident in the writings of Suger, who was fascinated by the mystical quality of such lighting.[21]
 Brick Gothic was a specific style of Gothic architecture common in Northeast and Central Europe especially in the regions in and around the Baltic Sea, which do not have resources of standing rock. The buildings are essentially built using bricks.
 During the Renaissance, Italy consisted of many states, and intense rivalry between them generated an increase in technical and artistic developments. The Medici Family, an Italian banking family and political dynasty, is famous for its financial support of Renaissance art and architecture.
 The period began in around 1452, when the architect and humanist Leon Battista Alberti (1404–1472) completed his treatise De Re Aedificatoria (On the Art of Building) after studying the ancient ruins of Rome and Vitruvius's De Architectura. His writings covered numerous subjects, including history, town planning, engineering, sacred geometry, humanism and philosophies of beauty, and set out the key elements of architecture and its ideal proportions. In the last decades of the 15th century, artists and architects began to visit Rome to study the ruins, especially the Colosseum and the Pantheon. They left behind precious records of their studies in the form of drawings. While humanist interest in Rome had been building up over more than a century (dating back at least to Petrarch in the 14th century), antiquarian considerations of monuments had focused on literary, epigraphic and historical information rather than on the physical remains. Although some artists and architects, such as Filippo Brunelleschi (1377–1446), Donatello (circa 1386–1466) and Leon Battista Alberti, are reported to have made studies of Roman sculpture and ruins, almost no direct evidence of this work survives. By the 1480s, prominent architects, such as Francesco di Giorgio (1439–1502) and Giuliano da Sangallo (circa 1445–1516), were making numerous studies of ancient monuments, undertaken in ways that demonstrated that the process of transforming the model into a new design had already begun. In many cases, drawing ruins in their fragmentary state necessitated a leap of imagination, as Francesco himself readily admitted in his annotation to his reconstruction of the Campidoglio, noting 'largely imagined by me, since very little can be understood from the ruins.[156]
 Soon, grand buildings were constructed in Florence using the new style, like the Pazzi Chapel (1441–1478) or the Palazzo Pitti (1458–1464). The Renaissance begun in Italy, but slowly spread to other parts of Europe, with varying interpretations.[149]
 Since Renaissance art is an attempt of reviving Ancient Rome's culture, it uses pretty much the same ornaments as the Ancient Greek and Roman. However, because most if not all resources that Renaissance artists had were Roman, Renaissance architecture and applied arts widely use certain motifs and ornaments that are specific to Ancient Rome. The most iconic one is the margent, a vertical arrangement of flowers, leaves or hanging vines, used at pilasters. Another ornament associated with the Renaissance is the round medallion, containing a profile of a person, similar with Ancient cameos. Renaissance, Baroque, Rococo, and other post-medieval styles use putti (chubby baby angels) much more often compared to Greco-Roman art and architecture. An ornament reintroduced during the Renaissance, that was of Ancient Roman descent, that will also be used in later styles, is the cartouche, an oval or oblong design with a slightly convex surface, typically edged with ornamental scrollwork.
 The Baroque emerged from the Counter Reformation as an attempt by the Catholic Church in Rome to convey its power and to emphasize the magnificence of God. The Baroque and its late variant the Rococo were the first truly global styles in the arts. Dominating more than two centuries of art and architecture in Europe, Latin America and beyond from circa 1580 to circa 1800. Born in the painting studios of Bologna and Rome in the 1580s and 1590s, and in Roman sculptural and architectural ateliers in the second and third decades of the 17th century, the Baroque spread swiftly throughout Italy, Spain and Portugal, Flanders, France, the Netherlands, England, Scandinavia, and Russia, as well as to central and eastern European centres from Munich (Germany) to Vilnius (Lithuania). The Portuguese, Spanish and French empires and the Dutch treading network had a leading role in spreading the two styles into the Americas and colonial Africa and Asia, to places such as Lima, Mozambique, Goa and the Philippines.[166] Due to its spread in regions with different architectural traditions, multiple kinds of Baroque appeared based on location, different in some aspects, but similar overall. For example, French Baroque appeared severe and detached by comparison, preempting Neoclassicism and the architecture of the Age of Enlightenment.[157] Hybrid Native American/European Baroque architecture first appeared in South America (as opposed to Mexico) in the late 17th century, after the indigenous symbols and styles that characterize this unusual variant of Baroque had been kept alive over the preceding century in other media, a very good example of this being the Jesuit Church in Arequipa (Peru).[167]
 The first Baroque buildings were cathedrals, churches and monasteries, soon joined by civic buildings, mansions, and palaces. Being characterized by dynamism, for the first time walls, façades and interiors curved,[168] a good example being San Carlo alle Quattro Fontane in Rome. Baroque architects took the basic elements of Renaissance architecture, including domes and colonnades, and made them higher, grander, more decorated, and more dramatic. The interior effects were often achieved with the use of quadratura, or trompe-l'œil painting combined with sculpture: the eye is drawn upward, giving the illusion that one is looking into the heavens. Clusters of sculpted angels and painted figures crowd the ceiling. Light was also used for dramatic effect; it streamed down from cupolas and was reflected from an abundance of gilding. Solomonic columns were often used, to give an illusion of upwards motion and other decorative elements occupied every available space. In Baroque palaces, grand stairways became a central element.[169] Besides architecture, Baroque painting and sculpture are characterized by dynamism too. This is in contrast with how static and peaceful Renaissance art is.
 Besides the building itself, the space where it was placed had a role too. Both Baroque and Rococo buildings try to seize viewers' attention and to dominate their surroundings, whether on a small scale such as the San Carlo alle Quattro Fontane in Rome, or on a massive one, like the new facade of the Santiago de Compostela Cathedral, designed to tower over the city. A manifestation of power and authority on the grandest scale, Baroque urban planning and renewal was promoted by the church and the state alike. It was the first era since antiquity to experience mass migration into cities, and urban planners took idealistic measures to regulate them. The most notable early example was Domenico Fontana's restructuring of Rome's street plan of Pope Sixtus V. Architects had experimented with idealized city schemes since the early Renaissance, examples being Leon Battista Alberti (1404–1472) planning a centralized model city, with streets leading to a central piazza, or Filarete (Antonio di Pietro Aver(u)lino, c. 1400-c. 1469) designing a round city named Sforzinda (1451–1456) that he based on parts of the human body in the idea that a healthy city should reflect the physiognomy of its inhabitants. However, none of these idealistic cities has ever been built. In fact, few such projects were put into practice in Europe as new cities were prohibitively costly and existing urban areas, with existing churches and palaces, could not be demolished. Only in the Americas, where architects often had a clean space to work with, were such cities possible, as in Lima (Peru) or Buenos Aires (Argentina). The earliest Baroque ideal city is Zamość, built north-east of Kraków (Poland) by the Italian architect Bernardo Morando (c. 1540-1600), being a centralized town focusing on a square with radiating streets. Where entire cities could not be rebuilt, patrons and architects compensated by creating spacious and symmetrical squares, often with avenues and radiating out at perpendicular angles and focusing on a fountain, statue or obelisk. A good example of this is the Place des Vosges (formerly Place Royale), commissioned by Henry IV probably after plans by Baptiste du Cerceau (1545–1590). The most famous Baroque space in the world is Gianlorenzo Bernini's St. Peter's Square in Rome.[170] Similar with ideal urban planning, Baroque gardens are characterized by straight and readapting avenues, with geometric spaces.
 The name Rococo derives from the French word rocaille, which describes shell-covered rock-work, and coquille, meaning seashell. Rococo architecture is fancy and fluid, accentuating asymmetry, with an abundant use of curves, scrolls, gilding and ornaments. The style enjoyed great popularity with the ruling elite of Europe during the first half of the 18th century. It developed in France out of a new fashion in interior decoration, and spread across Europe.[175] Domestic Rococo abandoned Baroque's high moral tone, its weighty allegories and its obsession with legitimacy: in fact, its abstract forms and carefree, pastoral subjects related more to notions of refuge and joy that created a more forgiving atmosphere for polite conversations. Rococo rooms are typically smaller than their Baroque counterparts, reflecting a movement towards domestic intimacy. Even the grander salons used for entertaining were more modest in scale, as social events involved smaller numbers of guests.
 Characteristic of the style were Rocaille motifs derived from the shells, icicles and rock-work or grotto decoration. Rocaille arabesques were mostly abstract forms, laid out symmetrically over and around architectural frames. A favourite motif was the scallop shell, whose top scrolls echoed the basic S and C framework scrolls of the arabesques and whose sinuous ridges echoed the general curvilinearity of the room decoration. While few Rococo exteriors were built in France, a number of Rococo churches are found in southern Germany.[176] Other widely-user motifs in decorative arts and interior architecture include: acanthus and other leaves, birds, bouquets of flowers, fruits, elements associated with love (putti, quivers with arrows ans arrowed hearts) trophies of arms, putti, medallions with faces, many many flowers, and Far Eastern elements (pagodes, dragons, monkeys, bizarre flowers, bamboo, and Chinese people).[177] Pastel colours were widely used, like light blue, mint green or pink. Rococo designers also loved mirrors (the more the better), an example being the Hall of Mirrors of the Amalienburg (Munich, Germany), by Johann Baptist Zimmermann. Generally, mirrors are also featured above fireplaces.
 The interactions between East and West brought on by colonialist exploration have had an impact on aesthetics. Because of being something rare and new to Westerners, some non-European styles were really appreciated during the 17th, 18th and 19th centuries. Some nobles and kings built little structures inspired by these styles in the gardens of their palaces, or fully decorated a handful of rooms of palaces like this. Because of not fully understanding the origins and principles that govern these exotic aesthetics, Europeans sometimes created hybrids of the style which they tried to replicate and which were the trends at that time. A good example of this is chinoiserie, a Western decorative style, popular during the 18th century, that was heavily inspired by Chinese arts, but also by Rococo at the same time. Because traveling to China or other Far Eastern countries was something hard at that time and so remained mysterious to most Westerners, European imagination were fuelled by perceptions of Asia as a place of wealth and luxury, and consequently patrons from emperors to merchants vied with each other in adorning their living quarters with Asian goods and decorating them in Asian styles. Where Asian objects were hard to obtain, European craftsmen and painters stepped up to fill the demand, creating a blend of Rococo forms and Asian figures, motifs and techniques.
 Chinese art was not the only foreign style with which Europeans experimented. Another was the Islamic one. Examples of this include the Garden Mosque of the Schwetzingen Palace in Germany (the only surviving example of an 18th-century European garden mosque), the Royal Pavilion in Brighton, or the Moorish Revival buildings from the 19th and early 20th centuries, with horseshoe arches and brick patterns. When it come to the Orient, Europeans also had an interest for the culture of Ancient Egypt. Compared to other cases of exoticism, the one with the land of pharaohs is the oldest one, since Ancient Greeks and Romans had this interest during Antiquity. The main periods when Egyptian Revival monuments were erected were the early 19th century, with Napoleon's military campaigns in Egypt, and the 1920s, when the Tomb of Tutankhamun was discovered in 1922, which caused an Egyptomania that lead to Art Deco sometimes using motifs inspired by Ancient Egypt. During the late 18th and early 19th century, Neoclassicism sometimes mixed Greco-Roman elements with Egyptian ones. Because of its association with pharaohs, death and eternity, multiple Egyptian Revival tombs or cemetery entry gates were built in this style. Besides mortuary structures, other buildings in this style include certain synagogues, like the Karlsruhe Synagogue or some Empire monuments built during the reign of Nepoleon, such as the Egyptian portico of the Hôtel Beauharnais or the Fontaine du Fellah. During the 1920s and 1930s, Pre-Columbian Mesoamerican architecture was of great interest for some American architects, particularly what the Mayans built. Several of Frank Lloyd Wright's California houses were erected in a Mayan Revival style, while other architects combined Mayan motifs with Art Deco ones.[186]
 Neoclassical architecture focused on Ancient Greek and Roman details, plain, white walls and grandeur of scale. Compared to the previous styles, Baroque and Rococo, Neoclassical exteriors tended to be more minimalist, featuring straight and angular lines, but being still ornamented. The style's clean lines and sense of balance and proportion worked well for grand buildings (such as the Panthéon in Paris) and for smaller structures alike (such as the Petit Trianon).
 Excavations during the 18th century at Pompeii and Herculaneum, which had both been buried under volcanic ash during the 79 AD eruption of Mount Vesuvius, inspired a return to order and rationality, largely thanks to the writings of Johann Joachim Winckelmann.[197][198] In the mid-18th century, antiquity was upheld as a standard for architecture as never before. Neoclassicism was a fundamental investigation of the very bases of architectural form and meaning. In the 1750s, an alliance between archaeological exploration and architectural theory started, which will continue in the 19th century. Marc-Antoine Laugier wrote in 1753 that 'Architecture owes all that is perfect to the Greeks'.[199]
 The style was adopted by progressive circles in other countries such as Sweden and Russia. Federal-style architecture is the name for the classicizing architecture built in North America between c. 1780 and 1830, and particularly from 1785 to 1815. This style shares its name with its era, the Federal Period. The term is also used in association with furniture design in the United States of the same time period. The style broadly corresponds to the middle-class classicism of Biedermeier style in the German-speaking lands, Regency style in Britain and to the French Empire style. In Central and Eastern Europe, the style is usually referred to as Classicism (German: Klassizismus, Russian: Классицизм), while the newer Revival styles of the 19th century until today are called neoclassical.
 Étienne-Louis Boullée (1728–1799) was a visionary architect of the period. His utopian projects, never built, included a monument to Isaac Newton (1784) in the form of an immense dome, with an oculus allowing the light to enter, giving the impression of a sky full of stars. His project for an enlargement of the Royal Library (1785) was even more dramatic, with a gigantic arch sheltering the collection of books. While none of his projects were ever built, the images were widely published and inspired architects of the period to look outside the traditional forms.[200]
 Similarly with the Renaissance and Baroque periods, during the Neoclassical one urban theories of how a good city should be appeared too. Enlightenment writers of the 18th century decried the problems of Paris at that time, the biggest one being the big number of narrow medieval streets crowded with modest houses. Voltaire openly criticized the failure of the French Royal administration to initiate public works, improve the quality of life in towns, and stimulate the economy. 'It is time for those who rule the most opulent capital in Europe to make it the most comfortable and the most magnificent of cities. There must be public markets, fountains which actually provide water and regular pavements. The narrow and infected streets must be widened, monuments that cannot be seen must be revealed and new ones built for all to see', Voltaire insisted in a polemical essay on 'The Embellishments of Paris' in 1749. In the same year, Étienne La Font de Saint-Yenne, criticized how Louis XIV's great east façade of the Louvre, was all but hidden from views by a dense quarter of modest houses. Voltaire also said that in order to transform Paris into a city that could rival ancient Rome, it was necessary to demolish more than it was to built. 'Our towns are still what they were, a mass of houses crowded together haphazardly without system, planning or design', Marc-Antoine Laugier complained in 1753. Writing a decade later, Pierre Patte promoted an urban reform in quest of health, social order, and security, launching at the same time a medical and organic metaphor which compared the operations of urban design to those of the surgeons. With bad air and lack of fresh water its current state was pathological, Patte asserted, calling for fountains to be placed at principal intersections and markets. Squares are recommended promote the circulation of air, and for the same reason houses on the city's bridges should be demolished. He also criticized the location of hospitals next to markets and protested continued burials in overcrowded city churchyards.[201] Besides cities, new ideas of how a garden should be appeared in 18th century England, making place for the English landscape garden (aka jardin à l'anglaise), characterized by an idealized view of nature, and the use of Greco-Roman or Gothic ruins, bridges, and other picturesque architecture, designed to recreate an idyllic pastoral landscape. It was the opposite of the symmetrical and geometrically planned Baroque garden (aka jardin à la française).
 The 19th century was dominated by a wide variety of stylistic revivals, variations, and interpretations. Revivalism in architecture is the use of visual styles that consciously echo the style of a previous architectural era. Modern-day Revival styles can be summarized within New Classical architecture, and sometimes under the umbrella term traditional architecture.
 The idea that architecture might represent the glory of kingdoms can be traced to the dawn of civilisation, but the notion that architecture can bear the stamp of national character is a modern idea, that appeared in the 18th century historical thinking and given political currency in the wake of the French Revolution. As the map of Europe was repeatedly changing, architecture was used to grant the aura of a glorious past to even the most recent nations. In addition to the credo of universal Classicism, two new, and often contradictory, attitudes on historical styles existed in the early 19th century. Pluralism promoted the simultaneous use of the expanded range of style, while Revivalism held that a single historical model was appropriate for modern architecture. Associations between styles and building types appeared, for example: Egyptian for prisons, Gothic for churches, or Renaissance Revival for banks and exchanges. These choices were the result of other associations: the pharaohs with death and eternity, the Middle Ages with Christianity, or the Medici family with the rise of banking and modern commerce.
 Whether their choice was Classical, medieval, or Renaissance, all revivalists shared the strategy of advocating a particular style based on national history, one of the great enterprises of historians in the early 19th century. Only one historic period was claimed to be the only one capable of providing models grounded in national traditions, institutions, or values. Issues of style became matters of state.[204]
 The most well-known Revivalist style is the Gothic Revival one, that appeared in the mid-18th century in the houses of a number of wealthy antiquarians in England, a notable example being the Strawberry Hill House. German Romantic writers and architects were the first to promote Gothic as a powerful expression of national character, and in turn use it as a symbol of national identity in territories still divided. Johann Gottfried Herder posed the question 'Why should we always imitate foreigners, as if we were Greeks or Romans?'.[205]
 In art and architecture history, the term Orientalism refers to the works of the Western artists who specialized in Oriental subjects, produced from their travels in Western Asia, during the 19th century. In that time, artists and scholars were described as Orientalists, especially in France.
 In India, during the British Raj, a new style, Indo-Saracenic, (also known as Indo-Gothic, Mughal-Gothic, Neo-Mughal, or Hindoo style) was getting developed, which incorporated varying degrees of Indian elements into the Western European style. The Churches and convents of Goa are another example of the blending of traditional Indian styles with western European architectural styles. Most Indo-Saracenic public buildings were constructed between 1858 and 1947, with the peaking at 1880.[206] The style has been described as ""part of a 19th-century movement to project themselves as the natural successors of the Mughals"".[207] They were often built for modern functions such as transport stations, government offices, and law courts. It is much more evident in British power centres in the subcontinent like Mumbai, Chennai, and Kolkata.[208]
 The Beaux-Arts style takes its name from the École des Beaux-Arts in Paris, where it developed and where many of the main exponents of the style studied. Due to the fact that international students studied here, there are buildings from the second half of the 19th century and the early 20th century of this type all over the world, designed by architects like Charles Girault, Thomas Hastings, Ion D. Berindey or Petre Antonescu. Today, from Bucharest to Buenos Aires and from San Francisco to Brussels, the Beaux-Arts style survives in opera houses, civic structures, university campuses commemorative monuments, luxury hotels and townhouses. The style was heavily influenced by the Paris Opéra House (1860–1875), designed by Charles Garnier, the masterpiece of the 19th century renovation of Paris, dominating its entire neighbourhood and continuing to astonish visitors with its majestic staircase and reception halls. The Opéra was an aesthetic and societal turning point in French architecture. Here, Garnier showed what he called a style actuel, which was influenced by the spirit of the time, aka Zeitgeist, and reflected the designer's personal taste.
 Beaux-Arts façades were usually imbricated, or layered with overlapping classical elements or sculpture. Often façades consisted of a high rusticated basement level, after it a few floors high level, usually decorated with pilasters or columns, and at the top an attic level and/or the roof. Beaux-Arts architects were often commissioned to design monumental civic buildings symbolic of the self-confidence of the town or city. The style aimed for a Baroque opulence through lavishly decorated monumental structures that evoked Louis XIV's Versailles. However, it was not just a revival of the Baroque, being more of a synthesis of Classicist styles, like Renaissance, Baroque, Rococo, Neoclassicism etc.[216][217][218]
 Because of the Industrial Revolution and the new technologies it brought, new types of buildings have appeared. By 1850 iron was quite present in dailylife at every scale, from mass-produced decorative architectural details and objects of apartment buildings and commercial buildings to train sheds. A well-known 19th century glass and iron building is the Crystal Palace from Hyde Park (London), built in 1851 to house the Great Exhibition, having an appearance similar with a greenhouse. Its scale was daunting.
 The marketplace pioneered novel uses of iron and glass to create an architecture of display and consumption that made the temporary display of the world fairs a permanent feature of modern urban life. Just after a year after the Crystal Palace was dismantaled, Aristide Boucicaut opened what historians of mass consumption have labelled the first department store, Le Bon Marché in Paris. As the store expanded, its exterior took on the form of a public monument, being highly decorated with French Renaissance Revival motifs. The entrances advanced subtly onto the pavemenet, hoping to captivate the attention of potential customers. Between 1872 and 1874, the interior was remodelled by Louis-Charles Boileau, in collaboration with the young engineering firm of Gustave Eiffel. In place of the open courtyard required to permit more daylight into the interior, the new building focused around three skylight atria.[224]
 Popular in many countries from the early 1890s until the outbreak of World War I in 1914, Art Nouveau was an influential although relatively brief art and design movement and philosophy. Despite being a short-lived fashion, it paved the way for the modern architecture of the 20th century. Between c. 1870 and 1900, a crisis of historicism occurred, during which the historicist culture was critiqued, one of the voices being Friedrich Nietzsche in 1874, who diagnosed 'a malignant historical fervour' as one of the crippling symptoms of a modern culture burdened by archaeological study and faith in the laws of historical progression.
 Focusing on natural forms, asymmetry, sinuous lines and whiplash curves, architects and designers aimed to escape the excessively ornamental styles and historical replications, popular during the 19th century. However, the style was not completely new, since Art Nouveau artists drew on a huge range of influences, particularly Beaux-Arts architecture, the Arts and Crafts movement, aestheticism and Japanese art. Buildings used materials associated in the 19th century with modernity, such as cast-iron and glass. A good example of this is the Paris Metro entrance at Porte Dauphine by Hector Guimard (1900). Its cast-iron and glass canopy is as much sculpture as it is architecture. In Paris, Art Nouveau was even called Le Style Métro by some. The interest for stylized organic forms of ornamentation originated in the mid 19th century, when it was promoted in The Grammar of Ornament (1854), a pattern book by British architect Owen Jones (architect) (1809–1874).
 Whiplash curves and sinuous organic lines are its most familiar hallmarks, however the style can not be summarized only to them, since its forms are much more varied and complex. The movement displayed many national interpretations. Depending on where it manifested, it was inspired by Celtic art, Gothic Revival, Rococo Revival, and Baroque Revival. In Hungary, Romania and Poland, for example, Art Nouveau incorporated folkloric elements. This is true especially in Romania, because it facilitated the appearance of the Romanian Revival style, which draws inspiration from Brâncovenesc architecture and traditional peasant houses and objects. The style also had different names, depending on countries. In Britain it was known as Modern Style, in the Netherlands as Nieuwe Kunst, in Germany and Austria as Jugendstil, in Italy as Liberty style, in Romania as Arta 1900, and in Japan as Shiro-Uma. It would be wrong to credit any particular place as the only one where the movement appeared, since it seems to have arisen in multiple locations.[233][234][235][236]
 Rejecting ornament and embracing minimalism and modern materials, Modernist architecture appeared across the world in the early 20th century. Art Nouveau paved the way for it, promoting the idea of non-historicist styles. It developed initially in Europe, focusing on functionalism and the avoidance of decoration. Modernism reached its peak during the 1930s and 1940s with the Bauhaus and the International Style, both characterised by asymmetry, flat roofs, large ribbon windows, metal, glass, white rendering and open-plan interiors.[240]
 Art Deco, named retrospectively after an exhibition held in Paris in 1925, originated in France as a luxurious, highly decorated style. It then spread quickly throughout the world - most dramatically in the United States - becoming more streamlined and modernistic through the 1930s. The style was pervasive and popular, finding its way into the design of everything from jewellery to film sets, from the interiors of ordinary homes to cinemas, luxury streamliners and hotels. Its exuberance and fantasy captured the spirit of the 'roaring 20s' and provided an escape from the realities of the Great Depression during the 1930s.[246]
 Although it ended with the start of World War II, its appeal has endured. Despite that it is an example of modern architecture, elements of the style drew on ancient Egyptian, Greek, Roman, African, Aztec and Japanese influences, but also on Futurism, Cubism and the Bauhaus. Bold colours were often applied on low-reliefs. Predominant materials include chrome plating, brass, polished steel and aluminium, inlaid wood, stone and stained glass.
 The International Style emerged in Europe after World War I, influenced by recent movements, including De Stijl and Streamline Moderne, and had a close relationship to the Bauhaus. The antithesis of nearly every other architectural movement that preceded it, the International Style eliminated extraneous ornament and used modern industrial materials such as steel, glass, reinforced concrete and chrome plating. Rectilinear, flat-roofed, asymmetrical and white, it became a symbol of modernity across the world. It seemed to offer a crisp, clean, rational future after the horrors of war. Named by the architect Philip Johnson and historian Henry-Russell Hitchcock (1903–1987) in 1932, the movement was epitomized by Charles-Edouard Jeanneret, or Le Corbusier and was clearly expressed in his statement that 'a house is a machine for living in'.[251]
 Based on social equality, Brutalism was inspired by Le Corbusier's 1947-1952 Unité d'habitation in Marseilles. It seems the term was originally coined by Swedish architect Hans Asplund (1921–1994), but Le Corbusier's use of the description béton brut, meaning raw concrete, for his choice of material for the Unité d'habitation was particularly influential. The style flourished from the 1950s to the mid-1970s, mainly using concrete, which although new in itself, was unconventional when exposed on facades. Before Brutalism, concrete was usually hidden beneath other materials.[257]
 Not one definable style, Postmodernism is an eclectic mix of approaches that appeared in the late 20th century in reaction against Modernism, which was increasingly perceived as monotonous and conservative. As with many movements, a complete antithesis to Modernism developed. In 1966, the architect Robert Venturi (1925–2018) had published his book, Complexity and Contradiction in Architecture, which praised the originality and creativity of Mannerist and Baroque architecture of Rome, and encouraged more ambiguity and complexity in contemporary design. Complaining about the austerity and tedium of so many smooth steel and glass Modernist buildings, and in deliberate denunciation of the famous Modernist 'Less is more', Venturi stated 'Less is a bore'. His theories became a majore influence on the development of Postmodernism.[258]
 Deconstructivism in architecture is a development of postmodern architecture that began in the late 1980s. It is characterized by ideas of fragmentation, non-linear processes of design, an interest in manipulating ideas of a structure's surface or skin, and apparent non-Euclidean geometry,[270] (i.e., non-rectilinear shapes) which serve to distort and dislocate some of the elements of architecture, such as structure and envelope. The finished visual appearance of buildings that exhibit the many deconstructivist ""styles"" is characterised by a stimulating unpredictability and a controlled chaos.
 Important events in the history of the Deconstructivist movement include the 1982 Parc de la Villette architectural design competition (especially the entry from the French philosopher Jacques Derrida and the American architect Peter Eisenman[271] and Bernard Tschumi's winning entry), the Museum of Modern Art's 1988 Deconstructivist Architecture exhibition in New York, organized by Philip Johnson and Mark Wigley, and the 1989 opening of the Wexner Center for the Arts in Columbus, designed by Peter Eisenman. The New York exhibition featured works by Frank Gehry, Daniel Libeskind, Rem Koolhaas, Peter Eisenman, Zaha Hadid, Coop Himmelblau, and Bernard Tschumi. Since the exhibition, many of the architects who were associated with Deconstructivism have distanced themselves from the term. Nonetheless, the term has stuck and has now, in fact, come to embrace a general trend within contemporary architecture.
 Dependencies and other territories
 Dependencies and other territories
 Territories
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Earth and heaven', 'Lavishly sculpted with hundreds of figures', 'local architectural styles', 'Futurism, Cubism and the Bauhaus', 'precision and technological advancement'], 'answer_start': [], 'answer_end': []}"
"
 Astrobiology is a scientific field within the life and environmental sciences that studies the origins, early evolution, distribution, and future of life in the universe by investigating its deterministic conditions and contingent events.[2] As a discipline, astrobiology is founded on the premise that life may exist beyond Earth.[3]
 Research in astrobiology comprises three main areas: the study of habitable environments in the Solar System and beyond, the search for planetary biosignatures of past or present extraterrestrial life, and the study of the origin and early evolution of life on Earth.
 The field of astrobiology has its origins in the 20th century with the advent of space exploration and the discovery of exoplanets. Early astrobiology research focused on the search for extraterrestrial life and the study of the potential for life to exist on other planets.[2] In the 1960s and 1970s, NASA began its astrobiology pursuits within  the Viking program, which was the first US mission to land on Mars and search for signs of life.[4] This mission, along with other early space exploration missions, laid the foundation for the development of astrobiology as a discipline.
 Regarding habitable environments, astrobiology investigates potential locations beyond Earth that could support life, such as Mars, Europa, and exoplanets, through research into the extremophiles populating austere environments on Earth, like volcanic and deep sea environments. Research within this topic is conducted utilising the methodology of the geosciences, especially geobiology, for astrobiological applications.
 The search for biosignatures involves the identification of signs of past or present life in the form of organic compounds, isotopic ratios, or microbial fossils. Research within this topic is conducted utilising the methodology of planetary and environmental science, especially atmospheric science, for astrobiological applications, and is often conducted through remote sensing and in situ missions.
 Astrobiology also concerns the study of the origin and early evolution of life on Earth to try to understand the conditions that are necessary for life to form on other planets.[5] This research seeks to understand how life emerged from non-living matter and how it evolved to become the diverse array of organisms we see today. Research within this topic is conducted utilising the methodology of paleosciences, especially paleobiology, for astrobiological applications.
 Astrobiology is a rapidly developing field with a strong interdisciplinary aspect that holds many challenges and opportunities for scientists. Astrobiology programs and research centres are present in many universities and research institutions around the world, and space agencies like NASA and ESA have dedicated departments and programs for astrobiology research.
 The term astrobiology was first proposed by the Russian astronomer Gavriil Tikhov in 1953.[6] It is etymologically derived from the Greek ἄστρον, ""star""; βίος, ""life""; and -λογία, -logia, ""study"". A close synonym is exobiology from the Greek Έξω, ""external""; βίος, ""life""; and -λογία, -logia, ""study"", coined by American molecular biologist Joshua Lederberg; exobiology is considered to have a narrow scope limited to search of life external to Earth.[7] Another associated term is xenobiology, from the Greek ξένος, ""foreign""; βίος, ""life""; and -λογία, ""study"", coined by American science fiction writer Robert Heinlein in his work The Star Beast;[8] xenobiology is now used in a more specialised sense, referring to 'biology based on foreign chemistry', whether of extraterrestrial or terrestrial (typically synthetic) origin.[9]
 While the potential for extraterrestrial life, especially intelligent life, has been explored throughout human history within philosophy and narrative, the question is a verifiable hypothesis and thus a valid line of scientific inquiry;[10][11] planetary scientist David Grinspoon calls it a field of natural philosophy, grounding speculation on the unknown in known scientific theory.[12]
 The modern field of astrobiology can be traced back to the 1950s and 1960s with the advent of space exploration, when scientists began to seriously consider the possibility of life on other planets. In 1957, the Soviet Union launched Sputnik 1, the first artificial satellite, which marked the beginning of the Space Age. This event led to an increase in the study of the potential for life on other planets, as scientists began to consider the possibilities opened up by the new technology of space exploration. In 1959, NASA funded its first exobiology project, and in 1960, NASA founded the Exobiology Program, now one of four main elements of NASA's current Astrobiology Program.[13] In 1971, NASA funded Project Cyclops,[14] part of the search for extraterrestrial intelligence, to search radio frequencies of the electromagnetic spectrum for interstellar communications transmitted by extraterrestrial life outside the Solar System. In the 1960s-1970s, NASA established the Viking program, which was the first US mission to land on Mars and search for metabolic signs of present life; the results were inconclusive.
 In the 1980s and 1990s, the field began to expand and diversify as new discoveries and technologies emerged. The discovery of microbial life in extreme environments on Earth, such as deep-sea hydrothermal vents, helped to clarify the feasibility of potential life existing in harsh conditions. The development of new techniques for the detection of biosignatures, such as the use of stable isotopes, also played a significant role in the evolution of the field.
 The contemporary landscape of astrobiology emerged in the early 21st century, focused on utilising Earth and environmental science for applications within comparate space environments. Missions included the ESA's Beagle 2, which failed minutes after landing on Mars, NASA's Phoenix lander, which probed the environment for past and present planetary habitability of microbial life on Mars and researched the history of water, and NASA's Curiosity rover, currently probing the environment for past and present planetary habitability of microbial life on Mars.
 Astrobiological research makes a number of simplifying assumptions when studying the necessary components for planetary habitability.
 Carbon and Organic Compounds: Carbon is the fourth most abundant element in the universe and the energy required to make or break a bond is at just the appropriate level for building molecules which are not only stable, but also reactive. The fact that carbon atoms bond readily to other carbon atoms allows for the building of extremely long and complex molecules. As such, astrobiological research presumes that the vast majority of life forms in the Milky Way galaxy are based on carbon chemistries, as are all life forms on Earth.[15][16] However, theoretical astrobiology entertains the potential for other organic molecular bases for life, thus astrobiological research often focuses on identifying environments that have the potential to support life based on the presence of organic compounds.
 Liquid water: Liquid water is a common molecule that provides an excellent environment for the formation of complicated carbon-based molecules, and is generally considered necessary for life as we know it to exist. Thus, astrobiological research presumes that extraterrestrial life similarly depends upon access to liquid water, and often focuses on identifying environments that have the potential to support liquid water.[17][18] Some researchers posit environments of water-ammonia mixtures as possible solvents for hypothetical types of biochemistry.[19]
 Environmental Stability: Where organisms adaptively evolve to the conditions of the environments in which they reside, environmental stability is considered necessary for life to exist. This presupposes the necessity of a stable temperature, pressure, and radiation levels; resultantly, astrobiological research focuses on planets orbiting Sun-like red dwarf stars.[20][16] This is because very large stars have relatively short lifetimes, meaning that life might not have time to emerge on planets orbiting them; very small stars provide so little heat and warmth that only planets in very close orbits around them would not be frozen solid, and in such close orbits these planets would be tidally locked to the star;[21] whereas the long lifetimes of red dwarfs could allow the development of habitable environments on planets with thick atmospheres.[22] This is significant as red dwarfs are extremely common. (See also: Habitability of red dwarf systems).
 Energy source: It is assumed that any life elsewhere in the universe would also require an energy source. Previously, it was assumed that this would necessarily be from a sun-like star, however with developments within extremophile research contemporary astrobiological research often focuses on identifying environments that have the potential to support life based on the availability of an energy source, such as the presence of volcanic activity on a planet or moon that could provide a source of heat and energy.
 It is important to note that these assumptions are based on our current understanding of life on Earth and the conditions under which it can exist. As our understanding of life and the potential for it to exist in different environments evolves, these assumptions may change.
 Astrobiological research concerning the study of habitable environments in our solar system and beyond utilises methods within the geosciences. Research within this branch primarily concerns the geobiology of organisms that can survive in extreme environments on Earth, such as in volcanic or deep sea environments, to understand the limits of life, and the conditions under which life might be able to survive on other planets. This includes, but is not limited to;
 Deep-sea extremophiles: Researchers are studying organisms that live in the extreme environments of deep-sea hydrothermal vents and cold seeps.[23] These organisms survive in the absence of sunlight, and some are able to survive in high temperatures and pressures, and use chemical energy instead of sunlight to produce food.
 Desert extremophiles: Researchers are studying organisms that can survive in extreme dry, high temperature conditions, such as in deserts.[24]
 Microbes in extreme environments: Researchers are investigating the diversity and activity of microorganisms in environments such as deep mines, subsurface soil, cold glaciers[25] and polar ice,[26] and high-altitude environments.
 Research also regards the long-term survival of life on Earth, and the possibilities and hazards of life on other planets, including;
 Biodiversity and ecosystem resilience: Scientists are studying how the diversity of life and the interactions between different species contribute to the resilience of ecosystems and their ability to recover from disturbances.[27]
 Climate change and extinction: Researchers are investigating the impacts of climate change on different species and ecosystems, and how they may lead to extinction or adaptation.[28] This includes the evolution of Earth's climate and geology, and their potential impact on the habitability of the planet in the future, especially for humans.
 Human impact on the biosphere: Scientists are studying the ways in which human activities, such as deforestation, pollution, and the introduction of invasive species, are affecting the biosphere and the long-term survival of life on Earth.[29]
 Long-term preservation of life: Researchers are exploring ways to preserve samples of life on Earth for long periods of time, such as cryopreservation and genomic preservation, in the event of a catastrophic event that could wipe out most of life on Earth.[30]
 Emerging astrobiological research concerning the search for planetary biosignatures of past or present extraterrestrial life utilise methodologies within planetary sciences. These include; 
 The study of microbial life in the subsurface of Mars:  Scientists are using data from Mars rover missions to study the composition of the subsurface of Mars, searching for biosignatures of past or present microbial life.[31]
The study of subsurface oceans on icy moons:   Discoveries of subsurface oceans on moons such as Europa[32][33][34] and Enceladus[35][36] showed habitability zones, making them viable targets for the search for extraterrestrial life. Currently,[when?] missions like the Europa Clipper were planned for searching for biosignatures within these environments.  The study of the atmospheres of planets:   Scientists are studying the potential for life to exist in the atmospheres of planets, with a focus on the study of the physical and chemical conditions necessary for such life to exist, namely the detection of organic molecules and biosignature gases; for example, the study of the possibility of life in the atmospheres of exoplanets that orbit red dwarfs and the study of the potential for microbial life in the upper atmosphere of Venus.[37]
 Telescopes and remote sensing of exoplanets: The discovery of thousands of exoplanets has opened up new opportunities for the search for biosignatures. Scientists are using telescopes such as the James Webb Space Telescope and the Transiting Exoplanet Survey Satellite to search for biosignatures on exoplanets. They are also developing new techniques for the detection of biosignatures, such as the use of remote sensing to search for biosignatures in the atmosphere of exoplanets.[38]
 SETI and CETI:  Scientists search for signals from intelligent extraterrestrial civilizations using radio and optical telescopes within the discipline of extraterrestrial intelligence communications (CETI). CETI focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. Communication attempts by humans have included broadcasting mathematical languages, pictorial systems such as the Arecibo message, and computational approaches to detecting and deciphering 'natural' language communication. While some high-profile scientists, such as Carl Sagan, have advocated the transmission of messages,[39][40] theoretical physicist Stephen Hawking warned against it, suggesting that aliens may raid Earth for its resources.[41]
 Emerging astrobiological research concerning the study of the origin and early evolution of life on Earth utilises methodologies within the palaeosciences. These include;
 The study of the early atmosphere: Researchers are investigating the role of the early atmosphere in providing the right conditions for the emergence of life, such as the presence of gases that could have helped to stabilise the climate and the formation of organic molecules.[42]
 The study of the early magnetic field: Researchers are investigating the role of the early magnetic field in protecting the Earth from harmful radiation and helping to stabilise the climate.[43] This research has immense astrobiological implications where the subjects of current astrobiological research like Mars lack such a field.
 The study of prebiotic chemistry: Scientists are studying the chemical reactions that could have occurred on the early Earth that led to the formation of the building blocks of life- amino acids, nucleotides, and lipids- and how these molecules could have formed spontaneously under early Earth conditions.[44]  The study of impact events: Scientists are investigating the potential role of impact events- especially meteorites- in the delivery of water and organic molecules to early Earth.[45]
 The study of the primordial soup:  Researchers are investigating the conditions and ingredients that were present on the early Earth that could have led to the formation of the first living organisms, such as the presence of water and organic molecules, and how these ingredients could have led to the formation of the first living organisms.[46] This includes the role of water in the formation of the first cells and in catalysing chemical reactions.
 The study of the role of minerals: Scientists are investigating the role of minerals like clay in catalysing the formation of organic molecules, thus playing a role in the emergence of life on Earth.[47]
 The study of the role of energy and electricity: Scientists are investigating the potential sources of energy and electricity that could have been available on the early Earth, and their role in the formation of organic molecules, thus the emergence of life.[48]
 The study of the early oceans: Scientists are investigating the composition and chemistry of the early oceans and how it may have played a role in the emergence of life, such as the presence of dissolved minerals that could have helped to catalyse the formation of organic molecules.[49]
 The study of hydrothermal vents: Scientists are investigating the potential role of hydrothermal vents in the origin of life, as these environments may have provided the energy and chemical building blocks needed for its emergence.[50]
 The study of plate tectonics: Scientists are investigating the role of plate tectonics in creating a diverse range of environments on the early Earth.[51]
 The study of the early biosphere: Researchers are investigating the diversity and activity of microorganisms in the early Earth, and how these organisms may have played a role in the emergence of life.[52]
 The study of microbial fossils: Scientists are investigating the presence of microbial fossils in ancient rocks, which can provide clues about the early evolution of life on Earth and the emergence of the first organisms.[53]
 The systematic search for possible life outside Earth is a valid multidisciplinary scientific endeavor.[54] However, hypotheses and predictions as to its existence and origin vary widely, and at the present, the development of hypotheses firmly grounded on science may be considered astrobiology's most concrete practical application. It has been proposed that viruses are likely to be encountered on other life-bearing planets,[55][56] and may be present even if there are no biological cells.[57]
 As of 2019[update], no evidence of extraterrestrial life has been identified.[58] Examination of the Allan Hills 84001 meteorite, which was recovered in Antarctica in 1984 and originated from Mars, is thought by David McKay, as well as few other scientists, to contain microfossils of extraterrestrial origin; this interpretation is controversial.[59][60][61]
 Yamato 000593, the second largest meteorite from Mars, was found on Earth in 2000. At a microscopic level, spheres are found in the meteorite that are rich in carbon compared to surrounding areas that lack such spheres. The carbon-rich spheres may have been formed by biotic activity according to some NASA scientists.[62][63][64]
 On 5 March 2011, Richard B. Hoover, a scientist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites in the fringe Journal of Cosmology, a story widely reported on by mainstream media.[65][66] However, NASA formally distanced itself from Hoover's claim.[67] According to American astrophysicist Neil deGrasse Tyson: ""At the moment, life on Earth is the only known life in the universe, but there are compelling arguments to suggest we are not alone.""[68]
 Most astronomy-related astrobiology research falls into the category of extrasolar planet (exoplanet) detection, the hypothesis being that if life arose on Earth, then it could also arise on other planets with similar characteristics. To that end, a number of instruments designed to detect Earth-sized exoplanets have been considered, most notably NASA's Terrestrial Planet Finder (TPF) and ESA's Darwin programs, both of which have been cancelled. NASA launched the Kepler mission in March 2009, and the French Space Agency launched the COROT space mission in 2006.[69][70] There are also several less ambitious ground-based efforts underway.
 The goal of these missions is not only to detect Earth-sized planets but also to directly detect light from the planet so that it may be studied spectroscopically. By examining planetary spectra, it would be possible to determine the basic composition of an extrasolar planet's atmosphere and/or surface.[71] Given this knowledge, it may be possible to assess the likelihood of life being found on that planet. A NASA research group, the Virtual Planet Laboratory,[72] is using computer modeling to generate a wide variety of virtual planets to see what they would look like if viewed by TPF or Darwin. It is hoped that once these missions come online, their spectra can be cross-checked with these virtual planetary spectra for features that might indicate the presence of life.
 An estimate for the number of planets with intelligent communicative extraterrestrial life can be gleaned from the Drake equation, essentially an equation expressing the probability of intelligent life as the product of factors such as the fraction of planets that might be habitable and the fraction of planets on which life might arise:[73]
 where:
 However, whilst the rationale behind the equation is sound, it is unlikely that the equation will be constrained to reasonable limits of error any time soon. The problem with the formula is that it is not used to generate or support hypotheses because it contains factors that can never be verified. The first term, R*, number of stars, is generally constrained within a few orders of magnitude. The second and third terms, fp, stars with planets and fe, planets with habitable conditions, are being evaluated for the star's neighborhood. Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference,[74] but some applications of the formula had been taken literally and related to simplistic or pseudoscientific arguments.[75] Another associated topic is the Fermi paradox, which suggests that if intelligent life is common in the universe, then there should be obvious signs of it.
 Another active research area in astrobiology is planetary system formation. It has been suggested that the peculiarities of the Solar System (for example, the presence of Jupiter as a protective shield)[76] may have greatly increased the probability of intelligent life arising on Earth.[77][78]
 Biology cannot state that a process or phenomenon, by being mathematically possible, has to exist forcibly in an extraterrestrial body. Biologists specify what is speculative and what is not.[75] The discovery of extremophiles, organisms able to survive in extreme environments, became a core research element for astrobiologists, as they are important to understand four areas in the limits of life in planetary context: the potential for panspermia, forward contamination due to human exploration ventures, planetary colonization by humans, and the exploration of extinct and extant extraterrestrial life.[79]
 Until the 1970s, life was thought to be entirely dependent on energy from the Sun. Plants on Earth's surface capture energy from sunlight to photosynthesize sugars from carbon dioxide and water, releasing oxygen in the process that is then consumed by oxygen-respiring organisms, passing their energy up the food chain. Even life in the ocean depths, where sunlight cannot reach, was thought to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did.[80] The world's ability to support life was thought to depend on its access to sunlight. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible Alvin, scientists discovered colonies of giant tube worms, clams, crustaceans, mussels, and other assorted creatures clustered around undersea volcanic features known as black smokers.[80] These creatures thrive despite having no access to sunlight, and it was soon discovered that they comprise an entirely independent ecosystem. Although most of these multicellular lifeforms need dissolved oxygen (produced by oxygenic photosynthesis) for their aerobic cellular respiration and thus are not completely independent from sunlight by themselves, the basis for their food chain is a form of bacterium that derives its energy from oxidization of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. Other lifeforms entirely decoupled from the energy from sunlight are green sulfur bacteria which are capturing geothermal light for anoxygenic photosynthesis or bacteria running chemolithoautotrophy based on the radioactive decay of uranium.[81] This chemosynthesis revolutionized the study of biology and astrobiology by revealing that life need not be sunlight-dependent; it only requires water and an energy gradient in order to exist.
 Biologists have found extremophiles that thrive in ice, boiling water, acid, alkali, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life.[82][83] This opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats. Characterization of these organisms, their environments and their evolutionary pathways, is considered a crucial component to understanding how life might evolve elsewhere in the universe. For example, some organisms able to withstand exposure to the vacuum and radiation of outer space include the lichen fungi Rhizocarpon geographicum and Rusavskia elegans,[84] the bacterium Bacillus safensis,[85] Deinococcus radiodurans,[85] Bacillus subtilis,[85] yeast Saccharomyces cerevisiae,[85] seeds from Arabidopsis thaliana ('mouse-ear cress'),[85] as well as the invertebrate animal Tardigrade.[85] While tardigrades are not considered true extremophiles, they are considered extremotolerant microorganisms that have contributed to the field of astrobiology. Their extreme radiation tolerance and presence of DNA protection proteins may provide answers as to whether life can survive away from the protection of the Earth's atmosphere.[86]
 Jupiter's moon, Europa,[83][87][88][89][90] and Saturn's moon, Enceladus,[91][35] are now considered the most likely locations for extant extraterrestrial life in the Solar System due to their subsurface water oceans where radiogenic and tidal heating enables liquid water to exist.[81]
 The origin of life, known as abiogenesis, distinct from the evolution of life, is another ongoing field of research. Oparin and Haldane postulated that the conditions on the early Earth were conducive to the formation of organic compounds from inorganic elements and thus to the formation of many of the chemicals common to all forms of life we see today. The study of this process, known as prebiotic chemistry, has made some progress, but it is still unclear whether or not life could have formed in such a manner on Earth. The alternative hypothesis of panspermia is that the first elements of life may have formed on another planet with even more favorable conditions (or even in interstellar space, asteroids, etc.) and then have been carried over to Earth.
 The cosmic dust permeating the universe contains complex organic compounds (""amorphous organic solids with a mixed aromatic-aliphatic structure"") that could be created naturally, and rapidly, by stars.[92][93][94] Further, a scientist suggested that these compounds may have been related to the development of life on Earth and said that, ""If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.""[92]
 More than 20% of the carbon in the universe may be associated with polycyclic aromatic hydrocarbons (PAHs), possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.[95] PAHs are subjected to interstellar medium conditions and are transformed through hydrogenation, oxygenation and hydroxylation, to more complex organics—""a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively"".[96][97]
 In October 2020, astronomers proposed the idea of detecting life on distant planets by studying the shadows of trees at certain times of the day to find patterns that could be detected through observation of exoplanets.[98][99]
 The Rare Earth hypothesis postulates that multicellular life forms found on Earth may actually be more of a rarity than scientists assume. According to this hypothesis, life on Earth (and more, multi-cellular life) is possible because of a conjunction of the right circumstances (galaxy and location within it, planetary system, star, orbit, planetary size, atmosphere, etc.); and the chance for all those circumstances to repeat elsewhere may be rare. It provides a possible answer to the Fermi paradox which suggests, ""If extraterrestrial aliens are common, why aren't they obvious?"" It is apparently in opposition to the principle of mediocrity, assumed by famed astronomers Frank Drake, Carl Sagan, and others. The principle of mediocrity suggests that life on Earth is not exceptional, and it is more than likely to be found on innumerable other worlds.
 Research into the environmental limits of life and the workings of extreme ecosystems is ongoing, enabling researchers to better predict what planetary environments might be most likely to harbor life. Missions such as the Phoenix lander, Mars Science Laboratory, ExoMars, Mars 2020 rover to Mars, and the Cassini probe to Saturn's moons aim to further explore the possibilities of life on other planets in the Solar System.
 The two Viking landers each carried four types of biological experiments to the surface of Mars in the late 1970s. These were the only Mars landers to carry out experiments looking specifically for metabolism by current microbial life on Mars. The landers used a robotic arm to collect soil samples into sealed test containers on the craft. The two landers were identical, so the same tests were carried out at two places on Mars' surface; Viking 1 near the equator and Viking 2 further north.[100] The result was inconclusive,[101] and is still disputed by some scientists.[102][103][104][105]
 Norman Horowitz was the chief of the Jet Propulsion Laboratory bioscience section for the Mariner and Viking missions from 1965 to 1976.  Horowitz considered that the great versatility of the carbon atom makes it the element most likely to provide solutions, even exotic solutions, to the problems of survival of life on other planets.[106]  However, he also considered that the conditions found on Mars were incompatible with carbon based life.
 Beagle 2 was an unsuccessful British Mars lander that formed part of the European Space Agency's 2003 Mars Express mission. Its primary purpose was to search for signs of life on Mars, past or present. Although it landed safely, it was unable to correctly deploy its solar panels and telecom antenna.[107]
 EXPOSE is a multi-user facility mounted in 2008 outside the International Space Station dedicated to astrobiology.[108][109] EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights that allow exposure of organic chemicals and biological samples to outer space in low Earth orbit.[110]
 The Mars Science Laboratory (MSL) mission landed the Curiosity rover that is currently in operation on Mars.[111] It was launched 26 November 2011, and landed at Gale Crater on 6 August 2012. Mission objectives are to help assess Mars' habitability and in doing so, determine whether Mars is or has ever been able to support life,[112] collect data for a future human mission, study Martian geology, its climate, and further assess the role that water, an essential ingredient for life as we know it, played in forming minerals on Mars.
 The Tanpopo mission is an orbital astrobiology experiment investigating the potential interplanetary transfer of life, organic compounds, and possible terrestrial particles in the low Earth orbit. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of microbial life as well as prebiotic organic compounds. Early mission results show evidence that some clumps of microorganism can survive for at least one year in space.[113] This may support the idea that clumps greater than 0.5 millimeters of microorganisms could be one way for life to spread from planet to planet.[113]
 ExoMars is a robotic mission to Mars to search for possible biosignatures of Martian life, past or present. This astrobiological mission is currently under development by the European Space Agency (ESA) in partnership with the Russian Federal Space Agency (Roscosmos); it is planned for a 2022 launch.[114][115][116]
 Mars 2020 successfully landed its rover Perseverance in Jezero Crater on 18 February 2021. It will investigate environments on Mars relevant to astrobiology, investigate its surface geological processes and history, including the assessment of its past habitability and potential for preservation of biosignatures and biomolecules within accessible geological materials.[117] The Science Definition Team is proposing the rover collect and package at least 31 samples of rock cores and soil for a later mission to bring back for more definitive analysis in laboratories on Earth. The rover could make measurements and technology demonstrations to help designers of a human expedition understand any hazards posed by Martian dust and demonstrate how to collect carbon dioxide (CO2), which could be a resource for making molecular oxygen (O2) and rocket fuel.[118][119]
 Europa Clipper is a mission planned by NASA for a 2025 launch that will conduct detailed reconnaissance of Jupiter's moon Europa and will investigate whether its internal ocean could harbor conditions suitable for life.[120][121] It will also aid in the selection of future landing sites.[122][123]
 Dragonfly is a NASA mission scheduled to land on Titan in 2036 to assess its microbial habitability and study its prebiotic chemistry. Dragonfly is a rotorcraft lander that will perform controlled flights between multiple locations on the surface, which allows sampling of diverse regions and geological contexts.[124]
 Icebreaker Life is a lander mission that was proposed for NASA's Discovery Program for the 2021 launch opportunity,[125] but it was not selected for development. It would have had a stationary lander that would be a near copy of the successful 2008 Phoenix and it would have carried an upgraded astrobiology scientific payload, including a 1-meter-long core drill to sample ice-cemented ground in the northern plains to conduct a search for organic molecules and evidence of current or past life on Mars.[126][127] One of the key goals of the Icebreaker Life mission is to test the hypothesis that the ice-rich ground in the polar regions has significant concentrations of organics due to protection by the ice from oxidants and radiation.
 Journey to Enceladus and Titan (JET) is an astrobiology mission concept to assess the habitability potential of Saturn's moons Enceladus and Titan by means of an orbiter.[128][129][130]
 Enceladus Life Finder (ELF) is a proposed astrobiology mission concept for a space probe intended to assess the habitability of the internal aquatic ocean of Enceladus, Saturn's sixth-largest moon.[131][132]
 Life Investigation For Enceladus (LIFE) is a proposed astrobiology sample-return mission concept. The spacecraft would enter into Saturn orbit and enable multiple flybys through Enceladus' icy plumes to collect icy plume particles and volatiles and return them to Earth on a capsule. The spacecraft may sample Enceladus' plumes, the E ring of Saturn, and the upper atmosphere of Titan.[133][134][135]
 Oceanus is an orbiter proposed in 2017 for the New Frontiers mission No. 4. It would travel to the moon of Saturn, Titan, to assess its habitability.[136] Oceanus' objectives are to reveal Titan's organic chemistry, geology, gravity, topography, collect 3D reconnaissance data, catalog the organics and determine where they may interact with liquid water.[137]
 Explorer of Enceladus and Titan (E2T) is an orbiter mission concept that would investigate the evolution and habitability of the Saturnian satellites Enceladus and Titan. The mission concept was proposed in 2017 by the European Space Agency.[138]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['geobiology of organisms that can survive in extreme environments on Earth', 'Enceladus and Titan', 'water', 'deterministic conditions and contingent events', 'astrobiological applications'], 'answer_start': [], 'answer_end': []}"
"
 Augmented reality (AR) is an interactive experience that combines the real world and computer-generated 3D content. The content can span multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory.[1] AR can be defined as a system that incorporates three basic features: a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects.[2] The overlaid sensory information can be constructive (i.e. additive to the natural environment), or destructive (i.e. masking of the natural environment).[3] As such, it is one of the key technologies in the reality-virtuality continuum.[4]
 This experience is seamlessly interwoven with the physical world such that it is perceived as an immersive aspect of the real environment.[3] In this way, augmented reality alters one's ongoing perception of a real-world environment, whereas virtual reality completely replaces the user's real-world environment with a simulated one.[5][6]
 Augmented reality is largely synonymous with mixed reality. There is also overlap in terminology with extended reality and computer-mediated reality.
 The primary value of augmented reality is the manner in which components of the digital world blend into a person's perception of the real world, not as a simple display of data, but through the integration of immersive sensations, which are perceived as natural parts of an environment. The earliest functional AR systems that provided immersive mixed reality experiences for users were invented in the early 1990s, starting with the Virtual Fixtures system developed at the U.S. Air Force's Armstrong Laboratory in 1992.[3][7][8] Commercial augmented reality experiences were first introduced in entertainment and gaming businesses.[9] Subsequently, augmented reality applications have spanned commercial industries such as education, communications, medicine, and entertainment. In education, content may be accessed by scanning or viewing an image with a mobile device or by using markerless AR techniques.[10][11][12]
 Augmented reality can be used to enhance natural environments or situations and offers perceptually enriched experiences. With the help of advanced AR technologies (e.g. adding computer vision, incorporating AR cameras into smartphone applications, and object recognition) the information about the surrounding real world of the user becomes interactive and digitally manipulated.[13] Information about the environment and its objects is overlaid on the real world. This information can be virtual. Augmented Reality is any experience which is artificial and which adds to the already existing reality.[14][15][16][17][18] or real, e.g. seeing other real sensed or measured information such as electromagnetic radio waves overlaid in exact alignment with where they actually are in space.[19][20][21] Augmented reality also has a lot of potential in the gathering and sharing of tacit knowledge. Augmentation techniques are typically performed in real-time and in semantic contexts with environmental elements. Immersive perceptual information is sometimes combined with supplemental information like scores over a live video feed of a sporting event. This combines the benefits of both augmented reality technology and heads up display technology (HUD).
 In virtual reality (VR), the users' perception of reality is completely based on virtual information. In augmented reality (AR) the user is provided with additional computer- generated information within the data collected from real life that enhances their perception of reality.[22][23] For example, in architecture, VR can be used to create a walk-through simulation of the inside of a new building; and AR can be used to show a building's structures and systems super-imposed on a real-life view. Another example is through the use of utility applications. Some AR applications, such as Augment, enable users to apply digital objects into real environments, allowing businesses to use augmented reality devices as a way to preview their products in the real world.[24] Similarly, it can also be used to demo what products may look like in an environment for customers, as demonstrated by companies such as Mountain Equipment Co-op or Lowe's who use augmented reality to allow customers to preview what their products might look like at home through the use of 3D models.[25]
 Augmented reality (AR) differs from virtual reality (VR) in the sense that in AR part of the surrounding environment is 'real' and AR is just adding layers of virtual objects to the real environment. On the other hand, in VR the surrounding environment is completely virtual and computer generated. A demonstration of how AR layers objects onto the real world can be seen with augmented reality games. WallaMe is an augmented reality game application that allows users to hide messages in real environments, utilizing geolocation technology in order to enable users to hide messages wherever they may wish in the world.[26] Such applications have many uses in the world, including in activism and artistic expression.[27]
 Augmented reality requires hardware components including a processor, display, sensors, and input devices. Modern mobile computing devices like smartphones and tablet computers contain these elements, which often include a camera and microelectromechanical systems (MEMS) sensors such as an accelerometer, GPS, and solid state compass, making them suitable AR platforms.[66][67]
 Various technologies can be used to display augmented reality, including optical projection systems, monitors, and handheld devices. Two of the display technologies used in augmented reality are diffractive waveguides and reflective waveguides.
 A head-mounted display (HMD) is a display device worn on the forehead, such as a harness or helmet-mounted. HMDs place images of both the physical world and virtual objects over the user's field of view. Modern HMDs often employ sensors for six degrees of freedom monitoring that allow the system to align virtual information to the physical world and adjust accordingly with the user's head movements.[68][69][70] HMDs can provide VR users with mobile and collaborative experiences.[71] Specific providers, such as uSens and Gestigon, include gesture controls for full virtual immersion.[72][73]
 Vuzix is a company that has produced a number of head-worn optical see through displays marketed for augmented reality.[74][75][76]
 AR displays can be rendered on devices resembling eyeglasses. Versions include eyewear that employs cameras to intercept the real world view and re-display its augmented view through the eyepieces[77] and devices in which the AR imagery is projected through or reflected off the surfaces of the eyewear lens pieces.[78][79][80]
 The EyeTap (also known as Generation-2 Glass[81]) captures rays of light that would otherwise pass through the center of the lens of the wearer's eye, and substitutes synthetic computer-controlled light for each ray of real light. The Generation-4 Glass[81] (Laser EyeTap) is similar to the VRD (i.e. it uses a computer-controlled laser light source) except that it also has infinite depth of focus and causes the eye itself to, in effect, function as both a camera and a display by way of exact alignment with the eye and resynthesis (in laser light) of rays of light entering the eye.[82]
 A head-up display (HUD) is a transparent display that presents data without requiring users to look away from their usual viewpoints. A precursor technology to augmented reality, heads-up displays were first developed for pilots in the 1950s, projecting simple flight data into their line of sight, thereby enabling them to keep their ""heads up"" and not look down at the instruments. Near-eye augmented reality devices can be used as portable head-up displays as they can show data, information, and images while the user views the real world. Many definitions of augmented reality only define it as overlaying the information.[83][84] This is basically what a head-up display does; however, practically speaking, augmented reality is expected to include registration and tracking between the superimposed perceptions, sensations, information, data, and images and some portion of the real world.[85]
 Contact lenses that display AR imaging are in development. These bionic contact lenses might contain the elements for display embedded into the lens including integrated circuitry, LEDs and an antenna for wireless communication. The first contact lens display was patented in 1999 by Steve Mann and was intended to work in combination with AR spectacles, but the project was abandoned,[86][87] then 11 years later in 2010–2011.[88][89][90][91] Another version of contact lenses, in development for the U.S. military, is designed to function with AR spectacles, allowing soldiers to focus on close-to-the-eye AR images on the spectacles and distant real world objects at the same time.[92][93]
 At CES 2013, a company called Innovega also unveiled similar contact lenses that required being combined with AR glasses to work.[94]
 Many scientists have been working on contact lenses capable of different technological feats. A patent filed by Samsung describes an AR contact lens, that, when finished, will include a built-in camera on the lens itself.[95] The design is intended to control its interface by blinking an eye. It is also intended to be linked with the user's smartphone to review footage, and control it separately. When successful, the lens would feature a camera, or sensor inside of it. It is said that it could be anything from a light sensor, to a temperature sensor.
 The first publicly unveiled working prototype of an AR contact lens not requiring the use of glasses in conjunction was developed by Mojo Vision and announced and shown off at CES 2020.[96][97][98]
 A virtual retinal display (VRD) is a personal display device under development at the University of Washington's Human Interface Technology Laboratory under Dr. Thomas A. Furness III.[99] With this technology, a display is scanned directly onto the retina of a viewer's eye. This results in bright images with high resolution and high contrast. The viewer sees what appears to be a conventional display floating in space.[100]
 Several of tests were done to analyze the safety of the VRD.[99] In one test, patients with partial loss of vision—having either macular degeneration (a disease that degenerates the retina) or keratoconus—were selected to view images using the technology. In the macular degeneration group, five out of eight subjects preferred the VRD images to the cathode-ray tube (CRT) or paper images and thought they were better and brighter and were able to see equal or better resolution levels. The Keratoconus patients could all resolve smaller lines in several line tests using the VRD as opposed to their own correction. They also found the VRD images to be easier to view and sharper. As a result of these several tests, virtual retinal display is considered safe technology.
 Virtual retinal display creates images that can be seen in ambient daylight and ambient room light. The VRD is considered a preferred candidate to use in a surgical display due to its combination of high resolution and high contrast and brightness. Additional tests show high potential for VRD to be used as a display technology for patients that have low vision.
 A Handheld display employs a small display that fits in a user's hand. All handheld AR solutions to date opt for video see-through. Initially handheld AR employed fiducial markers,[101] and later GPS units and MEMS sensors such as digital compasses and six degrees of freedom accelerometer–gyroscope. Today simultaneous localization and mapping (SLAM) markerless trackers such as PTAM (parallel tracking and mapping) are starting to come into use. Handheld display AR promises to be the first commercial success for AR technologies. The two main advantages of handheld AR are the portable nature of handheld devices and the ubiquitous nature of camera phones. The disadvantages are the physical constraints of the user having to hold the handheld device out in front of them at all times, as well as the distorting effect of classically wide-angled mobile phone cameras when compared to the real world as viewed through the eye.[102]
 Projection mapping augments real-world objects and scenes without the use of special displays such as monitors, head-mounted displays or hand-held devices. Projection mapping makes use of digital projectors to display graphical information onto physical objects. The key difference in projection mapping is that the display is separated from the users of the system. Since the displays are not associated with each user, projection mapping scales naturally up to groups of users, allowing for collocated collaboration between users.
 Examples include shader lamps, mobile projectors, virtual tables, and smart projectors. Shader lamps mimic and augment reality by projecting imagery onto neutral objects. This provides the opportunity to enhance the object's appearance with materials of a simple unit—a projector, camera, and sensor.
 Other applications include table and wall projections. Virtual showcases, which employ beam splitter mirrors together with multiple graphics displays, provide an interactive means of simultaneously engaging with the virtual and the real.
 A projection mapping system can display on any number of surfaces in an indoor setting at once. Projection mapping supports both a graphical visualization and passive haptic sensation for the end users. Users are able to touch physical objects in a process that provides passive haptic sensation.[18][43][103][104]
 Modern mobile augmented-reality systems use one or more of the following motion tracking technologies: digital cameras and/or other optical sensors, accelerometers, GPS, gyroscopes, solid state compasses, radio-frequency identification (RFID). These technologies offer varying levels of accuracy and precision. These technologies are implemented in the ARKit API by Apple and ARCore API by Google to allow tracking for their respective mobile device platforms.
 Techniques include speech recognition systems that translate a user's spoken words into computer instructions, and gesture recognition systems that interpret a user's body movements by visual detection or from sensors embedded in a peripheral device such as a wand, stylus, pointer, glove or other body wear.[105][106][107][108] Products which are trying to serve as a controller of AR headsets include Wave by Seebright Inc. and Nimble by Intugine Technologies.
 Computers are responsible for graphics in augmented reality. A computer analyzes the sensed visual and other data to synthesize and position virtual objects. With the improvement of technology and computers, augmented reality is going to lead to a drastic change on ones perspective of the real world.[109]
 Computers are improving at a very fast rate, leading to new ways to improve other technology. The more that computers progress, augmented reality will become more flexible and more common in society. Computers are the core of augmented reality.
[110] The computer receives data from the sensors which determine the relative position of an objects' surface. This translates to an input to the computer which then outputs to the users by adding something that would otherwise not be there. The computer comprises memory and a processor.[111] The computer takes the scanned environment then generates images or a video and puts it on the receiver for the observer to see. The fixed marks on an object's surface are stored in the memory of a computer. The computer also withdraws from its memory to present images realistically to the onlooker.
 Projectors can also be used to display AR contents. The projector can throw a virtual object on a projection screen and the viewer can interact with this virtual object. Projection surfaces can be many objects such as walls or glass panes.[112]
 Mobile augmented reality applications are gaining popularity because of the wide adoption of mobile and especially wearable devices. However, they often rely on computationally intensive computer vision algorithms with extreme latency requirements. To compensate for the lack of computing power, offloading data processing to a distant machine is often desired. Computation offloading introduces new constraints in applications, especially in terms of latency and bandwidth. Although there are a plethora of real-time multimedia transport protocols, there is a need for support from network infrastructure as well.[113]
 A key measure of AR systems is how realistically they integrate augmentations with the real world. The software must derive real world coordinates, independent of camera, and camera images. That process is called image registration, and uses different methods of computer vision, mostly related to video tracking.[114][115] Many computer vision methods of augmented reality are inherited from visual odometry.
 Usually those methods consist of two parts. The first stage is to detect interest points, fiducial markers or optical flow in the camera images. This step can use feature detection methods like corner detection, blob detection, edge detection or thresholding, and other image processing methods.[116][117] The second stage restores a real world coordinate system from the data obtained in the first stage. Some methods assume objects with known geometry (or fiducial markers) are present in the scene. In some of those cases the scene 3D structure should be calculated beforehand. If part of the scene is unknown simultaneous localization and mapping (SLAM) can map relative positions. If no information about scene geometry is available, structure from motion methods like bundle adjustment are used. Mathematical methods used in the second stage include: projective (epipolar) geometry, geometric algebra, rotation representation with exponential map, kalman and particle filters, nonlinear optimization, robust statistics.[citation needed]
 In augmented reality, the distinction is made between two distinct modes of tracking, known as marker and markerless. Markers are visual cues which trigger the display of the virtual information.[118] A piece of paper with some distinct geometries can be used. The camera recognizes the geometries by identifying specific points in the drawing. Markerless tracking, also called instant tracking, does not use markers. Instead, the user positions the object in the camera view preferably in a horizontal plane. It uses sensors in mobile devices to accurately detect the real-world environment, such as the locations of walls and points of intersection.[119]
 Augmented Reality Markup Language (ARML) is a data standard developed within the Open Geospatial Consortium (OGC),[120] which consists of Extensible Markup Language (XML) grammar to describe the location and appearance of virtual objects in the scene, as well as ECMAScript bindings to allow dynamic access to properties of virtual objects.
 
To enable rapid development of augmented reality applications, software development applications have emerged, including Lens Studio from Snapchat and Spark AR from Facebook. Augmented reality Software Development Kits (SDKs) have been launched by Apple and Google.[121][122]
 The implementation of augmented reality in consumer products requires considering the design of the applications and the related constraints of the technology platform. Since AR systems rely heavily on the immersion of the user and the interaction between the user and the system, design can facilitate the adoption of virtuality. For most augmented reality systems, a similar design guideline can be followed. The following lists some considerations for designing augmented reality applications:
 Context Design focuses on the end-user's physical surrounding, spatial space, and accessibility that may play a role when using the AR system. Designers should be aware of the possible physical scenarios the end-user may be in such as:
 By evaluating each physical scenario, potential safety hazards can be avoided and changes can be made to greater improve the end-user's immersion. UX designers will have to define user journeys for the relevant physical scenarios and define how the interface reacts to each.
 Another aspect of context design involves the design of the system's functionality and its ability to accommodate user preferences.[124][125] While accessibility tools are common in basic application design, some consideration should be made when designing time-limited prompts (to prevent unintentional operations), audio cues and overall engagement time. It is important to note that in some situations, the application's functionality may hinder the user's ability. For example, applications that is used for driving should reduce the amount of user interaction and use audio cues instead.
 Interaction design in augmented reality technology centers on the user's engagement with the end product to improve the overall user experience and enjoyment. The purpose of interaction design is to avoid alienating or confusing the user by organizing the information presented. Since user interaction relies on the user's input, designers must make system controls easier to understand and accessible. A common technique to improve usability for augmented reality applications is by discovering the frequently accessed areas in the device's touch display and design the application to match those areas of control.[126] It is also important to structure the user journey maps and the flow of information presented which reduce the system's overall cognitive load and greatly improves the learning curve of the application.[127]
 In interaction design, it is important for developers to utilize augmented reality technology that complement the system's function or purpose.[128] For instance, the utilization of exciting AR filters and the design of the unique sharing platform in Snapchat enables users to augment their in-app social interactions. In other applications that require users to understand the focus and intent, designers can employ a reticle or raycast from the device.[124]
 In general, graphic design is the appearance of the developing application that engages the user. To improve the graphic interface elements and user interaction, developers may use visual cues to inform the user what elements of UI are designed to interact with and how to interact with them. Since navigating in an AR application may appear difficult and seem frustrating, visual cue design can make interactions seem more natural.[123]
 In some augmented reality applications that use a 2D device as an interactive surface, the 2D control environment does not translate well in 3D space making users hesitant to explore their surroundings. To solve this issue, designers should apply visual cues to assist and encourage users to explore their surroundings.
 It is important to note the two main objects in AR when developing VR applications: 3D volumetric objects that are manipulated and realistically interact with light and shadow; and animated media imagery such as images and videos which are mostly traditional 2D media rendered in a new context for augmented reality.[123] When virtual objects are projected onto a real environment, it is challenging for augmented reality application designers to ensure a perfectly seamless integration relative to the real-world environment, especially with 2D objects. As such, designers can add weight to objects, use depths maps, and choose different material properties that highlight the object's presence in the real world. Another visual design that can be applied is using different lighting techniques or casting shadows to improve overall depth judgment. For instance, a common lighting technique is simply placing a light source overhead at the 12 o’clock position, to create shadows on virtual objects.[123]
 Augmented reality has been explored for many uses, from gaming and entertainment to medicine, education and business.[129] Example application areas described below include archaeology, architecture, commerce and education. Some of the earliest cited examples include augmented reality used to support surgery by providing virtual overlays to guide medical practitioners, to AR content for astronomy and welding.[8][130]
 AR has been used to aid archaeological research. By augmenting archaeological features onto the modern landscape, AR allows archaeologists to formulate possible site configurations from extant structures.[131] Computer generated models of ruins, buildings, landscapes or even ancient people have been recycled into early archaeological AR applications.[132][133][134] For example, implementing a system like VITA (Visual Interaction Tool for Archaeology) will allow users to imagine and investigate instant excavation results without leaving their home. Each user can collaborate by mutually ""navigating, searching, and viewing data"". Hrvoje Benko, a researcher in the computer science department at Columbia University, points out that these particular systems and others like them can provide ""3D panoramic images and 3D models of the site itself at different excavation stages"" all the while organizing much of the data in a collaborative way that is easy to use. Collaborative AR systems supply multimodal interactions that combine the real world with virtual images of both environments.[135]
 AR can aid in visualizing building projects. Computer-generated images of a structure can be superimposed onto a real-life local view of a property before the physical building is constructed there; this was demonstrated publicly by Trimble Navigation in 2004. AR can also be employed within an architect's workspace, rendering animated 3D visualizations of their 2D drawings. Architecture sight-seeing can be enhanced with AR applications, allowing users viewing a building's exterior to virtually see through its walls, viewing its interior objects and layout.[136][137][51]
 With continual improvements to GPS accuracy, businesses are able to use augmented reality to visualize georeferenced models of construction sites, underground structures, cables and pipes using mobile devices.[138] Augmented reality is applied to present new projects, to solve on-site construction challenges, and to enhance promotional materials.[139] Examples include the Daqri Smart Helmet, an Android-powered hard hat used to create augmented reality for the industrial worker, including visual instructions, real-time alerts, and 3D mapping.
 Following the Christchurch earthquake, the University of Canterbury released CityViewAR,[140] which enabled city planners and engineers to visualize buildings that had been destroyed.[141] This not only provided planners with tools to reference the previous cityscape, but it also served as a reminder of the magnitude of the resulting devastation, as entire buildings had been demolished.
 In educational settings, AR has been used to complement a standard curriculum. Text, graphics, video, and audio may be superimposed into a student's real-time environment. Textbooks, flashcards and other educational reading material may contain embedded ""markers"" or triggers that, when scanned by an AR device, produced supplementary information to the student rendered in a multimedia format.[142][143][144] The 2015 Virtual, Augmented and Mixed Reality: 7th International Conference mentioned Google Glass as an example of augmented reality that can replace the physical classroom.[145] First, AR technologies help learners engage in authentic exploration in the real world, and virtual objects such as texts, videos, and pictures are supplementary elements for learners to conduct investigations of the real-world surroundings.[146]
 As AR evolves, students can participate interactively and interact with knowledge more authentically. Instead of remaining passive recipients, students can become active learners, able to interact with their learning environment. Computer-generated simulations of historical events allow students to explore and learning details of each significant area of the event site.[147]
 In higher education, Construct3D, a Studierstube system, allows students to learn mechanical engineering concepts, math or geometry.[148] Chemistry AR apps allow students to visualize and interact with the spatial structure of a molecule using a marker object held in the hand.[149] Others have used HP Reveal, a free app, to create AR notecards for studying organic chemistry mechanisms or to create virtual demonstrations of how to use laboratory instrumentation.[150] Anatomy students can visualize different systems of the human body in three dimensions.[151] Using AR as a tool to learn anatomical structures has been shown to increase the learner knowledge and provide intrinsic benefits, such as increased engagement and learner immersion.[152][153]
 AR has been used to develop different safety training application for several types of disasters such as, earthquakes and building fire.[154][155] Further, several AR solutions have been proposed and tested to navigate building evacuees towards safe places in both large scale and small scale disasters.[156][157] Further, AR applications can have several overlapping with many other digital technologies, such as BIM, internet of things and artificial intelligence, to generate smarter safety training and navigation solutions.[158]
 AR is used to substitute paper manuals with digital instructions which are overlaid on the manufacturing operator's field of view, reducing mental effort required to operate.[159] AR makes machine maintenance efficient because it gives operators direct access to a machine's maintenance history.[160] Virtual manuals help manufacturers adapt to rapidly-changing product designs, as digital instructions are more easily edited and distributed compared to physical manuals.[159]
 Digital instructions increase operator safety by removing the need for operators to look at a screen or manual away from the working area, which can be hazardous. Instead, the instructions are overlaid on the working area.[161][162] The use of AR can increase operators' feeling of safety when working near high-load industrial machinery by giving operators additional information on a machine's status and safety functions, as well as hazardous areas of the workspace.[161][163]
 AR is used to integrate print and video marketing. Printed marketing material can be designed with certain ""trigger"" images that, when scanned by an AR-enabled device using image recognition, activate a video version of the promotional material. A major difference between augmented reality and straightforward image recognition is that one can overlay multiple media at the same time in the view screen, such as social media share buttons, the in-page video even audio and 3D objects. Traditional print-only publications are using augmented reality to connect different types of media.[164][165][166][167][168]
 AR can enhance product previews such as allowing a customer to view what's inside a product's packaging without opening it.[169] AR can also be used as an aid in selecting products from a catalog or through a kiosk. Scanned images of products can activate views of additional content such as customization options and additional images of the product in its use.[170]
 By 2010, virtual dressing rooms had been developed for e-commerce.[171]
 In 2012, a mint used AR techniques to market a commemorative coin for Aruba. The coin itself was used as an AR trigger, and when held in front of an AR-enabled device it revealed additional objects and layers of information that were not visible without the device.[172][173]
 In 2018, Apple announced Universal Scene Description (USDZ) AR file support for iPhones and iPads with iOS 12. Apple has created an AR QuickLook Gallery that allows masses to experience augmented reality on their own Apple device.[174]
 In 2018, Shopify, the Canadian e-commerce company, announced AR Quick Look integration. Their merchants will be able to upload 3D models of their products and their users will be able to tap on the models inside the Safari browser on their iOS devices to view them in their real-world environments.[175]
 In 2018, Twinkl released a free AR classroom application. Pupils can see how York looked over 1,900 years ago.[176] Twinkl launched the first ever multi-player AR game, Little Red[177] and has over 100 free AR educational models.[178]
 Augmented reality is becoming more frequently used for online advertising. Retailers offer the ability to upload a picture on their website and ""try on"" various clothes which are overlaid on the picture. Even further, companies such as Bodymetrics install dressing booths in department stores that offer full-body scanning. These booths render a 3-D model of the user, allowing the consumers to view different outfits on themselves without the need of physically changing clothes.[179] For example, JC Penney and Bloomingdale's use ""virtual dressing rooms"" that allow customers to see themselves in clothes without trying them on.[180] Another store that uses AR to market clothing to its customers is Neiman Marcus.[181] Neiman Marcus offers consumers the ability to see their outfits in a 360-degree view with their ""memory mirror"".[181] Makeup stores like L'Oreal, Sephora, Charlotte Tilbury, and Rimmel also have apps that utilize AR.[182] These apps allow consumers to see how the makeup will look on them.[182] According to Greg Jones, director of AR and VR at Google, augmented reality is going to ""reconnect physical and digital retail"".[182]
 AR technology is also used by furniture retailers such as IKEA, Houzz, and Wayfair.[182][180] These retailers offer apps that allow consumers to view their products in their home prior to purchasing anything.[182] [183]
In 2017, Ikea announced the Ikea Place app. It contains a catalogue of over 2,000 products—nearly the company's full collection of sofas, armchairs, coffee tables, and storage units which one can place anywhere in a room with their phone.[184] The app made it possible to have 3D and true-to-scale models of furniture in the customer's living space. IKEA realized that their customers are not shopping in stores as often or making direct purchases anymore.[185][186] Shopify's acquisition of Primer, an AR app aims to push small and medium-sized sellers towards interactive AR shopping with easy to use AR integration and user experience for both merchants and consumers.[187] AR helps the retail industry reduce operating costs. Merchants upload product information to the AR system, and consumers can use mobile terminals to search and generate 3D maps.[188]
 The first description of AR as it is known today was in Virtual Light, the 1994 novel by William Gibson. In 2011, AR was blended with poetry by ni ka from Sekai Camera in Tokyo, Japan. The prose of these AR poems come from Paul Celan, Die Niemandsrose, expressing the aftermath of the 2011 Tōhoku earthquake and tsunami.[189]
 AR applied in the visual arts allows objects or places to trigger artistic multidimensional experiences and interpretations of reality.
 The Australian new media artist Jeffrey Shaw pioneered Augmented Reality in three artworks: Viewpoint in 1975, Virtual Sculptures in 1987 and The Golden Calf in 1993.[191][192] He continues to explore new permutations of AR in numerous recent works.
 Augmented reality can aid in the progression of visual art in museums by allowing museum visitors to view artwork in galleries in a multidimensional way through their phone screens.[193] The Museum of Modern Art in New York has created an exhibit in their art museum showcasing AR features that viewers can see using an app on their smartphone.[194] The museum has developed their personal app, called MoMAR Gallery, that museum guests can download and use in the augmented reality specialized gallery in order to view the museum's paintings in a different way.[195] This allows individuals to see hidden aspects and information about the paintings, and to be able to have an interactive technological experience with artwork as well.
 AR technology was used in Nancy Baker Cahill's ""Margin of Error"" and ""Revolutions,""[196] the two public art pieces she created for the 2019 Desert X exhibition.[197]
 AR technology aided the development of eye tracking technology to translate a disabled person's eye movements into drawings on a screen.[198]
 A Danish artist, Olafur Eliasson, has placed objects like burning suns, extraterrestrial rocks, and rare animals, into the user's environment.[199] Martin & Muñoz started using Augmented Reality (AR) technology in 2020 to create and place virtual works, based on their snow globes, in their exhibitions and in user's environments. Their first AR work was presented at the Cervantes Institute in New York in early 2022.[200]
 AR hardware and software for use in fitness includes smart glasses made for biking and running, with performance analytics and map navigation projected onto the user's field of vision,[201] and boxing, martial arts, and tennis, where users remain aware of their physical environment for safety.[202] Fitness-related games and software include Pokémon Go and Jurassic World Alive.[203]
 Human–computer interaction (HCI) is an interdisciplinary area of computing that deals with design and implementation of systems that interact with people. Researchers in HCI come from a number of disciplines, including computer science, engineering, design, human factor, and social science, with a shared goal to solve problems in the design and the use of technology so that it can be used more easily, effectively, efficiently, safely, and with satisfaction.[204]
 According to a 2017 Time article, in about 15 to 20 years it is predicted that augmented reality and virtual reality are going to become the primary use for computer interactions.[205]
 Primary school children learn easily from interactive experiences. As an example, astronomical constellations and the movements of objects in the solar system were oriented in 3D and overlaid in the direction the device was held, and expanded with supplemental video information. Paper-based science book illustrations could seem to come alive as video without requiring the child to navigate to web-based materials.
 In 2013, a project was launched on Kickstarter to teach about electronics with an educational toy that allowed children to scan their circuit with an iPad and see the electric current flowing around.[206] While some educational apps were available for AR by 2016, it was not broadly used. Apps that leverage augmented reality to aid learning included SkyView for studying astronomy,[207] AR Circuits for building simple electric circuits,[208] and SketchAr for drawing.[209]
 AR would also be a way for parents and teachers to achieve their goals for modern education, which might include providing more individualized and flexible learning, making closer connections between what is taught at school and the real world, and helping students to become more engaged in their own learning.
 Augmented reality systems are used in public safety situations, from super storms to suspects at large.
 As early as 2009, two articles from Emergency Management discussed AR technology for emergency management. The first was ""Augmented Reality—Emerging Technology for Emergency Management"", by Gerald Baron.[210] According to Adam Crow,: ""Technologies like augmented reality (ex: Google Glass) and the growing expectation of the public will continue to force professional emergency managers to radically shift when, where, and how technology is deployed before, during, and after disasters.""[211]
 Another early example was a search aircraft looking for a lost hiker in rugged mountain terrain. Augmented reality systems provided aerial camera operators with a geographic awareness of forest road names and locations blended with the camera video. The camera operator was better able to search for the hiker knowing the geographic context of the camera image. Once located, the operator could more efficiently direct rescuers to the hiker's location because the geographic position and reference landmarks were clearly labeled.[212]
 AR can be used to facilitate social interaction. An augmented reality social network framework called Talk2Me enables people to disseminate information and view others' advertised information in an augmented reality way. The timely and dynamic information sharing and viewing functionalities of Talk2Me help initiate conversations and make friends for users with people in physical proximity.[213] However, use of an AR headset can inhibit the quality of an interaction between two people if one isn't wearing one if the headset becomes a distraction.[214]
 Augmented reality also gives users the ability to practice different forms of social interactions with other people in a safe, risk-free environment. Hannes Kauffman, Associate Professor for virtual reality at TU Vienna, says: ""In collaborative augmented reality multiple users may access a shared space populated by virtual objects, while remaining grounded in the real world. This technique is particularly powerful for educational purposes when users are collocated and can use natural means of communication (speech, gestures, etc.), but can also be mixed successfully with immersive VR or remote collaboration.""[This quote needs a citation] Hannes cites education as a potential use of this technology.
 The gaming industry embraced AR technology. A number of games were developed for prepared indoor environments, such as AR air hockey, Titans of Space, collaborative combat against virtual enemies, and AR-enhanced pool table games.[215][216][217]
 In 2010, Ogmento became the first AR gaming startup to receive VC Funding.  The company went on to produce early location-based AR games for titles like Paranormal Activity: Sanctuary, NBA: King of the Court, and Halo: King of the Hill. The companies computer vision technology was eventually repackaged and sold to Apple, became a major contribution to ARKit.[218]
 Augmented reality allows video game players to experience digital game play in a real-world environment. Niantic released the augmented reality mobile game Pokémon Go.[219] Disney has partnered with Lenovo to create the augmented reality game Star Wars: Jedi Challenges that works with a Lenovo Mirage AR headset, a tracking sensor and a Lightsaber controller, scheduled to launch in December 2017.[220]
 AR allows industrial designers to experience a product's design and operation before completion. Volkswagen has used AR for comparing calculated and actual crash test imagery.[221] AR has been used to visualize and modify car body structure and engine layout. It has also been used to compare digital mock-ups with physical mock-ups to find discrepancies between them.[222][223]
 One of the first applications of augmented reality was in healthcare, particularly to support the planning, practice, and training of surgical procedures. As far back as 1992, enhancing human performance during surgery was a formally stated objective when building the first augmented reality systems at U.S. Air Force laboratories.[3] Since 2005, a device called a near-infrared vein finder that films subcutaneous veins, processes and projects the image of the veins onto the skin has been used to locate veins.[224][225] AR provides surgeons with patient monitoring data in the style of a fighter pilot's heads-up display, and allows patient imaging records, including functional videos, to be accessed and overlaid. Examples include a virtual X-ray view based on prior tomography or on real-time images from ultrasound and confocal microscopy probes,[226] visualizing the position of a tumor in the video of an endoscope,[227] or radiation exposure risks from X-ray imaging devices.[228][229] AR can enhance viewing a fetus inside a mother's womb.[230] Siemens, Karl Storz and IRCAD have developed a system for laparoscopic liver surgery that uses AR to view sub-surface tumors and vessels.[231]
AR has been used for cockroach phobia treatment[232] and to reduce the fear of spiders.[233] Patients wearing augmented reality glasses can be reminded to take medications.[234] Augmented reality can be very helpful in the medical field.[235] It could be used to provide crucial information to a doctor or surgeon without having them take their eyes off the patient. On 30 April 2015 Microsoft announced the Microsoft HoloLens, their first attempt at augmented reality. The HoloLens has advanced through the years and is capable of projecting holograms for near infrared fluorescence based image guided surgery.[236] As augmented reality advances, it finds increasing applications in healthcare. Augmented reality and similar computer based-utilities are being used to train medical professionals.[237][238] In healthcare, AR can be used to provide guidance during diagnostic and therapeutic interventions e.g. during surgery. Magee et al.,[239] for instance, describe the use of augmented reality for medical training in simulating ultrasound-guided needle placement. A very recent study by Akçayır, Akçayır, Pektaş, and Ocak (2016) revealed that AR technology both improves university students' laboratory skills and helps them to build positive attitudes relating to physics laboratory work.[240] Recently, augmented reality began seeing adoption in neurosurgery, a field that requires heavy amounts of imaging before procedures.[241]
 Augmented reality applications, running on handheld devices utilized as virtual reality headsets, can also digitize human presence in space and provide a computer generated model of them, in a virtual space where they can interact and perform various actions. Such capabilities are demonstrated by Project Anywhere, developed by a postgraduate student at ETH Zurich, which was dubbed as an ""out-of-body experience"".[242][243][244]
 Building on decades of perceptual-motor research in experimental psychology, researchers at the Aviation Research Laboratory of the University of Illinois at Urbana–Champaign used augmented reality in the form of a flight path in the sky to teach flight students how to land an airplane using a flight simulator. An adaptive augmented schedule in which students were shown the augmentation only when they departed from the flight path proved to be a more effective training intervention than a constant schedule.[30][245] Flight students taught to land in the simulator with the adaptive augmentation learned to land a light aircraft more quickly than students with the same amount of landing training in the simulator but with constant augmentation or without any augmentation.[30]
 An interesting early application of AR occurred when Rockwell International created video map overlays of satellite and orbital debris tracks to aid in space observations at Air Force Maui Optical System. In their 1993 paper ""Debris Correlation Using the Rockwell WorldView System"" the authors describe the use of map overlays applied to video from space surveillance telescopes. The map overlays indicated the trajectories of various objects in geographic coordinates. This allowed telescope operators to identify satellites, and also to identify and catalog potentially dangerous space debris.[39]
 Starting in 2003 the US Army integrated the SmartCam3D augmented reality system into the Shadow Unmanned Aerial System to aid sensor operators using telescopic cameras to locate people or points of interest. The system combined fixed geographic information including street names, points of interest, airports, and railroads with live video from the camera system. The system offered a ""picture in picture"" mode that allows it to show a synthetic view of the area surrounding the camera's field of view. This helps solve a problem in which the field of view is so narrow that it excludes important context, as if ""looking through a soda straw"". The system displays real-time friend/foe/neutral location markers blended with live video, providing the operator with improved situational awareness.
 Researchers at USAF Research Lab (Calhoun, Draper et al.) found an approximately two-fold increase in the speed at which UAV sensor operators found points of interest using this technology.[246] This ability to maintain geographic awareness quantitatively enhances mission efficiency. The system is in use on the US Army RQ-7 Shadow and the MQ-1C Gray Eagle Unmanned Aerial Systems.
 In combat, AR can serve as a networked communication system that renders useful battlefield data onto a soldier's goggles in real time. From the soldier's viewpoint, people and various objects can be marked with special indicators to warn of potential dangers. Virtual maps and 360° view camera imaging can also be rendered to aid a soldier's navigation and battlefield perspective, and this can be transmitted to military leaders at a remote command center.[247] The combination of 360° view cameras visualization and AR can be used on board combat vehicles and tanks as circular review system.
 AR can be an effective tool for virtually mapping out the 3D topologies of munition storages in the terrain, with the choice of the munitions combination in stacks and distances between them with a visualization of risk areas.[248][unreliable source?] The scope of AR applications also includes visualization of data from embedded munitions monitoring sensors.[248]
 The NASA X-38 was flown using a hybrid synthetic vision system that overlaid map data on video to provide enhanced navigation for the spacecraft during flight tests from 1998 to 2002. It used the LandForm software which was useful for times of limited visibility, including an instance when the video camera window frosted over leaving astronauts to rely on the map overlays.[44] The LandForm software was also test flown at the Army Yuma Proving Ground in 1999. In the photo at right one can see the map markers indicating runways, air traffic control tower, taxiways, and hangars overlaid on the video.[45]
 AR can augment the effectiveness of navigation devices. Information can be displayed on an automobile's windshield indicating destination directions and meter, weather, terrain, road conditions and traffic information as well as alerts to potential hazards in their path.[249][250][251] Since 2012, a Swiss-based company WayRay has been developing holographic AR navigation systems that use holographic optical elements for projecting all route-related information including directions, important notifications, and points of interest right into the drivers' line of sight and far ahead of the vehicle.[252][253] Aboard maritime vessels, AR can allow bridge watch-standers to continuously monitor important information such as a ship's heading and speed while moving throughout the bridge or performing other tasks.[254]
 Augmented reality may have a positive impact on work collaboration as people may be inclined to interact more actively with their learning environment. It may also encourage tacit knowledge renewal which makes firms more competitive. AR was used to facilitate collaboration among distributed team members via conferences with local and virtual participants. AR tasks included brainstorming and discussion meetings utilizing common visualization via touch screen tables, interactive digital whiteboards, shared design spaces and distributed control rooms.[255][256][257]
 In industrial environments, augmented reality is proving to have a substantial impact with more and more use cases emerging across all aspect of the product lifecycle, starting from product design and new product introduction (NPI) to manufacturing to service and maintenance, to material handling and distribution. For example, labels were displayed on parts of a system to clarify operating instructions for a mechanic performing maintenance on a system.[258][259] Assembly lines benefited from the usage of AR. In addition to Boeing, BMW and Volkswagen were known for incorporating this technology into assembly lines for monitoring process improvements.[260][261][262] Big machines are difficult to maintain because of their multiple layers or structures. AR permits people to look through the machine as if with an x-ray, pointing them to the problem right away.[263]
 As AR technology has evolved and second and third generation AR devices come to market, the impact of AR in enterprise continues to flourish. In the Harvard Business Review, Magid Abraham and Marco Annunziata discuss how AR devices are now being used to ""boost workers' productivity on an array of tasks the first time they're used, even without prior training"".[264] They contend that ""these technologies increase productivity by making workers more skilled and efficient, and thus have the potential to yield both more economic growth and better jobs"".[264]
 Weather visualizations were the first application of augmented reality in television. It has now become common in weather casting to display full motion video of images captured in real-time from multiple cameras and other imaging devices. Coupled with 3D graphics symbols and mapped to a common virtual geospatial model, these animated visualizations constitute the first true application of AR to TV.
 AR has become common in sports telecasting. Sports and entertainment venues are provided with see-through and overlay augmentation through tracked camera feeds for enhanced viewing by the audience. Examples include the yellow ""first down"" line seen in television broadcasts of American football games showing the line the offensive team must cross to receive a first down. AR is also used in association with football and other sporting events to show commercial advertisements overlaid onto the view of the playing area. Sections of rugby fields and cricket pitches also display sponsored images. Swimming telecasts often add a line across the lanes to indicate the position of the current record holder as a race proceeds to allow viewers to compare the current race to the best performance. Other examples include hockey puck tracking and annotations of racing car performance[265] and snooker ball trajectories.[114][266]
 AR has been used to enhance concert and theater performances. For example, artists allow listeners to augment their listening experience by adding their performance to that of other bands/groups of users.[267][268][269]
 Travelers may use AR to access real-time informational displays regarding a location, its features, and comments or content provided by previous visitors. Advanced AR applications include simulations of historical events, places, and objects rendered into the landscape.[270][271][272]
 AR applications linked to geographic locations present location information by audio, announcing features of interest at a particular site as they become visible to the user.[273][274][275]
 AR systems such as Word Lens can interpret the foreign text on signs and menus and, in a user's augmented view, re-display the text in the user's language. Spoken words of a foreign language can be translated and displayed in a user's view as printed subtitles.[276][277][278]
 It has been suggested that augmented reality may be used in new methods of music production, mixing, control and visualization.[279][280][281][282]
 In a proof-of-concept project Ian Sterling, an interaction design student at California College of the Arts, and software engineer Swaroop Pal demonstrated a HoloLens app whose primary purpose is to provide a 3D spatial UI for cross-platform devices—the Android Music Player app and Arduino-controlled Fan and Light—and also allow interaction using gaze and gesture control.[283][284][285][286]
 Research by members of the CRIStAL at the University of Lille makes use of augmented reality to enrich musical performance. The ControllAR project allows musicians to augment their MIDI control surfaces with the remixed graphical user interfaces of music software.[287] The Rouages project proposes to augment digital musical instruments to reveal their mechanisms to the audience and thus improve the perceived liveness.[288] Reflets is a novel augmented reality display dedicated to musical performances where the audience acts as a 3D display by revealing virtual content on stage, which can also be used for 3D musical interaction and collaboration.[289]
 Snapchat users have access to augmented reality in the app through use of camera filters. In September 2017, Snapchat updated its app to include a camera filter that allowed users to render an animated, cartoon version of themselves called ""Bitmoji"". These animated avatars would be projected in the real world through the camera, and can be photographed or video recorded.[290] In the same month, Snapchat also announced a new feature called ""Sky Filters"" that will be available on its app. This new feature makes use of augmented reality to alter the look of a picture taken of the sky, much like how users can apply the app's filters to other pictures. Users can choose from sky filters such as starry night, stormy clouds, beautiful sunsets, and rainbow.[291]
 In a paper titled ""Death by Pokémon GO"", researchers at Purdue University's Krannert School of Management claim the game caused ""a disproportionate increase in vehicular crashes and associated vehicular damage, personal injuries, and fatalities in the vicinity of locations, called PokéStops, where users can play the game while driving.""[292] Using data from one municipality, the paper extrapolates what that might mean nationwide and concluded ""the increase in crashes attributable to the introduction of Pokémon GO is 145,632 with an associated increase in the number of injuries of 29,370 and an associated increase in the number of fatalities of 256 over the period of 6 July 2016, through 30 November 2016."" The authors extrapolated the cost of those crashes and fatalities at between $2bn and $7.3 billion for the same period. Furthermore, more than one in three surveyed advanced Internet users would like to edit out disturbing elements around them, such as garbage or graffiti.[293] They would like to even modify their surroundings by erasing street signs, billboard ads, and uninteresting shopping windows. So it seems that AR is as much a threat to companies as it is an opportunity. Although, this could be a nightmare to numerous brands that do not manage to capture consumer imaginations it also creates the risk that the wearers of augmented reality glasses may become unaware of surrounding dangers. Consumers want to use augmented reality glasses to change their surroundings into something that reflects their own personal opinions. Around two in five want to change the way their surroundings look and even how people appear to them. [citation needed]
 Next, to the possible privacy issues that are described below, overload and over-reliance issues are the biggest danger of AR. For the development of new AR-related products, this implies that the user-interface should follow certain guidelines as not to overload the user with information while also preventing the user from over-relying on the AR system such that important cues from the environment are missed.[18] This is called the virtually-augmented key.[18] Once the key is ignored, people might not desire the real world anymore.
 The concept of modern augmented reality depends on the ability of the device to record and analyze the environment in real time. Because of this, there are potential legal concerns over privacy. While the First Amendment to the United States Constitution allows for such recording in the name of public interest, the constant recording of an AR device makes it difficult to do so without also recording outside of the public domain. Legal complications would be found in areas where a right to a certain amount of privacy is expected or where copyrighted media are displayed.
 In terms of individual privacy, there exists the ease of access to information that one should not readily possess about a given person. This is accomplished through facial recognition technology. Assuming that AR automatically passes information about persons that the user sees, there could be anything seen from social media, criminal record, and marital status.[294]
 The Code of Ethics on Human Augmentation, which was originally introduced by Steve Mann in 2004 and further refined with Ray Kurzweil and Marvin Minsky in 2013, was ultimately ratified at the virtual reality Toronto conference on 25 June 2017.[295][296][297][298]
 The interaction of location-bound augmented reality with property law is largely undefined.[299][300] Several models have been analysed for how this interaction may be resolved in a common law context: an extension of real property rights to also cover augmentations on or near the property with a strong notion of trespassing, forbidding augmentations unless allowed by the owner; an 'open range' system, where augmentations are allowed unless forbidden by the owner; and a 'freedom to roam' system, where real property owners have no control over non-disruptive augmentations.[301]
 One issue experienced during the Pokémon Go craze was the game's players disturbing owners of private property while visiting nearby location-bound augmentations, which may have been on the properties or the properties may have been en route. The terms of service of Pokémon Go explicitly disclaim responsibility for players' actions, which may limit (but may not totally extinguish) the liability of its producer, Niantic, in the event of a player trespassing while playing the game: by Niantic's argument, the player is the one committing the trespass, while Niantic has merely engaged in permissible free speech. A theory advanced in lawsuits brought against Niantic is that their placement of game elements in places that will lead to trespass or an exceptionally large flux of visitors can constitute nuisance, despite each individual trespass or visit only being tenuously caused by Niantic.[302][303][304]
 Another claim raised against Niantic is that the placement of profitable game elements on land without permission of the land's owners is unjust enrichment.[305] More hypothetically, a property may be augmented with advertising or disagreeable content against its owner's wishes.[306] Under American law, these situations are unlikely to be seen as a violation of real property rights by courts without an expansion of those rights to include augmented reality (similarly to how English common law came to recognise air rights).[305]
 An article in the Michigan Telecommunications and Technology Law Review argues that there are three bases for this extension, starting with various understanding of property. The personality theory of property, outlined by Margaret Radin, is claimed to support extending property rights due to the intimate connection between personhood and ownership of property; however, her viewpoint is not universally shared by legal theorists.[307] Under the utilitarian theory of property, the benefits from avoiding the harms to real property owners caused by augmentations and the tragedy of the commons, and the reduction in transaction costs by making discovery of ownership easy, were assessed as justifying recognising real property rights as covering location-bound augmentations, though there does remain the possibility of a tragedy of the anticommons from having to negotiate with property owners slowing innovation.[308] Finally, following the 'property as the law of things' identification as supported by Thomas Merrill and Henry E Smith, location-based augmentation is naturally identified as a 'thing', and, while the non-rivalrous and ephemeral nature of digital objects presents difficulties to the excludeability prong of the definition, the article argues that this is not insurmountable.[309]
 Some attempts at legislative regulation have been made in the United States. Milwaukee County, Wisconsin attempted to regulate augmented reality games played in its parks, requiring prior issuance of a permit,[310] but this was criticised on free speech grounds by a federal judge;[311] and Illinois considered mandating a notice and take down procedure for location-bound augmentations.[312]
 An article for the Iowa Law Review observed that dealing with many local permitting processes would be arduous for a large-scale service,[313] and, while the proposed Illinois mechanism could be made workable,[314] it was reactive and required property owners to potentially continually deal with new augmented reality services; instead, a national-level geofencing registry, analogous to a do-not-call list, was proposed as the most desirable form of regulation to efficiently balance the interests of both providers of augmented reality services and real property owners.[315] An article in the Vanderbilt Journal of Entertainment and Technology Law, however, analyses a monolithic do-not-locate registry as an insufficiently flexible tool, either permitting unwanted augmentations or foreclosing useful applications of augmented reality.[316] Instead, it argues that an 'open range' model, where augmentations are permitted by default but property owners may restrict them on a case-by-case basis (and with noncompliance treated as a form of trespass), will produce the socially-best outcome.[317]
 The futuristic short film Sight[321] features contact lens-like augmented reality devices.[322][323]
  Media related to Augmented reality at Wikimedia Commons
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['the real world', 'Thomas Merrill and Henry E Smith', 'it is one of the key technologies in the reality-virtuality continuum', 'Augmented reality', 'latency and bandwidth'], 'answer_start': [], 'answer_end': []}"
"
 Globalization, or globalisation (Commonwealth English; see spelling differences), is the process of interaction and integration among people, companies, and governments worldwide. The term globalization first appeared in the early 20th century (supplanting an earlier French term mondialisation), developed its current meaning sometime in the second half of the 20th century, and came into popular use in the 1990s to describe the unprecedented international connectivity of the post-Cold War world.[1] Its origins can be traced back to 18th and 19th centuries due to advances in transportation and communications technology. This increase in global interactions has caused a growth in international trade and the exchange of ideas, beliefs, and culture. Globalization is primarily an economic process of interaction and integration that is associated with social and cultural aspects. However, disputes and international diplomacy are also large parts of the history of globalization, and of modern globalization.
 Economically, globalization involves goods, services, data, technology, and the economic resources of capital.[2] The expansion of global markets liberalizes the economic activities of the exchange of goods and funds. Removal of cross-border trade barriers has made the formation of global markets more feasible.[3] Advances in transportation, like the steam locomotive, steamship, jet engine, and container ships, and developments in telecommunication infrastructure such as the telegraph, the Internet, mobile phones, and smartphones, have been major factors in globalization and have generated further interdependence of economic and cultural activities around the globe.[4][5][6]
 Though many scholars place the origins of globalization in modern times, others trace its history to long before the European Age of Discovery and voyages to the New World, and some even to the third millennium BCE.[7] Large-scale globalization began in the 1820s, and in the late 19th century and early 20th century drove a rapid expansion in the connectivity of the world's economies and cultures.[8] The term global city was subsequently popularized by sociologist Saskia Sassen in her work The Global City: New York, London, Tokyo (1991).[9]
 In 2000, the International Monetary Fund (IMF) identified four basic aspects of globalization: trade and transactions, capital and investment movements, migration and movement of people, and the dissemination of knowledge.[10] Globalizing processes affect and are affected by business and work organization, economics, sociocultural resources, and the natural environment. Academic literature commonly divides globalization into three major areas: economic globalization, cultural globalization, and political globalization.[11]
 The word globalization was used in the English language as early as the 1930s, but only in the context of education, and the term failed to gain traction. Over the next few decades, the term was occasionally used by other scholars and media, but it was not clearly defined.[1] One of the first usages of the term in the meaning resembling the later, common usage was by French economist François Perroux in his essays from the early 1960s (in his French works he used the term ""mondialisation"" (literarly worldization in French), also translated as mundialization).[1] Theodore Levitt is often credited with popularizing the term and bringing it into the mainstream business audience in the later in the middle of 1980s.[1]
 Though often treated as synonyms, in French, globalization is seen as a stage following mondialisation, a stage that implies the dissolution of national identities and the abolishment of borders inside the world network of economic exchanges.[12]
 Since its inception, the concept of globalization has inspired competing definitions and interpretations. Its antecedents date back to the great movements of trade and empire across Asia and the Indian Ocean from the 15th century onward.[13][14]
 In 1848, Karl Marx noticed the increasing level of national inter-dependence brought on by capitalism, and predicted the universal character of the modern world society. He states:
 The bourgeoisie has through its exploitation of the world market given a cosmopolitan character to production and consumption in every country. To the great chagrin of Reactionists, it has drawn from under the feet of industry the national ground on which it stood. All old-established national industries have been destroyed or are daily being destroyed. . . . In place of the old local and national seclusion and self-sufficiency, we have intercourse in every direction, universal inter-dependence of nations.[15] Sociologists Martin Albrow and Elizabeth King define globalization as ""all those processes by which the people of the world are incorporated into a single world society.""[2] In The Consequences of Modernity, Anthony Giddens writes: ""Globalization can thus be defined as the intensification of worldwide social relations which link distant localities in such a way that local happenings are shaped by events occurring many miles away and vice versa.""[16] In 1992, Roland Robertson, professor of sociology at the University of Aberdeen and an early writer in the field, described globalization as ""the compression of the world and the intensification of the consciousness of the world as a whole.""[17]
 In Global Transformations, David Held and his co-writers state:
 Although in its simplistic sense globalization refers to the widening, deepening and speeding up of global interconnection, such a definition begs further elaboration. ... Globalization can be on a continuum with the local, national and regional. At one end of the continuum lie social and economic relations and networks which are organized on a local and/or national basis; at the other end lie social and economic relations and networks which crystallize on the wider scale of regional and global interactions. Globalization can refer to those spatial-temporal processes of change which underpin a transformation in the organization of human affairs by linking together and expanding human activity across regions and continents. Without reference to such expansive spatial connections, there can be no clear or coherent formulation of this term. ... A satisfactory definition of globalization must capture each of these elements: extensity (stretching), intensity, velocity and impact.[18] Held and his co-writers' definition of globalization in that same book as ""transformation in the spatial organization of social relations and transactions—assessed in terms of their extensity, intensity, velocity and impact—generating transcontinental or inter-regional flows"" was called ""probably the most widely-cited definition"" in the 2014 DHL Global Connectiveness Index.[19]
 Swedish journalist Thomas Larsson, in his book The Race to the Top: The Real Story of Globalization, states that globalization:
 ...is the process of world shrinkage, of distances getting shorter, things moving closer. It pertains to the increasing ease with which somebody on one side of the world can interact, to mutual benefit, with somebody on the other side of the world.[20] Paul James defines globalization with a more direct and historically contextualized emphasis:
 Globalization is the extension of social relations across world-space, defining that world-space in terms of the historically variable ways that it has been practiced and socially understood through changing world-time.[21] Manfred Steger, professor of global studies and research leader in the Global Cities Institute at RMIT University, identifies four main empirical dimensions of globalization: economic, political, cultural, and ecological. A fifth dimension—the ideological—cutting across the other four. The ideological dimension, according to Steger, is filled with a range of norms, claims, beliefs, and narratives about the phenomenon itself.[22]
 James and Steger stated that the concept of globalization ""emerged from the intersection of four interrelated sets of 'communities of practice' (Wenger, 1998): academics, journalists, publishers/editors, and librarians.""[1]: 424  They note the term was used ""in education to describe the global life of the mind""; in international relations to describe the extension of the European Common Market, and in journalism to describe how the ""American Negro and his problem are taking on a global significance"".[1] They have also argued that four forms of globalization can be distinguished that complement and cut across the solely empirical dimensions.[21][23] According to James, the oldest dominant form of globalization is embodied globalization, the movement of people. A second form is agency-extended globalization, the circulation of agents of different institutions, organizations, and polities, including imperial agents. Object-extended globalization, a third form, is the movement of commodities and other objects of exchange. He calls the transmission of ideas, images, knowledge, and information across world-space disembodied globalization, maintaining that it is currently the dominant form of globalization. James holds that this series of distinctions allows for an understanding of how, today, the most embodied forms of globalization such as the movement of refugees and migrants are increasingly restricted, while the most disembodied forms such as the circulation of financial instruments and codes are the most deregulated.[24]
 The journalist Thomas L. Friedman popularized the term ""flat world"", arguing that globalized trade, outsourcing, supply-chaining, and political forces had permanently changed the world, for better and worse. He asserted that the pace of globalization was quickening and that its impact on business organization and practice would continue to grow.[25]
 Economist Takis Fotopoulos defined ""economic globalization"" as the opening and deregulation of commodity, capital, and labor markets that led toward present neoliberal globalization. He used ""political globalization"" to refer to the emergence of a transnational élite and a phasing out of the nation-state. Meanwhile, he used ""cultural globalization"" to reference the worldwide homogenization of culture. Other of his usages included ""ideological globalization"", ""technological globalization"", and ""social globalization"".[26]
 Lechner and Boli (2012) define globalization as more people across large distances becoming connected in more and different ways.[27]
 ""Globophobia"" is used to refer to the fear of globalization, though it can also mean the fear of balloons.[28][29][30]
 There are both distal and proximate causes which can be traced in the historical factors affecting globalization. Large-scale globalization began in the 19th century.[31]
 Archaic globalization conventionally refers to a phase in the history of globalization including globalizing events and developments from the time of the earliest civilizations until roughly the 1600s. This term is used to describe the relationships between communities and states and how they were created by the geographical spread of ideas and social norms at both local and regional levels.[32]
 In this schema, three main prerequisites are posited for globalization to occur. The first is the idea of Eastern Origins, which shows how Western states have adapted and implemented learned principles from the East.[32] Without the spread of traditional ideas from the East, Western globalization would not have emerged the way it did. The interactions of states were not on a global scale and most often were confined to Asia, North Africa, the Middle East, and certain parts of Europe.[32] With early globalization, it was difficult for states to interact with others that were not close. Eventually, technological advances allowed states to learn of others' existence and thus another phase of globalization can occur. The third has to do with inter-dependency, stability, and regularity. If a state is not dependent on another, then there is no way for either state to be mutually affected by the other. This is one of the driving forces behind global connections and trade; without either, globalization would not have emerged the way it did and states would still be dependent on their own production and resources to work. This is one of the arguments surrounding the idea of early globalization. It is argued that archaic globalization did not function in a similar manner to modern globalization because states were not as interdependent on others as they are today.[32]
 Also posited is a ""multi-polar"" nature to archaic globalization, which involved the active participation of non-Europeans. Because it predated the Great Divergence in the nineteenth century, where Western Europe pulled ahead of the rest of the world in terms of industrial production and economic output, archaic globalization was a phenomenon that was driven not only by Europe but also by other economically developed Old World centers such as Gujarat, Bengal, coastal China, and Japan.[33]
 The German historical economist and sociologist Andre Gunder Frank argues that a form of globalization began with the rise of trade links between Sumer and the Indus Valley civilization in the third millennium BCE. This archaic globalization existed during the Hellenistic Age, when commercialized urban centers enveloped the axis of Greek culture that reached from India to Spain, including Alexandria and the other Alexandrine cities. Early on, the geographic position of Greece and the necessity of importing wheat forced the Greeks to engage in maritime trade. Trade in ancient Greece was largely unrestricted: the state controlled only the supply of grain.[7]
 Trade on the Silk Road was a significant factor in the development of civilizations from China, the Indian subcontinent, Persia, Europe, and Arabia, opening long-distance political and economic interactions between them.[34] Though silk was certainly the major trade item from China, common goods such as salt and sugar were traded as well; and religions, syncretic philosophies, and various technologies, as well as diseases, also traveled along the Silk Routes. In addition to economic trade, the Silk Road served as a means of carrying out cultural trade among the civilisations along its network.[35] The movement of people, such as refugees, artists, craftsmen, missionaries, robbers, and envoys, resulted in the exchange of religions, art, languages, and new technologies.[36]
 ""Early modern"" or ""proto-globalization"" covers a period of the history of globalization roughly spanning the years between 1600 and 1800. The concept of ""proto-globalization"" was first introduced by historians A. G. Hopkins and Christopher Bayly. The term describes the phase of increasing trade links and cultural exchange that characterized the period immediately preceding the advent of high ""modern globalization"" in the late 19th century.[37] This phase of globalization was characterized by the rise of maritime European empires, in the 15th and 17th centuries, first the Portuguese Empire (1415) followed by the Spanish Empire (1492), and later the Dutch and British Empires. In the 17th century, world trade developed further when chartered companies like the British East India Company (founded in 1600) and the Dutch East India Company (founded in 1602, often described as the first multinational corporation in which stock was offered) were established.[38]
 An alternative view from historians Dennis Flynn and Arturo Giraldez, postulated that: globalization began with the first circumnavigation of the globe under the Magellan-Elcano expedition which preluded the rise of global silver trade.[39][40]
 Early modern globalization is distinguished from modern globalization on the basis of expansionism, the method of managing global trade, and the level of information exchange. The period is marked by the shift of hegemony to Western Europe, the rise of larger-scale conflicts between powerful nations such as the Thirty Years' War, and demand for commodities, most particularly slaves. The triangular trade made it possible for Europe to take advantage of resources within the Western Hemisphere. The transfer of animal stocks, plant crops, and epidemic diseases associated with Alfred W. Crosby's concept of the Columbian exchange also played a central role in this process. European, Middle Eastern, Indian, Southeast Asian, and Chinese merchants were all involved in early modern trade and communications, particularly in the Indian Ocean region.
 According to economic historians Kevin H. O'Rourke, Leandro Prados de la Escosura, and Guillaume Daudin, several factors promoted globalization in the period 1815–1870:[41]
 During the 19th century, globalization approached its form as a direct result of the Industrial Revolution. Industrialization allowed standardized production of household items using economies of scale while rapid population growth created sustained demand for commodities. In the 19th century, steamships reduced the cost of international transportation significantly and railroads made inland transportation cheaper. The transportation revolution occurred some time between 1820 and 1850.[31] More nations embraced international trade.[31] Globalization in this period was decisively shaped by nineteenth-century imperialism such as in Africa and Asia. The invention of shipping containers in 1956 helped advance the globalization of commerce.[42][43]
 After World War II, work by politicians led to the agreements of the Bretton Woods Conference, in which major governments laid down the framework for international monetary policy, commerce, and finance, and the founding of several international institutions intended to facilitate economic growth by lowering trade barriers. Initially, the General Agreement on Tariffs and Trade (GATT) led to a series of agreements to remove trade restrictions. GATT's successor was the World Trade Organization (WTO), which provided a framework for negotiating and formalizing trade agreements and a dispute resolution process. Exports nearly doubled from 8.5% of total gross world product in 1970 to 16.2% in 2001.[44] The approach of using global agreements to advance trade stumbled with the failure of the Doha Development Round of trade negotiation. Many countries then shifted to bilateral or smaller multilateral agreements, such as the 2011 United States–Korea Free Trade Agreement.
 Since the 1970s, aviation has become increasingly affordable to middle classes in developed countries. Open skies policies and low-cost carriers have helped to bring competition to the market. In the 1990s, the growth of low-cost communication networks cut the cost of communicating between countries. More work can be performed using a computer without regard to location. This included accounting, software development, and engineering design.
 Student exchange programs became popular after World War II, and are intended to increase the participants' understanding and tolerance of other cultures, as well as improving their language skills and broadening their social horizons. Between 1963 and 2006 the number of students studying in a foreign country increased 9 times.[45]
 Since the 1980s, modern globalization has spread rapidly through the expansion of capitalism and neoliberal ideologies.[46] The implementation of neoliberal policies has allowed for the privatization of public industry, deregulation of laws or policies that interfered with the free flow of the market, as well as cut-backs to governmental social services.[47] These neoliberal policies were introduced to many developing countries in the form of structural adjustment programs (SAPs) that were implemented by the World Bank and the International Monetary Fund (IMF).[46] These programs required that the country receiving monetary aid would open its markets to capitalism, privatize public industry, allow free trade, cut social services like healthcare and education and allow the free movement of giant multinational corporations.[48] These programs allowed the World Bank and the IMF to become global financial market regulators that would promote neoliberalism and the creation of free markets for multinational corporations on a global scale.[49]
 In the late 19th and early 20th century, the connectedness of the world's economies and cultures grew very quickly. This slowed down from the 1910s onward due to the World Wars and the Cold War,[50] but picked up again in the 1980s and 1990s.[51] The revolutions of 1989 and subsequent liberalization in many parts of the world resulted in a significant expansion of global interconnectedness. The migration and movement of people can also be highlighted as a prominent feature of the globalization process. In the period between 1965 and 1990, the proportion of the labor force migrating approximately doubled. Most migration occurred between the developing countries and least developed countries (LDCs).[52] As economic integration intensified workers moved to areas with higher wages and most of the developing world oriented toward the international market economy. The collapse of the Soviet Union not only ended the Cold War's division of the world – it also left the United States its sole policeman and an unfettered advocate of free market.[according to whom?] It also resulted in the growing prominence of attention focused on the movement of diseases, the proliferation of popular culture and consumer values, the growing prominence of international institutions like the UN, and concerted international action on such issues as the environment and human rights.[53] Other developments as dramatic were the Internet's becoming influential in connecting people across the world; As of June 2012[update], more than 2.4 billion people—over a third of the world's human population—have used the services of the Internet.[54][55] Growth of globalization has never been smooth. One influential event was the late 2000s recession, which was associated with lower growth (in areas such as cross-border phone calls and Skype usage) or even temporarily negative growth (in areas such as trade) of global interconnectedness.[56][57]
 The China–United States trade war, starting in 2018, negatively affected trade between the two largest national economies. The economic impact of the COVID-19 pandemic included a massive decline in tourism and international business travel as many countries temporarily closed borders. The 2021–2022 global supply chain crisis resulted from temporary shutdowns of manufacturing and transportation facilities, and labor shortages. Supply problems incentivized some switches to domestic production.[58] The economic impact of the 2022 Russian invasion of Ukraine included a blockade of Ukrainian ports and international sanctions on Russia, resulting in some de-coupling of the Russian economy with global trade, especially with the European Union and other Western countries.
 Modern consensus for the last 15 years regards globalization as having run its course and gone into decline.[59] A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the global financial crisis. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum.[60][61]
 Economic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital.[63] Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market.[64] Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes[65] competition, technology, and corporations and industries.[63]
 Current globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration.[66]
 International standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization.[42] International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.
 A multinational corporation, or worldwide enterprise,[67] is an organization that owns or controls the production of goods or services in one or more countries other than their home country.[68] It can also be referred to as an international corporation, a transnational corporation, or a stateless corporation.[69]
 A free-trade area is the region encompassing a trade bloc whose member countries have signed a free-trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers –  import quotas and tariffs –  and to increase trade of goods and services with each other.[70]
 If people are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border. Arguably, the most significant free-trade area in the world is the European Union, a politico-economic union of 27 member states that are primarily located in Europe. The EU has developed European Single Market through a standardized system of laws that apply in all member states. EU policies aim to ensure the free movement of people, goods, services, and capital within the internal market,[71]
 Trade facilitation looks at how procedures and controls governing the movement of goods across national borders can be improved to reduce associated cost burdens and maximize efficiency while safeguarding legitimate regulatory objectives.
 Global trade in services is also significant. For example, in India, business process outsourcing has been described as the ""primary engine of the country's development over the next few decades, contributing broadly to GDP growth, employment growth, and poverty alleviation"".[72][73]
 William I. Robinson's theoretical approach to globalization is a critique of Wallerstein's World Systems Theory. He believes that the global capital experienced today is due to a new and distinct form of globalization which began in the 1980s. Robinson argues not only are economic activities expanded across national boundaries but also there is a transnational fragmentation of these activities.[74] One important aspect of Robinson's globalization theory is that production of goods are increasingly global. This means that one pair of shoes can be produced by six countries, each contributing to a part of the production process.
 Cultural globalization refers to the transmission of ideas, meanings, and values around the world in such a way as to extend and intensify social relations.[75] This process is marked by the common consumption of cultures that have been diffused by the Internet, popular culture media, and international travel. This has added to processes of commodity exchange and colonization which have a longer history of carrying cultural meaning around the globe. The circulation of cultures enables individuals to partake in extended social relations that cross national and regional borders. The creation and expansion of such social relations is not merely observed on a material level. Cultural globalization involves the formation of shared norms and knowledge with which people associate their individual and collective cultural identities. It brings increasing interconnectedness among different populations and cultures.[76]
 Cross-cultural communication is a field of study that looks at how people from differing cultural backgrounds communicate, in similar and different ways among themselves, and how they endeavour to communicate across cultures. Intercultural communication is a related field of study.
 Cultural diffusion is the spread of cultural items—such as ideas, styles, religions, technologies, languages etc.
Cultural globalization has increased cross-cultural contacts, but may be accompanied by a decrease in the uniqueness of once-isolated communities. For example, sushi is available in Germany as well as Japan, but Euro-Disney outdraws the city of Paris, potentially reducing demand for ""authentic"" French pastry.[77][78][79] Globalization's contribution to the alienation of individuals from their traditions may be modest compared to the impact of modernity itself, as alleged by existentialists such as Jean-Paul Sartre and Albert Camus. Globalization has expanded recreational opportunities by spreading pop culture, particularly via the Internet and satellite television. The cultural diffusion can create a homogenizing force, where globalisation is seen as synonymous with homogenizing force via connectedness of markets, cultures, politics and the desire for modernizations through imperial countries sphere of influence.[80]
 Religions were among the earliest cultural elements to globalize, being spread by force, migration, evangelists, imperialists, and traders. Christianity, Islam, Buddhism, and more recently sects such as Mormonism are among those religions which have taken root and influenced endemic cultures in places far from their origins.[81]
 Globalization has strongly influenced sports.[82] For example, the modern Olympic Games has athletes from more than 200 nations participating in a variety of competitions.[83] The FIFA World Cup is the most widely viewed and followed sporting event in the world, exceeding even the Olympic Games; a ninth of the entire population of the planet watched the 2006 FIFA World Cup Final.[84][85][86]
 The term globalization implies transformation. Cultural practices including traditional music can be lost or turned into a fusion of traditions. Globalization can trigger a state of emergency for the preservation of musical heritage. Archivists may attempt to collect, record, or transcribe repertoires before melodies are assimilated or modified, while local musicians may struggle for authenticity and to preserve local musical traditions. Globalization can lead performers to discard traditional instruments. Fusion genres can become interesting fields of analysis.[87]
 Music has an important role in economic and cultural development during globalization. Music genres such as jazz and reggae began locally and later became international phenomena. Globalization gave support to the world music phenomenon by allowing music from developing countries to reach broader audiences.[88] Though the term ""World Music"" was originally intended for ethnic-specific music, globalization is now expanding its scope such that the term often includes hybrid subgenres such as ""world fusion"", ""global fusion"", ""ethnic fusion"",[89] and worldbeat.[90][91]
 Bourdieu claimed that the perception of consumption can be seen as self-identification and the formation of identity. Musically, this translates into each individual having their own musical identity based on likes and tastes. These likes and tastes are greatly influenced by culture, as this is the most basic cause for a person's wants and behavior. The concept of one's own culture is now in a period of change due to globalization. Also, globalization has increased the interdependency of political, personal, cultural, and economic factors.[93]
 A 2005 UNESCO report[94] showed that cultural exchange is becoming more frequent from Eastern Asia, but that Western countries are still the main exporters of cultural goods. In 2002, China was the third largest exporter of cultural goods, after the UK and US. Between 1994 and 2002, both North America's and the European Union's shares of cultural exports declined while Asia's cultural exports grew to surpass North America. Related factors are the fact that Asia's population and area are several times that of North America. Americanization is related to a period of high political American clout and of significant growth of America's shops, markets and objects being brought into other countries.
 Some critics of globalization argue that it harms the diversity of cultures. As a dominating country's culture is introduced into a receiving country through globalization, it can become a threat to the diversity of local culture. Some argue that globalization may ultimately lead to Westernization or Americanization of culture, where the dominating cultural concepts of economically and politically powerful Western countries spread and cause harm to local cultures.[95]
 Globalization is a diverse phenomenon that relates to a multilateral political world and to the increase of cultural objects and markets between countries. The Indian experience particularly reveals the plurality of the impact of cultural globalization.[96]
 Transculturalism is defined as ""seeing oneself in the other"".[97] Transcultural[98] is in turn described as ""extending through all human cultures""[98] or ""involving, encompassing, or combining elements of more than one culture"".[99] Children brought up in transcultural backgrounds are sometimes called third-culture kids.
 Political globalization refers to the growth of the worldwide political system, both in size and complexity. That system includes national governments, their governmental and intergovernmental organizations as well as government-independent elements of global civil society such as international non-governmental organizations and social movement organizations. One of the key aspects of the political globalization is the declining importance of the nation-state and the rise of other actors on the political scene.
William R. Thompson has defined it as ""the expansion of a global political system, and its institutions, in which inter-regional transactions (including, but certainly not limited to trade) are managed"".[100] 
Political globalization is one of the three main dimensions of globalization commonly found in academic literature, with the two other being economic globalization and cultural globalization.[11]
 Intergovernmentalism is a term in political science with two meanings. The first refers to a theory of regional integration originally proposed by Stanley Hoffmann; the second treats states and the national government as the primary factors for integration. Multi-level governance is an approach in political science and public administration theory that originated from studies on European integration. Multi-level governance gives expression to the idea that there are many interacting authority structures at work in the emergent global political economy. It illuminates the intimate entanglement between the domestic and international levels of authority.
 Some people are citizens of multiple nation-states. Multiple citizenship, also called dual citizenship or multiple nationality or dual nationality, is a person's citizenship status, in which a person is concurrently regarded as a citizen of more than one state under the laws of those states.
 Increasingly, non-governmental organizations influence public policy across national boundaries, including humanitarian aid and developmental efforts.[102] Philanthropic organizations with global missions are also coming to the forefront of humanitarian efforts; charities such as the Bill and Melinda Gates Foundation, Accion International, the Acumen Fund (now Acumen) and the Echoing Green have combined the business model with philanthropy, giving rise to business organizations such as the Global Philanthropy Group and new associations of philanthropists such as the Global Philanthropy Forum. The Bill and Melinda Gates Foundation projects include a current multibillion-dollar commitment to funding immunizations in some of the world's more impoverished but rapidly growing countries.[103] The Hudson Institute estimates total private philanthropic flows to developing countries at US$59 billion in 2010.[104]
 As a response to globalization, some countries have embraced isolationist policies. For example, the North Korean government makes it very difficult for foreigners to enter the country and strictly monitors their activities when they do. Aid workers are subject to considerable scrutiny and excluded from places and regions the government does not wish them to enter. Citizens cannot freely leave the country.[105][106]
 Globalization has been a gendered process where giant multinational corporations have outsourced jobs to low-wage, low skilled, quota free economies like the ready made garment industry in Bangladesh where poor women make up the majority of labor force. Despite a large proportion of women workers in the garment industry, women are still heavily underemployed compared to men. Most women that are employed in the garment industry come from the countryside of Bangladesh triggering migration of women in search of garment work. It is still unclear as to whether or not access to paid work for women where it did not exist before has empowered them. The answers varied depending on whether it is the employers perspective or the workers and how they view their choices. Women workers did not see the garment industry as economically sustainable for them in the long run due to long hours standing and poor working conditions. Although women workers did show significant autonomy over their personal lives including their ability to negotiate with family, more choice in marriage, and being valued as a wage earner in the family. This did not translate into workers being able to collectively organize themselves in order to negotiate a better deal for themselves at work.[107]
 Another example of outsourcing in manufacturing includes the maquiladora industry in Ciudad Juarez, Mexico where poor women make up the majority of the labor force. Women in the maquiladora industry have produced high levels of turnover not staying long enough to be trained compared to men. A gendered two tiered system within the maquiladora industry has been created that focuses on training and worker loyalty. Women are seen as being untrainable, placed in un-skilled, low wage jobs, while men are seen as more trainable with less turnover rates, and placed in more high skilled technical jobs. The idea of training has become a tool used against women to blame them for their high turnover rates which also benefit the industry keeping women as temporary workers.[108]
 Scholars also occasionally discuss other, less common dimensions of globalization, such as environmental globalization (the internationally coordinated practices and regulations, often in the form of international treaties, regarding environmental protection)[109] or military globalization (growth in global extent and scope of security relationships).[110] Those dimensions, however, receive much less attention the three described above, as academic literature commonly subdivides globalization into three major areas: economic globalization, cultural globalization and political globalization.[11]
 An essential aspect of globalization is movement of people, and state-boundary limits on that movement have changed across history.[111] The movement of tourists and business people opened up over the last century. As transportation technology improved, travel time and costs decreased dramatically between the 18th and early 20th century. For example, travel across the Atlantic Ocean used to take up to 5 weeks in the 18th century, but around the time of the 20th century it took a mere 8 days.[112] Today, modern aviation has made long-distance transportation quick and affordable.
 Tourism is travel for pleasure. The developments in technology and transportation infrastructure, such as jumbo jets, low-cost airlines, and more accessible airports have made many types of tourism more affordable. At any given moment half a million people are in the air.[113] International tourist arrivals surpassed the milestone of 1 billion tourists globally for the first time in 2012.[114]
A visa is a conditional authorization granted by a country to a foreigner, allowing them to enter and temporarily remain within, or to leave that country. Some countries – such as those in the Schengen Area – have agreements with other countries allowing each other's citizens to travel between them without visas (for example, Switzerland is part of a Schengen Agreement allowing easy travel for people from countries within the European Union). The World Tourism Organization announced that the number of tourists who require a visa before traveling was at its lowest level ever in 2015.[115]
 Immigration is the international movement of people into a destination country of which they are not natives or where they do not possess citizenship in order to settle or reside there, especially as permanent residents or naturalized citizens, or to take-up employment as a migrant worker or temporarily as a foreign worker.[116][117][118] According to the International Labour Organization, as of 2014[update] there were an estimated 232 million international migrants in the world (defined as persons outside their country of origin for 12 months or more) and approximately half of them were estimated to be economically active (i.e. being employed or seeking employment).[119] International movement of labor is often seen as important to economic development. For example, freedom of movement for workers in the European Union means that people can move freely between member states to live, work, study or retire in another country.
 Globalization is associated with a dramatic rise in international education. The development of global cross-cultural competence in the workforce through ad-hoc training has deserved increasing attention in recent times.[121][122] More and more students are seeking higher education in foreign countries and many international students now consider overseas study a stepping-stone to permanent residency within a country.[123] The contributions that foreign students make to host nation economies, both culturally and financially has encouraged major players to implement further initiatives to facilitate the arrival and integration of overseas students, including substantial amendments to immigration and visa policies and procedures.[45]
 A transnational marriage is a marriage between two people from different countries. A variety of special issues arise in marriages between people from different countries, including those related to citizenship and culture, which add complexity and challenges to these kinds of relationships. In an age of increasing globalization, where a growing number of people have ties to networks of people and places across the globe, rather than to a current geographic location, people are increasingly marrying across national boundaries. Transnational marriage is a by-product of the movement and migration of people.
 Before electronic communications, long-distance communications relied on mail. Speed of global communications was limited by the maximum speed of courier services (especially horses and ships) until the mid-19th century. The electric telegraph was the first method of instant long-distance communication. For example, before the first transatlantic cable, communications between Europe and the Americas took weeks because ships had to carry mail across the ocean. The first transatlantic cable reduced communication time considerably, allowing a message and a response in the same day. Lasting transatlantic telegraph connections were achieved in the 1865–1866. The first wireless telegraphy transmitters were developed in 1895.
 The Internet has been instrumental in connecting people across geographical boundaries. For example, Facebook is a social networking service which has more than 1.65 billion monthly active users as of 31 March 2016[update].[125]
 Globalization can be spread by Global journalism which provides massive information and relies on the internet to interact, ""makes it into an everyday routine to investigate how people and their actions, practices, problems, life conditions, etc. in different parts of the world are interrelated. possible to assume that global threats such as climate change precipitate the further establishment of global journalism.""[126]
 In the current era of globalization, the world is more interdependent than at any other time. Efficient and inexpensive transportation has left few places inaccessible, and increased global trade has brought more and more people into contact with animal diseases that have subsequently jumped species barriers (see zoonosis).[127]
 Coronavirus disease 2019, abbreviated COVID-19, first appeared in Wuhan, China in November 2019. More than 180 countries have reported cases since then.[128] As of April 6, 2020[update], the U.S. has the most confirmed active cases in the world.[129] More than 3.4 million people from the worst-affected countries entered the U.S. in the first three months since the inception of the COVID-19 pandemic.[130] This has caused a detrimental impact on the global economy, particularly for SME's and Microbusinesses with unlimited liability/self-employed, leaving them vulnerable to financial difficulties, increasing the market share for oligopolistic markets as well as increasing the barriers of entry.
 One index of globalization is the KOF Index of Globalization, which measures three important dimensions of globalization: economic, social, and political.[131] Another is the A.T. Kearney / Foreign Policy Magazine Globalization Index.[132]
 Measurements of economic globalization typically focus on variables such as trade, Foreign Direct Investment (FDI), Gross Domestic Product (GDP), portfolio investment, and income. However, newer indices attempt to measure globalization in more general terms, including variables related to political, social, cultural, and even environmental aspects of globalization.[133][134]
 The DHL Global Connectedness Index studies four main types of cross-border flow: trade (in both goods and services), information, people (including tourists, students, and migrants), and capital. It shows that the depth of global integration fell by about one-tenth after 2008, but by 2013 had recovered well above its pre-crash peak.[19][56] The report also found a shift of economic activity to emerging economies.[19]
 Reactions to processes contributing to globalization have varied widely with a history as long as extraterritorial contact and trade. Philosophical differences regarding the costs and benefits of such processes give rise to a broad-range of ideologies and social movements. Proponents of economic growth, expansion, and development, in general, view globalizing processes as desirable or necessary to the well-being of human society.[135]
 Antagonists view one or more globalizing processes as detrimental to social well-being on a global or local scale;[135] this includes those who focus on social or natural sustainability of long-term and continuous economic expansion, the social structural inequality caused by these processes, and the colonial, imperialistic, or hegemonic ethnocentrism, cultural assimilation and cultural appropriation that underlie such processes.
 Globalization tends to bring people into contact with foreign people and cultures. Xenophobia is the fear of that which is perceived to be foreign or strange.[136][137] Xenophobia can manifest itself in many ways involving the relations and perceptions of an ingroup towards an outgroup, including a fear of losing identity, suspicion of its activities, aggression, and desire to eliminate its presence to secure a presumed purity.[138]
 Critiques of globalization generally stem from discussions surrounding the impact of such processes on the planet as well as the human costs. They challenge directly traditional metrics, such as GDP, and look to other measures, such as the Gini coefficient[139] or the Happy Planet Index,[140] and point to a ""multitude of interconnected fatal consequences–social disintegration, a breakdown of democracy, more rapid and extensive deterioration of the environment, the spread of new diseases, increasing poverty and alienation""[141] which they claim are the unintended consequences of globalization. Others point out that, while the forces of globalization have led to the spread of western-style democracy, this has been accompanied by an increase in inter-ethnic tension and violence as free market economic policies combine with democratic processes of universal suffrage as well as an escalation in militarization to impose democratic principles and as a means to conflict resolution.[142]
 On 9 August 2019, Pope Francis denounced isolationism and hinted that the Catholic Church will embrace globalization at the October 2019 Amazonia Synod, stating ""the whole is greater than the parts. Globalization and unity should not be conceived as a sphere, but as a polyhedron: each people retains its identity in unity with others""[143]
 As a complex and multifaceted phenomenon, globalization is considered by some as a form of capitalist expansion which entails the integration of local and national economies into a global, unregulated market economy.[144] A 2005 study by Peer Fis and Paul Hirsch found a large increase in articles negative towards globalization in the years prior. In 1998, negative articles outpaced positive articles by two to one.[145] The number of newspaper articles showing negative framing rose from about 10% of the total in 1991 to 55% of the total in 1999. This increase occurred during a period when the total number of articles concerning globalization nearly doubled.[145]
 A number of international polls have shown that residents of Africa and Asia tend to view globalization more favorably than residents of Europe or North America. In Africa, a Gallup poll found that 70% of the population views globalization favorably.[146] The BBC found that 50% of people believed that economic globalization was proceeding too rapidly, while 35% believed it was proceeding too slowly.[147]
 In 2004, Philip Gordon stated that ""a clear majority of Europeans believe that globalization can enrich their lives, while believing the European Union can help them take advantage of globalization's benefits while shielding them from its negative effects."" The main opposition consisted of socialists, environmental groups, and nationalists. Residents of the EU did not appear to feel threatened by globalization in 2004. The EU job market was more stable and workers were less likely to accept wage/benefit cuts. Social spending was much higher than in the US.[148] In a Danish poll in 2007, 76% responded that globalization is a good thing.[149]
 Fiss, et al., surveyed US opinion in 1993. Their survey showed that, in 1993, more than 40% of respondents were unfamiliar with the concept of globalization. When the survey was repeated in 1998, 89% of the respondents had a polarized view of globalization as being either good or bad. At the same time, discourse on globalization, which began in the financial community before shifting to a heated debate between proponents and disenchanted students and workers. Polarization increased dramatically after the establishment of the WTO in 1995; this event and subsequent protests led to a large-scale anti-globalization movement.[145]
Initially, college educated workers were likely to support globalization. Less educated workers, who were more likely to compete with immigrants and workers in developing countries, tended to be opponents. The situation changed after the financial crisis of 2007. According to a 1997 poll 58% of college graduates said globalization had been good for the US. By 2008 only 33% thought it was good. Respondents with high school education also became more opposed.[150]
 According to Takenaka Heizo and Chida Ryokichi, as of 1998[update] there was a perception in Japan that the economy was ""Small and Frail"". However, Japan was resource-poor and used exports to pay for its raw materials. Anxiety over their position caused terms such as internationalization and globalization to enter everyday language. However, Japanese tradition was to be as self-sufficient as possible, particularly in agriculture.[151]
 Many in developing countries see globalization as a positive force that lifts them out of poverty.[152] Those opposing globalization typically combine environmental concerns with nationalism. Opponents consider governments as agents of neo-colonialism that are subservient to multinational corporations.[153] Much of this criticism comes from the middle class; the Brookings Institution suggested this was because the middle class perceived upwardly mobile low-income groups as threatening to their economic security.[154]
 The literature analyzing the economics of free trade is extremely rich with extensive work having been done on the theoretical and empirical effects. Though it creates winners and losers, the broad consensus among economists is that free trade is a large and unambiguous net gain for society.[155][156] In a 2006 survey of 83 American economists, ""87.5% agree that the U.S. should eliminate remaining tariffs and other barriers to trade"" and ""90.1% disagree with the suggestion that the U.S. should restrict employers from outsourcing work to foreign countries.""[157]
 Quoting Harvard economics professor N. Gregory Mankiw, ""Few propositions command as much consensus among professional economists as that open world trade increases economic growth and raises living standards.""[158] In a survey of leading economists, none disagreed with the notion that ""freer trade improves productive efficiency and offers consumers better choices, and in the long run these gains are much larger than any effects on employment.""[159] Most economists would agree that although increasing returns to scale might mean that certain industry could settle in a geographical area without any strong economic reason derived from comparative advantage, this is not a reason to argue against free trade because the absolute level of output enjoyed by both ""winner"" and ""loser"" will increase with the ""winner"" gaining more than the ""loser"" but both gaining more than before in an absolute level.
 In the book The End of Poverty, Jeffrey Sachs discusses how many factors can affect a country's ability to enter the world market, including government corruption; legal and social disparities based on gender, ethnicity, or caste; diseases such as AIDS and malaria; lack of infrastructure (including transportation, communications, health, and trade); unstable political landscapes; protectionism; and geographic barriers.[160] Jagdish Bhagwati, a former adviser to the U.N. on globalization, holds that, although there are obvious problems with overly rapid development, globalization is a very positive force that lifts countries out of poverty by causing a virtuous economic cycle associated with faster economic growth.[152] However, economic growth does not necessarily mean a reduction in poverty; in fact, the two can coexist. Economic growth is conventionally measured using indicators such as GDP and GNI that do not accurately reflect the growing disparities in wealth.[161] Additionally, Oxfam International argues that poor people are often excluded from globalization-induced opportunities ""by a lack of productive assets, weak infrastructure, poor education and ill-health;""[162] effectively leaving these marginalized groups in a poverty trap. Economist Paul Krugman is another staunch supporter of globalization and free trade with a record of disagreeing with many critics of globalization. He argues that many of them lack a basic understanding of comparative advantage and its importance in today's world.[163]
 The flow of migrants to advanced economies has been claimed to provide a means through which global wages converge. An IMF study noted a potential for skills to be transferred back to developing countries as wages in those a countries rise.[10] Lastly, the dissemination of knowledge has been an integral aspect of globalization. Technological innovations (or technological transfer) are conjectured to benefit most developing and least developing countries (LDCs), as for example in the adoption of mobile phones.[52]
 There has been a rapid economic growth in Asia after embracing market orientation-based economic policies that encourage private property rights, free enterprise and competition. In particular, in East Asian developing countries, GDP per head rose at 5.9% a year from 1975 to 2001 (according to 2003 Human Development Report[164] of UNDP). Like this, the British economic journalist Martin Wolf says that incomes of poor developing countries, with more than half the world's population, grew substantially faster than those of the world's richest countries that remained relatively stable in its growth, leading to reduced international inequality and the incidence of poverty.
 Certain demographic changes in the developing world after active economic liberalization and international integration resulted in rising general welfare and, hence, reduced inequality. According to Wolf, in the developing world as a whole, life expectancy rose by four months each year after 1970 and infant mortality rate declined from 107 per thousand in 1970 to 58 in 2000 due to improvements in standards of living and health conditions. Also, adult literacy in developing countries rose from 53% in 1970 to 74% in 1998 and much lower illiteracy rate among the young guarantees that rates will continue to fall as time passes. Furthermore, the reduction in fertility rate in the developing world as a whole from 4.1 births per woman in 1980 to 2.8 in 2000 indicates improved education level of women on fertility, and control of fewer children with more parental attention and investment.[166] Consequently, more prosperous and educated parents with fewer children have chosen to withdraw their children from the labor force to give them opportunities to be educated at school improving the issue of child labor. Thus, despite seemingly unequal distribution of income within these developing countries, their economic growth and development have brought about improved standards of living and welfare for the population as a whole.
 Per capita gross domestic product (GDP) growth among post-1980 globalizing countries accelerated from 1.4 percent a year in the 1960s and 2.9 percent a year in the 1970s to 3.5 percent in the 1980s and 5.0 percent in the 1990s. This acceleration in growth seems even more remarkable given that the rich countries saw steady declines in growth from a high of 4.7 percent in the 1960s to 2.2 percent in the 1990s. Also, the non-globalizing developing countries seem to fare worse than the globalizers, with the former's annual growth rates falling from highs of 3.3 percent during the 1970s to only 1.4 percent during the 1990s. This rapid growth among the globalizers is not simply due to the strong performances of China and India in the 1980s and 1990s—18 out of the 24 globalizers experienced increases in growth, many of them quite substantial.[167]
 The globalization of the late 20th and early 21st centuries has led to the resurfacing of the idea that the growth of economic interdependence promotes peace.[168] This idea had been very powerful during the globalization of the late 19th and early 20th centuries, and was a central doctrine of classical liberals of that era, such as the young John Maynard Keynes (1883–1946).[169]
 Some opponents of globalization see the phenomenon as a promotion of corporate interests.[170] They also claim that the increasing autonomy and strength of corporate entities shapes the political policy of countries.[171][172] They advocate global institutions and policies that they believe better address the moral claims of poor and working classes as well as environmental concerns.[173] Economic arguments by fair trade theorists claim that unrestricted free trade benefits those with more financial leverage (i.e. the rich) at the expense of the poor.[174]
 Globalization allows corporations to outsource manufacturing and service jobs from high-cost locations, creating economic opportunities with the most competitive wages and worker benefits.[72] Critics of globalization say that it disadvantages poorer countries. While it is true that free trade encourages globalization among countries, some countries try to protect their domestic suppliers. The main export of poorer countries is usually agricultural productions. Larger countries often subsidize their farmers (e.g., the EU's Common Agricultural Policy), which lowers the market price for foreign crops.[175]
 Democratic globalization is a movement towards an institutional system of global democracy that would give world citizens a say in political organizations. This would, in their view, bypass nation-states, corporate oligopolies, ideological non-governmental organizations (NGO), political cults and mafias. One of its most prolific proponents is the British political thinker David Held. Advocates of democratic globalization argue that economic expansion and development should be the first phase of democratic globalization, which is to be followed by a phase of building global political institutions. Francesco Stipo, Director of the United States Association of the Club of Rome, advocates unifying nations under a world government, suggesting that it ""should reflect the political and economic balances of world nations. A world confederation would not supersede the authority of the State governments but rather complement it, as both the States and the world authority would have power within their sphere of competence"".[176] Former Canadian Senator Douglas Roche, O.C., viewed globalization as inevitable and advocated creating institutions such as a directly elected United Nations Parliamentary Assembly to exercise oversight over unelected international bodies.[177]
 Global civics suggests that civics can be understood, in a global sense, as a social contract between global citizens in the age of interdependence and interaction. The disseminators of the concept define it as the notion that we have certain rights and responsibilities towards each other by the mere fact of being human on Earth.[178] World citizen has a variety of similar meanings, often referring to a person who disapproves of traditional geopolitical divisions derived from national citizenship. An early incarnation of this sentiment can be found in Socrates, whom Plutarch quoted as saying: ""I am not an Athenian, or a Greek, but a citizen of the world.""[179] In an increasingly interdependent world, world citizens need a compass to frame their mindsets and create a shared consciousness and sense of global responsibility in world issues such as environmental problems and nuclear proliferation.[180]
 Baha'i-inspired author Meyjes, while favoring the single world community and emergent global consciousness, warns of globalization[181] as a cloak for an expeditious economic, social, and cultural Anglo-dominance that is insufficiently inclusive to inform the emergence of an optimal world civilization. He proposes a process of ""universalization"" as an alternative.
 Cosmopolitanism is the proposal that all human ethnic groups belong to a single community based on a shared morality. A person who adheres to the idea of cosmopolitanism in any of its forms is called a cosmopolitan or cosmopolite.[182] A cosmopolitan community might be based on an inclusive morality, a shared economic relationship, or a political structure that encompasses different nations. The cosmopolitan community is one in which individuals from different places (e.g. nation-states) form relationships based on mutual respect. For instance, Kwame Anthony Appiah suggests the possibility of a cosmopolitan community in which individuals from varying locations (physical, economic, etc.) enter relationships of mutual respect despite their differing beliefs (religious, political, etc.).[183]
 Canadian philosopher Marshall McLuhan popularized the term Global Village beginning in 1962.[184] His view suggested that globalization would lead to a world where people from all countries will become more integrated and aware of common interests and shared humanity.[185]
 Military cooperation – Past examples of international cooperation exist. One example is the security cooperation between the United States and the former Soviet Union after the end of the Cold War, which astonished international society. Arms control and disarmament agreements, including the Strategic Arms Reduction Treaty (see START I, START II, START III, and New START) and the establishment of NATO's Partnership for Peace, the Russia NATO Council, and the G8 Global Partnership against the Spread of Weapons and Materials of Mass Destruction, constitute concrete initiatives of arms control and de-nuclearization. The US–Russian cooperation was further strengthened by anti-terrorism agreements enacted in the wake of 9/11.[186]
 Environmental cooperation – One of the biggest successes of environmental cooperation has been the agreement to reduce chlorofluorocarbon (CFC) emissions, as specified in the Montreal Protocol, in order to stop ozone depletion. The most recent debate around nuclear energy and the non-alternative coal-burning power plants constitutes one more consensus on what not to do. Thirdly, significant achievements in IC can be observed through development studies.[186]
 Economic cooperation – One of the biggest challenges in 2019 with globalization is that many believe the progress made in the past decades are now back tracking. The back tracking of globalization has coined the term ""Slobalization."" Slobalization is a new, slower pattern of globalization.[187]
 Anti-globalization, or counter-globalization,[188] consists of a number of criticisms of globalization but, in general, is critical of the globalization of corporate capitalism.[189] The movement is also commonly referred to as the alter-globalization movement, anti-globalist movement, anti-corporate globalization movement,[190] or movement against neoliberal globalization. Opponents of globalization argue that power and respect in terms of international trade between the developed and underdeveloped countries of the world are unequally distributed.[191] The diverse subgroups that make up this movement include some of the following: trade unionists, environmentalists, anarchists, land rights and indigenous rights activists, organizations promoting human rights and sustainable development, opponents of privatization, and anti-sweatshop campaigners.[192]
 In The Revolt of the Elites and the Betrayal of Democracy, Christopher Lasch analyzes[193] the widening gap between the top and bottom of the social composition in the United States. For him, our epoch is determined by a social phenomenon: the revolt of the elites, in reference to The Revolt of the Masses (1929) by the Spanish philosopher José Ortega y Gasset. According to Lasch, the new elites, i.e. those who are in the top 20% in terms of income, through globalization which allows total mobility of capital, no longer live in the same world as their fellow-citizens. In this, they oppose the old bourgeoisie of the nineteenth and twentieth centuries, which was constrained by its spatial stability to a minimum of rooting and civic obligations. Globalization, according to the sociologist, has turned elites into tourists in their own countries. The denationalization of business enterprise tends to produce a class who see themselves as ""world citizens, but without accepting ... any of the obligations that citizenship in a polity normally implies"". Their ties to an international culture of work, leisure, information – make many of them deeply indifferent to the prospect of national decline. Instead of financing public services and the public treasury, new elites are investing their money in improving their voluntary ghettos: private schools in their residential neighborhoods, private police, garbage collection systems. They have ""withdrawn from common life"". Composed of those who control the international flows of capital and information, who preside over philanthropic foundations and institutions of higher education, manage the instruments of cultural production and thus fix the terms of public debate. So, the political debate is limited mainly to the dominant classes and political ideologies lose all contact with the concerns of the ordinary citizen. The result of this is that no one has a likely solution to these problems and that there are furious ideological battles on related issues. However, they remain protected from the problems affecting the working classes: the decline of industrial activity, the resulting loss of employment, the decline of the middle class, increasing the number of the poor, the rising crime rate, growing drug trafficking, the urban crisis.
 D.A. Snow et al. contend that the anti-globalization movement is an example of a new social movement, which uses tactics that are unique and use different resources than previously used before in other social movements.[194]
 One of the most infamous tactics of the movement is the Battle of Seattle in 1999, where there were protests against the World Trade Organization's Third Ministerial Meeting. All over the world, the movement has held protests outside meetings of institutions such as the WTO, the International Monetary Fund (IMF), the World Bank, the World Economic Forum, and the Group of Eight (G8).[192] Within the Seattle demonstrations the protesters that participated used both creative and violent tactics to gain the attention towards the issue of globalization.
 Capital markets have to do with raising and investing money in various human enterprises. Increasing integration of these financial markets between countries leads to the emergence of a global capital marketplace or a single world market. In the long run, increased movement of capital between countries tends to favor owners of capital more than any other group; in the short run, owners and workers in specific sectors in capital-exporting countries bear much of the burden of adjusting to increased movement of capital.[195]
 Those opposed to capital market integration on the basis of human rights issues are especially disturbed[according to whom?] by the various abuses which they think are perpetuated by global and international institutions that, they say, promote neoliberalism without regard to ethical standards. Common targets include the World Bank (WB), International Monetary Fund (IMF), the Organisation for Economic Co-operation and Development (OECD) and the World Trade Organization (WTO) and free trade treaties like the North American Free Trade Agreement (NAFTA), Free Trade Area of the Americas (FTAA), the Multilateral Agreement on Investment (MAI) and the General Agreement on Trade in Services (GATS). In light of the economic gap between rich and poor countries, movement adherents claim free trade without measures in place to protect the under-capitalized will contribute only to the strengthening the power of industrialized nations (often termed the ""North"" in opposition to the developing world's ""South"").[196][better source needed]
 Corporatist ideology, which privileges the rights of corporations (artificial or juridical persons) over those of natural persons, is an underlying factor in the recent rapid expansion of global commerce.[197] In recent years, there have been an increasing number of books (Naomi Klein's 2000 No Logo, for example) and films (e.g. The Corporation & Surplus) popularizing an anti-corporate ideology to the public.
 A related contemporary ideology, consumerism, which encourages the personal acquisition of goods and services, also drives globalization.[198] Anti-consumerism is a social movement against equating personal happiness with consumption and the purchase of material possessions. Concern over the treatment of consumers by large corporations has spawned substantial activism, and the incorporation of consumer education into school curricula. Social activists hold materialism is connected to global retail merchandizing and supplier convergence, war, greed, anomie, crime, environmental degradation, and general social malaise and discontent. One variation on this topic is activism by postconsumers, with the strategic emphasis on moving beyond addictive consumerism.[199]
 The global justice movement is the loose collection of individuals and groups—often referred to as a ""movement of movements""—who advocate fair trade rules and perceive current institutions of global economic integration as problems.[201] The movement is often labeled an anti-globalization movement by the mainstream media. Those involved, however, frequently deny that they are anti-globalization, insisting that they support the globalization of communication and people and oppose only the global expansion of corporate power.[202] The movement is based in the idea of social justice, desiring the creation of a society or institution based on the principles of equality and solidarity, the values of human rights, and the dignity of every human being.[203][204][205] Social inequality within and between nations, including a growing global digital divide, is a focal point of the movement. Many nongovernmental organizations have now arisen to fight these inequalities that many in Latin America, Africa and Asia face. A few very popular and well known non-governmental organizations (NGOs) include: War Child, Red Cross, Free The Children and CARE International. They often create partnerships where they work towards improving the lives of those who live in developing countries by building schools, fixing infrastructure, cleaning water supplies, purchasing equipment and supplies for hospitals, and other aid efforts.
 The economies of the world have developed unevenly, historically, such that entire geographical regions were left mired in poverty and disease while others began to reduce poverty and disease on a wholesale basis. From around 1980 through at least 2011, the GDP gap, while still wide, appeared to be closing and, in some more rapidly developing countries, life expectancies began to rise.[206] If we look at the Gini coefficient for world income, since the late 1980s, the gap between some regions has markedly narrowed—between Asia and the advanced economies of the West, for example—but huge gaps remain globally. Overall equality across humanity, considered as individuals, has improved very little. Within the decade between 2003 and 2013, income inequality grew even in traditionally egalitarian countries like Germany, Sweden and Denmark. With a few exceptions—France, Japan, Spain—the top 10 percent of earners in most advanced economies raced ahead, while the bottom 10 percent fell further behind.[207] By 2013, 85 multibillionaires had amassed wealth equivalent to all the wealth owned by the poorest half (3.5 billion) of the world's total population of 7 billion.[208]
 Critics of globalization argue that globalization results in weak labor unions: the surplus in cheap labor coupled with an ever-growing number of companies in transition weakened labor unions in high-cost areas. Unions become less effective and workers their enthusiasm for unions when membership begins to decline.[175] They also cite an increase in the exploitation of child labor: countries with weak protections for children are vulnerable to infestation by rogue companies and criminal gangs who exploit them. Examples include quarrying, salvage, and farm work as well as trafficking, bondage, forced labor, prostitution and pornography.[209]
 Women often participate in the workforce in precarious work, including export-oriented employment. Evidence suggests that while globalization has expanded women's access to employment, the long-term goal[whose?] of transforming gender inequalities remains unmet and appears unattainable without regulation of capital and a reorientation and expansion of the state's role in funding public goods and providing a social safety net.[210] Furthermore, the intersectionality of gender, race, class, and more remain overlooked[by whom?] when assessing the impact of globalization.[211]
 In 2016, a study published by the IMF posited that neoliberalism, the ideological backbone of contemporary globalized capitalism, has been ""oversold"", with the benefits of neoliberal policies being ""fairly difficult to establish when looking at a broad group of countries"" and the costs, most significantly higher income inequality within nations, ""hurt the level and sustainability of growth.""[212]
 Beginning in the 1930s, opposition arose to the idea of a world government, as advocated by organizations such as the World Federalist Movement (WFM). Those who oppose global governance typically do so on objections that the idea is unfeasible, inevitably oppressive, or simply unnecessary.[213] In general, these opponents are wary of the concentration of power or wealth that such governance might represent. Such reasoning dates back to the founding of the League of Nations and, later, the United Nations.
 Environmentalism is a broad philosophy, ideology[214][215][216] and social movement regarding concerns for environmental conservation and improvement of the health of the environment. Environmentalist concerns with globalization include issues such as global warming, global water supply and water crises, inequity in energy consumption and energy conservation, transnational air pollution and pollution of the world ocean, overpopulation, world habitat sustainability, deforestation, biodiversity loss and species extinction.
 One critique of globalization is that natural resources of the poor have been systematically taken over by the rich and the pollution promulgated by the rich is systematically dumped on the poor.[217] Some argue that Northern corporations are increasingly exploiting resources of less wealthy countries for their global activities while it is the South that is disproportionately bearing the environmental burden of the globalized economy. Globalization is thus leading to a type of"" environmental apartheid"".[218]
 Helena Norberg-Hodge, the director and founder of Local Futures/International Society for Ecology and Culture, criticizes globalization in many ways. In her book Ancient Futures, Norberg-Hodge claims that ""centuries of ecological balance and social harmony are under threat from the pressures of development and globalization."" She also criticizes the standardization and rationalization of globalization, as it does not always yield the expected growth outcomes. Although globalization takes similar steps in most countries, scholars such as Hodge claim that it might not be effective to certain countries and that globalization has actually moved some countries backward instead of developing them.[219]
 A related area of concern is the pollution haven hypothesis, which posits that, when large industrialized nations seek to set up factories or offices abroad, they will often look for the cheapest option in terms of resources and labor that offers the land and material access they require (see Race to the bottom).[220] This often comes at the cost of environmentally sound practices. Developing countries with cheap resources and labor tend to have less stringent environmental regulations, and conversely, nations with stricter environmental regulations become more expensive for companies as a result of the costs associated with meeting these standards. Thus, companies that choose to physically invest in foreign countries tend to (re)locate to the countries with the lowest environmental standards or weakest enforcement.
 The European Union–Mercosur Free Trade Agreement, which would form one of the world's largest free trade areas,[221] has been denounced by environmental activists and indigenous rights campaigners.[222] The fear is that the deal could lead to more deforestation of the Amazon rainforest as it expands market access to Brazilian beef.[223]
 Globalization is associated with a more efficient system of food production. This is because crops are grown in countries with optimum growing conditions. This improvement causes an increase in the world's food supply which encourages improved food security.[224] The political movement 'BREXIT' was considered a step back in globalisation; it has greatly disrupted food chains within the UK, as they import 26% of food produce from the EU.
 Norway's limited crop range advocates globalization of food production and availability. The northernmost country in Europe requires trade with other countries to ensure population food demands are met. The degree of self-sufficiency in food production was around 50% in Norway in 2007.[225]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Social inequality', 'Takenaka Heizo and Chida Ryokichi', 'Social inequality', 'economic globalization, cultural globalization and political globalization', 'competing definitions and interpretations'], 'answer_start': [], 'answer_end': []}"
