context,questions,answers
"Urban design is an approach to the design of buildings and the spaces between them that focuses on specific design processes and outcomes. In addition to designing and shaping the physical features of towns, cities, and regional spaces, urban design considers 'bigger picture' issues of economic, social and environmental value and social design. The scope of a project can range from a local street or public space to an entire city and surrounding areas. Urban designers connect the fields of architecture, landscape architecture and urban planning to better organize physical space and community environments.[1]
 Some important focuses of urban design on this page include its historical impact, paradigm shifts, its interdisciplinary nature, and issues related to urban design.
 Urban design deals with the larger scale of groups of buildings, infrastructure, streets, and public spaces, entire neighbourhoods and districts, and entire cities, with the goal of making urban environments that are equitable, beautiful, performative, and sustainable.[2][3][4]
 Urban design is an interdisciplinary field that utilizes the procedures and the elements of architecture and other related professions, including landscape design, urban planning, civil engineering, and municipal engineering.[5][6] It borrows substantive and procedural knowledge from public administration, sociology, law, urban geography, urban economics and other related disciplines from the social and behavioral sciences, as well as from the natural sciences.[7] In more recent times different sub-subfields of urban design have emerged such as strategic urban design, landscape urbanism, water-sensitive urban design, and sustainable urbanism. Urban design demands an understanding of a wide range of subjects from physical geography to social science, and an appreciation for disciplines, such as real estate development, urban economics, political economy, and social theory.
 Urban design theory deals primarily with the design and management of public space (i.e. the 'public environment', 'public realm' or 'public domain'), and the way public places are used and experienced. Public space includes the totality of spaces used freely on a day-to-day basis by the general public, such as streets, plazas, parks, and public infrastructure. Some aspects of privately owned spaces, such as building facades or domestic gardens, also contribute to public space and are therefore also considered by urban design theory. Important writers on urban design theory include Christopher Alexander, Peter Calthorpe, Gordon Cullen, Andrés Duany, Jane Jacobs, Jan Gehl, Allan B. Jacobs, Kevin Lynch, Aldo Rossi, Colin Rowe, Robert Venturi, William H. Whyte, Camillo Sitte, Bill Hillier (space syntax), and Elizabeth Plater-Zyberk.
 Although contemporary professional use of the term 'urban design' dates from the mid-20th century, urban design as such has been practiced throughout history. Ancient examples of carefully planned and designed cities exist in Asia, Africa, Europe, and the Americas, and are particularly well known within Classical Chinese, Roman, and Greek cultures. Specifically, Hippodamus of Miletus was a famous ancient Greek architect and urban planner, and all around academic that is often considered to be a ""father of European urban planning"", and the namesake of the ""Hippodamian plan"", also known as the grid plan of a city layout.[8]
 European Medieval cities are often, and often erroneously, regarded as exemplars of undesigned or 'organic' city development. There are many examples of considered urban design in the Middle Ages.[9] In England, many of the towns listed in the 9th-century Burghal Hidage were designed on a grid, examples including Southampton, Wareham, Dorset and Wallingford, Oxfordshire, having been rapidly created to provide a defensive network against Danish invaders.[10] 12th century western Europe brought renewed focus on urbanisation as a means of stimulating economic growth and generating revenue.[11] The burgage system dating from that time and its associated burgage plots brought a form of self-organising design to medieval towns.[12]
 Throughout history, the design of streets and deliberate configuration of public spaces with buildings have reflected contemporaneous social norms or philosophical and religious beliefs.[13] Yet the link between designed urban space and the human mind appears to be bidirectional. Indeed, the reverse impact of urban structure upon human behaviour and upon thought is evidenced by both observational study and historical records. There are clear indications of impact through Renaissance urban design on the thought of Johannes Kepler and Galileo Galilei.[14] Already René Descartes in his Discourse on the Method had attested to the impact Renaissance planned new towns had upon his own thought, and much evidence exists that the Renaissance streetscape was also the perceptual stimulus that had led to the development of coordinate geometry.[15]
 The beginnings of modern urban design in Europe are associated with the Renaissance but, especially, with the Age of Enlightenment.[16] Spanish colonial cities were often planned, as were some towns settled by other imperial cultures.[17] These sometimes embodied utopian ambitions as well as aims for functionality and good governance, as with James Oglethorpe's plan for Savannah, Georgia.[18] In the Baroque period the design approaches developed in French formal gardens such as Versailles were extended into urban development and redevelopment. In this period, when modern professional specializations did not exist, urban design was undertaken by people with skills in areas as diverse as sculpture, architecture, garden design, surveying, astronomy, and military engineering. In the 18th and 19th centuries, urban design was perhaps most closely linked with surveyors engineers and architects. The increase in urban populations brought with it problems of epidemic disease, the response to which was a focus on public health, the rise in the UK of municipal engineering and the inclusion in British legislation of provisions such as minimum widths of street in relation to heights of buildings in order to ensure adequate light and ventilation.[citation needed]
 Much of Frederick Law Olmsted's work was concerned with urban design, and the newly formed profession of landscape architecture also began to play a significant role in the late 19th century.[19]
 In the 19th century, cities were industrializing and expanding at a tremendous rate. Private businesses largely dictated the pace and style of this development. The expansion created many hardships for the working poor and concern for public health increased. However, the laissez-faire style of government, in fashion for most of the Victorian era, was starting to give way to a New Liberalism. This gave more power to the public. The public wanted the government to provide citizens, especially factory workers, with healthier environments. Around 1900, modern urban design emerged from developing theories on how to mitigate the consequences of the industrial age.
 The first modern urban planning theorist was Sir Ebenezer Howard. His ideas, although utopian, were adopted around the world because they were highly practical. He initiated the garden city movement.[20] in 1898.
His garden cities were intended to be planned, self-contained communities surrounded by parks. Howard wanted the cities to be proportional with separate areas of residences, industry, and agriculture. Inspired by the Utopian novel Looking Backward and Henry George's work Progress and Poverty, Howard published his book Garden Cities of To-morrow in 1898. His work is an important reference in the history of urban planning.[21] He envisioned the self-sufficient garden city to house 32,000 people on a site of 6,000 acres (2,428 ha). He planned on a concentric pattern with open spaces, public parks, and six radial boulevards, 120 ft (37 m) wide, extending from the center. When it reached full population, Howard wanted another garden city to be developed nearby. He envisaged a cluster of several garden cities as satellites of a central city of 50,000 people, linked by road and rail.[22] His model for a garden city was first created at Letchworth[23] and Welwyn Garden City in Hertfordshire. Howard's movement was extended by Sir Frederic Osborn to regional planning.[23]
 In the early 1900s, urban planning became professionalized. With input from utopian visionaries, civil engineers, and local councilors, new approaches to city design were developed for consideration by decision-makers such as elected officials. In 1899, the Town and Country Planning Association was founded. In 1909, the first academic course on urban planning was offered by the University of Liverpool.[24] Urban planning was first officially embodied in the Housing and Town Planning Act of 1909 Howard's 'garden city' compelled local authorities to introduce a system where all housing construction conformed to specific building standards.[25] In the United Kingdom following this Act, surveyor, civil engineers, architects, and lawyers began working together within local authorities. In 1910, Thomas Adams became the first Town Planning Inspector at the Local Government Board and began meeting with practitioners. In 1914, The Town Planning Institute was established. The first urban planning course in America wasn't established until 1924 at Harvard University. Professionals developed schemes for the development of land, transforming town planning into a new area of expertise.
 In the 20th century, urban planning was changed by the automobile industry. Car-oriented design impacted the rise of 'urban design'. City layouts now revolved around roadways and traffic patterns.
 In June 1928, the International Congresses of Modern Architecture (CIAM) was founded at the Chateau de la Sarraz in Switzerland, by a group of 28 European architects organized by Le Corbusier, Hélène de Mandrot, and Sigfried Giedion. The CIAM was one of many 20th century manifestos meant to advance the cause of ""architecture as a social art"".
 Team X was a group of architects and other invited participants who assembled starting in July 1953 at the 9th Congress of the International Congresses of Modern Architecture (CIAM) and created a schism within CIAM by challenging its doctrinaire approach to urbanism.
 In 1956, the term ""Urban design"" was first used at a series of conferences hosted by Harvard University. The event provided a platform for Harvard's Urban Design program. The program also utilized the writings of famous urban planning thinkers: Gordon Cullen, Jane Jacobs, Kevin Lynch, and Christopher Alexander.
 In 1961, Gordon Cullen published The Concise Townscape. He examined the traditional artistic approach to city design of theorists including Camillo Sitte, Barry Parker, and Raymond Unwin. Cullen also created the concept of 'serial vision'. It defined the urban landscape as a series of related spaces.
 Also in 1961, Jane Jacobs published The Death and Life of Great American Cities. She critiqued the modernism of CIAM (International Congresses of Modern Architecture). Jacobs also claimed crime rates in publicly owned spaces were rising because of the Modernist approach of 'city in the park'. She argued instead for an 'eyes on the street' approach to town planning through the resurrection of main public space precedents (e.g. streets, squares).
 In the same year, Kevin Lynch published The Image of the City. He was seminal to urban design, particularly with regards to the concept of legibility. He reduced urban design theory to five basic elements: paths, districts, edges, nodes, landmarks. He also made the use of mental maps to understand the city popular, rather than the two-dimensional physical master plans of the previous 50 years.
 Other notable works:
 The popularity of these works resulted in terms that become everyday language in the field of urban planning. Aldo Rossi introduced 'historicism' and 'collective memory' to urban design. Rossi also proposed a 'collage metaphor' to understand the collection of new and old forms within the same urban space. Peter Calthorpe developed a manifesto for sustainable urban living via medium-density living. He also designed a manual for building new settlements in his concept of Transit Oriented Development (TOD). Bill Hillier and Julienne Hanson introduced Space Syntax to predict how movement patterns in cities would contribute to urban vitality, anti-social behaviour, and economic success. 'Sustainability', 'livability', and 'high quality of urban components' also became commonplace in the field.
 Today, urban design seeks to create sustainable urban environments with long-lasting structures, buildings, and overall livability. Walkable urbanism is another approach to practice that is defined within the Charter of New Urbanism. It aims to reduce environmental impacts by altering the built environment to create smart cities that support sustainable transport. Compact urban neighborhoods encourage residents to drive less. These neighborhoods have significantly lower environmental impacts when compared to sprawling suburbs.[26] To prevent urban sprawl, Circular flow land use management was introduced in Europe to promote sustainable land use patterns.
 As a result of the recent New Classical Architecture movement, sustainable construction aims to develop smart growth, walkability, architectural tradition, and classical design.[27][28] It contrasts with modernist and globally uniform architecture. In the 1980s, urban design began to oppose the increasing solitary housing estates and suburban sprawl.[29]
Managed Urbanisation with the view to making the urbanising process completely culturally and economically, and environmentally sustainable, and as a possible solution to the urban sprawl, Frank Reale has submitted an interesting concept of Expanding Nodular Development (E.N.D.) that integrates many urban designs and ecological principles, to design and build smaller rural hubs with high-grade connecting freeways, rather than adding more expensive infrastructure to existing big cities and the resulting congestion.
 Throughout the young existence of the Urban Design discipline, many paradigm shifts have occurred that have affected the trajectory of the field regarding theory and practice. These paradigm shifts cover multiple subject areas outside of the traditional design disciplines.
 There have been many different theories and approaches applied to the practice of urban design.
 New Urbanism is an approach that began in the 1980s as a place-making initiative to combat suburban sprawl. Its goal is to increase density by creating compact and complete towns and neighborhoods. The 10 principles of new urbanism are walkability, connectivity, mixed-use and diversity, mixed housing, quality architecture and urban design, traditional neighborhood structure, increased density, smart transportation, sustainability, and quality of life. New urbanism and the developments that it has created are sources of debates within the discipline, primarily with the landscape urbanist approach but also due to its reproduction of idyllic architectural tropes that do not respond to the context. Andres Duany, Elizabeth Plater-Zyberk, Peter Calthorpe, and Jeff Speck are all strongly associated with New Urbanism and its evolution over the years.
 Landscape Urbanism is a theory that first surfaced in the 1990s, arguing that the city is constructed of interconnected and ecologically rich horizontal field conditions, rather than the arrangement of objects and buildings. Charles Waldheim, Mohsen Mostafavi, James Corner, and Richard Weller are closely associated with this theory. Landscape urbanism theorises sites, territories, ecosystems, networks, and infrastructures through landscape practice according to Corner,[30] while applying a dynamic concept to cities as ecosystems that grow, shrink or change phases of development according to Waldheim.[31]
 Everyday Urbanism is a concept introduced by Margaret Crawford and influenced by Henry Lefebvre that describes the everyday lived experience shared by urban residents including commuting, working, relaxing, moving through city streets and sidewalks, shopping, buying, eating food, and running errands. Everyday urbanism is not concerned with aesthetic value. Instead, it introduces the idea of eliminating the distance between experts and ordinary users and forces designers and planners to contemplate a 'shift of power' and address social life from a direct and ordinary perspective.
 Tactical Urbanism (also known as DIY Urbanism, Planning-by-Doing, Urban Acupuncture, or Urban Prototyping) is a city, organizational, or citizen-led approach to neighborhood-building that uses short-term, low-cost, and scalable interventions and policies to catalyze long term change.
 Top-up Urbanism is the theory and implementation of two techniques in urban design: top-down and bottom-up. Top-down urbanism is when the design is implemented from the top of the hierarchy - normally the government or planning department. Bottom-up or grassroots urbanism begins with the people or the bottom of the hierarchy. Top-up means that both methods are used together to make a more participatory design, so it is sure to be comprehensive and well regarded in order to be as successful as possible.
 Infrastructural Urbanism is the study of how the major investments that go into making infrastructural systems can be leveraged to be more sustainable for communities. Instead of the systems being solely about efficiency in both cost and production, infrastructural urbanism strives to utilize these investments to be more equitable for social and environmental issues as well. Linda Samuels is a designer investigating how to accomplish this change in infrastructure in what she calls ""next-generation infrastructure"" which is ""multifunctional; public; visible; socially productive; locally specific, flexible, and adaptable; sensitive to the eco-economy; composed of design prototypes or demonstration projects; symbiotic; technologically smart; and developed collaboratively across disciplines and agencies"".
 Sustainable Urbanism is the study from the 1990s of how a community can be beneficial for the ecosystem, the people, and the economy for which it is associated. It is based on Scott Campbell's planner's triangle which tries to find the balance between economy, equity, and the environment. Its main concept is to try and make cities as self-sufficient as possible while not damaging the ecosystem around them, today with an increased focus on climate stability.[4] A key designer working with sustainable urbanism is Douglas Farr.
 Feminist Urbanism is the study and critique of how the built environment affects genders differently because of patriarchal social and political structures in society. Typically, the people at the table making design decisions are men, so their conception about public space and the built environment relates to their life perspectives and experiences, which do not reflect the same experiences of women or children. Dolores Hayden is a scholar who has researched this topic from 1980 to the present day. Hayden's writing says, “when women, men, and children of all classes and races can identify the public domain as the place where they feel most comfortable as citizens, Americans will finally have homelike urban space.”
 Educational Urbanism is an emerging discipline, at the crossroads of urban planning, educational planning, and pedagogy. An approach that tackles the notion that economic activities, the need for new skills at the workplace, and the spatial configuration of the workplace rely on the spatial reorientation in the design of educational spaces and the urban dimension of educational planning.
 Black Urbanism is an approach in which black communities are active creators, innovators, and authors of the process of designing and creating the neighborhoods and spaces of the metropolitan areas they have done so much to help revive over the past half-century. The goal is not to build black cities for black people but to explore and develop the creative energy that exists in so-called black areas: that has the potential to contribute to the sustainable development of the whole city.
 Underlying the practice of urban design are the many theories about how to best design the city. Each theory makes a unique claim about how to effectively design thriving, sustainable urban environments. Debates over the efficacy of these approaches fill the urban design discourse. Landscape Urbanism and New Urbanism are commonly debated as distinct approaches to addressing suburban sprawl. While Landscape Urbanism proposes landscape as the basic building block of the city and embraces horizontality, flexibility, and adaptability, New Urbanism offers the neighborhood as the basic building block of the city and argues for increased density, mixed uses, and walkability. Opponents of Landscape Urbanism point out that most of its projects are urban parks, and as such, its application is limited. Opponents of New Urbanism claim that its preoccupation with traditional neighborhood structures is nostalgic, unimaginative, and culturally problematic. Everyday Urbanism argues for grassroots neighborhood improvements rather than master-planned, top-down interventions. Each theory elevates the roles of certain professions in the urban design process, further fueling the debate. In practice, urban designers often apply principles from many urban design theories. Emerging from the conversation is a universal acknowledgement of the importance of increased interdisciplinary collaboration in designing the modern city.[32]
 Urban designers work with architects, landscape architects, transportation engineers, urban planners, and industrial designers to reshape the city. Cooperation with public agencies, authorities and the interests of nearby property owners is necessary to manage public spaces. Users often compete over the spaces and negotiate across a variety of spheres. Input is frequently needed from a wide range of stakeholders. This can lead to different levels of participation as defined in Arnstein's Ladder of Citizen Participation.[33]
 While there are some professionals who identify themselves specifically as urban designers, a majority have backgrounds in urban planning, architecture, or landscape architecture. Many collegiate programs incorporate urban design theory and design subjects into their curricula. There is an increasing number of university programs offering degrees in urban design at the post-graduate level.
 Urban design considers:
 The original urban design was thought to be separated from architecture and urban planning. Urban Design has developed to a certain extent, and comes from the foundation of engineering. In Anglo-Saxon countries, it is often considered as a branch under the architecture, urban planning, and landscape architecture and limited as the construction of the urban physical environment. However Urban Design is more integrated into the social science-based, cultural, economic, political, and other aspects. Not only focus on space and architectural group, but also look at the whole city from a broader and more holistic perspective to shape a better living environment. Compared to architecture, the spatial and temporal scale of Urban Design processing is much larger. It deals with neighborhoods, communities, and even the entire city.
 The University of Liverpool's Department of Civic Design is the first urban design school in the world founded in 1909.[34] Following the 1956 Urban Design conference, Harvard University established the first graduate program with urban design in its title, The Master of Architecture in Urban Design, although as a subject taught in universities its history in Europe is far older. Urban design programs explore the built environment from diverse disciplinary backgrounds and points of view. The pedagogically innovative combination of interdisciplinary studios, lecture courses, seminars, and independent study creates an intimate and engaging educational atmosphere in which students thrive and learn. Soon after in 1961, Washington University in St. Louis founded their Master of Urban Design program. Today, twenty urban design programs exist in the United States:
 In the United Kingdom, Master's programmes in Urban Design at University of Manchester or University of Sheffield and Cardiff University or London South Bank University and City Design at the Royal College of Art or Queen's University Belfast are offered.
 The field of urban design holds enormous potential for helping us address today's biggest challenges: an expanding population, mass urbanization, rising inequality, and climate change. In its practice as well as its theories, urban design attempts to tackle these pressing issues. As climate change progresses, urban design can mitigate the results of flooding, temperature changes, and increasingly detrimental storm impacts through a mindset of sustainability and resilience. In doing so, the urban design discipline attempts to create environments that are constructed with longevity in mind, such as zero-carbon cities. Cities today must be designed to minimize resource consumption, waste generation, and pollution while also withstanding the unprecedented impacts of climate change.[4][35][36][37] To be truly resilient, our cities need to be able to not just bounce back from a catastrophic climate event but to bounce forward to an improved state.
 Another issue in this field is that it is often assumed that there are no mothers of planning and urban design. However, this is not the case, many women have made proactive contributions to the field, including the work of Mary Kingsbury Simkhovitch, Florence Kelley, and Lillian Wald, to name a few of whom were prominent leaders in the City Social movement. The City Social was a movement that steamed between the commonly known City Practical and City Beautiful movements. It was a movement that’s main concerns lay with the economic and social equalities regarding urban issues.[38]
 Justice is and will always be a key issue in urban design. As previously mentioned, past urban strategies have caused injustices within communities incapable of being remedied via simple means. As urban designers tackle the issue of justice, they often are required to look at the injustices of the past and must be careful not to overlook the nuances of race, place, and socioeconomic status in their design efforts. This includes ensuring reasonable access to basic services, transportation, and fighting against gentrification and the commodification of space for economic gain. Organizations such as the Divided Cities Initiatives at Washington University in St. Louis and the Just City Lab at Harvard work on promoting justice in urban design.
 Until the 1970s, the design of towns and cities took little account of the needs of people with disabilities. At that time, disabled people began to form movements demanding recognition of their potential contribution if social obstacles were removed. Disabled people challenged the 'medical model' of disability which saw physical and mental problems as an individual 'tragedy' and people with disabilities as 'brave' for enduring them. They proposed instead a 'social model' which said that barriers to disabled people result from the design of the built environment and attitudes of able-bodied people. 'Access Groups' were established composed of people with disabilities who audited their local areas, checked planning applications, and made representations for improvements. The new profession of 'access officer' was established around that time to produce guidelines based on the recommendations of access groups and to oversee adaptations to existing buildings as well as to check on the accessibility of new proposals. Many local authorities now employ access officers who are regulated by the Access Association. A new chapter of the Building Regulations (Part M) was introduced in 1992. Although it was beneficial to have legislation on this issue the requirements were fairly minimal but continue to be improved with ongoing amendments. The Disability Discrimination Act 1995 continues to raise awareness and enforce action on disability issues in the urban environment.
 
The issue of walkability has gained prominence in recent years, not only with the concerns of the aforementioned climate change, but also the health outcomes of residents. Car-centric urban design has an invariably negative effect on such outcomes. With proximity to internal combustion engines, residents tend to suffer from dangerous levels of air pollution which lead to cardiovascular complications ranging from the acute, in hypertension and alterations in heart rate, and the chronic, the outright development of atherosclerosis. More people die from air pollution each year than from car accidents.  This issue has been used to fuel movements for alternative forms of long to mid range transportation such as trains and bicycles, with walking as the primary means of short-range travel. This would bring benefits from two simultaneous avenues. The physical activity from walking, and the lack of particulate matter (carbon dioxide, sulfur dioxide, nitrogen dioxide, etc.) has shown to alleviate and lower the risk of many maladies such as diabetes, hypertension and cardiovascular disease. Physical activity levels from walking are closely related to the abundance of open public spaces, commercial shops, greenery, among others. These attributes also have been stated to contribute to stronger social and emotional health as the open public spaces facilitate more social interaction within communities. This issue is most prevalent in the United States, where the rise of neoliberalism directly and intentionally caused the car-centric infrastructure.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['urban design', 'Johannes Kepler and Galileo Galilei', 'impact', 'historical impact, paradigm shifts, its interdisciplinary nature', 'limited'], 'answer_start': [], 'answer_end': []}"
"A cognitive bias is a systematic pattern of deviation from norm or rationality in judgment.[1] Individuals create their own ""subjective reality"" from their perception of the input. An individual's construction of reality, not the objective input, may dictate their behavior in the world. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, and irrationality.[2][3][4]
 While cognitive biases may initially appear to be negative, some are adaptive. They may lead to more effective actions in a given context.[5] Furthermore, allowing cognitive biases enables faster decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics.[6] Other cognitive biases are a ""by-product"" of human processing limitations,[1] resulting from a lack of appropriate mental mechanisms (bounded rationality), the impact of an individual's constitution and biological state (see embodied cognition), or simply from a limited capacity for information processing.[7][8] Research suggests that cognitive biases can make individuals more inclined to endorsing pseudoscientific beliefs by requiring less evidence for claims that confirm their preconceptions. This can potentially distort their perceptions and lead to inaccurate judgments.[9]
 A continually evolving list of cognitive biases has been identified over the last six decades of research on human judgment and decision-making in cognitive science, social psychology, and behavioral economics. The study of cognitive biases has practical implications for areas including clinical judgment, entrepreneurship, finance, and management.[10][11]
 The notion of cognitive biases was introduced by Amos Tversky and Daniel Kahneman in 1972[12] and grew out of their experience of people's innumeracy, or inability to reason intuitively with the greater orders of magnitude. Tversky, Kahneman, and colleagues demonstrated several replicable ways in which human judgments and decisions differ from rational choice theory. Tversky and Kahneman explained human differences in judgment and decision-making in terms of heuristics. Heuristics involve mental shortcuts which provide swift estimates about the possibility of uncertain occurrences.[13] Heuristics are simple for the brain to compute but sometimes introduce ""severe and systematic errors.""[6] For example, the representativeness heuristic is defined as ""The tendency to judge the frequency or likelihood"" of an occurrence by the extent of which the event ""resembles the typical case.""[13]
 The ""Linda Problem"" illustrates the representativeness heuristic (Tversky & Kahneman, 1983[14]). Participants were given a description of ""Linda"" that suggests Linda might well be a feminist (e.g., she is said to be concerned about discrimination and social justice issues). They were then asked whether they thought Linda was more likely to be (a) a ""bank teller"" or (b) a ""bank teller and active in the feminist movement."" A majority chose answer (b). Independent of the information given about Linda, though, the more restrictive answer (b) is under any circumstance statistically less likely than answer (a). This is an example of the ""conjunction fallacy"". Tversky and Kahneman argued that respondents chose (b) because it seemed more ""representative"" or typical of persons who might fit the description of Linda. The representativeness heuristic may lead to errors such as activating stereotypes and inaccurate judgments of others (Haselton et al., 2005, p. 726).
 Critics of Kahneman and Tversky, such as Gerd Gigerenzer, alternatively argued that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases. They should rather conceive rationality as an adaptive tool, not identical to the rules of formal logic or the probability calculus.[15] Nevertheless, experiments such as the ""Linda problem"" grew into heuristics and biases research programs, which spread beyond academic psychology into other disciplines including medicine and political science.
 Biases can be distinguished on a number of dimensions. Examples of cognitive biases include -
 Other biases are due to the particular way the brain perceives, forms memories and makes judgments. This distinction is sometimes described as ""hot cognition"" versus ""cold cognition"", as motivated reasoning can involve a state of arousal. Among the ""cold"" biases,
 As some biases reflect motivation specifically the motivation to have positive attitudes to oneself.[20] It accounts for the fact that many biases are self-motivated or self-directed (e.g., illusion of asymmetric insight, self-serving bias). There are also biases in how subjects evaluate in-groups or out-groups; evaluating in-groups as more diverse and ""better"" in many respects, even when those groups are arbitrarily defined (ingroup bias, outgroup homogeneity bias).
 Some cognitive biases belong to the subgroup of attentional biases, which refers to paying increased attention to certain stimuli. It has been shown, for example, that people addicted to alcohol and other drugs pay more attention to drug-related stimuli. Common psychological tests to measure those biases are the Stroop task[21][22] and the dot probe task.
 Individuals' susceptibility to some types of cognitive biases can be measured by the Cognitive Reflection Test (CRT) developed by Shane Frederick (2005).[23][24]
 The following is a list of the more commonly studied cognitive biases:
 Many social institutions rely on individuals to make rational judgments.
 The securities regulation regime largely assumes that all investors act as perfectly rational persons. In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.
 A fair jury trial, for example, requires that the jury ignore irrelevant features of the case, weigh the relevant features appropriately, consider different possibilities open-mindedly and resist fallacies such as appeal to emotion. The various biases demonstrated in these psychological experiments suggest that people will frequently fail to do all these things.[35] However, they fail to do so in systematic, directional ways that are predictable.[4]
 In some academic disciplines, the study of bias is very popular. For instance, bias is a wide spread and well studied phenomenon because most decisions that concern the minds and hearts of entrepreneurs are computationally intractable.[11]
 Cognitive biases can create other issues that arise in everyday life. One study showed the connection between cognitive bias, specifically approach bias, and inhibitory control on how much unhealthy snack food a person would eat.[36] They found that the participants who ate more of the unhealthy snack food, tended to have less inhibitory control and more reliance on approach bias. Others have also hypothesized that cognitive biases could be linked to various eating disorders and how people view their bodies and their body image.[37][38]
 It has also been argued that cognitive biases can be used in destructive ways.[39] Some believe that there are people in authority who use cognitive biases and heuristics in order to manipulate others so that they can reach their end goals. Some medications and other health care treatments rely on cognitive biases in order to persuade others who are susceptible to cognitive biases to use their products. Many see this as taking advantage of one's natural struggle of judgement and decision-making. They also believe that it is the government's responsibility to regulate these misleading ads.
 Cognitive biases also seem to play a role in property sale price and value. Participants in the experiment were shown a residential property.[40] Afterwards, they were shown another property that was completely unrelated to the first property. They were asked to say what they believed the value and the sale price of the second property would be. They found that showing the participants an unrelated property did have an effect on how they valued the second property.
 Cognitive biases can be used in non-destructive ways. In team science and collective problem-solving, the superiority bias can be beneficial. It leads to a diversity of solutions within a group, especially in complex problems, by preventing premature consensus on suboptimal solutions. This example demonstrates how a cognitive bias, typically seen as a hindrance, can enhance collective decision-making by encouraging a wider exploration of possibilities.[41]
 Because they cause systematic errors, cognitive biases cannot be compensated for using a wisdom of the crowd technique of averaging answers from several people.[42] Debiasing is the reduction of biases in judgment and decision-making through incentives, nudges, and training. Cognitive bias mitigation and cognitive bias modification are forms of debiasing specifically applicable to cognitive biases and their effects. Reference class forecasting is a method for systematically debiasing estimates and decisions, based on what Daniel Kahneman has dubbed the outside view.
 Similar to Gigerenzer (1996),[43] Haselton et al. (2005) state the content and direction of cognitive biases are not ""arbitrary"" (p. 730).[1] Moreover, cognitive biases can be controlled. One debiasing technique aims to decrease biases by encouraging individuals to use controlled processing compared to automatic processing.[25] In relation to reducing the FAE, monetary incentives[44] and informing participants they will be held accountable for their attributions[45] have been linked to the increase of accurate attributions. Training has also shown to reduce cognitive bias. Carey K. Morewedge and colleagues (2015) found that research participants exposed to one-shot training interventions, such as educational videos and debiasing games that taught mitigating strategies, exhibited significant reductions in their commission of six cognitive biases immediately and up to 3 months later.[46]
 Cognitive bias modification refers to the process of modifying cognitive biases in healthy people and also refers to a growing area of psychological (non-pharmaceutical) therapies for anxiety, depression and addiction called cognitive bias modification therapy (CBMT). CBMT is sub-group of therapies within a growing area of psychological therapies based on modifying cognitive processes with or without accompanying medication and talk therapy, sometimes referred to as applied cognitive processing therapies (ACPT). Although cognitive bias modification can refer to modifying cognitive processes in healthy individuals, CBMT is a growing area of evidence-based psychological therapy, in which cognitive processes are modified to relieve suffering[47][48] from serious depression,[49] anxiety,[50] and addiction.[51] CBMT techniques are technology-assisted therapies that are delivered via a computer with or without clinician support. CBM combines evidence and theory from the cognitive model of anxiety,[52] cognitive neuroscience,[53] and attentional models.[54]
 Cognitive bias modification has also been used to help those with obsessive-compulsive beliefs and obsessive-compulsive disorder.[55][56] This therapy has shown that it decreases the obsessive-compulsive beliefs and behaviors.
 Bias arises from various processes that are sometimes difficult to distinguish. These include:
 People do appear to have stable individual differences in their susceptibility to decision biases such as overconfidence, temporal discounting, and bias blind spot.[65] That said, these stable levels of bias within individuals are possible to change. Participants in experiments who watched training videos and played debiasing games showed medium to large reductions both immediately and up to three months later in the extent to which they exhibited susceptibility to six cognitive biases: anchoring, bias blind spot, confirmation bias, fundamental attribution error, projection bias, and representativeness.[66]
 Individual differences in cognitive bias have also been linked to varying levels of cognitive abilities and functions.[67] The Cognitive Reflection Test (CRT) has been used to help understand the connection between cognitive biases and cognitive ability. There have been inconclusive results when using the Cognitive Reflection Test to understand ability. However, there does seem to be a correlation; those who gain a higher score on the Cognitive Reflection Test, have higher cognitive ability and rational-thinking skills. This in turn helps predict the performance on cognitive bias and heuristic tests. Those with higher CRT scores tend to be able to answer more correctly on different heuristic and cognitive bias tests and tasks.[68]
 Age is another individual difference that has an effect on one's ability to be susceptible to cognitive bias. Older individuals tend to be more susceptible to cognitive biases and have less cognitive flexibility. However, older individuals were able to decrease their susceptibility to cognitive biases throughout ongoing trials.[69] These experiments had both young and older adults complete a framing task. Younger adults had more cognitive flexibility than older adults. Cognitive flexibility is linked to helping overcome pre-existing biases.
 Cognitive bias theory loses the sight of any distinction between reason and bias. If every bias can be seen as a reason, and every reason can be seen as a bias, then the distinction is lost.[70]
 Criticism against theories of cognitive biases is usually founded in the fact that both sides of a debate often claim the other's thoughts to be subject to human nature and the result of cognitive bias, while claiming their own point of view to be above the cognitive bias and the correct way to ""overcome"" the issue. This rift ties to a more fundamental issue that stems from a lack of consensus in the field, thereby creating arguments that can be non-falsifiably used to validate any contradicting viewpoint.[citation needed]
 Gerd Gigerenzer is one of the main opponents to cognitive biases and heuristics.[71][72][73] Gigerenzer believes that cognitive biases are not biases, but rules of thumb, or as he would put it ""gut feelings"" that can actually help us make accurate decisions in our lives. His view shines a much more positive light on cognitive biases than many other researchers. Many view cognitive biases and heuristics as irrational ways of making decisions and judgements.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['discrimination and social justice issues', 'Amos Tversky and Daniel Kahneman', 'systematic pattern of deviation from norm or rationality in judgment', 'various eating disorders', 'practical implications for areas including clinical judgment, entrepreneurship, finance, and management'], 'answer_start': [], 'answer_end': []}"
"Marine engineering is the engineering of boats, ships, submarines, and any other marine vessel. Here it is also taken to include the engineering of other ocean systems and structures – referred to in certain academic and professional circles as ""ocean engineering"".
 Marine engineering applies a number of engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and ocean systems.[1] It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, as well as coastal and offshore structures.
 Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
 In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton's Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe. Around 50 years later the steam powered paddle wheels had a peak with the creation of the Great Eastern, which was as big as one of the cargo ships of today, 700 feet in length, weighing 22,000 tons. Paddle steamers would become the front runners of the steamship industry for the next thirty years till the next type of propulsion came around.[2]
 There are many ways to become a Marine Engineer, but all include a university or college degree. Primarily, training includes a Bachelor of Engineering (B.Eng. or B.E.), 
 Bachelor of Science (B.Sc. or B.S.), 
 Bachelor of Technology (B.Tech.), 
 Bachelor of Technology Management and Marine Engineering (B.TecMan & MarEng)
 Bachelor of Applied Science (B.A.Sc.) in Marine Engineering. 
 
Depending on the country and jurisdiction, to be licensed as a Marine engineer, a Master's degree; 
 Master of Engineering (M.Eng.), 
 Master of Science (M.Sc or M.S.)  
 Master of Applied Science (M.A.Sc.) 
 may be required. There are also Marine engineers who have come from other disciplines, e.g., from engineering fields like Mechanical Engineering, Civil Engineering, Electrical Engineering, Geomatics Engineering, Environmental Engineering or from science fields like Geology, Geophysics, Physics, Geomatics, Earth Science, Mathematics, However, this path requires taking a graduate degree such as M.Eng, M.S., M.Sc. or M.A.Sc. in Marine Engineering after graduating from a different quantitative undergraduate program to be qualified as a Marine engineer.
 The fundamental subjects of Marine engineering study usually include:
 In the engineering of seagoing vessels, naval architecture is concerned with the overall design of the ship and its propulsion through the water, while marine engineering ensures that the ship systems function as per the design.[3] Although they have distinctive disciplines, naval architects and marine engineers often work side-by-side.
 Ocean engineering is concerned with other structures and systems in or adjacent to the ocean, including offshore platforms, coastal structures such as piers and harbors, and other ocean systems such as ocean wave energy conversion and underwater life-support systems.[4] This in fact makes ocean engineering a distinctive field from marine engineering, which is concerned with the design and application of shipboard systems specifically.[5] However, on account of its similar nomenclature and multiple overlapping core disciplines (e.g. hydrodynamics, hydromechanics, and materials science), ""ocean engineering"" sometimes operates under the umbrella term of ""marine engineering"", especially in industry and academia outside of the U.S. The same combination has been applied to the rest of this article.
 Oceanography is a scientific field concerned with the acquisition and analysis of data to characterize the ocean. Although separate disciplines, marine engineering and oceanography are closely intertwined: marine engineers often use data gathered by oceanographers to inform their design and research, and oceanographers use tools designed by marine engineers (more specifically, oceanographic engineers) to advance their understanding and exploration of the ocean.[6]
 Marine engineering incorporates many aspects of mechanical engineering. One manifestation of this relationship lies in the design of shipboard propulsion systems. Mechanical engineers design the main propulsion plant, the powering and mechanization aspects of the ship functions such as steering, anchoring, cargo handling, heating, ventilation, air conditioning interior and exterior communication, and other related requirements. Electrical power generation and electrical power distribution systems are typically designed by their suppliers; the only design responsibility of the marine engineering is installation.
 Furthermore, an understanding of mechanical engineering topics such as fluid dynamics, fluid mechanics, linear wave theory, strength of materials, structural mechanics, and structural dynamics is essential to a marine engineer's repertoire of skills. These and other mechanical engineering subjects serve as an integral component of the marine engineering curriculum.[7]
 Civil engineering concepts play in an important role in many marine engineering projects such as the design and construction of ocean structures, ocean bridges and tunnels, and port/harbor design.
 Marine engineering often deals in the fields of electrical engineering and robotics, especially in applications related to employing deep-sea cables and UUVs.
 A series of transoceanic fiber optic cables are responsible for connecting much of the world's communication via the internet, carrying as much as 99 percent of total global internet and signal traffic. These cables must be engineered to withstand deep-sea environments that are remote and often unforgiving, with extreme pressures and temperatures as well as potential interference by fishing, trawling, and sea life.
 The use of unmanned underwater vehicles (UUVs) stands to benefit from the use of autonomous algorithms and networking. Marine engineers aim to learn how advancements in autonomy and networking can be used to enhance existing UUV technologies and facilitate the development of more capable underwater vehicles.
 A knowledge of marine engineering proves useful in the field of petroleum engineering, as hydrodynamics and seabed integration serve as key elements in the design and maintenance of offshore oil platforms.
 Marine construction is the process of building structures in or adjacent to large bodies of water, usually the sea. These structures can be built for a variety of purposes, including transportation, energy production, and recreation. Marine construction can involve the use of a variety of building materials, predominantly steel and concrete. Some examples of marine structures include ships, offshore platforms, moorings, pipelines, cables, wharves, bridges, tunnels, breakwaters and docks.
 In the same way that civil engineers design to accommodate wind loads on building and bridges, marine engineers design to accommodate a ship or submarine struck by waves millions of times over the course of the vessel's life. These load conditions are also found in marine construction and coastal engineering
 Any seagoing vessel has the constant need for hydrostatic stability. A naval architect, like an airplane designer, is concerned with stability. What makes the naval architect's job unique is that a ship operates in two fluids simultaneously: water and air. Even after a ship has been designed and put to sea, marine engineers face the challenge of balancing cargo, as stacking containers vertically increases the mass of the ship and shifts the center of gravity higher. The weight of fuel also presents a problem, as the pitch of the ship may cause the liquid to shift, resulting in an imbalance. In some vessels, this offset will be counteracted by storing water inside larger ballast tanks. Marine engineers are responsible for the task of balancing and tracking the fuel and ballast water of a ship. Floating offshore structures have similar constraints.
 The saltwater environment faced by seagoing vessels makes them highly susceptible to corrosion. In every project, marine engineers are concerned with surface protection and preventing galvanic corrosion. Corrosion can be inhibited through cathodic protection by introducing pieces of metal (e.g. zinc) to serve as a ""sacrificial anode"" in the corrosion reaction. This causes the metal to corrode instead of the ship's hull. Another way to prevent corrosion is by sending a controlled amount of low DC current through the ship's hull, thereby changing the hull's electrical charge and delaying the onset of electro-chemical corrosion. Similar problems are encountered in coastal and offshore structures.
 Anti-fouling is the process of eliminating obstructive organisms from essential components of seawater systems. Depending on the nature and location of marine growth, this process is performed in a number of different ways:
 
The burning of marine fuels releases harmful pollutants into the atmosphere. Ships burn marine diesel in addition to heavy fuel oil. Heavy fuel oil, being the heaviest of refined oils, releases sulfur dioxide when burned. Sulfur dioxide emissions have the potential to raise atmospheric and ocean acidity causing harm to marine life. However, heavy fuel oil may only be burned in international waters due to the pollution created. It is commercially advantageous due to the cost effectiveness compared to other marine fuels. It is prospected that heavy fuel oil will be phased out of commercial use by the year 2020 (Smith, 2018).[10] Water, oil, and other substances collect at the bottom of the ship in what is known as the bilge. Bilge water is pumped overboard, but must pass a pollution threshold test of 15 ppm (parts per million) of oil to be discharged. Water is tested and either discharged if clean or recirculated to a holding tank to be separated before being tested again. The tank it is sent back to, the oily water separator, utilizes gravity to separate the fluids due to their viscosity. Ships over 400 gross tons are required to carry the equipment to separate oil from bilge water. Further, as enforced by MARPOL, all ships over 400 gross tons and all oil tankers over 150 gross tons are required to log all oil transfers in an oil record book (EPA, 2011).[11]
 Cavitation is the process of forming an air bubble in a liquid due to the vaporization of that liquid cause by an area of low pressure. This area of low pressure lowers the boiling point of a liquid allowing it to vaporize into a gas. Cavitation can take place in pumps, which can cause damage to the impeller that moves the fluids through the system. Cavitation is also seen in propulsion. Low pressure pockets form on the surface of the propeller blades as its revolutions per minute increase (IIMS, 2015).[12] Cavitation on the propeller causes a small but violent implosion which could warp the propeller blade. To remedy the issue, more blades allow the same amount of propulsion force but at a lower rate of revolutions. This is crucial for submarines as the propeller needs to keep the vessel relatively quiet to stay hidden. With more propeller blades, the vessel is able to achieve the same amount of propulsion force at lower shaft revolutions.
 The following categories provide a number of focus areas in which marine engineers direct their efforts.
 In designing systems that operate in the arctic (especially scientific equipment such as meteorological instrumentation and oceanographic buoys), marine engineers must overcome an array of design challenges. Equipment must be able to operate at extreme temperatures for prolonged periods of time, often with little to no maintenance. This creates the need for exceptionally temperature-resistant materials and durable precision electronic components.[citation needed]
 Coastal engineering applies a mixture of civil engineering and other disciplines to create coastal solutions for areas along or near the ocean. In protecting coastlines from wave forces, erosion, and sea level rise, marine engineers must consider whether they will use a ""gray"" infrastructure solution - such as a breakwater, culvert, or sea wall made from rocks and concrete - or a ""green"" infrastructure solution that incorporates aquatic plants, mangroves, and/or marsh ecosystems.[13] It has been found that gray infrastructure costs more to build and maintain, but it may provide better protection against ocean forces in high-energy wave environments.[14] A green solution is generally less expensive and more well-integrated with local vegetation, but may be susceptible to erosion or damage if executed improperly.[15] In many cases engineers will select a hybrid approach that combines elements of both gray and green solutions.[16]
 The design of underwater life-support systems such as underwater habitats presents a unique set of challenges requiring a detailed knowledge of pressure vessels, diving physiology, and thermodynamics.
 Marine engineers may design or make frequent use of unmanned underwater vehicles, which operate underwater without a human aboard. UUVs often perform work in locations which would be otherwise impossible or difficult to access by humans due to a number of environmental factors (e.g. depth, remoteness, and/or temperature). UUVs can be remotely operated by humans, like in the case of remotely operated vehicles, semi-autonomous, or autonomous.
 The development of oceanographic sciences, subsea engineering and the ability to detect, track and destroy submarines (anti-submarine warfare) required the parallel development of a host of marine scientific instrumentation and sensors. Visible light is not transferred far underwater, so the medium for transmission of data is primarily acoustic. High-frequency sound is used to measure the depth of the ocean, determine the nature of the seafloor, and detect submerged objects. The higher the frequency, the higher the definition of the data that is returned. Sound Navigation and Ranging or SONAR was developed during the First World War to detect submarines, and has been greatly refined through to the present day. Submarines similarly use sonar equipment to detect and target other submarines and surface ships, and to detect submerged obstacles such as seamounts that pose a navigational obstacle. Simple echo-sounders point straight down and can give an accurate reading of ocean depth (or look up at the underside of sea-ice).
More advanced echo sounders use a fan-shaped beam or sound, or multiple beams to derive highly detailed images of the ocean floor. High power systems can penetrate the soil and seabed rocks to give information about the geology of the seafloor, and are widely used in geophysics for the discovery of hydrocarbons, or for engineering survey. 
For close-range underwater communications, optical transmission is possible, mainly using blue lasers. These have a high bandwidth compared with acoustic systems, but the range is usually only a few tens of metres, and ideally at night. 
As well as acoustic communications and navigation, sensors have been developed to measure ocean parameters such as temperature, salinity, oxygen levels and other properties including nitrate levels, levels of trace chemicals and environmental DNA. The industry trend has been towards smaller, more accurate and more affordable systems so that they can be purchased and used by university departments and small companies as well as large corporations, research organisations and governments. The sensors and instruments are fitted to autonomous and remotely-operated systems as well as ships, and are enabling these systems to take on tasks that hitherto required an expensive human-crewed platform.
Manufacture of marine sensors and instruments mainly takes place in Asia, Europe and North America. Products are advertised in specialist journals, and through Trade Shows such as Oceanology International and Ocean Business which help raise awareness of the products.
 In every coastal and offshore project, environmental sustainability is an important consideration for the preservation of ocean ecosystems and natural resources. Instances in which marine engineers benefit from knowledge of environmental engineering include creation of fisheries, clean-up of oil spills, and creation of coastal solutions.[17]
 A number of systems designed fully or in part by marine engineers are used offshore - far away from coastlines.
 The design of offshore oil platforms involves a number of marine engineering challenges. Platforms must be able to withstand ocean currents, wave forces, and saltwater corrosion while remaining structurally integral and fully anchored into the seabed. Additionally, drilling components must be engineered to handle these same challenges with a high factor of safety to prevent oil leaks and spills from contaminating the ocean.
 Offshore wind farms encounter many similar marine engineering challenges to oil platforms. They provide a source of renewable energy with a higher yield than wind farms on land, while encountering less resistance from the general public (see NIMBY).[18]
 Marine engineers continue to investigate the possibility of ocean wave energy as a viable source of power for distributed or grid applications. Many designs have been proposed and numerous prototypes have been built, but the problem of harnessing wave energy in a cost-effective manner remains largely unresolved.[19]
 A marine engineer may also deal with the planning, creation, expansion, and modification of port and harbor designs. Harbors can be natural or artificial and protect anchored ships from wind, waves, and currents.[20] Ports can be defined as a city, town, or place where ships are moored, loaded, or unloaded. Ports typically reside within a harbor and are made up of one or more individual terminals that handle a particular cargo including passengers, bulk cargo, or containerized cargo.[21] Marine engineers plan and design various types of marine terminals and structures found in ports, and they must understand the loads imposed on these structures over the course of their lifetime.
 Marine salvage techniques are continuously modified and improved to recover shipwrecks. Marine engineers use their skills to assist at some stages of this process.
 With a diverse engineering background, marine engineers work in a variety of industry jobs across every field of math, science, technology, and engineering. A few companies such as Oceaneering International and Van Oord specialize in marine engineering, while other companies consult marine engineers for specific projects. Such consulting commonly occurs in the oil industry, with companies such as ExxonMobil and BP hiring marine engineers to manage aspects of their offshore drilling projects.
 Marine engineering lends itself to a number of military applications – mostly related to the Navy. The United States Navy's Seabees, Civil Engineer Corps, and Engineering Duty Officers often perform work related to marine engineering. Military contractors (especially those in naval shipyards) and the Army Corps of Engineers play a role in certain marine engineering projects as well.
 In 2012, the average annual earnings for marine engineers in the U.S. were $96,140 with average hourly earnings of $46.22.[22] As a field, marine engineering is predicted to grow approximately 12% from 2016 to 2026. Currently, there are about 8,200 naval architects and marine engineers employed, however, this number is expected to increase to 9,200 by 2026 (BLS, 2017).[23] This is due at least in part to the critical role of the shipping industry on the global market supply chain; 80% of the world's trade by volume is done overseas by close to 50,000 ships, all of which require marine engineers aboard and shoreside (ICS, 2017).[24] Additionally, offshore energy continues to grow, and a greater need exists for coastal solutions due to sea level rise.
 Maritime universities are dedicated to teaching and training students in maritime professions. Marine engineers generally have a bachelor's degree in marine engineering, marine engineering technology, or marine systems engineering. Practical training is valued by employers alongside the bachelor's degree.
 A number of institutions - including MIT,[26] UC Berkeley,[27] the U.S. Naval Academy,[28] and Texas A&M University[29] - offer a four-year Bachelor of Science degree specifically in ocean engineering. Accredited programs consist of basic undergraduate math and science subjects such as calculus, statistics, chemistry, and physics; fundamental engineering subjects such as statics, dynamics, electrical engineering, and thermodynamics; and more specialized subjects such as ocean structural analysis, hydromechanics, and coastal management.
 Graduate students in ocean engineering take classes on more advanced, in-depth subjects while conducting research to complete a graduate-level thesis. The Massachusetts Institute of Technology offers master's and PhD degrees specifically in ocean engineering.[30] Additionally, MIT co-hosts a joint program with the Woods Hole Oceanographic Institution for students studying ocean engineering and other ocean-related topics at the graduate level.[31][32]
 Journals about ocean engineering include Ocean Engineering,[33] the IEEE Journal of Oceanic Engineering[34] and the Journal of Waterway, Port, Coastal, and Ocean Engineering.[35]
 Conferences in the field of marine engineering include the IEEE Oceanic Engineering Society's OCEANS Conference and Exposition[36] and the European Wave and Tidal Energy Conference (EWTEC).[37]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['ocean engineering', 'Oceaneering International and Van Oord', 'design and application of shipboard systems specifically', 'ocean structural analysis, hydromechanics, and coastal management', 'application of shipboard systems specifically'], 'answer_start': [], 'answer_end': []}"
"Dance music is music composed specifically to facilitate or accompany dancing. It can be either a whole piece or part of a larger musical arrangement. In terms of performance, the major categories are live dance music and recorded dance music. While there exist attestations of the combination of dance and music in ancient history (for example Ancient Greek vases sometimes show dancers accompanied by musicians), the earliest Western dance music that we can still reproduce with a degree of certainty are old-fashioned dances. In the Baroque period, the major dance styles were noble court dances (see Baroque dance). In the classical music era, the minuet was frequently used as a third movement, although in this context it would not accompany any dancing. The waltz also arose later in the classical era. Both remained part of the romantic music period, which also saw the rise of various other nationalistic dance forms like the barcarolle, mazurka, ecossaise, ballade and polonaise.
 Modern popular dance music initially emerged from late 19th century's Western ballroom and social dance music. During the early 20th century, ballroom dancing gained popularity among the working class who attended public dance halls. Dance music became enormously popular during the 1920s. In the 1930s, known as the Swing era, Swing music was the popular dance music in America. In the 1950s, rock and roll became the popular dance music. The late 1960s saw the rise of soul and R&B music. Dominican and Cuban New Yorkers created the popular salsa dance in the late 1960s which stemmed from the Latin music genre of salsa. The rise of disco in the early 1970s led to dance music becoming popular with the public. By the late 1970s, electronic dance music was developing. This music, made using electronics, is a style of popular music commonly played in nightclubs, radio stations, shows and raves. Many subgenres of electronic dance music  have evolved.
 Dancing to rhythmic music has long been a cherished tradition in both Western and Eastern African civilizations, where dynamic movements synchronized with percussion instruments such as drums, bells, and rattles serve as integral expressions of cultural identity, social cohesion, and spiritual significance. 
 Folk dance music is music accompanying traditional dance and may be contrasted with historical/classical, and popular/commercial dance music. An example of folk dance music in the United States is the old-time music played at square dances and contra dances.
 While there exist attestations of the combination of dance and music in ancient times (for example Ancient Greek vases sometimes show dancers accompanied by musicians), the earliest Western dance music that we can still reproduce with a degree of certainty are the surviving medieval dances such as carols and the Estampie. The earliest of these surviving dances are almost as old as Western staff-based music notation.
 The Renaissance dance music was written for instruments such as the lute, viol, tabor, pipe, and the sackbut.
 In the Baroque period, the major dance styles were noble court dances (see Baroque dance). Examples of dances include the French courante, sarabande, minuet and gigue. Collections of dances were often collected together as dance suites.
 In the classical music era, the minuet was frequently used as a third movement in four-movement non-vocal works such as sonatas, string quartets, and symphonies, although in this context it would not accompany any dancing. The waltz also arose later in the classical era, as the minuet evolved into the scherzo (literally, ""joke""; a faster-paced minuet).
 Both remained part of the romantic music period, which also saw the rise of various other nationalistic dance forms like the barcarolle, mazurka and polonaise. Also in the romantic music era, the growth and development of ballet extended the composition of dance music to a new height. Frequently, dance music was a part of opera.
 Modern popular dance music initially emerged from late 19th century's Western ballroom and social dance music.
 Dance music works often bear the name of the corresponding dance, e.g. waltzes, the tango, the bolero, the can-can, minuets, salsa, various kinds of jigs and the breakdown. Other dance forms include contradance, the merengue (Dominican Republic), and the cha-cha-cha. Often it is difficult to know whether the name of the music came first or the name of the dance.
 Ballads are commonly chosen for slow-dance routines. However ballads have been commonly deemed the opposite of dance music in terms of their tempo.[citation needed] Originally, the ballad was a type of dance as well (hence the name ""ballad"", from the same root as ""ballroom"" and ""ballet""). Ballads are still danced on the Faeroe Islands.
 ""Dansband"" (""Dance band"") is a term in Swedish for bands who play a kind of popular music, ""dansbandsmusik"" (""Dance band music""), to partner dance to. These terms came into use around 1970, and before that, many of the bands were classified as ""pop groups"". This type of music is mostly popular in the Nordic countries.
 Disco is a genre of dance music containing elements of funk, soul, pop, and salsa. It was most popular during the mid to late 1970s, though it has had brief resurgences afterwards. The first notable fully synthesized disco hit was ""I Feel Love"" by Donna Summer.[1] Looping,It inspired the electronic dance music genre.
 By 1981, a new form of dance music was developing. This music, made using electronics, is a style of popular music commonly played in dance music nightclubs, radio stations, shows and raves. During its gradual decline in the late 1970s, disco became influenced by electronic musical instruments such as synthesizers. sampling and segueing as found in disco continued to be used as creative techniques within trance music, techno music and especially house music.
 Electronic dance music experienced a boom in the late 1980s. In the UK, this manifested itself in the dance element of Tony Wilson's Haçienda scene (in Manchester) and London clubs like Delirium, The Trip, and Shoom. The scene rapidly expanded to the Summer Of Love in Ibiza, which became the European capital of house and trance. In 2018, the release of Fisher's ""Losing It,"" a significant tech-house crossover by the Australian EDM producer, marked a notable shift in trends within the dance music landscape.
 Many music genres that made use of electronic instruments developed into contemporary styles mainly due to the MIDI protocol, which enabled computers, synthesizers, sound cards, samplers, and drum machines to interact with each other and achieve the full synchronization of sounds. Electronic dance music is typically composed using synthesizers and computers, and rarely has any physical instruments. Instead, this is replaced by analogue and digital electronic sounds, with a 4/4 beat. Many producers of this kind of music however, such as Darren Tate and MJ Cole, were trained in classical music before they moved into the electronic medium.
 Associated with dance music are usually commercial tracks that may not easily be categorized, such as ""The Power"" by Snap!, ""No Limit"" by 2 Unlimited, ""Gonna Make You Sweat (Everybody Dance Now)"" by C+C Music Factory, and the Beatmasters' ""Rok da House"" but the term ""dance music"" is applied to many forms of electronic music, both commercial and non-commercial.
 Some of the most popular upbeat genres include house, techno, drum & bass, jungle, hardcore, electronica, industrial, breakbeat, trance, psychedelic trance, UK garage and electro. There are also much slower styles, such as downtempo, chillout and nu jazz.
 Many subgenres of electronic dance music have evolved. Subgenres of house include acid house, kwaito, electro house, hard house, funky house,deep house,afro house, tribal house, hip house, tech house and US garage. Subgenres of drum & bass include techstep, hardstep, jump-up, intelligent D&B/atmospheric D&B, liquid funk, sambass, drumfunk, neurofunk and ragga jungle. Subgenres of other styles include progressive breaks, booty bass, Goa trance, hard trance, hardstyle, minimal techno, gabber techno, breakcore, broken beat, trip hop, folktronica and glitch. Speed garage, breakstep, 2-step, bassline, grime, UK funky, future garage and the reggae-inspired dubstep are all subgenres of UK garage.
 During the early 20th century, ballroom dancing gained popularity among the working class who attended public dance halls.
 Dance music became enormously popular during the 1920s. Nightclubs were frequented by large numbers of people at which a form of jazz, which was characterized by fancy orchestras with strings instruments and complex arrangements, became the standard music at clubs. A particularly popular dance was the fox-trot. At the time this music was simply called jazz, although today people refer to it as ""white jazz"" or big band. Marabi evolved in South Africa in the 1920s, rooted in South African folk music, ragtime, jazz and blues. People were able to dance endlessly without having to have been familiar with the songs being played, before.[2][3]
 Genres: Swing music,mbube, Western swing. Duke Ellington, Benny Goodman and Glenn Miller gained swing jazz hits.
 Genres: Rock and roll, kwela
 In 1952, the television showed that  American Bandstand switched to a format where teenagers dance along as records are played. American Bandstand continued to be shown until 1989. Since the late 1950s, disc jockeys (commonly known as DJs) played recorded music at nightclubs.
 Genres: Rock and roll, R&B, funk, mbaqanga
 In 1960, Chubby Checker released his song ""The Twist"" setting off a dance craze.  The late 1960s saw the rise of soul and R&B music which used lavish orchestral arrangements.
 Genres: Disco, funk, R&B, hip hop
 In 1970, the television show Soul Train premiered featuring famous soul artists who would play or lipsync their hits while the audience danced along. In the early '70s, Kool and the Gang, Ohio Players, and B.T. Express were popular funk bands. By the mid-1970s, disco had become one of the main genres featured.  In 1974, Billboard added a Disco Action chart of top hits to its other charts (see List of Billboard number one dance club songs). Donna Summer, the Bee Gees, the Village People and Gloria Gaynor gained pop hits.[4] Disco was characterized by the use of real orchestral instruments, such as strings, which had largely been abandoned during the 1950s because of rock music. In contrast to the 1920s, however, the use of live orchestras in night clubs was extremely rare due to its expense.  The disco craze reached its peak in the late 1970s when the word ""disco"" became synonymous with ""dance music"" and nightclubs were referred to as ""discos"".
 Genres: Funk, hip hop, New jack swing,[5] R&B, bounce, Miami bass, boogie, disco,jaiva, contemporary R&B, new wave, dark wave, Italo disco, Euro disco, post-disco, synth-pop, dance-pop, dance-rock, house, kwaito, acid house, hip house, techno, freestyle, electro, hi-NRG, EBM, cosmic disco, Balearic beat, new beat
 Genres: New jack swing, contemporary R&B, dancehall, hip hop, G-funk, Miami bass, house, Italo dance, Italo house, Eurodance, Europop, hip house, electro, electroclash, progressive house, French house, techno, minimal techno, trance, alternative dance, drum and bass, jungle, big beat, breakbeat, breakbeat hardcore, rave, hardcore, happy hardcore, speed garage, UK garage, soca, reggaeton, psytrance, Goa trance, Afro house
 Genres: Trance,Afro-tech, electropop, dance-pop, snap, crunk, dancehall, reggaeton, dance-punk, nu-disco, electro house, minimal techno, dubstep, grime, bassline, UK funky, contemporary R&B, hip hop, drum and bass, progressive house, hardstyle, funky house
 Genres: Electropop, synthpop,gqom,amapiano, glitchpop, hip house, nu-disco, new wave, new rave, trance, house, hi-NRG, hard NRG, dance-pop, electro-industrial, deep house, drum and bass, dubstep, techstep, liquid funk, electro house, progressive house, breakbeat, hardstyle, dubstyle, drumstep, hip hop, ghetto house, Jersey club, trap, drill, moombahton, moombahcore, dancehall, tropical house, UK garage, Europop, hyperpop
 The Dance/Mix Show Airplay chart tracks the most popular tracks played by radio stations using a ""dance music"" format. Modern dance music is typically a core component of the rhythmic adult contemporary and rhythmic contemporary formats, and an occasional component of the contemporary hit radio format in the case of dance songs which chart.
 Mixshows are radio programmes which feature a sequence of dance music tracks where each track's outro is mixed into the intro of the next.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['live dance music and recorded dance music', 'Darren Tate and MJ Cole', 'the combination of dance and music in ancient history', 'live dance music and recorded dance music', 'specifically to facilitate or accompany dancing'], 'answer_start': [], 'answer_end': []}"
"
 
E-commerce (electronic commerce) is the activity of electronically buying or selling products on online services or over the Internet. E-commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. E-commerce is the largest sector of the electronics industry and is in turn driven by the technological advances of the semiconductor industry.
 The term was coined and first employed by Robert Jacobson, Principal Consultant to the California State Assembly's Utilities & Commerce Committee, in the title and text of California's Electronic Commerce Act, carried by the late Committee Chairwoman Gwen Moore (D-L.A.) and enacted in 1984.
 E-commerce typically uses the web for at least a part of a transaction's life cycle although it may also use other technologies such as e-mail. Typical e-commerce transactions include the purchase of products (such as books from Amazon) or services (such as music downloads in the form of digital distribution such as the iTunes Store).[1] There are three areas of e-commerce: online retailing, electronic markets, and online auctions. E-commerce is supported by electronic business.[2] The existence value of e-commerce is to allow consumers to shop online and pay online through the Internet, saving the time and space of customers and enterprises, greatly improving transaction efficiency, especially for busy office workers, and also saving a lot of valuable time.[3]
 E-commerce businesses may also employ some or all of the following:
 There are five essential categories of E-commerce:[6]
 Contemporary electronic commerce can be classified into two categories. The first category is business based on types of goods sold (involves everything from ordering ""digital"" content for immediate online consumption, to ordering conventional goods and services, to ""meta"" services to facilitate other types of electronic commerce). The second category is based on the nature of the participant (B2B, B2C, C2B and C2C).[7]
 On the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are pressing issues for electronic commerce.
 Aside from traditional e-commerce, the terms m-Commerce (mobile commerce) as well (around 2013) t-Commerce[8] have also been used.
 In the United States, California's Electronic Commerce Act (1984), enacted by the Legislature, the more recent California Privacy Rights Act (2020), enacted through a popular election proposition and to control specifically how electronic commerce may be conducted in California. In the US in its entirety, electronic commerce activities are regulated more broadly by the Federal Trade Commission (FTC). These activities include the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive.[9] Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information.[10] As a result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.
 The Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.[11]
 Conflict of laws in cyberspace is a major hurdle for harmonization of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996).[12]
 Internationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.
 There is also Asia Pacific Economic Cooperation. APEC was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.
 In Australia, trade is covered under Australian Treasury Guidelines for electronic commerce and the Australian Competition & Consumer Commission[13] regulates and offers advice on how to deal with businesses online,[14] and offers specific advice on what happens if things go wrong.[15]
 The European Union undertook an extensive enquiry into e-commerce in 2015-16 which observed significant growth in the development of e-commerce, along with some developments which raised concerns, such as increased use of selective distribution systems, which allow manufacturers to control routes to market, and ""increased use of contractual restrictions to better control product distribution"". The European Commission felt that some emerging practices might be justified if they could improve the quality of product distribution, but ""others may unduly prevent consumers from benefiting from greater product choice and lower prices in e-commerce and therefore warrant Commission action"" in order to promote compliance with EU competition rules.[16]
 In the United Kingdom, the Financial Services Authority (FSA)[17] was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority.[18] The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD requires the European Commission to report on the implementation and impact of the PSD by 1 November 2012.[19]
 In India, the Information Technology Act 2000 governs the basic applicability of e-commerce.
 In China, the Telecommunications Regulations of the People's Republic of China (promulgated on 25 September 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce.[20] On the same day, the Administrative Measures on Internet Information Services were released, the first administrative regulations to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China.[21] On 28 August 2004, the eleventh session of the tenth NPC Standing Committee adopted an Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China's e-commerce legislation. It was a milestone in the course of improving China's electronic commerce legislation, and also marks the entering of China's rapid development stage for electronic commerce legislation.[22]
 E-commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.[23][24]
 Cross-border e-Commerce is also an essential field for e-Commerce businesses.  It has responded to the trend of globalization. It shows that numerous firms have opened up new businesses, expanded new markets, and overcome trade barriers; more and more enterprises have started exploring the cross-border cooperation field. In addition, compared with traditional cross-border trade, the information on cross-border e-commerce is more concealed. In the era of globalization, cross-border e-commerce for inter-firm companies means the activities, interactions, or social relations of two or more e-commerce enterprises. However, the success of cross-border e-commerce promotes the development of small and medium-sized firms, and it has finally become a new transaction mode. It has helped the companies solve financial problems and realize the reasonable allocation of resources field. SMEs ( small and medium enterprises) can also precisely match the demand and supply in the market, having the industrial chain majorization and creating more revenues for companies.[25]
 In 2012, e-commerce sales topped $1 trillion for the first time in history.[26]
 Mobile devices are playing an increasing role in the mix of e-commerce, this is also commonly called mobile commerce, or m-commerce. In 2014, one estimate saw purchases made on mobile devices making up 25% of the market by 2017.[27]
 For traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested an enormous volume of investment in mobile applications. The DeLone and McLean Model stated that three perspectives contribute to a successful e-business: information system quality, service quality and users' satisfaction.[28] There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in the company.[29]
 Modern 3D graphics technologies, such as Facebook 3D Posts, are considered by some social media marketers and advertisers as a preferable way to promote consumer goods than static photos, and some brands like Sony are already paving the way for augmented reality commerce. Wayfair now lets you inspect a 3D version of its furniture in a home setting before buying.[30]
 Among emerging economies, China's e-commerce presence continues to expand every year. With 668 million Internet users, China's online shopping sales reached $253 billion in the first half of 2015, accounting for 10% of total Chinese consumer retail sales in that period.[31] The Chinese retailers have been able to help consumers feel more comfortable shopping online.[32] e-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade.[33] In 2013, Alibaba had an e-commerce market share of 80% in China.[34] In 2014, Alibaba still dominated the B2B marketplace in China with a market share of 44.82%, followed by several other companies including Made-in-China.com at 3.21%, and GlobalSources.com at 2.98%, with the total transaction value of China's B2B market exceeding 4.5 billion yuan.[35] In 2014, there were 600 million Internet users in China (twice as many as in the US), making it the world's biggest online market.[36]
 China is also the largest e-commerce market in the world by value of sales, with an estimated US$899 billion in 2016.[37] It accounted for 42.4% of worldwide retail e-commerce in that year, the most of any country.[38]: 110  Research shows that Chinese consumer motivations are different enough from Western audiences to require unique e-commerce app designs instead of simply porting Western apps into the Chinese market.[39]
 The expansion of e-commerce in China has resulted in the development of Taobao villages, clusters of e-commerce businesses operating in rural areas.[38]: 112  Because Taobao villages have increased the incomes or rural people and entrepreneurship in rural China, Taobao villages have become a component of rural revitalization strategies.[40]: 278 
 In 2019, the city of Hangzhou established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to e-commerce and internet-related intellectual property claims.[41]: 124 
 In 2010, the United Kingdom had the highest per capita e-commerce spending in the world.[42] As of 2013, the Czech Republic was the European country where e-commerce delivers the biggest contribution to the enterprises' total revenue. Almost a quarter (24%) of the country's total turnover is generated via the online channel.[43]
 The rate of growth of the number of internet users in the Arab countries has been rapid – 13.1% in 2015. A significant portion of the e-commerce market in the Middle East comprises people in the 30–34 year age group. Egypt has the largest number of internet users in the region, followed by Saudi Arabia and Morocco; these constitute 3/4th of the region's share. Yet, internet penetration is low: 35% in Egypt and 65% in Saudi Arabia.[44]
 The Gulf Cooperation Council countries have a rapidly growing market and are characterized by a population that becomes wealthier (Yuldashev). As such, retailers have launched Arabic-language websites as a means to target this population. Secondly, there are predictions of increased mobile purchases and an expanding internet audience (Yuldashev). The growth and development of the two aspects make the GCC countries become larger players in the electronic commerce market with time progress. Specifically, research shows that the e-commerce market is expected to grow to over $20 billion by 2020 among these GCC countries (Yuldashev). The e-commerce market has also gained much popularity among western countries, and in particular Europe and the U.S. These countries have been highly characterized by consumer-packaged goods (CPG) (Geisler, 34). However, trends show that there are future signs of a reverse. Similar to the GCC countries, there has been increased purchase of goods and services in online channels rather than offline channels. Activist investors are trying hard to consolidate and slash their overall cost and the governments in western countries continue to impose more regulation on CPG manufacturers (Geisler, 36). In these senses, CPG investors are being forced to adapt to e-commerce as it is effective as well as a means for them to thrive.
 The future trends in the GCC countries will be similar to that of the western countries. Despite the forces that push business to adapt e-commerce as a means to sell goods and products, the manner in which customers make purchases is similar in countries from these two regions. For instance, there has been an increased usage of smartphones which comes in conjunction with an increase in the overall internet audience from the regions. Yuldashev writes that consumers are scaling up to more modern technology that allows for mobile marketing.
However, the percentage of smartphone and internet users who make online purchases is expected to vary in the first few years. It will be independent on the willingness of the people to adopt this new trend (The Statistics Portal). For example, UAE has the greatest smartphone penetration of 73.8 per cent and has 91.9 per cent of its population has access to the internet. On the other hand, smartphone penetration in Europe has been reported to be at 64.7 per cent (The Statistics Portal). Regardless, the disparity in percentage between these regions is expected to level out in future because e-commerce technology is expected to grow to allow for more users.
 The e-commerce business within these two regions will result in competition. Government bodies at the country level will enhance their measures and strategies to ensure sustainability and consumer protection (Krings, et al.). These increased measures will raise the environmental and social standards in the countries, factors that will determine the success of the e-commerce market in these countries. For example, an adoption of tough sanctions will make it difficult for companies to enter the e-commerce market while lenient sanctions will allow ease of companies. As such, the future trends between GCC countries and the Western countries will be independent of these sanctions (Krings, et al.). These countries need to make rational conclusions in coming up with effective sanctions.
 India has an Internet user base of about 460 million as of December 2017.[45] Despite being the third largest user base in the world, the penetration of the Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around six million new entrants every month.[citation needed] In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities.[46][citation needed] The India retail market is expected to rise from 2.5% in 2016 to 5% in 2020.[47]
 In 2013, Brazil's e-commerce was growing quickly with retail e-commerce sales expected to grow at a double-digit pace through 2014. By 2016, eMarketer expected retail e-commerce sales in Brazil to reach $17.3 billion.[48]
 Logistics in e-commerce mainly concerns fulfillment. Online markets and retailers have to find the best possible way to fill orders and deliver products. Small companies usually control their own logistic operation because they do not have the ability to hire an outside company. Most large companies hire a fulfillment service that takes care of a company's logistic needs.[49] The optimization of logistics processes that contains long-term investment in an efficient storage infrastructure system and adoption of inventory management strategies is crucial to prioritize customer satisfaction throughout the entire process, from order placement to final delivery. [50]
 E-commerce markets are growing at noticeable rates. The online market is expected to grow by 56% in 2015–2020. In 2017, retail e-commerce sales worldwide amounted to 2.3 trillion US dollars and e-retail revenues are projected to grow to 4.891 trillion US dollars in 2021.[51] Traditional markets are only expected 2% growth during the same time. Brick and mortar retailers are struggling because of online retailer's ability to offer lower prices and higher efficiency. Many larger retailers are able to maintain a presence offline and online by linking physical and online offerings.[52]
 E-commerce allows customers to overcome geographical barriers and allows them to purchase products anytime and from anywhere. Online and traditional markets have different strategies for conducting business. Traditional retailers offer fewer assortment of products because of shelf space where, online retailers often hold no inventory but send customer orders directly to the manufacturer. The pricing strategies are also different for traditional and online retailers. Traditional retailers base their prices on store traffic and the cost to keep inventory. Online retailers base prices on the speed of delivery.
 There are two ways for marketers to conduct business through e-commerce: fully online or online along with a brick and mortar store. Online marketers can offer lower prices, greater product selection, and high efficiency rates. Many customers prefer online markets if the products can be delivered quickly at relatively low price. However, online retailers cannot offer the physical experience that traditional retailers can. It can be difficult to judge the quality of a product without the physical experience, which may cause customers to experience product or seller uncertainty. Another issue regarding the online market is concerns about the security of online transactions. Many customers remain loyal to well-known retailers because of this issue.[53]
 Security is a primary problem for e-commerce in developed and developing countries. E-commerce security is protecting businesses' websites and customers from unauthorized access, use, alteration, or destruction. The type of threats include: malicious codes, unwanted programs (ad ware, spyware), phishing, hacking, and cyber vandalism. E-commerce websites use different tools to avert security threats. These tools include firewalls, encryption software, digital certificates, and passwords.[citation needed]
 For a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.[54]
 E-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimized the capacity of information processing than companies used to have, and for the financial flows, e-commerce allows companies to have more efficient payment and settlement solutions.[54]
 In addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems, like SAP ERP, Xero, or Megaventory, have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.[54]
 E-commerce helps create new job opportunities due to information related services, software app and digital products. It also causes job losses. The areas with the greatest predicted job-loss are retail, postal, and travel agencies. The development of e-commerce will create jobs that require highly skilled workers to manage large amounts of information, customer demands, and production processes. In contrast, people with poor technical skills cannot enjoy the wages welfare. On the other hand, because e-commerce requires sufficient stocks that could be delivered to customers in time, the warehouse becomes an important element. Warehouse needs more staff to manage, supervise and organize, thus the condition of warehouse environment will be concerned by employees.[55]
 E-commerce brings convenience for customers as they do not have to leave home and only need to browse websites online, especially for buying products which are not sold in nearby shops. It could help customers buy a wider range of products and save customers' time. Consumers also gain power through online shopping. They are able to research products and compare prices among retailers. Thanks to the practice of user-generated ratings and reviews from companies like Bazaarvoice, Trustpilot, and Yelp, customers can also see what other people think of a product, and decide before buying if they want to spend money on it.[56][57] Also, online shopping often provides sales promotion or discounts code, thus it is more price effective for customers. Moreover, e-commerce provides products' detailed information; even the in-store staff cannot offer such detailed explanation. Customers can also review and track the order history online. 
 E-commerce technologies cut transaction costs by allowing both manufactures and consumers to skip through the intermediaries. This is achieved through by extending the search area best price deals and by group purchase. The success of e-commerce in urban and regional levels depend on how the local firms and consumers have adopted to e-commerce.[58]
 However, e-commerce lacks human interaction for customers, especially who prefer face-to-face connection. Customers are also concerned with the security of online transactions and tend to remain loyal to well-known retailers. In recent years, clothing retailers such as Tommy Hilfiger have started adding Virtual Fit platforms to their e-commerce sites to reduce the risk of customers buying the wrong sized clothes, although these vary greatly in their fit for purpose.[59] When the customer regret the purchase of a product, it involves returning goods and refunding process. This process is inconvenient as customers need to pack and post the goods. If the products are expensive, large or fragile, it refers to safety issues.[52]
 In 2018, E-commerce generated 1.3 million short tons (1.2 megatonnes) of container cardboard in North America, an increase from 1.1 million (1.00)) in 2017. Only 35 percent of North American cardboard manufacturing capacity is from recycled content. The recycling rate in Europe is 80 percent and Asia is 93 percent. Amazon, the largest user of boxes, has a strategy to cut back on packing material and has reduced packaging material used by 19 percent by weight since 2016. Amazon is requiring retailers to manufacture their product packaging in a way that does not require additional shipping packaging. Amazon also has an 85-person team researching ways to reduce and improve their packaging and shipping materials.[60]
 Accelerated movement of packages around the world includes accelerated movement of living things, with all its attendant risks.[61] Weeds, pests, and diseases all sometimes travel in packages of seeds.[61] Some of these packages are part of brushing manipulation of e-commerce reviews.[61]
 E-commerce has been cited as a major force for the failure of major U.S. retailers in a trend frequently referred to as a ""retail apocalypse.""[62] The rise of e-commerce outlets like Amazon has made it harder for traditional retailers to attract customers to their stores and forced companies to change their sales strategies. Many companies have turned to sales promotions and increased digital efforts to lure shoppers while shutting down brick-and-mortar locations.[63] The trend has forced some traditional retailers to shutter its brick and mortar operations.[64]
 In March 2020, global retail website traffic hit 14.3 billion visits[65] signifying an unprecedented growth of e-commerce during the lockdown of 2020. Later studies show that online sales increased by 25% and online grocery shopping increased by over 100% during the crisis in the United States.[66] Meanwhile, as many as 29% of surveyed shoppers state that they will never go back to shopping in person again; in the UK, 43% of consumers state that they expect to keep on shopping the same way even after the lockdown is over.[67]
 Retail sales of e-commerce shows that COVID-19 has a significant impact on e-commerce and its sales are expected to reach $6.5 trillion by 2023.[68]
 Some common applications related to electronic commerce are:
 A timeline for the development of e-commerce:
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['e-commerce', 'Saudi Arabia and Morocco', 'A timeline for the development of e-commerce', 'phishing, hacking, and cyber vandalism', 'A timeline for the development of e-commerce'], 'answer_start': [], 'answer_end': []}"
"""World music"" is an English phrase for styles of music from non-Western countries, including quasi-traditional, intercultural, and traditional music.  World music's broad nature and elasticity as a musical category pose obstacles to a universal definition, but its ethic of interest in the culturally exotic is encapsulated in Roots magazine's description of the genre as ""local music from out there"".[1][2]
 This music that does not follow ""North American or British pop and folk traditions""[3] was given the term ""world music"" by music industries in Europe and North America.[4] The term was popularized in the 1980s as a marketing category for non-Western traditional music.[5][6] It has grown to include subgenres such as ethnic fusion (Clannad, Ry Cooder, Enya, etc.)[7] and worldbeat.[8][9]
 The term ""world music"" has been credited to ethnomusicologist Robert E. Brown, who coined it in the early 1960s at Wesleyan University in Connecticut, where he developed undergraduate through doctoral programs in the discipline. To enhance the learning process (John Hill), he invited more than a dozen visiting performers from Africa and Asia and began a world music concert series.[10][11] The term became current in the 1980s as a marketing/classificatory device in the media and the music industry.[12] There are several conflicting definitions for world music. One is that it consists of ""all the music in the world"", though such a broad definition renders the term virtually meaningless.[13][14]
 Examples of popular forms of world music include the various forms of non-European classical music (e.g. Chinese guzheng music, Indian raga music, Tibetan chants), Eastern European folk music (e.g. the village music of the Balkans, The Mystery of the Bulgarian Voices), Nordic folk music, Latin music, Indonesian music, and the many forms of folk and tribal music of the Middle East, Africa, Asia, Oceania, Central and South America.
 The broad category of world music includes isolated forms of ethnic music from diverse geographical regions. These dissimilar strains of ethnic music are commonly categorized together by virtue of their indigenous roots. Over the 20th century, the invention of sound recording, low-cost international air travel, and common access to global communication among artists and the general public have given rise to a related phenomenon called ""crossover"" music. Musicians from diverse cultures and locations could readily access recorded music from around the world, see and hear visiting musicians from other cultures and visit other countries to play their own music, creating a melting pot of stylistic influences. While communication technology allows greater access to obscure forms of music, the pressures of commercialization also present the risk of increasing musical homogeneity, the blurring of regional identities, and the gradual extinction of traditional local music-making practices.[15]
 Since the music industry established this term, the fuller scope of what an average music consumer defines as ""world"" music in today's market has grown to include various blends of ethnic music tradition, style and interpretation,[9] and derivative world music genres have been coined to represent these hybrids, such as ethnic fusion and worldbeat. Good examples of hybrid, world fusion are the Irish-West African meld of Afro Celt Sound System,[16] the pan-cultural sound of AO Music[17] and the jazz / Finnish folk music of Värttinä,[18] each of which bear tinges of contemporary, Western influence—an increasingly noticeable element in the expansion genres of world music. Worldbeat and ethnic fusion can also blend specific indigenous sounds with more blatant elements of Western pop. Good examples are Paul Simon's album Graceland, on which South African mbaqanga music is heard; Peter Gabriel's work with Pakistani Sufi singer Nusrat Fateh Ali Khan; the Deep Forest project, in which vocal loops from West Africa are blended with Western, contemporary rhythmic textures and harmony structure; and the work of Mango, who combined pop and rock music with world elements.
 Depending on style and context, world music can sometimes share the new-age music genre, a category that often includes ambient music and textural expressions from indigenous roots sources. Good examples are Tibetan bowls, Tuvan throat singing, Gregorian chant or Native American flute music. World music blended with new-age music is a sound loosely classified as the hybrid genre 'ethnic fusion'. Examples of ethnic fusion are Nicholas Gunn's ""Face-to-Face"" from Beyond Grand Canyon, featuring authentic Native American flute combined with synthesizers, and ""Four Worlds"" from The Music of the Grand Canyon, featuring spoken word from Razor Saltboy of the Navajo Indian Nation.
 The subgenre world fusion is often mistakenly assumed to refer exclusively to a blending of Western jazz fusion elements with world music. Although such a hybrid expression falls easily into the world fusion category, the suffix ""fusion"" in the term world fusion should not be assumed to mean jazz fusion. Western jazz combined with strong elements of world music is more accurately termed world fusion jazz,[19] ethnic jazz or non-Western jazz. World fusion and global fusion are nearly synonymous with the genre term worldbeat, and though these are considered subgenres of popular music, they may also imply universal expressions of the more general term world music.[9] In the 1970s and 80s, fusion in the jazz music genre implied a blending of jazz and rock music, which is where the misleading assumption is rooted.[20]
 Millie Small released ""My Boy Lollipop"" in 1964. Small's version was a hit, reaching number 2 both in the UK Singles Chart[21] and in the US Billboard Hot 100. In the 1960s, Miriam Makeba and Hugh Masekela had popular hits in the USA. In 1969 Indian musician Ravi Shankar played sitar at the Woodstock festival.[22]
 In the 1970s, Manu Dibango's funky track ""Soul Makossa""[23] (1972) became a hit, and Osibisa released ""Sunshine Day"" (1976). Fela Kuti created Afrobeat[24] and Femi Kuti, Seun Kuti and Tony Allen followed Fela Kuti's funky music. Salsa musicians such as José Alberto ""El Canario"", Ray Sepúlveda, Johnny Pacheco, Fania All-Stars, Ray Barretto, Rubén Blades, Gilberto Santa Rosa, Roberto Roena, Bobby Valentín, Eddie Palmieri, Héctor Lavoe and Willie Colón developed Latin music.[25]
 The Breton musician Alan Stivell pioneered the connection between traditional folk music, modern rock music and world music with his 1972 album Renaissance of the Celtic Harp.[26] Around the same time, Stivell's contemporary, Welsh singer-songwriter Meic Stevens popularised Welsh folk music.[27] Neo-traditional Welsh language music featuring a fusion of modern instruments and traditional instruments such as the pibgorn and the Welsh harp has been further developed by Bob Delyn a'r Ebillion. Lebanese musical pioneer Lydia Canaan fused Middle-Eastern quarter notes and microtones with anglophone folk, and is listed in the catalog of the Rock and Roll Hall of Fame and Museum's Library and Archives[28][29] as the first rock star of the Middle East.[29][30][31][32][33]
 Although it primarily describes traditional music, the world music category also includes popular music from non-Western urban communities (e.g. South African ""township"" music) and non-European music forms that have been influenced by other so-called third-world musics (e.g. Afro-Cuban music).[34]
 The inspiration of Zimbabwe's Thomas Mapfumo in blending the Mbira (finger Piano) style onto the electric guitar, saw a host of other Zimbabwean musicians refining the genre, none more successfully than The Bhundu Boys. The Bhundu Jit music hit Europe with some force in 1986, taking Andy Kershaw and John Peel fully under its spell.
 For many years, Paris has attracted numerous musicians from former colonies in West and North Africa. This scene is aided by the fact that there are many concerts and institutions that help to promote the music.
 Algerian and Moroccan music have an important presence in the French capital. Hundreds of thousands of Algerian and Moroccan immigrants have settled in Paris, bringing the sounds of Amazigh (Berber), raï, and Gnawa music.
 The West African music community is also very large, integrated by people from Senegal, Mali, Ivory Coast, and Guinea.
 Unlike musical styles from other regions of the globe, the American music industry tends to categorize Latin music as its own genre and defines it as any music sung in Spanish from the Spanish-speaking world.[35]
 The most common name for this form of music is also ""folk music"", but is often called ""contemporary folk music"" or ""folk revival music"" to make the distinction.[36] The transition was somewhat centered in the US and is also called the American folk music revival.[37] Fusion genres such as folk rock and others also evolved within this phenomenon.
 On 29 June 1987, a meeting of interested parties gathered to capitalize on the marketing of non-Western folk music. Paul Simon had released the world music-influenced album Graceland in 1986.[38] The concept behind the album had been to express his own sensibilities using the sounds he had fallen in love with while listening to artists from Southern Africa, including Ladysmith Black Mambazo and Savuka. This project and the work of Peter Gabriel and Johnny Clegg among others had, to some degree, introduced non-Western music to a wider audience. They saw this as an opportunity.
 In an unprecedented move, all of the world music labels coordinated together and developed a compilation cassette for the cover of the music magazine NME. The overall running time was 90 minutes, each package containing a mini-catalog showing the other releases on offer.
 By the time of a second meeting it became clear that a successful campaign required its own dedicated press officer. The press officer would be able to juggle various deadlines and sell the music as a concept—not just to national stations, but also regional DJs keen to expand their musical variety. DJs were a key resource as it was important to make ""world music"" important to people outside London—most regions after all had a similarly heritage to tap into. A cost-effective way of achieving all this would be a leafleting campaign.
 The next step was to develop a world music chart, gathering together selling information from around fifty shops, so that it would finally be possible to see which were big sellers in the genre—so new listeners could see what was particularly popular. It was agreed that the NME could again be involved in printing the chart and also Music Week and the London listings magazine City Limits. It was also suggested that Andy Kershaw might be persuaded to do a run down of this chart on his show regularly.
 In most wealthy industrialized countries, large amounts of immigration from other regions has been ongoing for many decades. This has introduced non-Western music to Western audiences not only as ""exotic"" imports, but also as local music played by fellow citizens. But the process is ongoing and continues to produce new forms. In the 2010s several musicians from immigrant communities in the West rose to global popularity, such as Haitian-American Wyclef Jean, Somali-Canadian K'naan, Tamil-Briton M.I.A., often blending the music of their heritage with hip-hop or pop. Cuban-born singer-songwriter Addys Mercedes started her international career from Germany mixing traditional elements of Son with pop.[39]
 Once, an established Western artist might collaborate with an established African artist to produce an album or two. Now, new bands and new genres are built from the ground up by young performers. For example, the Punjabi-Irish fusion band Delhi 2 Dublin is from neither India nor Ireland, but Vancouver, British Columbia, Canada. Country for Syria, an Istanbul based music collective, blends American country music with the music of Syrian refugees and local Turkish music.[40] Musicians and composers also work collectively to create original compositions for various combinations of western and non western instruments.
 
The introduction of non-western music into western culture created a fusion that influenced both parties. (Feld 31)[41] With the quick demand for new music came the technicalities of ownership. As Feld states in page 31:[41] ""This complex traffic in sounds money and media is rooted in the nature of revitalization through appropriation."" There are collaborations between African and American popular music artists that raise questions on who is benefiting from said collaborations.(Feld 31)[41] Feld mentions the example of ""That was your mother"". Alton Rubin and his band the Twisters collaborated with Paul Simon on the song that possessed a zydeco feel, signature of Dopsie's band. Even though Paul Simon wrote and sang the lyrics with them, the whole copyright is attributed to Paul and not to the band as well. (Feld 34) [41] Because of crossovers like this one, where there was a disproportional gain when covering non-western music. Feld states that   ""...international music scene, where worldwide media contact, amalgamation of the music industry towards world record sales domination by three enormous companies, and extensive copyright controls by a few Western countries are having a riveting effect on the commodification of musical skill and styles, and on the power of musical ownership."" (Feld 32)[41] Immigration also heavily influences world music, providing a variety of options for the wider public. In the 1970s Punjabi music was greatly popular in the UK because of its growing Punjabi diaspora. (Schreffler 347)[42] Bhangra music was also greatly covered by its diaspora in cities like New York and Chicago. (Schreffler 351)[42] For a more mainstream integration, the Punjabi music scene integrated collaborations with rappers and started gaining more recognition. One of these successful attempts was a remix of the song ""Mundiān ton Bach ke"" called ""Beware of the Boys"" by Panjabi MC featuring Jay Z. (Schreffler 354)[43] Collaborations between outsider artists provided an integration of their music, even with foreign instrumentation, into the popular music scene.
 Immigration, being a great part of music exportation, plays a big role in cultural identity. Immigrant communities use music to feel as if they are home and future generations it plays the role of educating or giving insight into what their culture is about. In Punjabi culture, music became the carrier of culture around the world. (Schreffler 355)[43]
 World music radio programs today often play African hip hop or reggae artists, crossover Bhangra and Latin American jazz groups, etc. Common media for world music include public radio, webcasting, the BBC, NPR, and the Australian Broadcasting Corporation. By default, non-region-specific or multi-cultural world music projects are often listed under the generic category of world music.
 Examples of radio shows that feature world music include The Culture Cafe on WWUH West Hartford, World of Music on Voice of America, Transpacific Sound Paradise on WFMU, The Planet on Australia's ABC Radio National, DJ Edu presenting D.N.A: DestiNation Africa on BBC Radio 1Xtra, Adil Ray on the BBC Asian Network, Andy Kershaw's show on BBC Radio 3 and Charlie Gillett's show[44] on the BBC World Service.
 The BBC Radio 3 Awards for World Music was an award given to world music artists between 2002 and 2008, sponsored by BBC Radio 3. The award was thought up by fRoots magazine's editor Ian Anderson, inspired by the BBC Radio 2 Folk Awards. Award categories included: Africa, Asia/Pacific, Americas, Europe, Mid East and North Africa, Newcomer, Culture Crossing, Club Global, Album of the Year, and Audience Award. Initial lists of nominees in each category were selected annually by a panel of several thousand industry experts. Shortlisted nominees were voted on by a twelve-member jury, which selected the winners in every category except for the Audience Award category. These jury members were appointed and presided over by the BBC.[45] The annual awards ceremony was held at the BBC Proms and winners were given an award called a ""Planet"". In March 2009, the BBC made a decision to axe the BBC Radio 3 Awards for World Music.[46][47]
 In response to the BBC's decision to end its awards program, the British world music magazine Songlines launched the Songlines Music Awards in 2009 ""to recognise outstanding talent in world music"".[48]
 The WOMEX Awards were introduced in 1999 to honor the high points of world music on an international level and to acknowledge musical excellence, social importance, commercial success, political impact and lifetime achievement.[49] Every October at the WOMEX event, the award figurine—an ancient mother goddess statue dating back about 6000 years to the Neolithic age—is presented in an award ceremony to a worthy member of the world music community.
 Many festivals are identified as being ""world music""; here's a small representative selection:
 Australia
 Bangladesh
 Belgium
 Canada
 Croatia
 France
 Germany
 Ghana
 (Free Electronic Dance Music Festival) was established in (2020) at Busua Beach in the Western Region, by Djsky S K Y M U S I C.[54]
 Hungary
 Iceland
 India
 Indonesia
 Iran
 Italy
 North Macedonia
 Malaysia
 Mali
 Morocco
 New Zealand
 Nigeria
 Poland
 Portugal
 Romania
 Serbia
 Spain 
Spain's most important world music festivals are:
 Sweden
 Tanzania
 Turkey
 Uganda
 Ukraine
 United Kingdom
 United States
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['traditional music', 'Peter Gabriel and Johnny Clegg', 'all the music in the world', 'quasi-traditional, intercultural, and traditional music', 'various forms of non-European classical music'], 'answer_start': [], 'answer_end': []}"
"
 
 
 Genetic engineering, also called genetic modification or genetic manipulation, is the modification and  manipulation of an organism's genes using technology. It is a set of technologies used to change the genetic makeup of cells, including the transfer of genes within and across species boundaries to produce improved or novel organisms. 
 New DNA is obtained by either isolating and copying the genetic material of interest using recombinant DNA methods or by artificially synthesising the DNA. A construct is usually created and used to insert this DNA into the host organism. The first recombinant DNA molecule was made by Paul Berg in 1972 by combining DNA from the monkey virus SV40 with the lambda virus. 
 As well as inserting genes, the process can be used to remove, or ""knock out"", genes. The new DNA can be inserted randomly, or targeted to a specific part of the genome.[1]
 An organism that is generated through genetic engineering is considered to be genetically modified (GM) and the resulting entity is a genetically modified organism (GMO). The first GMO was a bacterium generated by Herbert Boyer and Stanley Cohen in 1973. Rudolf Jaenisch created the first GM animal when he inserted foreign DNA into a mouse in 1974. The first company to focus on genetic engineering, Genentech, was founded in 1976 and started the production of human proteins. Genetically engineered human insulin was produced in 1978 and insulin-producing bacteria were commercialised in 1982. Genetically modified food has been sold since 1994, with the release of the Flavr Savr tomato. The Flavr Savr was engineered to have a longer shelf life, but most current GM crops are modified to increase resistance to insects and herbicides. GloFish, the first GMO designed as a pet, was sold in the United States in December 2003. In 2016 salmon modified with a growth hormone were sold.
 Genetic engineering has been applied in numerous fields including research, medicine, industrial biotechnology and agriculture. In research, GMOs are used to study gene function and expression through loss of function, gain of function, tracking and expression experiments. By knocking out genes responsible for certain conditions it is possible to create animal model organisms of human diseases. As well as producing hormones, vaccines and other drugs, genetic engineering has the potential to cure genetic diseases through gene therapy. Chinese hamster ovary (CHO) cells are used in industrial genetic engineering. Additionally mRNA vaccines are made through genetic engineering to treat viruses such as COVID-19. The same techniques that are used to produce drugs can also have industrial applications such as producing enzymes for laundry detergent, cheeses and other products.
 The rise of commercialised genetically modified crops has provided economic benefit to farmers in many different countries, but has also been the source of most of the controversy surrounding the technology. This has been present since its early use; the first field trials were destroyed by anti-GM activists. Although there is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, critics consider GM food safety a leading concern. Gene flow, impact on non-target organisms, control of the food supply and intellectual property rights have also been raised as potential issues. These concerns have led to the development of a regulatory framework, which started in 1975. It has led to an international treaty, the Cartagena Protocol on Biosafety, that was adopted in 2000. Individual countries have developed their own regulatory systems regarding GMOs, with the most marked differences occurring between the United States and Europe.
 Genetic engineering: Process of inserting new genetic information into existing cells in order to modify a specific organism for the purpose of changing its characteristics.
 Note: Adapted from ref.[2][3]
  Genetic engineering is a process that alters the genetic structure of an organism by either removing or introducing DNA, or modifying existing genetic material in situ. Unlike traditional animal and plant breeding, which involves doing multiple crosses and then selecting for the organism with the desired phenotype, genetic engineering takes the gene directly from one organism and delivers it to the other. This is much faster, can be used to insert any genes from any organism (even ones from different domains) and prevents other undesirable genes from also being added.[4]
 Genetic engineering could potentially fix severe genetic disorders in humans by replacing the defective gene with a functioning one.[5] It is an important tool in research that allows the function of specific genes to be studied.[6] Drugs, vaccines and other products have been harvested from organisms engineered to produce them.[7] Crops have been developed that aid food security by increasing yield, nutritional value and tolerance to environmental stresses.[8]
 The DNA can be introduced directly into the host organism or into a cell that is then fused or hybridised with the host.[9] This relies on recombinant nucleic acid techniques to form new combinations of heritable genetic material followed by the incorporation of that material either indirectly through a vector system or directly through micro-injection, macro-injection or micro-encapsulation. 
 Genetic engineering does not normally include traditional breeding, in vitro fertilisation, induction of polyploidy, mutagenesis and cell fusion techniques that do not use recombinant nucleic acids or a genetically modified organism in the process.[9] However, some broad definitions of genetic engineering include selective breeding.[10] Cloning and stem cell research, although not considered genetic engineering,[11] are closely related and genetic engineering can be used within them.[12] Synthetic biology is an emerging discipline that takes genetic engineering a step further by introducing artificially synthesised material into an organism.[13]
 Plants, animals or microorganisms that have been changed through genetic engineering are termed genetically modified organisms or GMOs.[14] If genetic material from another species is added to the host, the resulting organism is called transgenic. If genetic material from the same species or a species that can naturally breed with the host is used the resulting organism is called cisgenic.[15] If genetic engineering is used to remove genetic material from the target organism the resulting organism is termed a knockout organism.[16] In Europe genetic modification is synonymous with genetic engineering while within the United States of America and Canada genetic modification can also be used to refer to more conventional breeding methods.[17][18][19]
 Humans have altered the genomes of species for thousands of years through selective breeding, or artificial selection[20]: 1 [21]: 1  as contrasted with natural selection. More recently, mutation breeding has used exposure to chemicals or radiation to produce a high frequency of random mutations, for selective breeding purposes. Genetic engineering as the direct manipulation of DNA by humans outside breeding and mutations has only existed since the 1970s. The term ""genetic engineering"" was coined by the Russian-born geneticist Nikolay Timofeev-Ressovsky in his 1934 paper ""The Experimental Production of Mutations"", published in the British journal Biological Reviews.[22] Jack Williamson used the term in his science fiction novel Dragon's Island, published in 1951[23] – one year before DNA's role in heredity was confirmed by Alfred Hershey and Martha Chase,[24] and two years before James Watson and Francis Crick showed that the DNA molecule has a double-helix structure – though the general concept of direct genetic manipulation was explored in rudimentary form in Stanley G. Weinbaum's 1936 science fiction story Proteus Island.[25][26]
 In 1972, Paul Berg created the first recombinant DNA molecules by combining DNA from the monkey virus SV40 with that of the lambda virus.[27] In 1973 Herbert Boyer and Stanley Cohen created the first transgenic organism by inserting antibiotic resistance genes into the plasmid of an Escherichia coli bacterium.[28][29] A year later Rudolf Jaenisch created a transgenic mouse by introducing foreign DNA into its embryo, making it the world's first transgenic animal[30] These achievements led to concerns in the scientific community about potential risks from genetic engineering, which were first discussed in depth at the Asilomar Conference in 1975. One of the main recommendations from this meeting was that government oversight of recombinant DNA research should be established until the technology was deemed safe.[31][32]
 In 1976 Genentech, the first genetic engineering company, was founded by Herbert Boyer and Robert Swanson and a year later the company produced a human protein (somatostatin) in E. coli. Genentech announced the production of genetically engineered human insulin in 1978.[33] In 1980, the U.S. Supreme Court in the Diamond v. Chakrabarty case ruled that genetically altered life could be patented.[34] The insulin produced by bacteria was approved for release by the Food and Drug Administration (FDA) in 1982.[35]
 In 1983, a biotech company, Advanced Genetic Sciences (AGS) applied for U.S. government authorisation to perform field tests with the ice-minus strain of Pseudomonas syringae to protect crops from frost, but environmental groups and protestors delayed the field tests for four years with legal challenges.[36] In 1987, the ice-minus strain of P. syringae became the first genetically modified organism (GMO) to be released into the environment[37] when a strawberry field and a potato field in California were sprayed with it.[38] Both test fields were attacked by activist groups the night before the tests occurred: ""The world's first trial site attracted the world's first field trasher"".[37]
 The first field trials of genetically engineered plants occurred in France and the US in 1986, tobacco plants were engineered to be resistant to herbicides.[39] The People's Republic of China was the first country to commercialise transgenic plants, introducing a virus-resistant tobacco in 1992.[40] In 1994 Calgene attained approval to commercially release the first genetically modified food, the Flavr Savr, a tomato engineered to have a longer shelf life.[41] In 1994, the European Union approved tobacco engineered to be resistant to the herbicide bromoxynil, making it the first genetically engineered crop commercialised in Europe.[42] In 1995, Bt potato was approved safe by the Environmental Protection Agency, after having been approved by the FDA, making it the first pesticide producing crop to be approved in the US.[43] In 2009 11 transgenic crops were grown commercially in 25 countries, the largest of which by area grown were the US, Brazil, Argentina, India, Canada, China, Paraguay and South Africa.[44]
 In 2010, scientists at the J. Craig Venter Institute created the first synthetic genome and inserted it into an empty bacterial cell. The resulting bacterium, named Mycoplasma laboratorium, could replicate and produce proteins.[45][46] Four years later this was taken a step further when a bacterium was developed that replicated a plasmid containing a unique base pair, creating the first organism engineered to use an expanded genetic alphabet.[47][48] In 2012, Jennifer Doudna and Emmanuelle Charpentier collaborated to develop the CRISPR/Cas9 system,[49][50] a technique which can be used to easily and specifically alter the genome of almost any organism.[51]
 Creating a GMO is a multi-step process. Genetic engineers must first choose what gene they wish to insert into the organism. This is driven by what the aim is for the resultant organism and is built on earlier research. Genetic screens can be carried out to determine potential genes and further tests then used to identify the best candidates. The development of microarrays, transcriptomics and genome sequencing has made it much easier to find suitable genes.[52] Luck also plays its part; the Roundup Ready gene was discovered after scientists noticed a bacterium thriving in the presence of the herbicide.[53]
 The next step is to isolate the candidate gene. The cell containing the gene is opened and the DNA is purified.[54] The gene is separated by using restriction enzymes to cut the DNA into fragments[55] or polymerase chain reaction (PCR) to amplify up the gene segment.[56] These segments can then be extracted through gel electrophoresis. If the chosen gene or the donor organism's genome has been well studied it may already be accessible from a genetic library. If the DNA sequence is known, but no copies of the gene are available, it can also be artificially synthesised.[57] Once isolated the gene is ligated into a plasmid that is then inserted into a bacterium. The plasmid is replicated when the bacteria divide, ensuring unlimited copies of the gene are available.[58] The RK2 plasmid is notable for its ability to replicate in a wide variety of single-celled organisms, which makes it suitable as a genetic engineering tool.[59]
 Before the gene is inserted into the target organism it must be combined with other genetic elements. These include a promoter and terminator region, which initiate and end transcription. A selectable marker gene is added, which in most cases confers antibiotic resistance, so researchers can easily determine which cells have been successfully transformed. The gene can also be modified at this stage for better expression or effectiveness. These manipulations are carried out using recombinant DNA techniques, such as restriction digests, ligations and molecular cloning.[60]
 There are a number of techniques used to insert genetic material into the host genome. Some bacteria can naturally take up foreign DNA. This ability can be induced in other bacteria via stress (e.g. thermal or electric shock), which increases the cell membrane's permeability to DNA; up-taken DNA can either integrate with the genome or exist as extrachromosomal DNA. DNA is generally inserted into animal cells using microinjection, where it can be injected through the cell's nuclear envelope directly into the nucleus, or through the use of viral vectors.[61]
 Plant genomes can be engineered by physical methods or by use of Agrobacterium for the delivery of sequences hosted in T-DNA binary vectors. In plants the DNA is often inserted using Agrobacterium-mediated transformation,[62] taking advantage of the Agrobacteriums T-DNA sequence that allows natural insertion of genetic material into plant cells.[63] Other methods include biolistics, where particles of gold or tungsten are coated with DNA and then shot into young plant cells,[64] and electroporation, which involves using an electric shock to make the cell membrane permeable to plasmid DNA.
 As only a single cell is transformed with genetic material, the organism must be regenerated from that single cell. In plants this is accomplished through the use of tissue culture.[65][66] In animals it is necessary to ensure that the inserted DNA is present in the embryonic stem cells.[67] Bacteria consist of a single cell and reproduce clonally so regeneration is not necessary. Selectable markers are used to easily differentiate transformed from untransformed cells. These markers are usually present in the transgenic organism, although a number of strategies have been developed that can remove the selectable marker from the mature transgenic plant.[68]
 Further testing using PCR, Southern hybridization, and DNA sequencing is conducted to confirm that an organism contains the new gene.[69] These tests can also confirm the chromosomal location and copy number of the inserted gene. The presence of the gene does not guarantee it will be expressed at appropriate levels in the target tissue so methods that look for and measure the gene products (RNA and protein) are also used. These include northern hybridisation, quantitative RT-PCR, Western blot, immunofluorescence, ELISA and phenotypic analysis.[70]
 The new genetic material can be inserted randomly within the host genome or targeted to a specific location. The technique of gene targeting uses homologous recombination to make desired changes to a specific endogenous gene.  This tends to occur at a relatively low frequency in plants and animals and generally requires the use of selectable markers. The frequency of gene targeting can be greatly enhanced through genome editing. Genome editing uses artificially engineered nucleases that create specific double-stranded breaks at desired locations in the genome, and use the cell's endogenous mechanisms to repair the induced break by the natural processes of homologous recombination and nonhomologous end-joining. There are four families of engineered nucleases: meganucleases,[71][72] zinc finger nucleases,[73][74] transcription activator-like effector nucleases (TALENs),[75][76] and the Cas9-guideRNA system (adapted from CRISPR).[77][78] TALEN and CRISPR are the two most commonly used and each has its own advantages.[79] TALENs have greater target specificity, while CRISPR is easier to design and more efficient.[79] In addition to enhancing gene targeting, engineered nucleases can be used to introduce mutations at endogenous genes that generate a gene knockout.[80][81]
 Genetic engineering has applications in medicine, research, industry and agriculture and can be used on a wide range of plants, animals and microorganisms. Bacteria, the first organisms to be genetically modified, can have plasmid DNA inserted containing new genes that code for medicines or enzymes that process food and other substrates.[82][83] Plants have been modified for insect protection, herbicide resistance, virus resistance, enhanced nutrition, tolerance to environmental pressures and the production of edible vaccines.[84] Most commercialised GMOs are insect resistant or herbicide tolerant crop plants.[85] Genetically modified animals have been used for research, model animals and the production of agricultural or pharmaceutical products. The genetically modified animals include animals with genes knocked out, increased susceptibility to disease, hormones for extra growth and the ability to express proteins in their milk.[86]
 Genetic engineering has many applications to medicine that include the manufacturing of drugs, creation of model animals that mimic human conditions and gene therapy. One of the earliest uses of genetic engineering was to mass-produce human insulin in bacteria.[33] This application has now been applied to human growth hormones, follicle stimulating hormones (for treating infertility), human albumin, monoclonal antibodies, antihemophilic factors, vaccines and many other drugs.[87][88] Mouse hybridomas, cells fused together to create monoclonal antibodies, have been adapted through genetic engineering to create human monoclonal antibodies.[89] Genetically engineered viruses are being developed that can still confer immunity, but lack the infectious sequences.[90]
 Genetic engineering is also used to create animal models of human diseases. Genetically modified mice are the most common genetically engineered animal model.[91] They have been used to study and model cancer (the oncomouse), obesity, heart disease, diabetes, arthritis, substance abuse, anxiety, aging and Parkinson disease.[92] Potential cures can be tested against these mouse models. 
 Gene therapy is the genetic engineering of humans, generally by replacing defective genes with effective ones. Clinical research using somatic gene therapy has been conducted with several diseases, including X-linked SCID,[93] chronic lymphocytic leukemia (CLL),[94][95] and Parkinson's disease.[96] In 2012, Alipogene tiparvovec became the first gene therapy treatment to be approved for clinical use.[97][98] In 2015 a virus was used to insert a healthy gene into the skin cells of a boy suffering from a rare skin disease, epidermolysis bullosa, in order to grow, and then graft healthy skin onto 80 percent of the boy's body which was affected by the illness.[99]
 Germline gene therapy would result in any change being inheritable, which has raised concerns within the scientific community.[100][101] In 2015, CRISPR was used to edit the DNA of non-viable human embryos,[102][103] leading scientists of major world academies to call for a moratorium on inheritable human genome edits.[104] There are also concerns that the technology could be used not just for treatment, but for enhancement, modification or alteration of a human beings' appearance, adaptability, intelligence, character or behavior.[105] The distinction between cure and enhancement can also be difficult to establish.[106] In November 2018, He Jiankui announced that he had edited the genomes of two human embryos, to attempt to disable the CCR5 gene, which codes for a receptor that HIV uses to enter cells. The work was widely condemned as unethical, dangerous, and premature.[107] Currently, germline modification is banned in 40 countries. Scientists that do this type of research will often let embryos grow for a few days without allowing it to develop into a baby.[108]
 Researchers are altering the genome of pigs to induce the growth of human organs, with the aim of increasing the success of pig to human organ transplantation.[109] Scientists are creating ""gene drives"", changing the genomes of mosquitoes to make them immune to malaria, and then looking to spread the genetically altered mosquitoes throughout the mosquito population in the hopes of eliminating the disease.[110]
 Genetic engineering is an important tool for natural scientists, with the creation of transgenic organisms one of the most important tools for analysis of gene function.[111] Genes and other genetic information from a wide range of organisms can be inserted into bacteria for storage and modification, creating genetically modified bacteria in the process. Bacteria are cheap, easy to grow, clonal, multiply quickly, relatively easy to transform and can be stored at -80 °C almost indefinitely. Once a gene is isolated it can be stored inside the bacteria providing an unlimited supply for research.[112]
 Organisms are genetically engineered to discover the functions of certain genes. This could be the effect on the phenotype of the organism, where the gene is expressed or what other genes it interacts with. These experiments generally involve loss of function, gain of function, tracking and expression.
 Organisms can have their cells transformed with a gene coding for a useful protein, such as an enzyme, so that they will overexpress the desired protein. Mass quantities of the protein can then be manufactured by growing the transformed organism in bioreactor equipment using industrial fermentation, and then purifying the protein.[116] Some genes do not work well in bacteria, so yeast, insect cells or mammalian cells can also be used.[117] These techniques are used to produce medicines such as insulin, human growth hormone, and vaccines, supplements such as tryptophan, aid in the production of food (chymosin in cheese making) and fuels.[118] Other applications with genetically engineered bacteria could involve making them perform tasks outside their natural cycle, such as making biofuels,[119] cleaning up oil spills, carbon and other toxic waste[120] and detecting arsenic in drinking water.[121] Certain genetically modified microbes can also be used in biomining and bioremediation, due to their ability to extract heavy metals from their environment and incorporate them into compounds that are more easily recoverable.[122]
 In materials science, a genetically modified virus has been used in a research laboratory as a scaffold for assembling a more environmentally friendly lithium-ion battery.[123][124] Bacteria have also been engineered to function as sensors by expressing a fluorescent protein under certain environmental conditions.[125]
 One of the best-known and controversial applications of genetic engineering is the creation and use of genetically modified crops or genetically modified livestock to produce genetically modified food. Crops have been developed to increase production, increase tolerance to abiotic stresses, alter the composition of the food, or to produce novel products.[127]
 The first crops to be released commercially on a large scale provided protection from insect pests or tolerance to herbicides. Fungal and virus resistant crops have also been developed or are in development.[128][129] This makes the insect and weed management of crops easier and can indirectly increase crop yield.[130][131] GM crops that directly improve yield by accelerating growth or making the plant more hardy (by improving salt, cold or drought tolerance) are also under development.[132] In 2016 Salmon have been genetically modified with growth hormones to reach normal adult size much faster.[133]
 GMOs have been developed that modify the quality of produce by increasing the nutritional value or providing more industrially useful qualities or quantities.[132] The Amflora potato produces a more industrially useful blend of starches. Soybeans and canola have been genetically modified to produce more healthy oils.[134][135] The first commercialised GM food was a tomato that had delayed ripening, increasing its shelf life.[136]
 Plants and animals have been engineered to produce materials they do not normally make.  Pharming uses crops and animals as bioreactors to produce vaccines, drug intermediates, or the drugs themselves; the useful product is purified from the harvest and then used in the standard pharmaceutical production process.[137] Cows and goats have been engineered to express drugs and other proteins in their milk, and in 2009 the FDA approved a drug produced in goat milk.[138][139]
 Genetic engineering has potential applications in conservation and natural area management. Gene transfer through viral vectors has been proposed as a means of controlling invasive species as well as vaccinating threatened fauna from disease.[140] Transgenic trees have been suggested as a way to confer resistance to pathogens in wild populations.[141] With the increasing risks of maladaptation in organisms as a result of climate change and other perturbations, facilitated adaptation through gene tweaking could be one solution to reducing extinction risks.[142] Applications of genetic engineering in conservation are thus far mostly theoretical and have yet to be put into practice.
 Genetic engineering is also being used to create microbial art.[143] Some bacteria have been genetically engineered to create black and white photographs.[144] Novelty items such as lavender-colored carnations,[145] blue roses,[146] and glowing fish[147][148] have also been produced through genetic engineering.
 The regulation of genetic engineering concerns the approaches taken by governments to assess and manage the risks associated with the development and release of GMOs. The development of a regulatory framework began in 1975, at Asilomar, California.[149] The Asilomar meeting recommended a set of voluntary guidelines regarding the use of recombinant technology.[31] As the technology improved the US established a committee at the Office of Science and Technology,[150] which assigned regulatory approval of GM food to the USDA, FDA and EPA.[151] The Cartagena Protocol on Biosafety, an international treaty that governs the transfer, handling, and use of GMOs,[152] was adopted on 29 January 2000.[153] One hundred and fifty-seven countries are members of the Protocol, and many use it as a reference point for their own regulations.[154]
 The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.[155][156][157][158] Some countries allow the import of GM food with authorisation, but either do not allow its cultivation (Russia, Norway, Israel) or have provisions for cultivation even though no GM products are yet produced (Japan, South Korea).  Most countries that do not allow GMO cultivation do permit research.[159] Some of the most marked differences occur between the US and Europe. The US policy focuses on the product (not the process), only looks at verifiable scientific risks and uses the concept of substantial equivalence.[160] The European Union by contrast has possibly the most stringent GMO regulations in the world.[161] All GMOs, along with irradiated food, are considered ""new food"" and subject to extensive, case-by-case, science-based food evaluation by the European Food Safety Authority.  The criteria for authorisation fall in four broad categories: ""safety"", ""freedom of choice"", ""labelling"", and ""traceability"".[162] The level of regulation in other countries that cultivate GMOs lie in between Europe and the United States.
 One of the key issues concerning regulators is whether GM products should be labeled.  The European Commission says that mandatory labeling and traceability are needed to allow for informed choice, avoid potential false advertising[173] and facilitate the withdrawal of products if adverse effects on health or the environment are discovered.[174] The American Medical Association[175] and the American Association for the Advancement of Science[176] say that absent scientific evidence of harm even voluntary labeling is misleading and will falsely alarm consumers. Labeling of GMO products in the marketplace is required in 64 countries.[177] Labeling can be mandatory up to a threshold GM content level (which varies between countries) or voluntary. In Canada and the US labeling of GM food is voluntary,[178] while in Europe all food (including processed food) or feed which contains greater than 0.9% of approved GMOs must be labelled.[161]
 Critics have objected to the use of genetic engineering on several grounds, including ethical, ecological and economic concerns. Many of these concerns involve GM crops and whether food produced from them is safe and what impact growing them will have on the environment. These controversies have led to litigation, international trade disputes, and protests, and to restrictive regulation of commercial products in some countries.[179]
 Accusations that scientists are ""playing God"" and other religious issues have been ascribed to the technology from the beginning.[180] Other ethical issues raised include the patenting of life,[181] the use of intellectual property rights,[182] the level of labeling on products,[183][184] control of the food supply[185] and the objectivity of the regulatory process.[186] Although doubts have been raised,[187] economically most studies have found growing GM crops to be beneficial to farmers.[188][189][190]
 Gene flow between GM crops and compatible plants, along with increased use of selective herbicides, can increase the risk of ""superweeds"" developing.[191] Other environmental concerns involve potential impacts on non-target organisms, including soil microbes,[192] and an increase in secondary and resistant insect pests.[193][194] Many of the environmental impacts regarding GM crops may take many years to be understood and are also evident in conventional agriculture practices.[192][195] With the commercialisation of genetically modified fish there are concerns over what the environmental consequences will be if they escape.[196]
 There are three main concerns over the safety of genetically modified food: whether they may provoke an allergic reaction; whether the genes could transfer from the food into human cells; and whether the genes not approved for human consumption could outcross to other crops.[197] There is a scientific consensus[198][199][200][201] that currently available food derived from GM crops poses no greater risk to human health than conventional food,[202][203][204][205][206] but that each GM food needs to be tested on a case-by-case basis before introduction.[207][208][209] Nonetheless, members of the public are less likely than scientists to perceive GM foods as safe.[210][211][212][213]
 Genetic engineering features in many science fiction stories.[214] Frank Herbert's novel The White Plague describes the deliberate use of genetic engineering to create a pathogen which specifically kills women.[214] Another of Herbert's creations, the Dune series of novels, uses genetic engineering to create the powerful Tleilaxu.[215] Few films have informed audiences about genetic engineering, with the exception of the 1978 The Boys from Brazil and the 1993 Jurassic Park, both of which make use of a lesson, a demonstration, and a clip of scientific film.[216][217] Genetic engineering methods are weakly represented in film; Michael Clark, writing for the Wellcome Trust, calls the portrayal of genetic engineering and biotechnology ""seriously distorted""[217] in films such as The 6th Day. In Clark's view, the biotechnology is typically ""given fantastic but visually arresting forms"" while the science is either relegated to the background or fictionalised to suit a young audience.[217]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Genetic engineering', 'Alfred Hershey and Martha Chase', 'its ability to replicate in a wide variety of single-celled organisms', '1978 The Boys from Brazil and the 1993 Jurassic Park', 'potential risks from genetic engineering'], 'answer_start': [], 'answer_end': []}"
"Mining in the engineering discipline is the extraction of minerals from the ground. Mining engineering is associated with many other disciplines, such as mineral processing, exploration, excavation, geology, metallurgy, geotechnical engineering and surveying. A mining engineer may manage any phase of mining operations, from exploration and discovery of the mineral resources, through feasibility study, mine design, development of plans, production and operations to mine closure.[not verified in body]
 From prehistoric times to the present, mining has played a significant role in the existence of the human race. Since the beginning of civilization, people have used stone and ceramics and, later, metals found on or close to the Earth's surface. These were used to manufacture early tools and weapons. For example, high-quality flint found in northern France and southern England were used to set fire and break rock.[1] Flint mines have been found in chalk areas where seams of the stone were followed underground by shafts and galleries. The oldest known mine on the archaeological record is the ""Lion Cave"" in Eswatini. At this site, which radiocarbon dating indicates to be about 43,000 years old, paleolithic humans mined mineral hematite, which contained iron and was ground to produce the red pigment ochre.[2][3]
 The ancient Romans were innovators of mining engineering. They developed large-scale mining methods, such as the use of large volumes of water brought to the minehead by aqueducts for hydraulic mining. The exposed rock was then attacked by fire-setting, where fires were used to heat the rock, which would be quenched with a stream of water. The thermal shock cracked the rock, enabling it to be removed. In some mines, the Romans utilized water-powered machinery such as reverse overshot water-wheels. These were used extensively in the copper mines at Rio Tinto in Spain, where one sequence comprised 16 such wheels arranged in pairs, lifting water about 80 feet (24 m).[4]
 Black powder was first used in mining in Banská Štiavnica, Kingdom of Hungary (present-day Slovakia) in 1627.[5]  This allowed blasting of rock and earth to loosen and reveal ore veins, which was much faster than fire-setting. The Industrial Revolution saw further advances in mining technologies, including improved explosives and steam-powered pumps, lifts, and drills.
 Becoming an accredited mining engineer requires a university or college degree. Training includes a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Technology (B.Tech.) or Bachelor of Applied Science (B.A.Sc.) in mining engineering. Depending on the country and jurisdiction, to be licensed as a mining engineer may require a Master of Engineering (M.Eng.), Master of Science (M.Sc or M.S.) or Master of Applied Science (M.A.Sc.) degree.
 Some mining engineers who have come from other disciplines, primarily from engineering fields (e.g.: mechanical, civil, electrical, geomatics or environmental engineering) or from science fields (e.g.: geology, geophysics, physics, geomatics, earth science, or mathematics), typically completing a graduate degree such as M.Eng, M.S., M.Sc. or M.A.Sc. in mining engineering after graduating from a different quantitative undergraduate program.
 The fundamental subjects of mining engineering study usually include:
 In the United States, about 14 universities offer a B.S. degree in mining and mineral engineering. The top rated universities[according to whom?] include Michigan Technological University, South Dakota School of Mines and Technology, Virginia Tech, the University of Kentucky, the University of Arizona,  Pennsylvania State University, and Colorado School of Mines.[6] Most of these universities offer M.S. and Ph.D. degrees.
 In Canada, there are 19 undergraduate degree programs in mining engineering or equivalent.[7] McGill University Faculty of Engineering offers both undergraduate (B.Sc., B.Eng.) and graduate (M.Sc., Ph.D.) degrees in Mining Engineering.[8][9] and the University of British Columbia in Vancouver offers a Bachelor of Applied Science (B.A.Sc.) in Mining Engineering[10] and also graduate degrees (M.A.Sc. or M.Eng and Ph.D.) in Mining Engineering.[11][promotion?]
 In Europe, most programs are integrated (B.S. plus M.S. into one) after the Bologna Process and take five years to complete. In Portugal, the University of Porto offers an M.Eng. in Mining and Geo-Environmental Engineering[12] and in Spain the Technical University of Madrid offers degrees in Mining Engineering with tracks in Mining Technology, Mining Operations, Fuels and Explosives, Metallurgy.[13] In the United Kingdom, The Camborne School of Mines offers a wide choice of BEng and MEng degrees in Mining engineering and other Mining related disciplines. This is done through the University of Exeter.[14] In Romania, the University of Petroșani (formerly known as the Petroşani Institute of Mines, or rarely as the Petroşani Institute of Coal) is the only university that offers a degree in Mining Engineering, Mining Surveying or Underground Mining Constructions, albeit, after the closure of Jiu Valley coal mines, those degrees had fallen out of interest for most high-school graduates.[15]
 In South Africa, leading institutions include the University of Pretoria, offering a 4-year Bachelor of Engineering (B.Eng in Mining Engineering) as well as post-graduate studies in various specialty fields such as rock engineering and numerical modelling, explosives engineering, ventilation engineering, underground mining methods and mine design;[16] and the University of the Witwatersrand offering a 4-year Bachelor of Science in Engineering (B.Sc.(Eng.)) in Mining Engineering[17] as well as graduate programs (M.Sc.(Eng.) and Ph.D.) in Mining Engineering.[18]
 Some mining engineers go on to pursue Doctorate degree programs such as Doctor of Philosophy (Ph.D., DPhil), Doctor of Engineering (D.Eng., Eng.D.). These programs involve a significant original research component and are usually seen as entry points into academia.
 In the Russian Federation, 85 universities across all federal districts are training specialists for the mineral resource sector. 36 universities are training specialists for extracting and processing solid minerals (mining). 49 are training specialists for extracting, primary processing, and transporting liquid and gaseous minerals (oil and gas). 37 are training specialists for geological exploration (applied geology, geological exploration). Among the universities that train specialists for the mineral resource sector, 7 are federal universities, and 13 are national research universities of Russia.[19] Personnel training for the mineral resource sector in Russian universities is currently carried out in the following main specializations of training (specialist's degree): ""Applied Geology"" with the qualification of mining engineer (5 years of training);  ""Geological Exploration"" with the qualification of mining engineer (5 years of training); ""Mining"" with the qualification of mining engineer (5.5 years of training); ""Physical Processes in Mining or Oil and Gas Production"" with the qualification of mining engineer (5.5 years of training); ""Oil and Gas Engineering and Technologies"" with the qualification of mining engineer (5.5 years of training). Universities develop and implement the main professional educational programs of higher education in the directions and specializations of training by forming their profile (name of the program). For example, within the framework of the specialization ""Mining"", universities often adhere to the classical names of the programs ""Open-pit mining"", ""Underground mining of mineral deposits"", ""Surveying"", ""Mineral enrichment"", ""Mining machines"", ""Technological safety and mine rescue"", ""Mine and underground construction"", ""Blasting work"", ""Electrification of the mining industry"", etc. In the last ten years, under the influence of various factors, new names of programs have begun to appear, such as: ""Mining and geological information systems"", ""Mining ecology"", etc. Thus, universities, using their freedom to form new training programs for specialists, can look to the future and try to foresee new professions of mining engineers. After the specialist's degree, you can immediately enroll in postgraduate school (analog of Doctorate degree programs, four years of training).[19]
 Mining salaries are usually determined by the level of skill required, where the position is, and what kind of organization the engineer works for.[citation needed]
 Mining engineers in India earn relatively high salaries in comparison to many other professions,[20] with an average salary of $15,250 [relevant?]. However, in comparison to mining engineer salaries in other regions, such as Canada, the United States, Australia, and the United Kingdom, Indian salaries are low.  In the United States, there are an estimated 6,150 employed mining engineers, with a mean yearly wage of US$103,710.[21]
 As there is considerable capital expenditure required for mining operations, an array of pre-mining activities are normally carried out to assess whether a mining operation would be worthwhile.
 Mineral exploration is the process of locating minerals and assessing their concentrations (grade) and quantities (tonnage), to determine if they are commercially viable ores for mining. Mineral exploration is much more intensive, organized, involved, and professional than mineral prospecting – though it frequently utilizes services exploration, enlisting geologists and surveyors in the necessary pre-feasibility study of the possible mining operation. Mineral exploration and estimation of the reserve can determine the profitability conditions and advocate the form and type of mining required.[citation needed]
 Mineral discovery can be made from research of mineral maps, academic geological reports, or government geological reports. Other sources of information include property assays and local word of mouth. Mineral research usually includes sampling and analyzing sediments, soil, and drill cores. Soil sampling and analysis is one of the most popular mineral exploration tools.[22][23] Other common tools include satellite and aerial surveys or airborne geophysics, including magnetometric and gamma-spectrometric maps.[24] Unless the mineral exploration is done on public property, the owners of the property may play a significant role in the exploration process and might be the original discoverers of the mineral deposit.[25]
 After a prospective mineral is located, the mining geologist and engineer determine the ore properties. This may involve chemical analysis of the ore to determine the sample's composition. Once the mineral properties are identified, the next step is determining the quantity of the ore. This involves determining the extent of the deposit and the purity of the ore.[26] The geologist drills additional core samples to find the limits of the deposit or seam and estimates the quantity of valuable material present.
 Once the mineral identification and reserve amount are reasonably determined, the next step is to determine the feasibility of recovering the mineral deposit. A preliminary survey shortly after the discovery of the deposit examines the market conditions, such as the supply and demand of the mineral, the amount of ore needed to be moved to recover a certain quantity of that mineral, and analysis of the cost associated with the operation. This pre-feasibility study determines whether the mining project is likely to be profitable; if so, a more in-depth analysis of the deposit is undertaken. After the full extent of the ore body is known and has been examined by engineers, the feasibility study examines the cost of initial capital investment, methods of extraction, the cost of operation, an estimated length of time to pay back the investment, the gross revenue and net profit margin, any possible resale price of the land, the total life of the reserve, the full value of the account, investment in future projects, and the property owner or owners' contract. In addition, environmental impact, reclamation, possible legal ramifications, and all government permitting are considered.[27][28] These steps of analysis determine whether the mining company and its investors should proceed with the extraction of the minerals or whether the project should be abandoned. The mining company may decide to sell the rights to the reserve to a third party rather than develop it themselves. Alternatively, the decision to proceed with extraction may be postponed indefinitely until market conditions become favorable.
 Mining engineers working in an established mine may work as an engineer for operations improvement, further mineral exploration, and operation capitalization by determining where in the mine to add equipment and personnel. The engineer may also work in supervision and management or as an equipment and mineral salesperson. In addition to engineering and operations, the mining engineer may work as an environmental, health, and safety manager or design engineer.
 The act of mining requires different methods of extraction depending on the mineralogy, geology, and location of the resources. Characteristics such as mineral hardness, the mineral stratification, and access to that mineral will determine the method of extraction.
 Generally, mining is either done from the surface or underground. Mining can also occur with surface and covert operations on the same reserve. Mining activity varies as to what method is employed to remove the mineral.
 Surface mining comprises 90% of the world's mineral tonnage output. Also called open pit mining, surface mining removes minerals in formations near the surface. Ore retrieval is done by material removal from the land in its natural state. Surface mining often alters the land's characteristics, shape, topography, and geological makeup.
 Surface mining involves quarrying and excavating minerals through cutting, cleaving, and breaking machinery. Explosives are usually used to facilitate breakage. Hard rocks such as limestone, sand, gravel, and slate are generally quarried into benches.
 Using mechanical shovels, track dozers, and front-end loaders, strip mining is done on softer minerals such as clays and phosphate removed. Smoother coal seams can also be extracted this way.
 With placer mining, dredge mining can also remove minerals from the bottoms of lakes, rivers, streams, and even the ocean. In addition, in-situ mining can be done from the surface using dissolving agents on the ore body and retrieving the ore via pumping. The pumped material is then set to leach for further processing. Hydraulic mining is utilized as water jets to wash away either overburden or the ore itself.[29]
 Legal attention to health and safety in mining began in the late 19th century. In the 20th century, it progressed to a comprehensive and stringent codification of enforcement and mandatory health and safety regulation. In whatever role, a mining engineer must follow all mine safety laws.
 The United States Congress, through the passage of the Federal Mine Safety and Health Act of 1977, known as the Miner's Act, created the Mine Safety and Health Administration (MSHA) under the US Department of Labor. The act provides miners with rights against retaliation for reporting violations, consolidated regulation of coal mines with metallic and nonmetallic mines, and created the independent Federal Mine Safety and Health Review Commission to review violations reported to MSHA.[31]
 The act codified in Code of Federal Regulations § 30 (CFR § 30) covers all miners at an active mine. When a mining engineer works at an active mine, they are subject to the same rights, violations, mandatory health and safety regulations, and compulsory training as any other worker at the mine. The mining engineer can be legally identified as a ""miner"".[32]
 The act establishes the rights of miners. The miner may report at any time a hazardous condition and request an inspection. The miners may elect a miners' representative to participate during an inspection, pre-inspection meeting, and post-inspection conference. The miners and miners' representatives shall be paid for their time during all inspections and investigations.[33]
 Waste and uneconomic material generated from the mineral extraction process are the primary source of pollution in the vicinity of mines. Mining activities, by their nature, cause a disturbance of the natural environment in and around which the minerals are located. Mining engineers should therefore be concerned not only with the production and processing of mineral commodities but also with the mitigation of damage to the environment both during and after mining as a result of the change in the mining area.
  This article incorporates text by Petrov, V. L. available under the CC BY 4.0 license.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Mining engineering', 'geologists and surveyors', 'Mining in the engineering discipline is the extraction of minerals from the ground', 'rights, violations, mandatory health and safety regulations', 'environmental impact, reclamation, possible legal ramifications, and all government permitting'], 'answer_start': [], 'answer_end': []}"
"
 International humanitarian law (IHL), also referred to as the laws of armed conflict, is the law that regulates the conduct of war (jus in bello).[1][2] It is a branch of international law that seeks to limit the effects of armed conflict by protecting persons who are not participating in hostilities and by restricting and regulating the means and methods of warfare available to combatants.
 International humanitarian law is inspired by considerations of humanity and the mitigation of human suffering. It comprises a set of rules, which is established by treaty or custom and that seeks to protect persons and property/objects that are or may be affected by armed conflict, and it limits the rights of parties to a conflict to use methods and means of warfare of their choice.[3] Sources of international law include international agreements (the Geneva Conventions), customary international law, general principles of nations, and case law.[2][4] It defines the conduct and responsibilities of belligerent nations, neutral nations, and individuals engaged in warfare, in relation to each other and to protected persons, usually meaning non-combatants. It is designed to balance humanitarian concerns and military necessity, and subjects warfare to the rule of law by limiting its destructive effect and alleviating human suffering.[3] Serious violations of international humanitarian law are called war crimes. 
 While IHL (jus in bello) concerns the rules and principles governing the conduct of warfare once armed conflict has begun, jus ad bellum pertains to the justification for resorting to war and includes the crime of aggression. Together the jus in bello and jus ad bellum comprise the two strands of the laws of war governing all aspects of international armed conflicts. The law is mandatory for nations bound by the appropriate treaties. There are also other customary unwritten rules of war, many of which were explored at the Nuremberg trials. IHL operates on a strict division between rules applicable in international armed conflict and internal armed conflict.[5]
 International humanitarian law is traditionally seen as distinct from international human rights law (which governs the conduct of a state towards its people), although the two branches of law are complementary and in some ways overlap.[6][7][8]
 Modern international humanitarian law is made up of two historical streams:
 The two streams take their names from a number of international conferences which drew up treaties relating to war and conflict, in particular the Hague Conventions of 1899 and 1907, and the Geneva Conventions, the first of which was drawn up in 1863.  Both deal with jus in bello, which deals with the question of whether certain practices are acceptable during armed conflict.[10]
 The Law of The Hague, or the laws of war proper, ""determines the rights and duties of belligerents in the conduct of operations and limits the choice of means in doing harm"".[11] In particular, it concerns itself with
 Systematic attempts to limit the savagery of warfare only began to develop in the 19th century.  Such concerns were able to build on the changing view of warfare by states influenced by the Age of Enlightenment. The purpose of warfare was to overcome the enemy state, which could be done by disabling the enemy combatants. Thus, ""the distinction between combatants and civilians, the requirement that wounded and captured enemy combatants must be treated humanely, and that quarter must be given, some of the pillars of modern humanitarian law, all follow from this principle"".[13]
 Fritz Munch sums up historical military practice before 1800: ""The essential points seem to be these: In battle and in towns taken by force, combatants and non-combatants were killed and property was destroyed or looted.""[14]  In the 17th century, the Dutch jurist Hugo Grotius, widely regarded as the founder or father of public international law, wrote that ""wars, for the attainment of their objects, it cannot be denied, must employ force and terror as their most proper agents"".[15]
 Even in the midst of the carnage of history, however, there have been frequent expressions and invocation of humanitarian norms for the protection of the victims of armed conflicts: the wounded, the sick and the shipwrecked. These date back to ancient times.[16]
 In the Old Testament, the King of Israel prevents the slaying of the captured, following the prophet Elisha's admonition to spare enemy prisoners. In answer to a question from the King, Elisha said, ""You shall not slay them. Would you slay those whom you have taken captive with your sword and with your bow? Set bread and water before them, that they may eat and drink and go to their master.""[17]
 In ancient India there are records (the Laws of Manu, for example) describing the types of weapons that should not be used: ""When he fights with his foes in battle, let him not strike with weapons concealed (in wood), nor with (such as are) barbed, poisoned, or the points of which are blazing with fire.""[18] There is also the command not to strike a eunuch nor the enemy ""who folds his hands in supplication ... Nor one who sleeps, nor one who has lost his coat of mail, nor one who is naked, nor one who is disarmed, nor one who looks on without taking part in the fight.""[19]
 Islamic law states that ""non-combatants who did not take part in fighting such as women, children, monks and hermits, the aged, blind, and insane"" were not to be molested.[20] The first Caliph, Abu Bakr, proclaimed, ""Do not mutilate. Do not kill little children or old men or women. Do not cut off the heads of palm trees or burn them. Do not cut down fruit trees. Do not slaughter livestock except for food.""[21] Islamic jurists have held that a prisoner should not be killed, as he ""cannot be held responsible for mere acts of belligerency"".[22] 
However, the prohibition against killing non-combatants is not necessarily absolute in Islamic Law. For example, in situations where an ""enemy retreats inside fortifications and one-to-one combat is not an option"", Islamic jurists have been unanimous as to the permissibility on the use of less discriminating weapons such as mangonels (a weapon for catapulting large stones) if required by military necessity but have differed with respect to the use of fire in such cases. [23]
 The most important antecedent of IHL is the current Armistice Agreement and Regularization of War, signed and ratified in 1820 between the authorities of the then Government of Great Colombia and the Chief of the Expeditionary Forces of the Spanish Crown, in the Venezuelan city of Santa Ana de Trujillo. This treaty was signed under the conflict of Independence, being the first of its kind in the West.
 It was not until the second half of the 19th century, however, that a more systematic approach was initiated. In the United States, a German immigrant, Francis Lieber, drew up a code of conduct in 1863, which came to be known as the Lieber Code, for the Union Army during the American Civil War. The Lieber Code included the humane treatment of civilian populations in areas of conflict, and also forbade the execution of POWs.
 At the same time, the involvement during the Crimean War of a number of such individuals as Florence Nightingale and Henry Dunant, a Genevese businessman who had worked with wounded soldiers at the Battle of Solferino, led to more systematic efforts to prevent the suffering of war victims. Dunant wrote a book, which he titled A Memory of Solferino, in which he described the horrors he had witnessed. His reports were so shocking that they led to the founding of the International Committee of the Red Cross (ICRC) in 1863, and the convening of a conference in Geneva in 1864, which drew up the Geneva Convention for the Amelioration of the Condition of the Wounded in Armies in the Field.[24]
 The Law of Geneva is directly inspired by the principle of humanity. It relates to those who are not participating in the conflict, as well as to military personnel hors de combat. It provides the legal basis for protection and humanitarian assistance carried out by impartial humanitarian organizations such as the ICRC.[25]  This focus can be found in the Geneva Conventions.
 The Geneva Conventions are the result of a process that developed in a number of stages between 1864 and 1949. It focused on the protection of civilians and those who can no longer fight in an armed conflict. As a result of World War II, all four conventions were revised, based on previous revisions and on some of the 1907 Hague Conventions, and readopted by the international community in 1949. Later conferences have added provisions prohibiting certain methods of warfare and addressing issues of civil wars.[26]
 The first three Geneva Conventions were revised, expanded, and replaced, and the fourth one was added, in 1949.
 There are three additional amendment protocols to the Geneva Convention:
 The Geneva Conventions of 1949 may be seen, therefore, as the result of a process which began in 1864. Today they have ""achieved universal participation with 194 parties"". This means that they apply to almost any international armed conflict.[30] The Additional Protocols, however, have yet to achieve near-universal acceptance, since the United States and several other significant military powers (like Iran, Israel, India and Pakistan) are currently not parties to them.[31]
 With the adoption of the 1977 Additional Protocols to the Geneva Conventions, the two strains of law began to converge, although provisions focusing on humanity could already be found in the Hague law (i.e. the protection of certain prisoners of war and civilians in occupied territories). The 1977 Additional Protocols, relating to the protection of victims in both international and internal conflict, not only incorporated aspects of both the Law of The Hague and the Law of Geneva, but also important human rights provisions.[32]
 Well-known examples of such rules include the prohibition on attacking doctors or ambulances displaying a red cross. It is also prohibited to fire at a person or vehicle bearing a white flag, since that, being considered the flag of truce, indicates an intent to surrender or a desire to communicate. In either case, the persons protected by the Red Cross or the white flag are expected to maintain neutrality, and may not engage in warlike acts themselves; engaging in war activities under a white flag or a red cross is itself a violation of the laws of war.
 These examples of the laws of war address:
 It is a violation of the laws of war to engage in combat without meeting certain requirements, among them the wearing of a distinctive uniform or other easily identifiable badge, and the carrying of weapons openly. Impersonating soldiers of the other side by wearing the enemy's uniform is allowed, though fighting in that uniform is unlawful perfidy, as is the taking of hostages.
 International humanitarian law now includes several treaties that outlaw specific weapons. These conventions were created largely because these weapons cause deaths and injuries long after conflicts have ended. Unexploded land mines have caused up to 7,000 deaths a year; unexploded bombs, particularly from cluster bombs that scatter many small ""bomblets"", have also killed many. An estimated 98% of the victims are civilian; farmers tilling their fields and children who find these explosives have been common victims. For these reasons, the following conventions have been adopted:
 The ICRC is the only institution explicitly named under international humanitarian law as a controlling authority. The legal mandate of the ICRC stems from the four Geneva Conventions of 1949, as well as from its own Statutes.
 The International Committee of the Red Cross (ICRC) is an impartial, neutral, and independent organization whose exclusively humanitarian mission is to protect the lives and dignity of victims of war and internal violence and to provide them with assistance. During conflict, punishment for violating the laws of war may consist of a specific, deliberate and limited violation of the laws of war in reprisal.
 Combatants who break specific provisions of the laws of war lose the protections and status afforded to them as prisoners of war, but only after facing a ""competent tribunal"".[35] At that point, they become unlawful combatants, but must still be ""treated with humanity and, in case of trial, shall not be deprived of the rights of fair and regular trial"", because they are still covered by GC IV, Article 5.
 Spies and terrorists are only protected by the laws of war if the ""power"" which holds them is in a state of armed conflict or war, and until they are found to be an ""unlawful combatant"". Depending on the circumstances, they may be subject to civilian law or a military tribunal for their acts. In practice, they have often have been subjected to torture and execution. The laws of war neither approve nor condemn such acts, which fall outside their scope.[citation needed] Spies may only be punished following a trial; if captured after rejoining their own army, they must be treated as prisoners of war.[36] Suspected terrorists who are captured during an armed conflict, without having participated in the hostilities, may be detained only in accordance with the GC IV, and are entitled to a regular trial.[37] Countries that have signed the UN Convention Against Torture have committed themselves not to use torture on anyone for any reason.
 After a conflict has ended, persons who have committed any breach of the laws of war, and especially atrocities, may be held individually accountable for war crimes through process of law.
 Reparation for victims of serious violations of International Humanitarian Law acknowledges the suffering endured by individuals and communities and seeks to provide a form of redress for the harms inflicted upon them. The evolving legal landscape, notably through the mechanisms of international courts like the ICC, has reinforced the notion that victims of war crimes and other serious breaches of International Humanitarian Law have a recognized right to seek reparations. These reparations can take various forms, including restitution, compensation, rehabilitation, satisfaction, and guarantees of non-repetition, aimed at addressing the physical, psychological, and material damage suffered by victims.[38]
 The Fourth Geneva Convention focuses on the civilian population. The two additional protocols adopted in 1977 extend and strengthen civilian protection in international (AP I) and non-international (AP II) armed conflict: for example, by introducing the prohibition of direct attacks against civilians. A ""civilian"" is defined as ""any person not belonging to the armed forces"", including non-nationals and refugees.[39] However, it is accepted that operations may cause civilian casualties. Luis Moreno Ocampo, chief prosecutor of the international criminal court, wrote in 2006: ""International humanitarian law and the Rome statute permit belligerents to carry out proportionate attacks against military objectives, even when it is known that some civilian deaths or injuries will occur. A crime occurs if there is an intentional attack directed against civilians (principle of distinction) ... or an attack is launched on a military objective in the knowledge that the incidental civilian injuries would be clearly excessive in relation to the anticipated military advantage (principle of proportionality).""[40]
 The provisions and principles of IHL which seek to protect civilians are:[41]
 The principle of distinction protects civilian population and civilian objects from the effects of military operations. It requires parties to an armed conflict to distinguish at all times, and under all circumstances, between combatants and military objectives on the one hand, and civilians and civilian objects on the other; and only to target the former. It also provides that civilians lose such protection should they take a direct part in hostilities.[42] The principle of distinction has also been found by the ICRC to be reflected in state practice; it is therefore an established norm of customary international law in both international and non-international armed conflicts.[43]
 Necessity and proportionality are established principles in humanitarian law. Under IHL, a belligerent may apply only the amount and kind of force necessary to defeat the enemy. Further, attacks on military objects must not cause loss of civilian life considered excessive in relation to the direct military advantage anticipated.[44] Every feasible precaution must be taken by commanders to avoid civilian casualties.[45] The principle of proportionality has also been found by the ICRC to form part of customary international law in international and non-international armed conflicts.[46]
 The principle of humane treatment requires that civilians be treated humanely at all times.[47] Common Article 3 of the GCs prohibits violence to life and person (including cruel treatment and torture), the taking of hostages, humiliating and degrading treatment, and execution without regular trial against non-combatants, including persons hors de combat (wounded, sick and shipwrecked). Civilians are entitled to respect for their physical and mental integrity, their honour, family rights, religious convictions and practices, and their manners and customs.[48] This principle of humane treatment has been affirmed by the ICRC as a norm of customary international law, applicable in both international and non-international armed conflicts.[46]
 The principle of non-discrimination is a core principle of IHL. Adverse distinction based on race, sex, nationality, religious belief or political opinion is prohibited in the treatment of prisoners of war,[49] civilians,[50] and persons hors de combat.[51] All protected persons shall be treated with the same consideration by parties to the conflict, without distinction based on race, religion, sex or political opinion.[52] Each and every person affected by armed conflict is entitled to his fundamental rights and guarantees, without discrimination.[48] The prohibition against adverse distinction is also considered by the ICRC to form part of customary international law in international and non-international armed conflict.[46]
 Women must be protected from rape, forced prostitution and from any form of indecent assault. Children under the age of eighteen must not be permitted to take part in hostilities, cannot be evacuated to a foreign country by a country other than theirs, except temporarily due to a compelling threat to their health and safety, and if orphaned or separated from their families, must be maintained and receive an education.[53]
 The European Union has made significant changes to its sanctions policy to better safeguard humanitarian efforts, in response to UN Security Council Resolution 2664. This includes incorporating humanitarian exemptions into EU sanctions regimes, ensuring that aid can reach those in need without legal barriers. This shift has led to the inclusion of comprehensive humanitarian exemptions in new sanctions frameworks for Niger and Sudan, and the amendment of existing regimes to incorporate similar exemptions, thereby covering key humanitarian contexts in countries like Lebanon, Myanmar, and Venezuela.[54]
 IHL emphasises, in various provisions in the GCs and APs, the concept of formal equality and non-discrimination. Protections should be provided ""without any adverse distinction founded on sex"". For example, with regard to female prisoners of war, women are required to receive treatment ""as favourable as that granted to men"".[55] In addition to claims of formal equality, IHL mandates special protections to women, providing female prisoners of war with separate dormitories from men, for example,[56] and prohibiting sexual violence against women.[57]
 The reality of women's and men's lived experiences of conflict has highlighted some of the gender limitations of IHL. Feminist critics have challenged IHL's focus on male combatants and its relegation of women to the status of victims, and its granting them legitimacy almost exclusively as child-rearers. A study of the 42 provisions relating to women within the Geneva Conventions and the Additional Protocols found that almost half address women who are expectant or nursing mothers.[58] Others have argued that the issue of sexual violence against men in conflict has not yet received the attention it deserves.[59]
 Soft-law instruments have been relied on to supplement the protection of women in armed conflict:
 Read together with other legal mechanisms, in particular the UN Convention for the Elimination of All Forms of Discrimination Against Women (CEDAW), these can enhance interpretation and implementation of IHL.
 In addition, international criminal tribunals (like the International Criminal Tribunals for the former Yugoslavia and Rwanda) and mixed tribunals (like the Special Court for Sierra Leone) have contributed to expanding the scope of definitions of sexual violence and rape in conflict. They have effectively prosecuted sexual and gender-based crimes committed during armed conflict. There is now well-established jurisprudence on gender-based crimes. Nonetheless, there remains an urgent need to further develop constructions of gender within international humanitarian law.[60]
 IHL has generally not been subject to the same debates and criticisms of ""cultural relativism"" as have international human rights. Although the modern codification of IHL in the Geneva Conventions and the Additional Protocols is relatively new, and European in name, the core concepts are not new, and laws relating to warfare can be found in all cultures. Indeed, non-Western participants played important roles in the development of this area of law at the global level as early as the 1907 Second Hague Conference, and have continued to do so since.[61]
 ICRC studies on the Middle East, Somalia, Latin America, and the Pacific, for example have found that there are traditional and long-standing practices in various cultures that preceded, but are generally consistent with, modern IHL. It is important to respect local and cultural practices that are in line with IHL. Relying on these links and on local practices can help to promote awareness of and adherence to IHL principles among local groups and communities.[citation needed]
 Durham cautions that, although traditional practices and IHL legal norms are largely compatible, it is important not to assume perfect alignment. There are areas in which legal norms and cultural practices clash. Violence against women, for example, is frequently legitimized by arguments from culture, and yet is prohibited in IHL and other international law. In such cases, it is important to ensure that IHL is not negatively affected.
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['Middle East, Somalia, Latin America, and the Pacific', 'Florence Nightingale and Henry Dunant', 'those who are not participating in the conflict', 'those who are not participating in the conflict', 'addressing the physical, psychological, and material damage suffered by victims'], 'answer_start': [], 'answer_end': []}"
"
 Public health is ""the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals"".[1][2] Analyzing the determinants of health of a population and the threats it faces is the basis for public health.[3] The public can be as small as a handful of people or as large as a village or an entire city; in the case of a pandemic it may encompass several continents. The concept of health takes into account physical, psychological, and social well-being.[1][4]
 Public health is an interdisciplinary field. For example, epidemiology, biostatistics, social sciences and management of health services are all relevant. Other important sub-fields include environmental health, community health, behavioral health, health economics, public policy, mental health, health education, health politics, occupational safety, disability, oral health, gender issues in health, and sexual and reproductive health.[5] Public health, together with primary care, secondary care, and tertiary care, is part of a country's overall healthcare system. Public health is implemented through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promotion of hand-washing and breastfeeding, delivery of vaccinations, promoting ventilation and improved air quality both indoors and outdoors, suicide prevention, smoking cessation, obesity education, increasing healthcare accessibility and distribution of condoms to control the spread of sexually transmitted diseases.
 There is a significant disparity in access to health care and public health initiatives between developed countries and developing countries, as well as within developing countries. In developing countries, public health infrastructures are still forming. There may not be enough trained healthcare workers, monetary resources, or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention.[6][7] A major public health concern in developing countries is poor maternal and child health, exacerbated by malnutrition and poverty coupled with governments' reluctance in implementing public health policies.
 From the beginnings of human civilization, communities promoted health and fought disease at the population level.[8][9] In complex, pre-industrialized societies, interventions designed to reduce health risks could be the initiative of different stakeholders, such as army generals, the clergy or rulers. Great Britain became a leader in the development of public health initiatives, beginning in the 19th century, due to the fact that it was the first modern urban nation worldwide.[10] The public health initiatives that began to emerge initially focused on sanitation (for example, the Liverpool and London sewerage systems), control of infectious diseases (including vaccination and quarantine) and an evolving infrastructure of various sciences, e.g. statistics, microbiology, epidemiology, sciences of engineering.[10]
 Public health has been defined as ""the science and art of preventing disease"", prolonging life and improving quality of life through organized efforts and informed choices of society, organizations (public and private), communities and individuals.[2]  The public can be as small as a handful of people or as large as a village or an entire city. The concept of health takes into account physical, psychological, and social well-being. As such, according to the World Health Organization, ""health is a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity"".[4]
 Public health is related to global health which is the health of populations in the worldwide context.[11] It has been defined as ""the area of study, research and practice that places a priority on improving health and achieving equity in ""Health for all"" people worldwide"".[12] International health is a field of health care, usually with a public health emphasis, dealing with health across regional or national boundaries.[citation needed] Public health is not the same as public healthcare (publicly funded health care).
 The term preventive medicine is related to public health. The American Board of Preventive Medicine separates three categories of preventive medicine: aerospace health, occupational health, and public health and general preventative medicine. Jung, Boris and Lushniak argue that preventive medicine should be considered the medical specialty for public health but note that the American College of Preventive Medicine and American Board of Preventive Medicine do not prominently use the term ""public health"".[13]: 1  Preventive medicine specialists are trained as clinicians and address complex health needs of a population such as by assessing the need for disease prevention programs, using the best methods to implement them, and assessing their effectiveness.[13]: 1, 3 
 Since the 1990s many scholars in public health have been using the term population health.[14]: 3  There are no medical specialties directly related to population health.[13]: 4  Valles argues that consideration of health equity is a fundamental part of population health. Scholars such as Coggon and Pielke express concerns about bringing general issues of wealth distribution into population health. Pielke worries about ""stealth issue advocacy"" in population health.[14]: 163  Jung, Boris and Lushniak consider population health to be a concept that is the goal of an activity called public health practiced through the specialty preventive medicine.[13]: 4 
 Lifestyle medicine uses individual lifestyle modification to prevent or revert disease and can be considered a component of preventive medicine and public health. It is implemented as part of primary care rather than a specialty in its own right.[13]: 3  Valles argues that the term social medicine has a narrower and more biomedical focus than the term population health.[14]: 7 
 The purpose of a public health intervention is to prevent and mitigate diseases, injuries and other health conditions. The overall goal is to improve the health of populations and increase life expectancy.[citation needed]
 Public health is a complex term, composed of many elements and different practices. It is a multi-faceted, interdisciplinary field.[10] For example, epidemiology, biostatistics, social sciences and management of health services are all relevant. Other important sub-fields include environmental health, community health, behavioral health, health economics, public policy, mental health, health education, health politics, occupational safety, disability, gender issues in health, and sexual and reproductive health.[5]
 Modern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, physician assistants, public health nurses, midwives, medical microbiologists, pharmacists, economists, sociologists, geneticists, data managers, environmental health officers (public health inspectors), bioethicists, gender experts, sexual and reproductive health specialists, physicians, and veterinarians.[15]
 The elements and priorities of public health have evolved over time, and are continuing to evolve.[10] Different regions in the world can have different public health concerns at a given time.[citation needed]
 Common public health initiatives include promotion of hand-washing and breastfeeding, delivery of vaccinations, suicide prevention, smoking cessation, obesity education, increasing healthcare accessibility and distribution of condoms to control the spread of sexually transmitted diseases.[citation needed]
 Public health aims are achieved through surveillance of cases and the promotion of healthy behaviors, communities and environments. Analyzing the determinants of health of a population and the threats it faces is the basis for public health.[3]
 Many diseases are preventable through simple, nonmedical methods. For example, research has shown that the simple act of handwashing with soap can prevent the spread of many contagious diseases.[16] In other cases, treating a disease or controlling a pathogen can be vital to preventing its spread to others, either during an outbreak of infectious disease or through contamination of food or water supplies. Public health communications programs, vaccination programs and distribution of condoms are examples of common preventive public health measures.[citation needed]
 Public health, together with primary care, secondary care, and tertiary care, is part of a country's overall health care system. Many interventions of public health interest are delivered outside of health facilities, such as food safety surveillance, distribution of condoms and needle-exchange programs for the prevention of transmissible diseases.Public health plays an important role in disease prevention efforts in both the developing world and in developed countries through local health systems and non-governmental organizations.[citation needed]
 Public health requires Geographic Information Systems (GIS) because risk, vulnerability and exposure involve geographic aspects.[17]
 A dilemma in public health ethics is dealing with the conflict between individual rights and maximizing right to health.[18]: 28  Public health is justified by consequentialist utilitarian ideas,[18]: 153  but is constrained and critiqued by liberal,[18] deontological, principlist and libertarian philosophies[18]: 99, 95, 74, 123  Stephen Holland argues that it can be easy to find a particular framework to justify any viewpoint on public health issues, but that the correct approach is to find a framework that best describes a situation and see what it implies about public health policy.[18]: 154 
 The definition of health is vague and there are many conceptualizations. Public health practitioners definition of health can different markedly from members of the public or clinicians. This can mean that members of the public view the values behind public health interventions as alien which can cause resentment amongst the public towards certain interventions.[18]: 230  Such vagueness can be a problem for health promotion.[18]: 241  Critics have argued that public health tends to place more focus on individual factors associated with health at the expense of factors operating at the population level.[14]: 9 
 Historically, public health campaigns have been criticized as a form of ""healthism"",  as moralistic in nature rather than being focused on health. Medical doctors, Petr Shkrabanek and James McCormick wrote a series of publications on this topic in the late 1980s and early 1990s criticizing the UK's the Health of The Nation campaign. These publications exposed abuse of epidemiology and statistics by the public health movement to support lifestyle interventions and screening programs.[19]: 85 [20] A combination of inculcating a fear of ill-health and a strong notion of individual responsibility has been criticized as a form of ""health fascism"" by a number of scholars, objectifying the individual with no considerations of emotional or social factors.[21]: 8 [20]: 7 [22]: 81 
 When public health initiatives began to emerge in England in modern times (18th century onwards) there were three core strands of public health which were all related to statecraft: Supply of clean water and sanitation (for example London sewerage system); control of infectious diseases (including vaccination and quarantine); an evolving infrastructure of various sciences, e.g. statistics, microbiology, epidemiology, sciences of engineering.[10] Great Britain was a leader in the development of public health during that time period out of necessity: Great Britain was the first modern urban nation (by 1851 more than half of the population lived in settlements of more than 2000 people).[10] This led to a certain type of distress which then led to public health initiatives.[10] Later that particular concern faded away.
 With the onset of the epidemiological transition and as the prevalence of infectious diseases decreased through the 20th century, public health began to put more focus on chronic diseases such as cancer and heart disease. Previous efforts in many developed countries had already led to dramatic reductions in the infant mortality rate using preventive methods. In Britain, the infant mortality rate fell from over 15% in 1870 to 7% by 1930.[23]
 A major public health concern in developing countries is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year.[24]
 Public health surveillance has led to the identification and prioritization of many public health issues facing the world today, including HIV/AIDS, diabetes, waterborne diseases, zoonotic diseases, and antibiotic resistance leading to the reemergence of infectious diseases such as tuberculosis. Antibiotic resistance, also known as drug resistance, was the theme of World Health Day 2011.
 For example, the WHO reports that at least 220 million people worldwide have diabetes. Its incidence is increasing rapidly, and it is projected that the number of diabetes deaths will double by 2030.[25] In a June 2010 editorial in the medical journal The Lancet, the authors opined that ""The fact that type 2 diabetes, a largely preventable disorder, has reached epidemic proportion is a public health humiliation.""[26] The risk of type 2 diabetes is closely linked with the growing problem of obesity. The WHO's latest estimates as of June 2016[update] highlighted that globally approximately 1.9 billion adults were overweight in 2014, and 41 million children under the age of five were overweight in 2014.[27] Once considered a problem in high-income countries, it is now on the rise in low-income countries, especially in urban settings.[citation needed]
 Many public health programs are increasingly dedicating attention and resources to the issue of obesity, with objectives to address the underlying causes including healthy diet and physical exercise. The National Institute for Health and Care Research (NIHR) has published a review of research on what local authorities can do to tackle obesity.[28] The review covers interventions in the food environment (what people buy and eat), the built and natural environments, schools, and the community, as well as those focussing on active travel, leisure services and public sports, weight management programmes, and system-wide approaches.[citation needed]
 Health inequalities, driven by the social determinants of health, are also a growing area of concern in public health.  A central challenge to securing health equity is that the same social structures that contribute to health inequities also operate and are reproduced by public health organizations.[29] In other words, public health organizations have evolved to better meet the needs of some groups more than others. The result is often that those most in need of preventative interventions are least likely to receive them[30] and interventions can actually aggravate inequities[31] as they are often inadvertently tailored to the needs of the normative group.[32] Identifying bias within public health research and practice is essential to ensuring public health efforts mitigate and don't aggravate health inequities.
 The World Health Organization (WHO) is a specialized agency of the United Nations responsible for international public health.[33] The WHO Constitution, which establishes the agency's governing structure and principles, states its main objective as ""the attainment by all peoples of the highest possible level of health"".[34] The WHO's broad mandate includes advocating for universal healthcare, monitoring public health risks, coordinating responses to health emergencies, and promoting human health and well-being.[35] The WHO has played a leading role in several public health achievements, most notably the eradication of smallpox, the near-eradication of polio, and the development of an Ebola vaccine. Its current priorities include communicable diseases, particularly HIV/AIDS, Ebola, COVID-19, malaria and tuberculosis; non-communicable diseases such as heart disease and cancer; healthy diet, nutrition, and food security; occupational health; and substance abuse.[citation needed]
 Most countries have their own governmental public health agency, often called the ministry of health, with responsibility for domestic health issues.
 For example, in the United States, state and local health departments are on the front line of public health initiatives. In addition to their national duties, the United States Public Health Service (PHS), led by the Surgeon General of the United States Public Health Service, and the Centers for Disease Control and Prevention, headquartered in Atlanta, are also involved with international health activities.[36]
 Most governments recognize the importance of public health programs in reducing the incidence of disease, disability, and the effects of aging and other physical and mental health conditions. However, public health generally receives significantly less government funding compared with medicine.[37] Although the collaboration of local health and government agencies is considered best practice to improve public health, the pieces of evidence available to support this is limited.[38] Public health programs providing vaccinations have made major progress in promoting health, including substantially reducing the occurrence of cholera and polio and eradicating smallpox, diseases that have plagued humanity for thousands of years.[39]
 The World Health Organization (WHO) identifies core functions of public health programs including:[40]
 In particular, public health surveillance programs can:[41]
 Many health problems are due to maladaptive personal behaviors. From an evolutionary psychology perspective, over consumption of novel substances that are harmful is due to the activation of an evolved reward system for substances such as drugs, tobacco, alcohol, refined salt, fat, and carbohydrates. New technologies such as modern transportation also cause reduced physical activity. Research has found that behavior is more effectively changed by taking evolutionary motivations into consideration instead of only presenting information about health effects. The marketing industry has long known the importance of associating products with high status and attractiveness to others. Films are increasingly being recognized as a public health tool.[citation needed] In fact, film festivals and competitions have been established to specifically promote films about health.[42] Conversely, it has been argued that emphasizing the harmful and undesirable effects of tobacco smoking on other persons and imposing smoking bans in public places have been particularly effective in reducing tobacco smoking.[43]
 As well as seeking to improve population health through the implementation of specific population-level interventions, public health contributes to medical care by identifying and assessing population needs for health care services, including:[44][45][46][47]
 Some programs and policies associated with public health promotion and prevention can be controversial. One such example is programs focusing on the prevention of HIV transmission through safe sex campaigns and needle-exchange programs. Another is the control of tobacco smoking. Many nations have implemented major initiatives to cut smoking, such as increased taxation and bans on smoking in some or all public places. Supporters argue by presenting evidence that smoking is one of the major killers, and that therefore governments have a duty to reduce the death rate, both through limiting passive (second-hand) smoking and by providing fewer opportunities for people to smoke. Opponents say that this undermines individual freedom and personal responsibility, and worry that the state may be encouraged to remove more and more choice in the name of better population health overall.[citation needed]
 Psychological research confirms this tension between concerns about public health and concerns about personal liberty: (i) the best predictor of complying with public health recommendations such as hand-washing, mask-wearing, and staying at home (except for essential activity) during the COVID-19 pandemic was people's perceived duties to prevent harm but (ii) the best predictor of flouting such public health recommendations was valuing liberty more than equality.[48]
 Simultaneously, while communicable diseases have historically ranged uppermost as a global health priority, non-communicable diseases and the underlying behavior-related risk factors have been at the bottom. This is changing, however, as illustrated by the United Nations hosting its first General Assembly Special Summit on the issue of non-communicable diseases in September 2011.[49]
 There is a significant disparity in access to health care and public health initiatives between developed countries and developing countries, as well as within developing countries. In developing countries, public health infrastructures are still forming. There may not be enough trained health workers, monetary resources or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention.[6][7] As a result, a large majority of disease and mortality in developing countries results from and contributes to extreme poverty. For example, many African governments spend less than $100 USD per person per year on health care, while, in the United States, the federal government spent approximately $10,600 USD per capita in 2019.[50] However, expenditures on health care should not be confused with spending on public health. Public health measures may not generally be considered ""health care"" in the strictest sense. For example, mandating the use of seat belts in cars can save countless lives and contribute to the health of a population, but typically money spent enforcing this rule would not count as money spent on health care.
 Large parts of the world remained plagued by largely preventable or treatable infectious diseases. In addition to this however, many developing countries are also experiencing an epidemiological shift and polarization in which populations are now experiencing more of the effects of chronic diseases as life expectancy increases, the poorer communities being heavily affected by both chronic and infectious diseases.[7] Another major public health concern in the developing world is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year.[24] Intermittent preventive therapy aimed at treating and preventing malaria episodes among pregnant women and young children is one public health measure in endemic countries.
 Since the 1980s, the growing field of population health has broadened the focus of public health from individual behaviors and risk factors to population-level issues such as inequality, poverty, and education. Modern public health is often concerned with addressing determinants of health across a population. There is a recognition that health is affected by many factors including class, race, income, educational status, region of residence, and social relationships; these are known as ""social determinants of health"". The upstream drivers such as environment, education, employment, income, food security, housing, social inclusion and many others effect the distribution of health between and within populations and are often shaped by policy.[53] A social gradient in health runs through society. The poorest generally have the worst health, but even the middle classes will generally have worse health outcomes than those of a higher social level.[54] The new public health advocates for population-based policies that improve health in an equitable manner.
 The health sector is one of Europe's most labor-intensive industries. In late 2020, it accounted for more than 21 million employment in the European Union when combined with social work.[55] According to the WHO, several countries began the COVID-19 pandemic with insufficient health and care professionals, inappropriate skill mixtures, and unequal geographical distributions. These issues were worsened by the pandemic, reiterating the importance of public health.[56] In the United States, a history of underinvestment in public health undermined the public health workforce and support for population health, long before the pandemic added to stress, mental distress, job dissatisfaction, and accelerated departures among public health workers.[57]
 Health aid to developing countries is an important source of public health funding for many developing countries.[59] Health aid to developing countries has shown a significant increase after World War II as concerns over the spread of disease as a result of globalization increased and the HIV/AIDS epidemic in sub-Saharan Africa surfaced.[60][61] From 1990 to 2010, total health aid from developed countries increased from 5.5 billion to 26.87 billion with wealthy countries continuously donating billions of dollars every year with the goal of improving population health.[61] Some efforts, however, receive a significantly larger proportion of funds such as HIV which received an increase in funds of over $6 billion between 2000 and 2010 which was more than twice the increase seen in any other sector during those years.[59] Health aid has seen an expansion through multiple channels including private philanthropy, non-governmental organizations, private foundations such as the Rockefeller Foundation or the Bill & Melinda Gates Foundation, bilateral donors, and multilateral donors such as the World Bank or UNICEF.[61] The result has been a sharp rise in uncoordinated and fragmented funding of an ever-increasing number of initiatives and projects. To promote better strategic cooperation and coordination between partners, particularly among bilateral development agencies and funding organizations, the Swedish International Development Cooperation Agency (Sida) spearheaded the establishment of ESSENCE,[62] an initiative to facilitate dialogue between donors/funders, allowing them to identify synergies. ESSENCE brings together a wide range of funding agencies to coordinate funding efforts.
 In 2009 health aid from the OECD amounted to $12.47 billion which amounted to 11.4% of its total bilateral aid.[63] In 2009, Multilateral donors were found to spend 15.3% of their total aid on bettering public healthcare.[63]
 Debates exist questioning the efficacy of international health aid. Supporters of aid claim that health aid from wealthy countries is necessary in order for developing countries to escape the poverty trap. Opponents of health aid claim that international health aid actually disrupts developing countries' course of development, causes dependence on aid, and in many cases the aid fails to reach its recipients.[59] For example, recently, health aid was funneled towards initiatives such as financing new technologies like antiretroviral medication, insecticide-treated mosquito nets, and new vaccines. The positive impacts of these initiatives can be seen in the eradication of smallpox and polio; however, critics claim that misuse or misplacement of funds may cause many of these efforts to never come into achievement.[59]
 Economic modeling based on the Institute for Health Metrics and Evaluation and the World Health Organization has shown a link between international health aid in developing countries and a reduction in adult mortality rates.[61] However, a 2014–2016 study suggests that a potential confounding variable for this outcome is the possibility that aid was directed at countries once they were already on track for improvement.[59] That same study, however, also suggests that 1 billion dollars in health aid was associated with 364,000 fewer deaths occurring between ages 0 and 5 in 2011.[59]
 To address current and future challenges in addressing health issues in the world, the United Nations have developed the Sustainable Development Goals to be completed by 2030.[64] These goals in their entirety encompass the entire spectrum of development across nations, however Goals 1–6 directly address health disparities, primarily in developing countries.[65] These six goals address key issues in global public health, poverty, hunger and food security, health, education, gender equality and women's empowerment, and water and sanitation.[65] Public health officials can use these goals to set their own agenda and plan for smaller scale initiatives for their organizations. These goals are designed to lessen the burden of disease and inequality faced by developing countries and lead to a healthier future. The links between the various sustainable development goals and public health are numerous and well established.[66][67]
 From the beginnings of human civilization, communities promoted health and fought disease at the population level.[8][9] Definitions of health as well as methods to pursue it differed according to the medical, religious and natural-philosophical ideas groups held, the resources they had, and the changing circumstances in which they lived. Yet few early societies displayed the hygienic stagnation or even apathy often attributed to them.[68][69][70] The latter reputation is mainly based on the absence of present-day bioindicators, especially immunological and statistical tools developed in light of the germ theory of disease transmission.[citation needed]
 Public health was born neither in Europe nor as a response to the Industrial Revolution. Preventive health interventions are attested almost anywhere historical communities have left their mark. In Southeast Asia, for instance, Ayurvedic medicine and subsequently Buddhism fostered occupational, dietary and sexual regimens that promised balanced bodies, lives and communities, a notion strongly present in Traditional Chinese Medicine as well.[71][72] Among the Mayans, Aztecs and other early civilizations in the Americas, population centers pursued hygienic programs, including by holding medicinal herbal markets.[73] And among Aboriginal Australians, techniques for preserving and protecting water and food sources, micro-zoning to reduce pollution and fire risks, and screens to protect people against flies were common, even in temporary camps.[74][75]
 Western European, Byzantine and Islamicate civilizations, which generally adopted a Hippocratic, Galenic or humoral medical system, fostered preventive programs as well.[76][77][78][79] These were developed on the basis of evaluating the quality of local climates, including topography, wind conditions and exposure to the sun, and the properties and availability of water and food, for both humans and nonhuman animals. Diverse authors of medical, architectural, engineering and military manuals explained how to apply such theories to groups of different origins and under different circumstances.[80][81][82] This was crucial, since under Galenism bodily constitutions were thought to be heavily shaped by their material environments, so their balance required specific regimens as they traveled during different seasons and between climate zones.[83][84][85]
 In complex, pre-industrialized societies, interventions designed to reduce health risks could be the initiative of different stakeholders. For instance, in Greek and Roman antiquity, army generals learned to provide for soldiers' wellbeing, including off the battlefield, where most combatants died prior to the twentieth century.[86][87] In Christian monasteries across the Eastern Mediterranean and western Europe since at least the fifth century CE, monks and nuns pursued strict but balanced regimens, including nutritious diets, developed explicitly to extend their lives.[88] And royal, princely and papal courts, which were often mobile as well, likewise adapted their behavior to suit environmental conditions in the sites they occupied. They could also choose sites they considered salubrious for their members and sometimes had them modified.[89]
 In cities, residents and rulers developed measures to benefit the general population, which faced a broad array of recognized health risks. These provide some of the most sustained evidence for preventive measures in earlier civilizations. In numerous sites the upkeep of infrastructures, including roads, canals and marketplaces, as well as zoning policies, were introduced explicitly to preserve residents' health.[90] Officials such as the muhtasib in the Middle East and the Road master in Italy, fought the combined threats of pollution through sin, ocular intromission and miasma.[91][92][93][94] Craft guilds were important agents of waste disposal and promoted harm reduction through honesty and labor safety among their members. Medical practitioners, including public physicians,[95] collaborated with urban governments in predicting and preparing for calamities and identifying and isolating people perceived as lepers, a disease with strong moral connotations.[96][97] Neighborhoods were also active in safeguarding local people's health, by monitoring at-risk sites near them and taking appropriate social and legal action against artisanal polluters and neglectful owners of animals. Religious institutions, individuals and charitable organizations in both Islam and Christianity likewise promoted moral and physical wellbeing by endowing urban amenities such as wells, fountains, schools and bridges, also in the service of pilgrims.[98][99] In western Europe and Byzantium, religious processions commonly took place, which purported to act as both preventive and curative measures for the entire community.[100]
 Urban residents and other groups also developed preventive measures in response to calamities such as war, famine, floods and widespread disease.[101][102][103][104] During and after the Black Death (1346–53), for instance, inhabitants of the Eastern Mediterranean and Western Europe reacted to massive population decline in part on the basis of existing medical theories and protocols, for instance concerning meat consumption and burial, and in part by developing new ones.[105][106][107] The latter included the establishment of quarantine facilities and health boards, some of which eventually became regular urban (and later national) offices.[108][109] Subsequent measures for protecting cities and their regions included issuing health passports for travelers, deploying guards to create sanitary cordons for protecting local inhabitants, and gathering morbidity and mortality statistics.[110][111][112]  Such measures relied in turn on better transportation and communication networks, through which news on human and animal disease was efficiently spread.
 With the onset of the Industrial Revolution, living standards amongst the working population began to worsen, with cramped and unsanitary urban conditions. In the first four decades of the 19th century alone, London's population doubled and even greater growth rates were recorded in the new industrial towns, such as Leeds and Manchester. This rapid urbanization exacerbated the spread of disease in the large conurbations that built up around the workhouses and factories. These settlements were cramped and primitive with no organized sanitation. Disease was inevitable and its incubation in these areas was encouraged by the poor lifestyle of the inhabitants. Unavailable housing led to the rapid growth of slums and the per capita death rate began to rise alarmingly, almost doubling in Birmingham and Liverpool. Thomas Malthus warned of the dangers of overpopulation in 1798. His ideas, as well as those of Jeremy Bentham, became very influential in government circles in the early years of the 19th century.[113] The latter part of the century brought the establishment of the basic pattern of improvements in public health over the next two centuries: a social evil was identified, private philanthropists brought attention to it, and changing public opinion led to government action.[113] The 18th century saw rapid growth in voluntary hospitals in England.[114]
 The practice of vaccination began in the 1800s, following the pioneering work of Edward Jenner in treating smallpox. James Lind's discovery of the causes of scurvy amongst sailors and its mitigation via the introduction of fruit on lengthy voyages was published in 1754 and led to the adoption of this idea by the Royal Navy.[115] Efforts were also made to promulgate health matters to the broader public; in 1752 the British physician Sir John Pringle published Observations on the Diseases of the Army in Camp and Garrison, in which he advocated for the importance of adequate ventilation in the military barracks and the provision of latrines for the soldiers.[116]
 The first attempts at sanitary reform and the establishment of public health institutions were made in the 1840s. Thomas Southwood Smith, physician at the London Fever Hospital, began to write papers on the importance of public health, and was one of the first physicians brought in to give evidence before the Poor Law Commission in the 1830s, along with Neil Arnott and James Phillips Kay.[117] Smith advised the government on the importance of quarantine and sanitary improvement for limiting the spread of infectious diseases such as cholera and yellow fever.[118][119]
 The Poor Law Commission reported in 1838 that ""the expenditures necessary to the adoption and maintenance of measures of prevention would ultimately amount to less than the cost of the disease now constantly engendered"". It recommended the implementation of large scale government engineering projects to alleviate the conditions that allowed for the propagation of disease.[113] The Health of Towns Association was formed at Exeter Hall London on 11 December 1844, and vigorously campaigned for the development of public health in the United Kingdom.[120] Its formation followed the 1843 establishment of the Health of Towns Commission, chaired by Sir Edwin Chadwick, which produced a series of reports on poor and insanitary conditions in British cities.[120]
 These national and local movements led to the Public Health Act, finally passed in 1848. It aimed to improve the sanitary condition of towns and populous places in England and Wales by placing the supply of water, sewerage, drainage, cleansing and paving under a single local body with the General Board of Health as a central authority. The Act was passed by the Liberal government of Lord John Russell, in response to the urging of Edwin Chadwick. Chadwick's seminal report on The Sanitary Condition of the Labouring Population was published in 1842[121] and was followed up with a supplementary report a year later.[122] During this time, James Newlands (appointed following the passing of the 1846 Liverpool Sanatory Act championed by the Borough of Liverpool Health of Towns Committee) designed the world's first integrated sewerage system, in Liverpool (1848–1869), with Joseph Bazalgette later creating London's sewerage system (1858–1875).
 The Vaccination Act 1853 introduced compulsory smallpox vaccination in England and Wales.[123] By 1871 legislation required a comprehensive system of registration run by appointed vaccination officers.[124]
 Further interventions were made by a series of subsequent Public Health Acts, notably the 1875 Act. Reforms included the building of sewers, the regular collection of garbage followed by incineration or disposal in a landfill, the provision of clean water and the draining of standing water to prevent the breeding of mosquitoes.
 The Infectious Disease (Notification) Act 1889 (52 & 53 Vict. c. 72) mandated the reporting of infectious diseases to the local sanitary authority, which could then pursue measures such as the removal of the patient to hospital and the disinfection of homes and properties.[125]
 In the United States, the first public health organization based on a state health department and local boards of health was founded in New York City in 1866.[126]
 In Germany during The Weimar Republic the country faced many public health catastrophes.[examples  needed] The Nazi Party had a goal of modernizing health care with Volksgesundheit, German for people's public health; this modernization was based on the growing field of eugenics and measures prioritizing group health over any care for the health of individuals.  The end of World War 2 led to the Nuremberg Code, a set of research ethics concerning human experimentation.[127]
 The science of epidemiology was founded by John Snow's identification of a polluted public water well as the source of an 1854 cholera outbreak in London. Snow believed in the germ theory of disease as opposed to the prevailing miasma theory. By talking to local residents (with the help of Reverend Henry Whitehead), he identified the source of the outbreak as the public water pump on Broad Street (now Broadwick Street). Although Snow's chemical and microscope examination of a water sample from the Broad Street pump did not conclusively prove its danger, his studies of the pattern of the disease were convincing enough to persuade the local council to close the well pump by removing its handle.[128]
 Snow later used a dot map to illustrate the cluster of cholera cases around the pump. He also used statistics to illustrate the connection between the quality of the water source and cholera cases. He showed that the Southwark and Vauxhall Waterworks Company was taking water from sewage-polluted sections of the Thames and delivering the water to homes, leading to an increased incidence of cholera. Snow's study was a major event in the history of public health and geography. It is regarded as the founding event of the science of epidemiology.[129][130]
 With the pioneering work in bacteriology of French chemist Louis Pasteur and German scientist Robert Koch, methods for isolating the bacteria responsible for a given disease and vaccines for remedy were developed at the turn of the 20th century. British physician Ronald Ross identified the mosquito as the carrier of malaria and laid the foundations for combating the disease.[131] Joseph Lister revolutionized surgery by the introduction of antiseptic surgery to eliminate infection. French epidemiologist Paul-Louis Simond proved that plague was carried by fleas on the back of rats,[132] and Cuban scientist Carlos J. Finlay and U.S. Americans Walter Reed and James Carroll demonstrated that mosquitoes carry the virus responsible for yellow fever.[133][134] Brazilian scientist Carlos Chagas identified a tropical disease and its vector.[135]
 Education and training of public health professionals is available throughout the world in Schools of Public Health, Medical Schools, Veterinary Schools, Schools of Nursing, and Schools of Public Affairs. The training typically requires a university degree with a focus on core disciplines of biostatistics, epidemiology, health services administration, health policy, health education, behavioral science, gender issues, sexual and reproductive health, public health nutrition, and occupational and environmental health.[136][137]
 In the global context, the field of public health education has evolved enormously in recent decades, supported by institutions such as the World Health Organization and the World Bank, among others. Operational structures are formulated by strategic principles, with educational and career pathways guided by competency frameworks, all requiring modulation according to local, national and global realities. Moreover, integrating technology or digital platforms to connect to low health literacy LHL groups could be a way to increase health literacy. [138]It is critically important for the health of populations that nations assess their public health human resource needs and develop their ability to deliver this capacity, and not depend on other countries to supply it.[139]
 In the United States, the Welch-Rose Report of 1915[140] has been viewed as the basis for the critical movement in the history of the institutional schism between public health and medicine because it led to the establishment of schools of public health supported by the Rockefeller Foundation.[141] The report was authored by William Welch, founding dean of the Johns Hopkins Bloomberg School of Public Health, and Wickliffe Rose of the Rockefeller Foundation. The report focused more on research than practical education.[141][142] Some have blamed the Rockefeller Foundation's 1916 decision to support the establishment of schools of public health for creating the schism between public health and medicine and legitimizing the rift between medicine's laboratory investigation of the mechanisms of disease and public health's nonclinical concern with environmental and social influences on health and wellness.[141][143]
 Even though schools of public health had already been established in Canada, Europe and North Africa, the United States had still maintained the traditional system of housing faculties of public health within their medical institutions.  A $25,000 donation from businessman Samuel Zemurray instituted the School of Public Health and Tropical Medicine at Tulane University in 1912 conferring its first doctor of public health degree in 1914.[144][145]  The Yale School of Public Health was founded by Charles-Edward Amory Winslow in 1915.[146] The Johns Hopkins School of Hygiene and Public Health was founded in 1916 and became an independent, degree-granting institution for research and training in public health, and the largest public health training facility in the United States.[147][148][149] By 1922, schools of public health were established at Columbia and Harvard on the Hopkins model. By 1999 there were twenty nine schools of public health in the US, enrolling around fifteen thousand students.[136][141]
 Over the years, the types of students and training provided have also changed. In the beginning, students who enrolled in public health schools typically had already obtained a medical degree; public health school training was largely a second degree for medical professionals. However, in 1978, 69% of American students enrolled in public health schools had only a bachelor's degree.[136]
 Schools of public health offer a variety of degrees generally fall into two categories: professional or academic.[151] The two major postgraduate degrees are the Master of Public Health (MPH) or the Master of Science in Public Health (MSPH). Doctoral studies in this field include Doctor of Public Health (DrPH) and Doctor of Philosophy (PhD) in a subspecialty of greater Public Health disciplines. DrPH is regarded as a professional degree and PhD as more of an academic degree.
 Professional degrees are oriented towards practice in public health settings. The Master of Public Health, Doctor of Public Health, Doctor of Health Science (DHSc/DHS) and the Master of Health Care Administration are examples of degrees which are geared towards people who want careers as practitioners of public health in health departments, managed care and community-based organizations, hospitals and consulting firms, among others. Master of Public Health degrees broadly fall into two categories, those that put more emphasis on an understanding of epidemiology and statistics as the scientific basis of public health practice and those that include a more wide range of methodologies.  A Master of Science of Public Health is similar to an MPH but is considered an academic degree (as opposed to a professional degree) and places more emphasis on scientific methods and research.  The same distinction can be made between the DrPH and the DHSc. The DrPH is considered a professional degree and the DHSc is an academic degree.[citation needed]
 Academic degrees are more oriented towards those with interests in the scientific basis of public health and preventive medicine who wish to pursue careers in research, university teaching in graduate programs, policy analysis and development, and other high-level public health positions. Examples of academic degrees are the Master of Science, Doctor of Philosophy, Doctor of Science (ScD), and Doctor of Health Science (DHSc). The doctoral programs are distinct from the MPH and other professional programs by the addition of advanced coursework and the nature and scope of a dissertation research project.
 In Canada, the Public Health Agency of Canada is the national agency responsible for public health, emergency preparedness and response, and infectious and chronic disease control and prevention.[164]
 Since the 1959 Cuban Revolution the Cuban government has devoted extensive resources to the improvement of health conditions for its entire population via universal access to health care. Infant mortality has plummeted.[165] Cuban medical internationalism as a policy has seen the Cuban government sent doctors as a form of aid and export to countries in need in Latin America, especially Venezuela, as well as Oceania and Africa countries.
 Public health was important elsewhere in Latin America in consolidating state power and integrating marginalized populations into the nation-state.  In Colombia, public health was a means for creating and implementing ideas of citizenship.[166] In Bolivia, a similar push came after their 1952 revolution.[167]
 Though curable and preventive, malaria remains a major public health issue and is the third leading cause of death in Ghana.[168] In the absence of a vaccine, mosquito control, or access to anti-malaria medication, public health methods become the main strategy for reducing the prevalence and severity of malaria.[169] These methods include reducing breeding sites, screening doors and windows, insecticide sprays, prompt treatment following infection, and usage of insecticide treated mosquito nets.[169] Distribution and sale of insecticide-treated mosquito nets is a common, cost-effective anti-malaria public health intervention; however, barriers to use exist including cost, household and family organization, access to resources, and social and behavioral determinants which have not only been shown to affect malaria prevalence rates but also mosquito net use.[170][169]
 Public health issues were important for the Spanish Empire during the colonial era. Epidemic disease was the main factor in the decline of indigenous populations in the era immediately following the sixteenth-century conquest era and was a problem during the colonial era. The Spanish crown took steps in eighteenth-century Mexico to bring in regulations to make populations healthier.[175] In the late nineteenth century, Mexico was in the process of modernization, and public health issues were again tackled from a scientific point of view.[176][177][178] As in the U.S., food safety became a public health issue, particularly focusing on meat slaughterhouses and meatpacking.[179]
 The United States Public Health Service (USPHS or PHS) is a collection of agencies of the Department of Health and Human Services concerned with public health, containing nine out of the department's twelve operating divisions. The Assistant Secretary for Health oversees the PHS. The Public Health Service Commissioned Corps (PHSCC) is the federal uniformed service of the PHS, and is one of the eight uniformed services of the United States.
 The United States lacks a coherent system for the governmental funding of public health, relying on a variety of agencies and programs at the federal, state and local levels.[185]
Between 1960 and 2001, public health spending in the United States tended to grow,
based on increasing expenditures by state and local government, which made up 80–90% of
total public health spending.  Spending in support of public health in the United States peaked in 2002 and declined in the following decade.[186] State cuts to public health funding during the Great Recession of 2007–2008 were not restored in subsequent years.[187]
As of 2012, a panel for the  U.S. Institute of Medicine panel warned that the United States spends disproportionately far more on clinical care than it does on public health, neglecting ""population-based activities that offer efficient and effective approaches to improving the nation's health.""[188][186]  As of 2018[update], about 3% of government health spending was directed to public health and prevention.[39][189][190] This situation has been described as an ""uneven patchwork""[191] and ""chronic underfunding"".[192][193][194][195]
The COVID-19 pandemic has been seen as drawing attention to problems in the public health system in the United States and to a lack of understanding of public health and its important role as a common good.[39]
","[{'input_text': 'What is the main topic discussed?'}, {'input_text': 'Who are the key figures mentioned?'}, {'input_text': 'What is the historical significance of the topic?'}, {'input_text': 'What are the major events related to the topic?'}, {'input_text': 'What are the applications or implications of this topic?'}]","{'input_text': ['research than practical education', 'army generals, the clergy or rulers', 'institutional schism between public health and medicine', 'problems', 'Further interventions'], 'answer_start': [], 'answer_end': []}"
